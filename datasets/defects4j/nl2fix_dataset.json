{
    "Math_19_src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java_504_561": {
        "src": "private void checkParameters() {\n        final double[] init = getStartPoint();\n        final double[] lB = getLowerBound();\n        final double[] uB = getUpperBound();\n\n        // Checks whether there is at least one finite bound value.\n        boolean hasFiniteBounds = false;\n        for (int i = 0; i < lB.length; i++) {\n            if (!Double.isInfinite(lB[i]) ||\n                !Double.isInfinite(uB[i])) {\n                hasFiniteBounds = true;\n                break;\n            }\n        }\n        // Checks whether there is at least one infinite bound value.\n        boolean hasInfiniteBounds = false;\n        if (hasFiniteBounds) {\n            for (int i = 0; i < lB.length; i++) {\n                if (Double.isInfinite(lB[i]) ||\n                    Double.isInfinite(uB[i])) {\n                    hasInfiniteBounds = true;\n                    break;\n                }\n            }\n\n            if (hasInfiniteBounds) {\n                // If there is at least one finite bound, none can be infinite,\n                // because mixed cases are not supported by the current code.\n                throw new MathUnsupportedOperationException();\n            } else {\n                // Convert API to internal handling of boundaries.\n                boundaries = new double[2][];\n                boundaries[0] = lB;\n                boundaries[1] = uB;\n\n                // Abort early if the normalization will overflow (cf. \"encode\" method).\n            }\n        } else {\n            // Convert API to internal handling of boundaries.\n            boundaries = null;\n        }\n\n        if (inputSigma != null) {\n            if (inputSigma.length != init.length) {\n                throw new DimensionMismatchException(inputSigma.length, init.length);\n            }\n            for (int i = 0; i < init.length; i++) {\n                if (inputSigma[i] < 0) {\n                    throw new NotPositiveException(inputSigma[i]);\n                }\n                if (boundaries != null) {\n                    if (inputSigma[i] > boundaries[1][i] - boundaries[0][i]) {\n                        throw new OutOfRangeException(inputSigma[i], 0, boundaries[1][i] - boundaries[0][i]);\n                    }\n                }\n            }\n        }\n    }",
        "src_wo_comments": "private void checkParameters ( ) { final double [ ] init = getStartPoint ( ) ; final double [ ] lB = getLowerBound ( ) ; final double [ ] uB = getUpperBound ( ) ; boolean hasFiniteBounds = false ; for ( int i = 0 ; i < lB . length ; i ++ ) { if ( ! Double . isInfinite ( lB [ i ] ) || ! Double . isInfinite ( uB [ i ] ) ) { hasFiniteBounds = true ; break ; } } boolean hasInfiniteBounds = false ; if ( hasFiniteBounds ) { for ( int i = 0 ; i < lB . length ; i ++ ) { if ( Double . isInfinite ( lB [ i ] ) || Double . isInfinite ( uB [ i ] ) ) { hasInfiniteBounds = true ; break ; } } if ( hasInfiniteBounds ) { throw new MathUnsupportedOperationException ( ) ; } else { boundaries = new double [ 2 ] [ ] ; boundaries [ 0 ] = lB ; boundaries [ 1 ] = uB ; } } else { boundaries = null ; } if ( inputSigma != null ) { if ( inputSigma . length != init . length ) { throw new DimensionMismatchException ( inputSigma . length , init . length ) ; } for ( int i = 0 ; i < init . length ; i ++ ) { if ( inputSigma [ i ] < 0 ) { throw new NotPositiveException ( inputSigma [ i ] ) ; } if ( boundaries != null ) { if ( inputSigma [ i ] > boundaries [ 1 ] [ i ] - boundaries [ 0 ] [ i ] ) { throw new OutOfRangeException ( inputSigma [ i ] , 0 , boundaries [ 1 ] [ i ] - boundaries [ 0 ] [ i ] ) ; } } } } }",
        "fixed_src": "private void checkParameters() {\n        final double[] init = getStartPoint();\n        final double[] lB = getLowerBound();\n        final double[] uB = getUpperBound();\n\n        // Checks whether there is at least one finite bound value.\n        boolean hasFiniteBounds = false;\n        for (int i = 0; i < lB.length; i++) {\n            if (!Double.isInfinite(lB[i]) ||\n                !Double.isInfinite(uB[i])) {\n                hasFiniteBounds = true;\n                break;\n            }\n        }\n        // Checks whether there is at least one infinite bound value.\n        boolean hasInfiniteBounds = false;\n        if (hasFiniteBounds) {\n            for (int i = 0; i < lB.length; i++) {\n                if (Double.isInfinite(lB[i]) ||\n                    Double.isInfinite(uB[i])) {\n                    hasInfiniteBounds = true;\n                    break;\n                }\n            }\n\n            if (hasInfiniteBounds) {\n                // If there is at least one finite bound, none can be infinite,\n                // because mixed cases are not supported by the current code.\n                throw new MathUnsupportedOperationException();\n            } else {\n                // Convert API to internal handling of boundaries.\n                boundaries = new double[2][];\n                boundaries[0] = lB;\n                boundaries[1] = uB;\n\n                // Abort early if the normalization will overflow (cf. \"encode\" method).\n                for (int i = 0; i < lB.length; i++) {\n                    if (Double.isInfinite(boundaries[1][i] - boundaries[0][i])) {\n                        final double max = Double.MAX_VALUE + boundaries[0][i];\n                        final NumberIsTooLargeException e\n                            = new NumberIsTooLargeException(boundaries[1][i],\n                                                            max,\n                                                            true);\n                        e.getContext().addMessage(LocalizedFormats.OVERFLOW);\n                        e.getContext().addMessage(LocalizedFormats.INDEX, i);\n\n                        throw e;\n                    }\n                }\n            }\n        } else {\n            // Convert API to internal handling of boundaries.\n            boundaries = null;\n        }\n\n        if (inputSigma != null) {\n            if (inputSigma.length != init.length) {\n                throw new DimensionMismatchException(inputSigma.length, init.length);\n            }\n            for (int i = 0; i < init.length; i++) {\n                if (inputSigma[i] < 0) {\n                    throw new NotPositiveException(inputSigma[i]);\n                }\n                if (boundaries != null) {\n                    if (inputSigma[i] > boundaries[1][i] - boundaries[0][i]) {\n                        throw new OutOfRangeException(inputSigma[i], 0, boundaries[1][i] - boundaries[0][i]);\n                    }\n                }\n            }\n        }\n    }",
        "fixed_src_wo_comments": "private void checkParameters ( ) { final double [ ] init = getStartPoint ( ) ; final double [ ] lB = getLowerBound ( ) ; final double [ ] uB = getUpperBound ( ) ; boolean hasFiniteBounds = false ; for ( int i = 0 ; i < lB . length ; i ++ ) { if ( ! Double . isInfinite ( lB [ i ] ) || ! Double . isInfinite ( uB [ i ] ) ) { hasFiniteBounds = true ; break ; } } boolean hasInfiniteBounds = false ; if ( hasFiniteBounds ) { for ( int i = 0 ; i < lB . length ; i ++ ) { if ( Double . isInfinite ( lB [ i ] ) || Double . isInfinite ( uB [ i ] ) ) { hasInfiniteBounds = true ; break ; } } if ( hasInfiniteBounds ) { throw new MathUnsupportedOperationException ( ) ; } else { boundaries = new double [ 2 ] [ ] ; boundaries [ 0 ] = lB ; boundaries [ 1 ] = uB ; for ( int i = 0 ; i < lB . length ; i ++ ) { if ( Double . isInfinite ( boundaries [ 1 ] [ i ] - boundaries [ 0 ] [ i ] ) ) { final double max = Double . MAX_VALUE + boundaries [ 0 ] [ i ] ; final NumberIsTooLargeException e = new NumberIsTooLargeException ( boundaries [ 1 ] [ i ] , max , true ) ; e . getContext ( ) . addMessage ( LocalizedFormats . OVERFLOW ) ; e . getContext ( ) . addMessage ( LocalizedFormats . INDEX , i ) ; throw e ; } } } } else { boundaries = null ; } if ( inputSigma != null ) { if ( inputSigma . length != init . length ) { throw new DimensionMismatchException ( inputSigma . length , init . length ) ; } for ( int i = 0 ; i < init . length ; i ++ ) { if ( inputSigma [ i ] < 0 ) { throw new NotPositiveException ( inputSigma [ i ] ) ; } if ( boundaries != null ) { if ( inputSigma [ i ] > boundaries [ 1 ] [ i ] - boundaries [ 0 ] [ i ] ) { throw new OutOfRangeException ( inputSigma [ i ] , 0 , boundaries [ 1 ] [ i ] - boundaries [ 0 ] [ i ] ) ; } } } } }",
        "summary": "Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function",
        "Description": "If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-865",
        "comments": [
            "Also, if the first call to the fitness function returns NaN and gets stored in the bestValue local variable in doOptimize() before the generationLoop, then the optimization will completely fail.  This is because the later comparison if(bestValue > bestFitness) will always return false if bestValue is NaN and no more optimal result will ever be found.",
            "Attached test program that demonstrates the problem.",
            "Another issue is, if you use bounds then the bounded range is mapped onto the interval [0,1] where the fit is performed, and then blown back up to your specified bounds after getting a result.  This means if you use a large range, the discreteness of Double variables becomes very noticeable in the fit results.  For example, if you set the boundsMagnitude to 5e15 (approx. 1./Math.ulp(1.)) in the Math865Test program I attached, the fitter is unable to fit anything to a precision smaller than 1.",
            "bq. My guess is this is due to FitnessFunction.encode [...] generating NaN\n\nGood guess.\n\nI've introduced a check on the bounds that now throws an exception in case of overflow.\nPlease test revision 1388552.\n\nThanks for the report.\n",
            "Yes, it throws the exception for me now (revision 1388555), thanks.  The secondary issue i mentioned in the comment is still a problem, but I'll open a new bug for that.",
            "This issue is superseded by MATH-867 whose fix entails that an overflow cannot happen anymore.\n"
        ]
    },
    "Compress_16_src/main/java/org/apache/commons/compress/archivers/ArchiveStreamFactory.java_197_258": {
        "src": "public ArchiveInputStream createArchiveInputStream(final InputStream in)\n            throws ArchiveException {\n        if (in == null) {\n            throw new IllegalArgumentException(\"Stream must not be null.\");\n        }\n\n        if (!in.markSupported()) {\n            throw new IllegalArgumentException(\"Mark is not supported.\");\n        }\n\n        final byte[] signature = new byte[12];\n        in.mark(signature.length);\n        try {\n            int signatureLength = in.read(signature);\n            in.reset();\n            if (ZipArchiveInputStream.matches(signature, signatureLength)) {\n                return new ZipArchiveInputStream(in);\n            } else if (JarArchiveInputStream.matches(signature, signatureLength)) {\n                return new JarArchiveInputStream(in);\n            } else if (ArArchiveInputStream.matches(signature, signatureLength)) {\n                return new ArArchiveInputStream(in);\n            } else if (CpioArchiveInputStream.matches(signature, signatureLength)) {\n                return new CpioArchiveInputStream(in);\n            }\n\n            // Dump needs a bigger buffer to check the signature;\n            final byte[] dumpsig = new byte[32];\n            in.mark(dumpsig.length);\n            signatureLength = in.read(dumpsig);\n            in.reset();\n            if (DumpArchiveInputStream.matches(dumpsig, signatureLength)) {\n                return new DumpArchiveInputStream(in);\n            }\n\n            // Tar needs an even bigger buffer to check the signature; read the first block\n            final byte[] tarheader = new byte[512];\n            in.mark(tarheader.length);\n            signatureLength = in.read(tarheader);\n            in.reset();\n            if (TarArchiveInputStream.matches(tarheader, signatureLength)) {\n                return new TarArchiveInputStream(in);\n            }\n            // COMPRESS-117 - improve auto-recognition\n            if (signatureLength >= 512) {\n                try {\n                    TarArchiveInputStream tais = new TarArchiveInputStream(new ByteArrayInputStream(tarheader));\n                    // COMPRESS-191 - verify the header checksum\n                    tais.getNextEntry();\n                        return new TarArchiveInputStream(in);\n                } catch (Exception e) { // NOPMD\n                    // can generate IllegalArgumentException as well\n                    // as IOException\n                    // autodetection, simply not a TAR\n                    // ignored\n                }\n            }\n        } catch (IOException e) {\n            throw new ArchiveException(\"Could not use reset and mark operations.\", e);\n        }\n\n        throw new ArchiveException(\"No Archiver found for the stream signature\");\n    }",
        "src_wo_comments": "public ArchiveInputStream createArchiveInputStream ( final InputStream in ) throws ArchiveException { if ( in == null ) { throw new IllegalArgumentException ( \"Stream must not be null.\" ) ; } if ( ! in . markSupported ( ) ) { throw new IllegalArgumentException ( \"Mark is not supported.\" ) ; } final byte [ ] signature = new byte [ 12 ] ; in . mark ( signature . length ) ; try { int signatureLength = in . read ( signature ) ; in . reset ( ) ; if ( ZipArchiveInputStream . matches ( signature , signatureLength ) ) { return new ZipArchiveInputStream ( in ) ; } else if ( JarArchiveInputStream . matches ( signature , signatureLength ) ) { return new JarArchiveInputStream ( in ) ; } else if ( ArArchiveInputStream . matches ( signature , signatureLength ) ) { return new ArArchiveInputStream ( in ) ; } else if ( CpioArchiveInputStream . matches ( signature , signatureLength ) ) { return new CpioArchiveInputStream ( in ) ; } final byte [ ] dumpsig = new byte [ 32 ] ; in . mark ( dumpsig . length ) ; signatureLength = in . read ( dumpsig ) ; in . reset ( ) ; if ( DumpArchiveInputStream . matches ( dumpsig , signatureLength ) ) { return new DumpArchiveInputStream ( in ) ; } final byte [ ] tarheader = new byte [ 512 ] ; in . mark ( tarheader . length ) ; signatureLength = in . read ( tarheader ) ; in . reset ( ) ; if ( TarArchiveInputStream . matches ( tarheader , signatureLength ) ) { return new TarArchiveInputStream ( in ) ; } if ( signatureLength >= 512 ) { try { TarArchiveInputStream tais = new TarArchiveInputStream ( new ByteArrayInputStream ( tarheader ) ) ; tais . getNextEntry ( ) ; return new TarArchiveInputStream ( in ) ; } catch ( Exception e ) { } } } catch ( IOException e ) { throw new ArchiveException ( \"Could not use reset and mark operations.\" , e ) ; } throw new ArchiveException ( \"No Archiver found for the stream signature\" ) ; }",
        "fixed_src": "public ArchiveInputStream createArchiveInputStream(final InputStream in)\n            throws ArchiveException {\n        if (in == null) {\n            throw new IllegalArgumentException(\"Stream must not be null.\");\n        }\n\n        if (!in.markSupported()) {\n            throw new IllegalArgumentException(\"Mark is not supported.\");\n        }\n\n        final byte[] signature = new byte[12];\n        in.mark(signature.length);\n        try {\n            int signatureLength = in.read(signature);\n            in.reset();\n            if (ZipArchiveInputStream.matches(signature, signatureLength)) {\n                return new ZipArchiveInputStream(in);\n            } else if (JarArchiveInputStream.matches(signature, signatureLength)) {\n                return new JarArchiveInputStream(in);\n            } else if (ArArchiveInputStream.matches(signature, signatureLength)) {\n                return new ArArchiveInputStream(in);\n            } else if (CpioArchiveInputStream.matches(signature, signatureLength)) {\n                return new CpioArchiveInputStream(in);\n            }\n\n            // Dump needs a bigger buffer to check the signature;\n            final byte[] dumpsig = new byte[32];\n            in.mark(dumpsig.length);\n            signatureLength = in.read(dumpsig);\n            in.reset();\n            if (DumpArchiveInputStream.matches(dumpsig, signatureLength)) {\n                return new DumpArchiveInputStream(in);\n            }\n\n            // Tar needs an even bigger buffer to check the signature; read the first block\n            final byte[] tarheader = new byte[512];\n            in.mark(tarheader.length);\n            signatureLength = in.read(tarheader);\n            in.reset();\n            if (TarArchiveInputStream.matches(tarheader, signatureLength)) {\n                return new TarArchiveInputStream(in);\n            }\n            // COMPRESS-117 - improve auto-recognition\n            if (signatureLength >= 512) {\n                try {\n                    TarArchiveInputStream tais = new TarArchiveInputStream(new ByteArrayInputStream(tarheader));\n                    // COMPRESS-191 - verify the header checksum\n                    if (tais.getNextTarEntry().isCheckSumOK()) {\n                        return new TarArchiveInputStream(in);\n                    }\n                } catch (Exception e) { // NOPMD\n                    // can generate IllegalArgumentException as well\n                    // as IOException\n                    // autodetection, simply not a TAR\n                    // ignored\n                }\n            }\n        } catch (IOException e) {\n            throw new ArchiveException(\"Could not use reset and mark operations.\", e);\n        }\n\n        throw new ArchiveException(\"No Archiver found for the stream signature\");\n    }",
        "fixed_src_wo_comments": "public ArchiveInputStream createArchiveInputStream ( final InputStream in ) throws ArchiveException { if ( in == null ) { throw new IllegalArgumentException ( \"Stream must not be null.\" ) ; } if ( ! in . markSupported ( ) ) { throw new IllegalArgumentException ( \"Mark is not supported.\" ) ; } final byte [ ] signature = new byte [ 12 ] ; in . mark ( signature . length ) ; try { int signatureLength = in . read ( signature ) ; in . reset ( ) ; if ( ZipArchiveInputStream . matches ( signature , signatureLength ) ) { return new ZipArchiveInputStream ( in ) ; } else if ( JarArchiveInputStream . matches ( signature , signatureLength ) ) { return new JarArchiveInputStream ( in ) ; } else if ( ArArchiveInputStream . matches ( signature , signatureLength ) ) { return new ArArchiveInputStream ( in ) ; } else if ( CpioArchiveInputStream . matches ( signature , signatureLength ) ) { return new CpioArchiveInputStream ( in ) ; } final byte [ ] dumpsig = new byte [ 32 ] ; in . mark ( dumpsig . length ) ; signatureLength = in . read ( dumpsig ) ; in . reset ( ) ; if ( DumpArchiveInputStream . matches ( dumpsig , signatureLength ) ) { return new DumpArchiveInputStream ( in ) ; } final byte [ ] tarheader = new byte [ 512 ] ; in . mark ( tarheader . length ) ; signatureLength = in . read ( tarheader ) ; in . reset ( ) ; if ( TarArchiveInputStream . matches ( tarheader , signatureLength ) ) { return new TarArchiveInputStream ( in ) ; } if ( signatureLength >= 512 ) { try { TarArchiveInputStream tais = new TarArchiveInputStream ( new ByteArrayInputStream ( tarheader ) ) ; if ( tais . getNextTarEntry ( ) . isCheckSumOK ( ) ) { return new TarArchiveInputStream ( in ) ; } } catch ( Exception e ) { } } } catch ( IOException e ) { throw new ArchiveException ( \"Could not use reset and mark operations.\" , e ) ; } throw new ArchiveException ( \"No Archiver found for the stream signature\" ) ; }",
        "summary": "Too relaxed tar detection in ArchiveStreamFactory",
        "Description": "The relaxed tar detection logic added in COMPRESS-117 unfortunately matches also some non-tar files like a [test AIFF file|https://svn.apache.org/repos/asf/tika/trunk/tika-parsers/src/test/resources/test-documents/testAIFF.aif] that Apache Tika uses. It would be good to improve the detection heuristics to still match files like the one in COMPRESS-117 but avoid false positives like the AIFF file in Tika.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-191",
        "comments": [
            "The attached patch adds heuristics for verifying the tar header checksum, and uses that mechanism to better avoid false positives in ArchiveStreamFactory.",
            "patch applied and a testcase added based on Tika's AIFF file.\n\nsvn revisions 1358504 and 1358506\n\nMany thanks!"
        ],
        "summarized_discussion": "\n\nThe bug was solved by applying a patch which added heuristics for verifying the tar header checksum and used that mechanism to better avoid false positives in ArchiveStreamFactory. The patch was applied and a testcase was added based on Tika's AIFF file. The SVN revisions for the patch were 1358504 and 1358506."
    },
    "Compress_41_src/main/java/org/apache/commons/compress/archivers/zip/ZipArchiveInputStream.java_219_324": {
        "src": "public ZipArchiveEntry getNextZipEntry() throws IOException {\n        boolean firstEntry = true;\n        if (closed || hitCentralDirectory) {\n            return null;\n        }\n        if (current != null) {\n            closeEntry();\n            firstEntry = false;\n        }\n\n        try {\n            if (firstEntry) {\n                // split archives have a special signature before the\n                // first local file header - look for it and fail with\n                // the appropriate error message if this is a split\n                // archive.\n                readFirstLocalFileHeader(LFH_BUF);\n            } else {\n                readFully(LFH_BUF);\n            }\n        } catch (final EOFException e) {\n            return null;\n        }\n\n        final ZipLong sig = new ZipLong(LFH_BUF);\n        if (sig.equals(ZipLong.CFH_SIG) || sig.equals(ZipLong.AED_SIG)) {\n            hitCentralDirectory = true;\n            skipRemainderOfArchive();\n        }\n        if (!sig.equals(ZipLong.LFH_SIG)) {\n            return null;\n        }\n\n        int off = WORD;\n        current = new CurrentEntry();\n\n        final int versionMadeBy = ZipShort.getValue(LFH_BUF, off);\n        off += SHORT;\n        current.entry.setPlatform((versionMadeBy >> ZipFile.BYTE_SHIFT) & ZipFile.NIBLET_MASK);\n\n        final GeneralPurposeBit gpFlag = GeneralPurposeBit.parse(LFH_BUF, off);\n        final boolean hasUTF8Flag = gpFlag.usesUTF8ForNames();\n        final ZipEncoding entryEncoding = hasUTF8Flag ? ZipEncodingHelper.UTF8_ZIP_ENCODING : zipEncoding;\n        current.hasDataDescriptor = gpFlag.usesDataDescriptor();\n        current.entry.setGeneralPurposeBit(gpFlag);\n\n        off += SHORT;\n\n        current.entry.setMethod(ZipShort.getValue(LFH_BUF, off));\n        off += SHORT;\n\n        final long time = ZipUtil.dosToJavaTime(ZipLong.getValue(LFH_BUF, off));\n        current.entry.setTime(time);\n        off += WORD;\n\n        ZipLong size = null, cSize = null;\n        if (!current.hasDataDescriptor) {\n            current.entry.setCrc(ZipLong.getValue(LFH_BUF, off));\n            off += WORD;\n\n            cSize = new ZipLong(LFH_BUF, off);\n            off += WORD;\n\n            size = new ZipLong(LFH_BUF, off);\n            off += WORD;\n        } else {\n            off += 3 * WORD;\n        }\n\n        final int fileNameLen = ZipShort.getValue(LFH_BUF, off);\n\n        off += SHORT;\n\n        final int extraLen = ZipShort.getValue(LFH_BUF, off);\n        off += SHORT;\n\n        final byte[] fileName = new byte[fileNameLen];\n        readFully(fileName);\n        current.entry.setName(entryEncoding.decode(fileName), fileName);\n\n        final byte[] extraData = new byte[extraLen];\n        readFully(extraData);\n        current.entry.setExtra(extraData);\n\n        if (!hasUTF8Flag && useUnicodeExtraFields) {\n            ZipUtil.setNameAndCommentFromExtraFields(current.entry, fileName, null);\n        }\n\n        processZip64Extra(size, cSize);\n\n        if (current.entry.getCompressedSize() != ArchiveEntry.SIZE_UNKNOWN) {\n            if (current.entry.getMethod() == ZipMethod.UNSHRINKING.getCode()) {\n                current.in = new UnshrinkingInputStream(new BoundedInputStream(in, current.entry.getCompressedSize()));\n            } else if (current.entry.getMethod() == ZipMethod.IMPLODING.getCode()) {\n                current.in = new ExplodingInputStream(\n                        current.entry.getGeneralPurposeBit().getSlidingDictionarySize(),\n                        current.entry.getGeneralPurposeBit().getNumberOfShannonFanoTrees(),\n                        new BoundedInputStream(in, current.entry.getCompressedSize()));\n            } else if (current.entry.getMethod() == ZipMethod.BZIP2.getCode()) {\n                current.in = new BZip2CompressorInputStream(new BoundedInputStream(in, current.entry.getCompressedSize()));\n            }\n        }\n        \n        entriesRead++;\n        return current.entry;\n    }",
        "src_wo_comments": "public ZipArchiveEntry getNextZipEntry ( ) throws IOException { boolean firstEntry = true ; if ( closed || hitCentralDirectory ) { return null ; } if ( current != null ) { closeEntry ( ) ; firstEntry = false ; } try { if ( firstEntry ) { readFirstLocalFileHeader ( LFH_BUF ) ; } else { readFully ( LFH_BUF ) ; } } catch ( final EOFException e ) { return null ; } final ZipLong sig = new ZipLong ( LFH_BUF ) ; if ( sig . equals ( ZipLong . CFH_SIG ) || sig . equals ( ZipLong . AED_SIG ) ) { hitCentralDirectory = true ; skipRemainderOfArchive ( ) ; } if ( ! sig . equals ( ZipLong . LFH_SIG ) ) { return null ; } int off = WORD ; current = new CurrentEntry ( ) ; final int versionMadeBy = ZipShort . getValue ( LFH_BUF , off ) ; off += SHORT ; current . entry . setPlatform ( ( versionMadeBy >> ZipFile . BYTE_SHIFT ) & ZipFile . NIBLET_MASK ) ; final GeneralPurposeBit gpFlag = GeneralPurposeBit . parse ( LFH_BUF , off ) ; final boolean hasUTF8Flag = gpFlag . usesUTF8ForNames ( ) ; final ZipEncoding entryEncoding = hasUTF8Flag ? ZipEncodingHelper . UTF8_ZIP_ENCODING : zipEncoding ; current . hasDataDescriptor = gpFlag . usesDataDescriptor ( ) ; current . entry . setGeneralPurposeBit ( gpFlag ) ; off += SHORT ; current . entry . setMethod ( ZipShort . getValue ( LFH_BUF , off ) ) ; off += SHORT ; final long time = ZipUtil . dosToJavaTime ( ZipLong . getValue ( LFH_BUF , off ) ) ; current . entry . setTime ( time ) ; off += WORD ; ZipLong size = null , cSize = null ; if ( ! current . hasDataDescriptor ) { current . entry . setCrc ( ZipLong . getValue ( LFH_BUF , off ) ) ; off += WORD ; cSize = new ZipLong ( LFH_BUF , off ) ; off += WORD ; size = new ZipLong ( LFH_BUF , off ) ; off += WORD ; } else { off += 3 * WORD ; } final int fileNameLen = ZipShort . getValue ( LFH_BUF , off ) ; off += SHORT ; final int extraLen = ZipShort . getValue ( LFH_BUF , off ) ; off += SHORT ; final byte [ ] fileName = new byte [ fileNameLen ] ; readFully ( fileName ) ; current . entry . setName ( entryEncoding . decode ( fileName ) , fileName ) ; final byte [ ] extraData = new byte [ extraLen ] ; readFully ( extraData ) ; current . entry . setExtra ( extraData ) ; if ( ! hasUTF8Flag && useUnicodeExtraFields ) { ZipUtil . setNameAndCommentFromExtraFields ( current . entry , fileName , null ) ; } processZip64Extra ( size , cSize ) ; if ( current . entry . getCompressedSize ( ) != ArchiveEntry . SIZE_UNKNOWN ) { if ( current . entry . getMethod ( ) == ZipMethod . UNSHRINKING . getCode ( ) ) { current . in = new UnshrinkingInputStream ( new BoundedInputStream ( in , current . entry . getCompressedSize ( ) ) ) ; } else if ( current . entry . getMethod ( ) == ZipMethod . IMPLODING . getCode ( ) ) { current . in = new ExplodingInputStream ( current . entry . getGeneralPurposeBit ( ) . getSlidingDictionarySize ( ) , current . entry . getGeneralPurposeBit ( ) . getNumberOfShannonFanoTrees ( ) , new BoundedInputStream ( in , current . entry . getCompressedSize ( ) ) ) ; } else if ( current . entry . getMethod ( ) == ZipMethod . BZIP2 . getCode ( ) ) { current . in = new BZip2CompressorInputStream ( new BoundedInputStream ( in , current . entry . getCompressedSize ( ) ) ) ; } } entriesRead ++ ; return current . entry ; }",
        "fixed_src": "public ZipArchiveEntry getNextZipEntry() throws IOException {\n        boolean firstEntry = true;\n        if (closed || hitCentralDirectory) {\n            return null;\n        }\n        if (current != null) {\n            closeEntry();\n            firstEntry = false;\n        }\n\n        try {\n            if (firstEntry) {\n                // split archives have a special signature before the\n                // first local file header - look for it and fail with\n                // the appropriate error message if this is a split\n                // archive.\n                readFirstLocalFileHeader(LFH_BUF);\n            } else {\n                readFully(LFH_BUF);\n            }\n        } catch (final EOFException e) {\n            return null;\n        }\n\n        final ZipLong sig = new ZipLong(LFH_BUF);\n        if (sig.equals(ZipLong.CFH_SIG) || sig.equals(ZipLong.AED_SIG)) {\n            hitCentralDirectory = true;\n            skipRemainderOfArchive();\n            return null;\n        }\n        if (!sig.equals(ZipLong.LFH_SIG)) {\n            throw new ZipException(String.format(\"Unexpected record signature: 0X%X\", sig.getValue()));\n        }\n\n        int off = WORD;\n        current = new CurrentEntry();\n\n        final int versionMadeBy = ZipShort.getValue(LFH_BUF, off);\n        off += SHORT;\n        current.entry.setPlatform((versionMadeBy >> ZipFile.BYTE_SHIFT) & ZipFile.NIBLET_MASK);\n\n        final GeneralPurposeBit gpFlag = GeneralPurposeBit.parse(LFH_BUF, off);\n        final boolean hasUTF8Flag = gpFlag.usesUTF8ForNames();\n        final ZipEncoding entryEncoding = hasUTF8Flag ? ZipEncodingHelper.UTF8_ZIP_ENCODING : zipEncoding;\n        current.hasDataDescriptor = gpFlag.usesDataDescriptor();\n        current.entry.setGeneralPurposeBit(gpFlag);\n\n        off += SHORT;\n\n        current.entry.setMethod(ZipShort.getValue(LFH_BUF, off));\n        off += SHORT;\n\n        final long time = ZipUtil.dosToJavaTime(ZipLong.getValue(LFH_BUF, off));\n        current.entry.setTime(time);\n        off += WORD;\n\n        ZipLong size = null, cSize = null;\n        if (!current.hasDataDescriptor) {\n            current.entry.setCrc(ZipLong.getValue(LFH_BUF, off));\n            off += WORD;\n\n            cSize = new ZipLong(LFH_BUF, off);\n            off += WORD;\n\n            size = new ZipLong(LFH_BUF, off);\n            off += WORD;\n        } else {\n            off += 3 * WORD;\n        }\n\n        final int fileNameLen = ZipShort.getValue(LFH_BUF, off);\n\n        off += SHORT;\n\n        final int extraLen = ZipShort.getValue(LFH_BUF, off);\n        off += SHORT;\n\n        final byte[] fileName = new byte[fileNameLen];\n        readFully(fileName);\n        current.entry.setName(entryEncoding.decode(fileName), fileName);\n\n        final byte[] extraData = new byte[extraLen];\n        readFully(extraData);\n        current.entry.setExtra(extraData);\n\n        if (!hasUTF8Flag && useUnicodeExtraFields) {\n            ZipUtil.setNameAndCommentFromExtraFields(current.entry, fileName, null);\n        }\n\n        processZip64Extra(size, cSize);\n\n        if (current.entry.getCompressedSize() != ArchiveEntry.SIZE_UNKNOWN) {\n            if (current.entry.getMethod() == ZipMethod.UNSHRINKING.getCode()) {\n                current.in = new UnshrinkingInputStream(new BoundedInputStream(in, current.entry.getCompressedSize()));\n            } else if (current.entry.getMethod() == ZipMethod.IMPLODING.getCode()) {\n                current.in = new ExplodingInputStream(\n                        current.entry.getGeneralPurposeBit().getSlidingDictionarySize(),\n                        current.entry.getGeneralPurposeBit().getNumberOfShannonFanoTrees(),\n                        new BoundedInputStream(in, current.entry.getCompressedSize()));\n            } else if (current.entry.getMethod() == ZipMethod.BZIP2.getCode()) {\n                current.in = new BZip2CompressorInputStream(new BoundedInputStream(in, current.entry.getCompressedSize()));\n            }\n        }\n        \n        entriesRead++;\n        return current.entry;\n    }",
        "fixed_src_wo_comments": "public ZipArchiveEntry getNextZipEntry ( ) throws IOException { boolean firstEntry = true ; if ( closed || hitCentralDirectory ) { return null ; } if ( current != null ) { closeEntry ( ) ; firstEntry = false ; } try { if ( firstEntry ) { readFirstLocalFileHeader ( LFH_BUF ) ; } else { readFully ( LFH_BUF ) ; } } catch ( final EOFException e ) { return null ; } final ZipLong sig = new ZipLong ( LFH_BUF ) ; if ( sig . equals ( ZipLong . CFH_SIG ) || sig . equals ( ZipLong . AED_SIG ) ) { hitCentralDirectory = true ; skipRemainderOfArchive ( ) ; return null ; } if ( ! sig . equals ( ZipLong . LFH_SIG ) ) { throw new ZipException ( String . format ( \"Unexpected record signature: 0X%X\" , sig . getValue ( ) ) ) ; } int off = WORD ; current = new CurrentEntry ( ) ; final int versionMadeBy = ZipShort . getValue ( LFH_BUF , off ) ; off += SHORT ; current . entry . setPlatform ( ( versionMadeBy >> ZipFile . BYTE_SHIFT ) & ZipFile . NIBLET_MASK ) ; final GeneralPurposeBit gpFlag = GeneralPurposeBit . parse ( LFH_BUF , off ) ; final boolean hasUTF8Flag = gpFlag . usesUTF8ForNames ( ) ; final ZipEncoding entryEncoding = hasUTF8Flag ? ZipEncodingHelper . UTF8_ZIP_ENCODING : zipEncoding ; current . hasDataDescriptor = gpFlag . usesDataDescriptor ( ) ; current . entry . setGeneralPurposeBit ( gpFlag ) ; off += SHORT ; current . entry . setMethod ( ZipShort . getValue ( LFH_BUF , off ) ) ; off += SHORT ; final long time = ZipUtil . dosToJavaTime ( ZipLong . getValue ( LFH_BUF , off ) ) ; current . entry . setTime ( time ) ; off += WORD ; ZipLong size = null , cSize = null ; if ( ! current . hasDataDescriptor ) { current . entry . setCrc ( ZipLong . getValue ( LFH_BUF , off ) ) ; off += WORD ; cSize = new ZipLong ( LFH_BUF , off ) ; off += WORD ; size = new ZipLong ( LFH_BUF , off ) ; off += WORD ; } else { off += 3 * WORD ; } final int fileNameLen = ZipShort . getValue ( LFH_BUF , off ) ; off += SHORT ; final int extraLen = ZipShort . getValue ( LFH_BUF , off ) ; off += SHORT ; final byte [ ] fileName = new byte [ fileNameLen ] ; readFully ( fileName ) ; current . entry . setName ( entryEncoding . decode ( fileName ) , fileName ) ; final byte [ ] extraData = new byte [ extraLen ] ; readFully ( extraData ) ; current . entry . setExtra ( extraData ) ; if ( ! hasUTF8Flag && useUnicodeExtraFields ) { ZipUtil . setNameAndCommentFromExtraFields ( current . entry , fileName , null ) ; } processZip64Extra ( size , cSize ) ; if ( current . entry . getCompressedSize ( ) != ArchiveEntry . SIZE_UNKNOWN ) { if ( current . entry . getMethod ( ) == ZipMethod . UNSHRINKING . getCode ( ) ) { current . in = new UnshrinkingInputStream ( new BoundedInputStream ( in , current . entry . getCompressedSize ( ) ) ) ; } else if ( current . entry . getMethod ( ) == ZipMethod . IMPLODING . getCode ( ) ) { current . in = new ExplodingInputStream ( current . entry . getGeneralPurposeBit ( ) . getSlidingDictionarySize ( ) , current . entry . getGeneralPurposeBit ( ) . getNumberOfShannonFanoTrees ( ) , new BoundedInputStream ( in , current . entry . getCompressedSize ( ) ) ) ; } else if ( current . entry . getMethod ( ) == ZipMethod . BZIP2 . getCode ( ) ) { current . in = new BZip2CompressorInputStream ( new BoundedInputStream ( in , current . entry . getCompressedSize ( ) ) ) ; } } entriesRead ++ ; return current . entry ; }",
        "summary": "ZipArchiveInputStream.getNextZipEntry() should differentiate between \"invalid entry encountered\" and \"no more entries\"",
        "Description": "ZipArchiveInputStream.getNextZipEntry() currently returns null if an invalid entry is encountered.  Thus, it's not possible to differentiate between \"no more entries\" and \"invalid entry encountered\" conditions.\n\nInstead, it should throw an exception if an invalid entry is encountered.\n\nI've created a test case and fix. I will submit a pull request shortly.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-367",
        "comments": [
            "Attached is fix, new test, and fix for existing test.",
            "Patch applied, many thanks!"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed with a patch, and a new test and fix for the existing test have been applied."
    },
    "JacksonDatabind_93_src/main/java/com/fasterxml/jackson/databind/jsontype/impl/SubTypeValidator.java_67_99": {
        "src": "public void validateSubType(DeserializationContext ctxt, JavaType type) throws JsonMappingException\n    {\n        // There are certain nasty classes that could cause problems, mostly\n        // via default typing -- catch them here.\n        final Class<?> raw = type.getRawClass();\n        String full = raw.getName();\n\n        main_check:\n        do {\n            if (_cfgIllegalClassNames.contains(full)) {\n                break;\n            }\n\n            // 18-Dec-2017, tatu: As per [databind#1855], need bit more sophisticated handling\n            //    for some Spring framework types\n            // 05-Jan-2017, tatu: ... also, only applies to classes, not interfaces\n            if (full.startsWith(PREFIX_STRING)) {\n                for (Class<?> cls = raw; cls != Object.class; cls = cls.getSuperclass()) {\n                    String name = cls.getSimpleName();\n                    // looking for \"AbstractBeanFactoryPointcutAdvisor\" but no point to allow any is there?\n                    if (\"AbstractPointcutAdvisor\".equals(name)\n                            // ditto  for \"FileSystemXmlApplicationContext\": block all ApplicationContexts\n                            || \"AbstractApplicationContext\".equals(name)) {\n                        break main_check;\n                    }\n                }\n            }\n            return;\n        } while (false);\n\n        throw JsonMappingException.from(ctxt,\n                String.format(\"Illegal type (%s) to deserialize: prevented for security reasons\", full));\n    }",
        "src_wo_comments": "public void validateSubType ( DeserializationContext ctxt , JavaType type ) throws JsonMappingException { final Class < ? > raw = type . getRawClass ( ) ; String full = raw . getName ( ) ; main_check : do { if ( _cfgIllegalClassNames . contains ( full ) ) { break ; } if ( full . startsWith ( PREFIX_STRING ) ) { for ( Class < ? > cls = raw ; cls != Object . class ; cls = cls . getSuperclass ( ) ) { String name = cls . getSimpleName ( ) ; if ( \"AbstractPointcutAdvisor\" . equals ( name ) || \"AbstractApplicationContext\" . equals ( name ) ) { break main_check ; } } } return ; } while ( false ) ; throw JsonMappingException . from ( ctxt , String . format ( \"Illegal type (%s) to deserialize: prevented for security reasons\" , full ) ) ; }",
        "fixed_src": "public void validateSubType(DeserializationContext ctxt, JavaType type) throws JsonMappingException\n    {\n        // There are certain nasty classes that could cause problems, mostly\n        // via default typing -- catch them here.\n        final Class<?> raw = type.getRawClass();\n        String full = raw.getName();\n\n        main_check:\n        do {\n            if (_cfgIllegalClassNames.contains(full)) {\n                break;\n            }\n\n            // 18-Dec-2017, tatu: As per [databind#1855], need bit more sophisticated handling\n            //    for some Spring framework types\n            // 05-Jan-2017, tatu: ... also, only applies to classes, not interfaces\n            if (!raw.isInterface() && full.startsWith(PREFIX_STRING)) {\n                for (Class<?> cls = raw; (cls != null) && (cls != Object.class); cls = cls.getSuperclass()) {\n                    String name = cls.getSimpleName();\n                    // looking for \"AbstractBeanFactoryPointcutAdvisor\" but no point to allow any is there?\n                    if (\"AbstractPointcutAdvisor\".equals(name)\n                            // ditto  for \"FileSystemXmlApplicationContext\": block all ApplicationContexts\n                            || \"AbstractApplicationContext\".equals(name)) {\n                        break main_check;\n                    }\n                }\n            }\n            return;\n        } while (false);\n\n        throw JsonMappingException.from(ctxt,\n                String.format(\"Illegal type (%s) to deserialize: prevented for security reasons\", full));\n    }",
        "fixed_src_wo_comments": "public void validateSubType ( DeserializationContext ctxt , JavaType type ) throws JsonMappingException { final Class < ? > raw = type . getRawClass ( ) ; String full = raw . getName ( ) ; main_check : do { if ( _cfgIllegalClassNames . contains ( full ) ) { break ; } if ( ! raw . isInterface ( ) && full . startsWith ( PREFIX_STRING ) ) { for ( Class < ? > cls = raw ; ( cls != null ) && ( cls != Object . class ) ; cls = cls . getSuperclass ( ) ) { String name = cls . getSimpleName ( ) ; if ( \"AbstractPointcutAdvisor\" . equals ( name ) || \"AbstractApplicationContext\" . equals ( name ) ) { break main_check ; } } } return ; } while ( false ) ; throw JsonMappingException . from ( ctxt , String . format ( \"Illegal type (%s) to deserialize: prevented for security reasons\" , full ) ) ; }",
        "summary": "`NullPointerException` in `SubTypeValidator.validateSubType` when validating Spring interface",
        "Description": "In jackson-databind-2.8.11 jackson-databind-2.9.3 and  jackson-databind-2.9.4-SNAPSHOT `SubTypeValidator.validateSubType` fails with a `NullPointerException` if the `JavaType.getRawClass()` is an interface that starts with `org.springframework.` For example, the following will fail:\r\n\r\n```java\r\npackage org.springframework.security.core;\r\n\r\nimport java.util.*;\r\n\r\npublic class Authentication {\r\n\tprivate List<GrantedAuthority> authorities = new ArrayList<GrantedAuthority>();\r\n\r\n\tpublic List<GrantedAuthority> getAuthorities() {\r\n\t\treturn this.authorities;\r\n\t}\r\n\r\n\tpublic void setAuthorities(List<GrantedAuthority> authorities) {\r\n\t\tthis.authorities = authorities;\r\n\t}\r\n}\r\n```\r\n\r\n```java\r\npackage org.springframework.security.core;\r\n\r\npublic interface GrantedAuthority {\r\n\tString getAuthority();\r\n}\r\n```\r\n\r\n```java\r\n@Test\r\npublic void validateSubTypeFailsWithNPE() throws Exception {\r\n\tObjectMapper mapper = new ObjectMapper();\r\n\tmapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL, JsonTypeInfo.As.PROPERTY);\r\n\r\n\tString json = \"{\\\"@class\\\":\\\"org.springframework.security.core.Authentication\\\",\\\"authorities\\\":[\\\"java.util.ArrayList\\\",[]]}\";\r\n\r\n\tAuthentication authentication = mapper.readValue(json, Authentication.class);\r\n}\r\n```\r\n\r\nwith the following stacktrace:\r\n\r\n```\r\njava.lang.NullPointerException\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.SubTypeValidator.validateSubType(SubTypeValidator.java:86)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerFactory._validateSubType(BeanDeserializerFactory.java:916)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.createBeanDeserializer(BeanDeserializerFactory.java:135)\r\n\tat com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer2(DeserializerCache.java:411)\r\n\tat com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer(DeserializerCache.java:349)\r\n\tat com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:264)\r\n\tat com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244)\r\n\tat com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.findContextualValueDeserializer(DeserializationContext.java:444)\r\n\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.createContextual(CollectionDeserializer.java:183)\r\n\tat com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.createContextual(CollectionDeserializer.java:27)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.handlePrimaryContextualization(DeserializationContext.java:651)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.resolve(BeanDeserializerBase.java:471)\r\n\tat com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:293)\r\n\tat com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244)\r\n\tat com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.findRootValueDeserializer(DeserializationContext.java:477)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._findRootDeserializer(ObjectMapper.java:4178)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3997)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2992)\r\n```\r\nIn prior versions, the test works.   ",
        "issue_url": null,
        "comments": [
            {
                "content": "That sounds like a very interesting behavior. :)\r\nThank you for reporting this, will see what gives.\r\n"
            },
            {
                "content": "Ah. Probably not due to package name, but due to interface. Will verify.\r\n"
            },
            {
                "content": "I left out an analysis on the original report, but perhaps this will help....\r\n\r\nAs far as I can tell, it must be both interface and start with `org.springframework.`\r\n\r\nIf it does not start with `org.springframework.` it will not enter this [if block](https://github.com/FasterXML/jackson-databind/blob/2dff214e36ca5c63b2d2c108e7ff327887bda6e2/src/main/java/com/fasterxml/jackson/databind/jsontype/impl/SubTypeValidator.java#L84).\r\n\r\nThe `NullPointerException` is caused at [this line due](https://github.com/FasterXML/jackson-databind/blob/2dff214e36ca5c63b2d2c108e7ff327887bda6e2/src/main/java/com/fasterxml/jackson/databind/jsontype/impl/SubTypeValidator.java#L85) to the fact that interfaces do not extend `Object`. One way to resolve the issue is to change the line:\r\n\r\n```java\r\nfor (Class<?> cls = raw; cls != Object.class; cls = cls.getSuperclass()) {\r\n```\r\n\r\nto\r\n\r\n```java\r\nfor (Class<?> cls = raw; cls != Object.class && cls != null; cls = cls.getSuperclass()) {\r\n```\r\n"
            },
            {
                "content": "Actually correction, both. Guard statement for spring packages. So nicely isolates problem to the area being ... patched. :-/\r\n\r\nI think this alone would warrant micro-patch, so there's I guess that upside. I'll backport the other fix and then could use help verifying there aren't other issues.\r\n"
            },
            {
                "content": "As always...Thanks for the fast response!\r\n\r\nI am tracking what I believe is one last issue I am having with 2.9.4-SNAPSHOT, but not sure if I will get anything reported until Monday. I want to spend some time isolating the problem and ensuring the problem is not on my end."
            },
            {
                "content": "@rwinch Your help is much appreciated. I do want to make as sure as possible that we can close this branch of problems before 2.9.4 and 2.8.11.1, and will coordinate with you before going with release.\r\nMonday (or next week in general) is fine from my perspective since although I do want to get 2.9.4 out, I don't want to rush things. Plus there may be couple of other things I could perhaps solve by then.\r\n"
            },
            {
                "content": "Thanks for your responsiveness!\r\n\r\nOn a related note... It appears that [2.9.4-SNAPSHOT](https://oss.sonatype.org/content/repositories/snapshots/com/fasterxml/jackson/core/jackson-databind/2.9.4-SNAPSHOT/) hasn't been updated since December. Any chance you could fix that? I believe it is related to that it appears that [Travis is not building 2.9 branch](https://travis-ci.org/FasterXML/jackson-databind/branches), but may be wrong on that.\r\n\r\nPS: At this point I am fairly certain at this point that the remaining issue with updating Jackson in Spring Security is on my end. I will be sure to validate 2.9.4 and 2.8.11.1 before the end of Monday.  "
            },
            {
                "content": "@rwinch Yes, I'll do mvn deploy from home. I thought settings used should allow build of `master` and `2.9` (from `.travis.yml` of `master`) but... not quite sure why 2.9 wouldn't be enabled.\r\n"
            },
            {
                "content": "Found it. I think I didn't fully understand branch whitelisting (applies to each commit separately), plus, 2.9 settings were wrong wrt that."
            },
            {
                "content": "@cowtowncoder \r\n\r\n> I do want to make as sure as possible that we can close this branch of problems before 2.9.4 and 2.8.11.1, and will coordinate with you before going with release.\r\nMonday (or next week in general) is fine from my perspective since although I do want to get 2.9.4 out, I don't want to rush things. Plus there may be couple of other things I could perhaps solve by then.\r\n\r\nThanks for waiting and sorry for my delayed response. My delay was due to deliberating on an additional issue. I ended up reporting #1880 \r\n\r\nInitially I was thinking perhaps this is just a bug fix which broke passivity (in which case it wasn't an issue), but the more I thought about it I thought it was a bug.\r\n\r\nThanks again for all your help."
            },
            {
                "content": "Fix included in `2.8.11.1` / `2.9.4`.\r\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was due to an interface not extending Object, causing a NullPointerException. The solution was to change the line in the source code from \"for (Class<?> cls = raw; cls != Object.class; cls = cls.getSuperclass()) {\" to \"for (Class<?> cls = raw; cls != Object.class && cls != null; cls = cls.getSuperclass()) {\". The fix was included in versions 2.8.11.1 and 2.9.4."
    },
    "Math_2_src/main/java/org/apache/commons/math3/distribution/HypergeometricDistribution.java_267_269": {
        "src": "public double getNumericalMean() {\n        return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();\n    }",
        "src_wo_comments": "public double getNumericalMean ( ) { return ( double ) ( getSampleSize ( ) * getNumberOfSuccesses ( ) ) / ( double ) getPopulationSize ( ) ; }",
        "fixed_src": "public double getNumericalMean() {\n        return getSampleSize() * (getNumberOfSuccesses() / (double) getPopulationSize());\n    }",
        "fixed_src_wo_comments": "public double getNumericalMean ( ) { return getSampleSize ( ) * ( getNumberOfSuccesses ( ) / ( double ) getPopulationSize ( ) ) ; }",
        "summary": "HypergeometricDistribution.sample suffers from integer overflow",
        "Description": "Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values -- the example code below should return a sample between 0 and 50, but usually returns -50.\n\n{code}\nimport org.apache.commons.math3.distribution.HypergeometricDistribution;\n\npublic class Foo {\n  public static void main(String[] args) {\n    HypergeometricDistribution a = new HypergeometricDistribution(\n        43130568, 42976365, 50);\n    System.out.printf(\"%d %d%n\", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints \"0 50\"\n    System.out.printf(\"%d%n\",a.sample());                                             // Prints \"-50\"\n  }\n}\n{code}\n\nIn the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() -- instead of doing\n{code}\nreturn (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();\n{code}\nit could do:\n{code}\nreturn getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());\n{code}\nThis seemed to fix it, based on a quick test.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-1021",
        "comments": [
            "Thanks for the report and suggested fix.\nCommitted in revision 1512546.",
            "Closing all resolved issue now available in released 3.3 version."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in revision 1512546 and is now available in the released 3.3 version. The issue has been closed."
    },
    "Math_58_src/main/java/org/apache/commons/math/optimization/fitting/GaussianFitter.java_119_122": {
        "src": "public double[] fit() {\n        final double[] guess = (new ParameterGuesser(getObservations())).guess();\n        return fit(new Gaussian.Parametric(), guess);\n    }",
        "src_wo_comments": "public double [ ] fit ( ) { final double [ ] guess = ( new ParameterGuesser ( getObservations ( ) ) ) . guess ( ) ; return fit ( new Gaussian . Parametric ( ) , guess ) ; }",
        "fixed_src": "public double[] fit() {\n        final double[] guess = (new ParameterGuesser(getObservations())).guess();\n        return fit(guess);\n    }",
        "fixed_src_wo_comments": "public double [ ] fit ( ) { final double [ ] guess = ( new ParameterGuesser ( getObservations ( ) ) ) . guess ( ) ; return fit ( guess ) ; }",
        "summary": "GaussianFitter Unexpectedly Throws NotStrictlyPositiveException",
        "Description": "Running the following:\n\n    \tdouble[] observations = \n    \t{ \n    \t\t\t1.1143831578403364E-29, \n    \t\t\t 4.95281403484594E-28, \n    \t\t\t 1.1171347211930288E-26, \n    \t\t\t 1.7044813962636277E-25, \n    \t\t\t 1.9784716574832164E-24, \n    \t\t\t 1.8630236407866774E-23, \n    \t\t\t 1.4820532905097742E-22, \n    \t\t\t 1.0241963854632831E-21, \n    \t\t\t 6.275077366673128E-21, \n    \t\t\t 3.461808994532493E-20, \n    \t\t\t 1.7407124684715706E-19, \n    \t\t\t 8.056687953553974E-19, \n    \t\t\t 3.460193945992071E-18, \n    \t\t\t 1.3883326374011525E-17, \n    \t\t\t 5.233894983671116E-17, \n    \t\t\t 1.8630791465263745E-16, \n    \t\t\t 6.288759227922111E-16, \n    \t\t\t 2.0204433920597856E-15, \n    \t\t\t 6.198768938576155E-15, \n    \t\t\t 1.821419346860626E-14, \n    \t\t\t 5.139176445538471E-14, \n    \t\t\t 1.3956427429045787E-13, \n    \t\t\t 3.655705706448139E-13, \n    \t\t\t 9.253753324779779E-13, \n    \t\t\t 2.267636001476696E-12, \n    \t\t\t 5.3880460095836855E-12, \n    \t\t\t 1.2431632654852931E-11 \n    \t};\n  \n    \tGaussianFitter g = \n    \t\tnew GaussianFitter(new LevenbergMarquardtOptimizer());\n    \t\n    \tfor (int index = 0; index < 27; index++)\n    \t{\n    \t\tg.addObservedPoint(index, observations[index]);\n    \t}\n       \tg.fit();\n\nResults in:\n\norg.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0)\n\tat org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184)\n\tat org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129)\n\n\nI'm guessing the initial guess for sigma is off.  ",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-519",
        "comments": [
            "Will you investigate?\nA good starting point would be to prepare a unit test.\n\nIf it's indeed the guesser, we should add a check that the guessed sigma is positive (and, if not, return the opposite value).\n",
            "Yes I'm going to start looking at it this weekend.",
            "Gilles - BTW - Did you ever figure out what was happening with this test in the ParametricGaussianFunction test:\n\n    /**\n     * The parameters d is 0.\n     *\n     * @throws MathUserException in the event of a test case error\n     */\n    @Test(expected=ZeroException.class)\n    public void testValue03() throws MathUserException {\n        Gaussian.Parametric f = new Gaussian.Parametric();\n        f.value(0.0, new double[] {1.0, 1.0, 0.0});\n    }\n\n",
            "It seems the optimizer is producing a negative value for sigma...",
            "I tried commenting out the validateParameters(param) line in Gaussian.value() and now it converges:\n\nMean: 53.15727927329495\nSigma: 5.752146229144571",
            "I guess that the clean solution would be to be able to specify constraints such that the optimizer won't try invalid parameters.\n\nAn ugly workaround would be that the \"CurveFitter\" internally catches all \"RuntimeException\"s generated by the function, and consider the residuals to be infinite. But this would violate the stated policy that CM does not catch \"MathUserException\"s (and could have nasty side-effects when the exception is really unexpected).\n\nI've tried another workaround in \"GaussianFitter\" (see attached file) whereby an invalid parameter is turned into the function returning \"NaN\" (\"POSITIVE_INFINITY\" also works).\nLet me know if you see any issues with this solution or if would be an adequate solution for this problem.\n",
            "I like the last option the best.  I'm wondering though whether we are placing constraints on variables that are not supposed to be constrained?\n\nPerhaps the optimizer should be allowed to call a method on Gaussian that does not effectively constrain the valid values of sigma?\n\n",
            "In the case of the Gaussian, that parameter (\"sigma\") is used squared, so negative values don't really matter; however, the semantics of \"sigma\" calls for it to be non-negative.\nBut it certainly cannot be zero, so that's a constraint that cannot be removed.\n",
            "I think that while the optimizer is searching for a solution, it should be able to pass parameters that are not necessarily valid, but enable it to proceed.  If it finds a solution that is not valid, then an exception should be thrown. \n\n",
            "{quote}\n[...] pass parameters that are not necessarily valid [...]\n{quote}\n\nWhat would that mean?\nTo direct its search, the optimizer needs feedback from the objective function; in case of invalid parameters, the objective function cannot provide feedback (i.e. compute the theoretical curve)!\n\nThe only issue is how to \"tell\" the optimizer that it should discard invalid parameters (i.e. make it consider that they are far away from a solution).\n",
            "I'm assuming that the value that results from passing in a negative sigma is closer to the optimal than POSITIVE infinity or NaN, and that this will result in faster convergence.  How about only returning Nan or POSITIVE_INFINITY if the optimizer passes in zero for sigma, but letting it proceed otherwise?\n\n\n",
            "Maybe this discussion should be held on the dev list since it becomes long.\nAnyway, returning NaN or POSITIVE_INFINITY would work only with some optimizers.\nI guess a proper solution would be to have constrained optimization available (see MATH-196).\nFor simple bounds on estimated parameters, this can be done using intermediate variables and mapping functions, but for general non-linear constraints, we need Lagrangian multipliers and all this stuff.",
            "{quote}\nI'm assuming that the value that results from passing in a negative sigma is closer to the optimal than POSITIVE infinity or NaN [...]\n{quote}\n\nPOSITIVE_INFINITY or NaN are the returned values of the objective function and its gradient, specially chosen because they are quite likely to be different from the actual values of the function and its gradient; it's not \"sigma\" that is assumed to infinity or NaN.\n\nIf you think is that we can accept a negative sigma as the result of the fitting, I don't agree. In the case of the Gaussian, it's by \"chance\" that a semantically invalid parameter (negative sigma) would still be usable (as it is being squared before use).\nIn most case you cannot expect such a forgiving situation. For example, if you want to fit \"a\" in the following function:\n{noformat}\n  log(a * x)\n{noformat}\nno invalid values for \"a\" are usable.\nThe \"Gaussian\" class should not be unsafe (no validation of sigma) just because of its particular use here.\n[Moreover the workaround is useful in showing users how to setup a fitting of a function that can raise an exception.]\n\n{quote}\n[...] and that this will result in faster convergence.\n{quote}\n\nDid you try?\n",
            "Yes!  I figured out how to quote!\n\nFirst of all I hope we are talking about this function:\nhttp://en.wikipedia.org/wiki/Gaussian_function\n\nAs the objective function right?  If I got that wrong then ignore the below.\n\n{quote}\nAnyway, returning NaN or POSITIVE_INFINITY would work only with some optimizers.\n{quote}\n\nSeems to me that if the optimizer does not understand POSITIVE_INFINITY then that's a bug.  \n\n{quote}\nIf you think is that we can accept a negative sigma as the result of the fitting...\n{quote}\n\nNo no no - Not at all.  I'm saying that we should let the optimizer try negative values for sigma if it wants to while it's in the middle of trying to find the optimal sigma.  If it returns a negative sigma as a result, then we need to throw a NotStrictlyPositiveException.\n\n{quote}\nDid you try?\n{quote}\n\nI could give it a whirl, but it does not necessarily prove anything.  Even it it converges quicker, does that mean it will do so in all cases?  It just seems to me like POSITIVE_INFINITE is as far from the optimal as you can get, and therefore it will take longer to get to the optimal.\n\nAlso, I changed my mind about an earlier comment.  If sigma is zero then the gaussian function is zero, so we should probably just return zero.\n",
            "{quote}\nI'm saying that we should let the optimizer try negative values for sigma if it wants to while it's in the middle of trying to find the optimal sigma.\n{quote}\n\nThe point is that we cannot allow invalid parameters because, for those values (of the parameters), the objective function is, by definition of \"invalid\", undefined.\n\n{quote}\n[...] Even it it converges quicker [...]\n{quote}\n\nWell, actually it doesn't (cf. my mail on the \"dev\" ML).\n\n{quote}\nIt just seems to me like POSITIVE_INFINITE is as far from the optimal as you can get, [...]\n{quote}\n\nIndeed, that's exactly the intention: it tells the optimizer to step back from this wrong value for sigma.\n[Note that POSITIVE_INFINITY is not a value of sigma, it is the value of the objective function for any negative sigma.]\n\n{quote}\nIf sigma is zero then the gaussian function is zero, [...]\n{quote}\n\nNo, when sigma is strictly zero, there is no Gaussian anymore: the value at \"b\" (mean) is undefined.\nAlso, cf. http://en.wikipedia.org/wiki/Dirac_delta\n\n",
            "OK - I see what you mean - I also get 17009 iterations with just the validate line commented out.  I'm on board with POSITIVE_INFINITY.    ",
            "If this works, go for it.",
            "Workaround in revision 1073554."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to add a check that the guessed sigma is positive and return the opposite value if it is not. Additionally, the objective function should return NaN or POSITIVE_INFINITY if the optimizer passes in zero for sigma. This will enable the optimizer to discard invalid parameters and consider them to be far away from a solution."
    },
    "JacksonDatabind_85_src/main/java/com/fasterxml/jackson/databind/ser/std/DateTimeSerializerBase.java_48_95": {
        "src": "@Override\n    public JsonSerializer<?> createContextual(SerializerProvider serializers,\n            BeanProperty property) throws JsonMappingException\n    {\n        if (property == null) {\n            return this;\n        }\n        JsonFormat.Value format = findFormatOverrides(serializers, property, handledType());\n        if (format == null) {\n            return this;\n        }\n        // Simple case first: serialize as numeric timestamp?\n        JsonFormat.Shape shape = format.getShape();\n        if (shape.isNumeric()) {\n            return withFormat(Boolean.TRUE, null);\n        }\n\n        // 08-Jun-2017, tatu: With [databind#1648], this gets bit tricky..\n        // First: custom pattern will override things\n                if ((shape == JsonFormat.Shape.STRING) || format.hasPattern()\n                                || format.hasLocale() || format.hasTimeZone()) {\n                    TimeZone tz = format.getTimeZone();\n                    final String pattern = format.hasPattern()\n                                    ? format.getPattern()\n                                    : StdDateFormat.DATE_FORMAT_STR_ISO8601;\n            final Locale loc = format.hasLocale()\n                            ? format.getLocale()\n                            : serializers.getLocale();\n                    SimpleDateFormat df = new SimpleDateFormat(pattern, loc);\n                    if (tz == null) {\n                        tz = serializers.getTimeZone();\n                    }\n            df.setTimeZone(tz);\n            return withFormat(Boolean.FALSE, df);\n        }\n\n        // Otherwise, need one of these changes:\n\n\n        // Jackson's own `StdDateFormat` is quite easy to deal with...\n\n        // 08-Jun-2017, tatu: Unfortunately there's no generally usable\n        //    mechanism for changing `DateFormat` instances (or even clone()ing)\n        //    So: require it be `SimpleDateFormat`; can't config other types\n//            serializers.reportBadDefinition(handledType(), String.format(\n            // Ugh. No way to change `Locale`, create copy; must re-crete completely:\n        return this;\n    }",
        "src_wo_comments": "@ Override public JsonSerializer < ? > createContextual ( SerializerProvider serializers , BeanProperty property ) throws JsonMappingException { if ( property == null ) { return this ; } JsonFormat . Value format = findFormatOverrides ( serializers , property , handledType ( ) ) ; if ( format == null ) { return this ; } JsonFormat . Shape shape = format . getShape ( ) ; if ( shape . isNumeric ( ) ) { return withFormat ( Boolean . TRUE , null ) ; } if ( ( shape == JsonFormat . Shape . STRING ) || format . hasPattern ( ) || format . hasLocale ( ) || format . hasTimeZone ( ) ) { TimeZone tz = format . getTimeZone ( ) ; final String pattern = format . hasPattern ( ) ? format . getPattern ( ) : StdDateFormat . DATE_FORMAT_STR_ISO8601 ; final Locale loc = format . hasLocale ( ) ? format . getLocale ( ) : serializers . getLocale ( ) ; SimpleDateFormat df = new SimpleDateFormat ( pattern , loc ) ; if ( tz == null ) { tz = serializers . getTimeZone ( ) ; } df . setTimeZone ( tz ) ; return withFormat ( Boolean . FALSE , df ) ; } return this ; }",
        "fixed_src": "@Override\n    public JsonSerializer<?> createContextual(SerializerProvider serializers,\n            BeanProperty property) throws JsonMappingException\n    {\n        if (property == null) {\n            return this;\n        }\n        JsonFormat.Value format = findFormatOverrides(serializers, property, handledType());\n        if (format == null) {\n            return this;\n        }\n        // Simple case first: serialize as numeric timestamp?\n        JsonFormat.Shape shape = format.getShape();\n        if (shape.isNumeric()) {\n            return withFormat(Boolean.TRUE, null);\n        }\n\n        // 08-Jun-2017, tatu: With [databind#1648], this gets bit tricky..\n        // First: custom pattern will override things\n        if (format.hasPattern()) {\n            final Locale loc = format.hasLocale()\n                            ? format.getLocale()\n                            : serializers.getLocale();\n            SimpleDateFormat df = new SimpleDateFormat(format.getPattern(), loc);\n            TimeZone tz = format.hasTimeZone() ? format.getTimeZone()\n                    : serializers.getTimeZone();\n            df.setTimeZone(tz);\n            return withFormat(Boolean.FALSE, df);\n        }\n\n        // Otherwise, need one of these changes:\n        final boolean hasLocale = format.hasLocale();\n        final boolean hasTZ = format.hasTimeZone();\n        final boolean asString = (shape == JsonFormat.Shape.STRING);\n\n        if (!hasLocale && !hasTZ && !asString) {\n            return this;\n        }\n\n        DateFormat df0 = serializers.getConfig().getDateFormat();\n        // Jackson's own `StdDateFormat` is quite easy to deal with...\n        if (df0 instanceof StdDateFormat) {\n            StdDateFormat std = (StdDateFormat) df0;\n            if (format.hasLocale()) {\n                std = std.withLocale(format.getLocale());\n            }\n            if (format.hasTimeZone()) {\n                std = std.withTimeZone(format.getTimeZone());\n            }\n            return withFormat(Boolean.FALSE, std);\n        }\n\n        // 08-Jun-2017, tatu: Unfortunately there's no generally usable\n        //    mechanism for changing `DateFormat` instances (or even clone()ing)\n        //    So: require it be `SimpleDateFormat`; can't config other types\n        if (!(df0 instanceof SimpleDateFormat)) {\n//            serializers.reportBadDefinition(handledType(), String.format(\n            serializers.reportMappingProblem(\n\"Configured `DateFormat` (%s) not a `SimpleDateFormat`; can not configure `Locale` or `TimeZone`\",\ndf0.getClass().getName());\n        }\n        SimpleDateFormat df = (SimpleDateFormat) df0;\n        if (hasLocale) {\n            // Ugh. No way to change `Locale`, create copy; must re-crete completely:\n            df = new SimpleDateFormat(df.toPattern(), format.getLocale());\n        } else {\n            df = (SimpleDateFormat) df.clone();\n        }\n        TimeZone newTz = format.getTimeZone();\n        boolean changeTZ = (newTz != null) && !newTz.equals(df.getTimeZone());\n        if (changeTZ) {\n            df.setTimeZone(newTz);\n        }\n        return withFormat(Boolean.FALSE, df);\n    }",
        "fixed_src_wo_comments": "@ Override public JsonSerializer < ? > createContextual ( SerializerProvider serializers , BeanProperty property ) throws JsonMappingException { if ( property == null ) { return this ; } JsonFormat . Value format = findFormatOverrides ( serializers , property , handledType ( ) ) ; if ( format == null ) { return this ; } JsonFormat . Shape shape = format . getShape ( ) ; if ( shape . isNumeric ( ) ) { return withFormat ( Boolean . TRUE , null ) ; } if ( format . hasPattern ( ) ) { final Locale loc = format . hasLocale ( ) ? format . getLocale ( ) : serializers . getLocale ( ) ; SimpleDateFormat df = new SimpleDateFormat ( format . getPattern ( ) , loc ) ; TimeZone tz = format . hasTimeZone ( ) ? format . getTimeZone ( ) : serializers . getTimeZone ( ) ; df . setTimeZone ( tz ) ; return withFormat ( Boolean . FALSE , df ) ; } final boolean hasLocale = format . hasLocale ( ) ; final boolean hasTZ = format . hasTimeZone ( ) ; final boolean asString = ( shape == JsonFormat . Shape . STRING ) ; if ( ! hasLocale && ! hasTZ && ! asString ) { return this ; } DateFormat df0 = serializers . getConfig ( ) . getDateFormat ( ) ; if ( df0 instanceof StdDateFormat ) { StdDateFormat std = ( StdDateFormat ) df0 ; if ( format . hasLocale ( ) ) { std = std . withLocale ( format . getLocale ( ) ) ; } if ( format . hasTimeZone ( ) ) { std = std . withTimeZone ( format . getTimeZone ( ) ) ; } return withFormat ( Boolean . FALSE , std ) ; } if ( ! ( df0 instanceof SimpleDateFormat ) ) { serializers . reportMappingProblem ( \"Configured `DateFormat` (%s) not a `SimpleDateFormat`; can not configure `Locale` or `TimeZone`\" , df0 . getClass ( ) . getName ( ) ) ; } SimpleDateFormat df = ( SimpleDateFormat ) df0 ; if ( hasLocale ) { df = new SimpleDateFormat ( df . toPattern ( ) , format . getLocale ( ) ) ; } else { df = ( SimpleDateFormat ) df . clone ( ) ; } TimeZone newTz = format . getTimeZone ( ) ; boolean changeTZ = ( newTz != null ) && ! newTz . equals ( df . getTimeZone ( ) ) ; if ( changeTZ ) { df . setTimeZone ( newTz ) ; } return withFormat ( Boolean . FALSE , df ) ; }",
        "summary": "`DateTimeSerializerBase` ignores configured date format when creating contextual ",
        "Description": "`DateTimeSerializerBase#createContextual` creates a new serializer with `StdDateFormat.DATE_FORMAT_STR_ISO8601` format instead of re-using the actual format that may have been specified on the configuration. See the following code:\r\n\r\n```\r\nfinal String pattern = format.hasPattern()\r\n                                    ? format.getPattern()\r\n                                    : StdDateFormat.DATE_FORMAT_STR_ISO8601;\r\n```\r\n\r\nUsing the `@JsonFormat` annotation on a field will therefore reset the format to Jackson's default even if the annotation doesn't specify any custom format.\r\n\r\n\r\n`DateBasedDeserializer#createContextual` behaves differently and tries to re-use the configured format:\r\n\r\n```\r\nDateFormat df = ctxt.getConfig().getDateFormat();\r\n// one shortcut: with our custom format, can simplify handling a bit\r\nif (df.getClass() == StdDateFormat.class) {\r\n   ...\r\n   StdDateFormat std = (StdDateFormat) df;\r\n   std = std.withTimeZone(tz);\r\n   ...\r\n} else {\r\n  // otherwise need to clone, re-set timezone:\r\n  df = (DateFormat) df.clone();\r\n  df.setTimeZone(tz);\r\n}\r\n```\r\n\r\nShouldn't the serializer follow the same approach ?\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "That sounds like a bug.\r\n\r\nWould it be possible to create a stand-alone unit test to reproduce the problem? That would help verify the fix, and guard against future regression.\r\n"
            },
            {
                "content": "Yes, this definitely looks like a problem to me; code should not be by-passing contextual default."
            },
            {
                "content": "@cowtowncoder Maybe I should open another issue, but here is an interesting scenario...\r\nImagine I register a custom *serialiser* for `java.util.Date` (by means of a module for instance) instead of changing the date format itself on the mapper.\r\n\r\n```\r\nmapper.registerModule(\r\n\tnew SimpleModule().addSerializer(\r\n\t\tnew DateSerializer(\r\n\t\t\tBoolean.FALSE, \r\n\t\t\tnew SimpleDateFormat(\"yyyy/MM/dd'T'HH:mm:ss.SSSXXX\"))));\r\n```\r\n\r\nNow suppose I annotate my pojo as follows:\r\n```\r\npublic class Pojo {\r\n   @JsonFormat(timezone=\"GMT+4\")\r\n   private Date date;\r\n...\r\n```\r\n\r\nThe annotation doesn't change the format but only the timezone - so I expect my custom format will be used. It won't be the case unfortunately: the handling of the annotation is such that it reverts to the format specified on the mapper itself and will bypass my custom module entirely.\r\n\r\nSee what I mean ?"
            },
            {
                "content": "Hmmh. To be honest, I did not design `DateSerializer` to be extensible/sub-classable, so its functioning (if any) is serendipitous... :-)\r\n\r\nNow, I am not against trying to make it work (I don't see usage necessarily as wrong), but the better way, in my opinion, is to try to use \"config overrides\" functionality, added in 2.8, as it allows things like:\r\n\r\n```java\r\n        mapper.configOverride(Date.class)\r\n            .setFormat(JsonFormat.Value.forPattern(\"yyyy.dd.MM\"));\r\n```\r\n\r\nand is designed to work with default handlers.\r\n\r\nSo... I don't mind your filing a separate issue for using `DateSerializer` if you wish.\r\nBut I would recommend using config overrides approach for actual usage, as it is better supported and likely more robust approach.\r\n\r\n"
            },
            {
                "content": "I'll try the latter approach.\r\nIn short, you tell me a Module isn't necessary the best option to override the behaviour of types handled natively by Jackson - which is basically the case of my example.\r\n\r\nHow is you approach different from `objectMapper.setDateFormat(new SimpleDateFormat(...))` ?\r\n"
            },
            {
                "content": "@brenuart Well, use of custom (de)serializers should typically be the last resort, if other approaches are not available or don't work. Config Overrides were added to support equivalent of couple of annotations (specifically `@JsonInclude`, `@JsonFormat`), but through configuration. Since Jackson itself controls application (although custom (de)serializers can easily access these too), it can take care to make things work more reliably together, that's all.\r\nUse of `Module` may still make sense, although there's no real benefit wrt configuring date format: it may make sense as packaging unit for projects.\r\n\r\nNow: as to `DateFormat`... that will work ok with `Date` case, and it may well be what you want to use.\r\nIt is bit more limited just because it won't work with Joda Date or Java 8 date/time types: problem being that JDK's old `DateFormat` is ... not a very good API, and can not really be reused (due to multiple reasons) by Joda/Java 8. But since Jackson's Date support started before explicit support for either of those, I chose to just date `DateFormat`.\r\nIn retrospect, it might have made sense to instead just take String pattern -- and this is what `@JsonFormat` and config overrides do. This makes it more feasible to use general/centralized configuration.\r\n\r\nSo, long story short: if setting `DateFormat` works, nothing wrong with it.\r\n"
            },
            {
                "content": "@cowtowncoder Thanks for the very detailed info.\r\n\r\nFor your background, we had to adapt the serialization/deserialization of `java.util.Date` slightly for the following reasons:\r\n- make sure dates have their timezone offset serialized with a column (`:`)  - i.e. the format `yyyy-MM-dd'T'HH:mm:ss.SSSXXX` - which is not the case by default. The primary reason is to guarantee interoperability with applications making use of Java8 Zoned/OffsetDateTime (and also other platforms like iOS)\r\n- Java8 is likely to send Time info with nano second precision. Joda can handle them - it drops the extra digits and keep only the millis. We wanted same behaviour for java.util.Date as well.\r\n- provide a fix for parsing the 'zulu' dates\r\n\r\nMaybe you'll find some of the reasons interesting enough ;-)\r\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to use the config overrides functionality added in Jackson 2.8, which allows for the annotation @JsonFormat(timezone=\"GMT+4\") to be used and for the format of the date to be specified through configuration. Additionally, the use of custom (de)serializers should be the last resort, as config overrides are more reliable and better supported."
    },
    "JacksonDatabind_1_src/main/java/com/fasterxml/jackson/databind/ser/BeanPropertyWriter.java_582_624": {
        "src": "public void serializeAsColumn(Object bean, JsonGenerator jgen, SerializerProvider prov)\n        throws Exception\n    {\n        Object value = get(bean);\n        if (value == null) { // nulls need specialized handling\n            if (_nullSerializer != null) {\n                _nullSerializer.serialize(null, jgen, prov);\n            } else { // can NOT suppress entries in tabular output\n                jgen.writeNull();\n            }\n        }\n        // otherwise find serializer to use\n        JsonSerializer<Object> ser = _serializer;\n        if (ser == null) {\n            Class<?> cls = value.getClass();\n            PropertySerializerMap map = _dynamicSerializers;\n            ser = map.serializerFor(cls);\n            if (ser == null) {\n                ser = _findAndAddDynamic(map, cls, prov);\n            }\n        }\n        // and then see if we must suppress certain values (default, empty)\n        if (_suppressableValue != null) {\n            if (MARKER_FOR_EMPTY == _suppressableValue) {\n                if (ser.isEmpty(value)) { // can NOT suppress entries in tabular output\n                    serializeAsPlaceholder(bean, jgen, prov);\n                    return;\n                }\n            } else if (_suppressableValue.equals(value)) { // can NOT suppress entries in tabular output\n                serializeAsPlaceholder(bean, jgen, prov);\n                return;\n            }\n        }\n        // For non-nulls: simple check for direct cycles\n        if (value == bean) {\n            _handleSelfReference(bean, ser);\n        }\n        if (_typeSerializer == null) {\n            ser.serialize(value, jgen, prov);\n        } else {\n            ser.serializeWithType(value, jgen, prov, _typeSerializer);\n        }\n    }",
        "src_wo_comments": "public void serializeAsColumn ( Object bean , JsonGenerator jgen , SerializerProvider prov ) throws Exception { Object value = get ( bean ) ; if ( value == null ) { if ( _nullSerializer != null ) { _nullSerializer . serialize ( null , jgen , prov ) ; } else { jgen . writeNull ( ) ; } } JsonSerializer < Object > ser = _serializer ; if ( ser == null ) { Class < ? > cls = value . getClass ( ) ; PropertySerializerMap map = _dynamicSerializers ; ser = map . serializerFor ( cls ) ; if ( ser == null ) { ser = _findAndAddDynamic ( map , cls , prov ) ; } } if ( _suppressableValue != null ) { if ( MARKER_FOR_EMPTY == _suppressableValue ) { if ( ser . isEmpty ( value ) ) { serializeAsPlaceholder ( bean , jgen , prov ) ; return ; } } else if ( _suppressableValue . equals ( value ) ) { serializeAsPlaceholder ( bean , jgen , prov ) ; return ; } } if ( value == bean ) { _handleSelfReference ( bean , ser ) ; } if ( _typeSerializer == null ) { ser . serialize ( value , jgen , prov ) ; } else { ser . serializeWithType ( value , jgen , prov , _typeSerializer ) ; } }",
        "fixed_src": "public void serializeAsColumn(Object bean, JsonGenerator jgen, SerializerProvider prov)\n        throws Exception\n    {\n        Object value = get(bean);\n        if (value == null) { // nulls need specialized handling\n            if (_nullSerializer != null) {\n                _nullSerializer.serialize(null, jgen, prov);\n            } else { // can NOT suppress entries in tabular output\n                jgen.writeNull();\n            }\n            return;\n        }\n        // otherwise find serializer to use\n        JsonSerializer<Object> ser = _serializer;\n        if (ser == null) {\n            Class<?> cls = value.getClass();\n            PropertySerializerMap map = _dynamicSerializers;\n            ser = map.serializerFor(cls);\n            if (ser == null) {\n                ser = _findAndAddDynamic(map, cls, prov);\n            }\n        }\n        // and then see if we must suppress certain values (default, empty)\n        if (_suppressableValue != null) {\n            if (MARKER_FOR_EMPTY == _suppressableValue) {\n                if (ser.isEmpty(value)) { // can NOT suppress entries in tabular output\n                    serializeAsPlaceholder(bean, jgen, prov);\n                    return;\n                }\n            } else if (_suppressableValue.equals(value)) { // can NOT suppress entries in tabular output\n                serializeAsPlaceholder(bean, jgen, prov);\n                return;\n            }\n        }\n        // For non-nulls: simple check for direct cycles\n        if (value == bean) {\n            _handleSelfReference(bean, ser);\n        }\n        if (_typeSerializer == null) {\n            ser.serialize(value, jgen, prov);\n        } else {\n            ser.serializeWithType(value, jgen, prov, _typeSerializer);\n        }\n    }",
        "fixed_src_wo_comments": "public void serializeAsColumn ( Object bean , JsonGenerator jgen , SerializerProvider prov ) throws Exception { Object value = get ( bean ) ; if ( value == null ) { if ( _nullSerializer != null ) { _nullSerializer . serialize ( null , jgen , prov ) ; } else { jgen . writeNull ( ) ; } return ; } JsonSerializer < Object > ser = _serializer ; if ( ser == null ) { Class < ? > cls = value . getClass ( ) ; PropertySerializerMap map = _dynamicSerializers ; ser = map . serializerFor ( cls ) ; if ( ser == null ) { ser = _findAndAddDynamic ( map , cls , prov ) ; } } if ( _suppressableValue != null ) { if ( MARKER_FOR_EMPTY == _suppressableValue ) { if ( ser . isEmpty ( value ) ) { serializeAsPlaceholder ( bean , jgen , prov ) ; return ; } } else if ( _suppressableValue . equals ( value ) ) { serializeAsPlaceholder ( bean , jgen , prov ) ; return ; } } if ( value == bean ) { _handleSelfReference ( bean , ser ) ; } if ( _typeSerializer == null ) { ser . serialize ( value , jgen , prov ) ; } else { ser . serializeWithType ( value , jgen , prov , _typeSerializer ) ; } }",
        "summary": "NULL values are duplicated when serializing as array [via @JsonFormat(shape = JsonFormat.Shape.ARRAY)]",
        "Description": "Example:\n\n``` java\npublic class TestOuter {\n\n    @JsonFormat(shape = JsonFormat.Shape.ARRAY)\n    public ArrayList<TestInner> array;\n\n    public TestOuter() {\n        this.array = new ArrayList<TestInner>();\n        this.array.add(new TestInner(1, \"one\"));\n        this.array.add(new TestInner(0, null));\n    }\n\n    private class TestInner {\n        public int i;\n        public String mayBeNull;\n\n        public TestInner(int i, String s) {\n            this.i = i;\n            this.mayBeNull = s;\n        }\n    }\n}\n```\n\nSerializing an instance of TestOuter will produce the following incorrect result (as of Jackson 2.2.1):\n\n``` json\n\"array\": [[1, \"one\"], [0, null, null]]\n```\n\nwhere the null value is duplicated. The expected result would be:\n\n``` json\n\"array\": [[1, \"one\"], [0, null]]\n```\n\nI tracked the issue down to:\n\n``` java\npackage com.fasterxml.jackson.databind.ser;\n// ...\npublic class BeanPropertyWriter {\n// ...\n    public void serializeAsColumn(Object bean, JsonGenerator jgen, SerializerProvider prov)\n        throws Exception\n    {\n        Object value = get(bean);\n        if (value == null) { // nulls need specialized handling\n            if (_nullSerializer != null) {\n                _nullSerializer.serialize(null, jgen, prov);\n            } else { // can NOT suppress entries in tabular output\n                jgen.writeNull();\n            }\n        }\n        // otherwise find serializer to use\n        JsonSerializer<Object> ser = _serializer;\n    // ... ...\n```\n\nwhere I suspect there is a missing \"return\", to exit the function once handling of the null value in the dedicated branch is done.\nAs it is now, a null value is first serialized in the dedicated branch (jgen.writeNull()), and then execution continues on the \"normal\" (non-null) path and eventually the value is serialized once again.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Good catch. Thank you for doing the detective work -- this does indeed sound like the root cause.\nWill fix it for 2.2.2.\n\nI am also very interested in getting more feedback for serialize-as-array: it is relatively new feature, and not as well tested as more mature ones are (although I thought there was a test for checking this part... apparently not a good test).\nBut I think there is potential benefits for size reduction, performance improvements.\nI mean, when it works fully\n"
            },
            {
                "content": "Thank you for the very quick reply and fix. I'll be watching http://wiki.fasterxml.com/JacksonRelease222 for the release date.\n\nYes, the serialize-as-array feature is <b>very</b> useful for reducing size when dealing with hundreds or thousands of records of data. And having the serialization \"as array\" done in Jackson (e.g. in conjunction with @ResponseBody in Spring MVC) allows the M(odel) part on the server side to be clear and easy to maintain (no need to resort to actual array of arrays with generic values, just keep a hierarchy of objects with typed properties).\n\nActually, I'll use the opportunity :smile: to request an improvement for the serialize-as-array feature. The way we are using this feature is in conjunction with a \"meta-data\" array holding the names of the columns in the \"data\" array. Like:\n\n``` json\n\"array\": {\n    \"cols\": [\"i\", \"mayBeNull\"],\n    \"data\": [[1, \"one\"], [0, null]]\n}\n```\n\nThis allows the JSON response to be self-contained, and the client side code to be more robust.\nCurrently, to ensure consistency in the generated JSON output, we would use @JsonPropertyOrder on the InnerTest class, in conjunction with just a hard-coded matching \"cols\" array. \n\nSo from this standpoint ideally the serialize-as-array feature would incorporate the meta-data array, allowing to specify it in the @JsonFormat(shape = JsonFormat.Shape.ARRAY) annotation perhaps as an optional meta-data array name. Well actually there would be two additional optional properties for shape = ARRAY, one for the meta-data array, and one for the data array... Like:\n\n``` java\n@JsonFormat(shape = JsonFormat.Shape.ARRAY, metaDataArray = \"cols\", dataArray = \"data\")\npublic ArrayList<TestInner> array;\n```\n\nwhich would generate the JSON:\n\n``` json\n\"array\": {\n    \"cols\": [\"i\", \"mayBeNull\"],\n    \"data\": [[1, \"one\"], [0, null]]\n}\n```\n\nThanks for listening and thanks again for the quick fix! Looking forward to release 2.2.2.\n"
            },
            {
                "content": "You are welcome wrt fix. Glad to know feature has proven useful so far.\n\nColumn names as metadata is something people have been doing for a while anyway.\nSupporting that would require quite a bit of work; little bit for annotation (since JsonFormat is a generic annotation,\nthis would need more thought since it should not be something only applicable to POJO/array case, but bit more abstraction),  but mostly with serializers/deserializers.\nAnd since it would need to coordinate handling of all kinds of JSON list structures (arrays, Collections) with POJOs, it'd be a big undertaking; Jackson is designed to maximized modularity and (de)serializers only interact via small set of interfaces (wrt. including type & identity information and such cross-cutting concerns).\nBut then again, it's easier to see simplifications when actually working on implementing something.\n\nCould you file this as a separate request however? This way I can mark particular bug as fixed, and new entry would track this improvement idea.\n"
            },
            {
                "content": "Thanks for the reply. I suspected it would not be trivial to support such an enhancement (I suspect in fact that starting to actually work on implementing something like this would uncover more complications, rather than simplifications :smile:). Also, this extended serialization format is easily obtainable using existing functionality, so it would really just be a \"nice-to-have\".\n\nI filed this enhancement request separately as item https://github.com/FasterXML/jackson-databind/issues/229\nThanks!\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug discussed was related to the serialize-as-array feature of Jackson, and the solution was to fix it for the 2.2.2 release. The user also requested an improvement for the serialize-as-array feature, which would incorporate a meta-data array, allowing users to specify it in the @JsonFormat(shape = JsonFormat.Shape.ARRAY) annotation. The user filed a separate enhancement request for this, which is tracked at https://github.com/FasterXML/jackson-databind/issues/229."
    },
    "Math_74_src/main/java/org/apache/commons/math/ode/nonstiff/EmbeddedRungeKuttaIntegrator.java_191_359": {
        "src": "@Override\n  public double integrate(final FirstOrderDifferentialEquations equations,\n                          final double t0, final double[] y0,\n                          final double t, final double[] y)\n  throws DerivativeException, IntegratorException {\n\n    sanityChecks(equations, t0, y0, t, y);\n    setEquations(equations);\n    resetEvaluations();\n    final boolean forward = t > t0;\n\n    // create some internal working arrays\n    final int stages = c.length + 1;\n    if (y != y0) {\n      System.arraycopy(y0, 0, y, 0, y0.length);\n    }\n    final double[][] yDotK = new double[stages][y0.length];\n    final double[] yTmp = new double[y0.length];\n\n    // set up an interpolator sharing the integrator arrays\n    AbstractStepInterpolator interpolator;\n    if (requiresDenseOutput() || (! eventsHandlersManager.isEmpty())) {\n      final RungeKuttaStepInterpolator rki = (RungeKuttaStepInterpolator) prototype.copy();\n      rki.reinitialize(this, yTmp, yDotK, forward);\n      interpolator = rki;\n    } else {\n      interpolator = new DummyStepInterpolator(yTmp, forward);\n    }\n    interpolator.storeTime(t0);\n\n    // set up integration control objects\n    stepStart         = t0;\n    double  hNew      = 0;\n    boolean firstTime = true;\n    for (StepHandler handler : stepHandlers) {\n        handler.reset();\n    }\n    CombinedEventsManager manager = addEndTimeChecker(t0, t, eventsHandlersManager);\n    boolean lastStep = false;\n\n    // main integration loop\n    while (!lastStep) {\n\n      interpolator.shift();\n\n      double error = 0;\n      for (boolean loop = true; loop;) {\n\n        if (firstTime || !fsal) {\n          // first stage\n          computeDerivatives(stepStart, y, yDotK[0]);\n        }\n\n        if (firstTime) {\n          final double[] scale;\n          if (vecAbsoluteTolerance == null) {\n              scale = new double[y0.length];\n              java.util.Arrays.fill(scale, scalAbsoluteTolerance);\n            } else {\n              scale = vecAbsoluteTolerance;\n            }\n          hNew = initializeStep(equations, forward, getOrder(), scale,\n                                stepStart, y, yDotK[0], yTmp, yDotK[1]);\n          firstTime = false;\n        }\n\n        stepSize = hNew;\n\n        // next stages\n        for (int k = 1; k < stages; ++k) {\n\n          for (int j = 0; j < y0.length; ++j) {\n            double sum = a[k-1][0] * yDotK[0][j];\n            for (int l = 1; l < k; ++l) {\n              sum += a[k-1][l] * yDotK[l][j];\n            }\n            yTmp[j] = y[j] + stepSize * sum;\n          }\n\n          computeDerivatives(stepStart + c[k-1] * stepSize, yTmp, yDotK[k]);\n\n        }\n\n        // estimate the state at the end of the step\n        for (int j = 0; j < y0.length; ++j) {\n          double sum    = b[0] * yDotK[0][j];\n          for (int l = 1; l < stages; ++l) {\n            sum    += b[l] * yDotK[l][j];\n          }\n          yTmp[j] = y[j] + stepSize * sum;\n        }\n\n        // estimate the error at the end of the step\n        error = estimateError(yDotK, y, yTmp, stepSize);\n        if (error <= 1.0) {\n\n          // discrete events handling\n          interpolator.storeTime(stepStart + stepSize);\n          if (manager.evaluateStep(interpolator)) {\n              final double dt = manager.getEventTime() - stepStart;\n              if (Math.abs(dt) <= Math.ulp(stepStart)) {\n                  // rejecting the step would lead to a too small next step, we accept it\n                  loop = false;\n              } else {\n                  // reject the step to match exactly the next switch time\n                  hNew = dt;\n              }\n          } else {\n            // accept the step\n            loop = false;\n          }\n\n        } else {\n          // reject the step and attempt to reduce error by stepsize control\n          final double factor =\n              Math.min(maxGrowth,\n                       Math.max(minReduction, safety * Math.pow(error, exp)));\n          hNew = filterStep(stepSize * factor, forward, false);\n        }\n\n      }\n\n      // the step has been accepted\n      final double nextStep = stepStart + stepSize;\n      System.arraycopy(yTmp, 0, y, 0, y0.length);\n      manager.stepAccepted(nextStep, y);\n      lastStep = manager.stop();\n\n      // provide the step data to the step handler\n      interpolator.storeTime(nextStep);\n      for (StepHandler handler : stepHandlers) {\n          handler.handleStep(interpolator, lastStep);\n      }\n      stepStart = nextStep;\n\n      if (fsal) {\n        // save the last evaluation for the next step\n        System.arraycopy(yDotK[stages - 1], 0, yDotK[0], 0, y0.length);\n      }\n\n      if (manager.reset(stepStart, y) && ! lastStep) {\n        // some event handler has triggered changes that\n        // invalidate the derivatives, we need to recompute them\n        computeDerivatives(stepStart, y, yDotK[0]);\n      }\n\n      if (! lastStep) {\n        // in some rare cases we may get here with stepSize = 0, for example\n        // when an event occurs at integration start, reducing the first step\n        // to zero; we have to reset the step to some safe non zero value\n          stepSize = filterStep(stepSize, forward, true);\n\n        // stepsize control for next step\n        final double factor = Math.min(maxGrowth,\n                                       Math.max(minReduction,\n                                                safety * Math.pow(error, exp)));\n        final double  scaledH    = stepSize * factor;\n        final double  nextT      = stepStart + scaledH;\n        final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t);\n        hNew = filterStep(scaledH, forward, nextIsLast);\n      }\n\n    }\n\n    final double stopTime = stepStart;\n    resetInternalState();\n    return stopTime;\n\n  }",
        "src_wo_comments": "@ Override public double integrate ( final FirstOrderDifferentialEquations equations , final double t0 , final double [ ] y0 , final double t , final double [ ] y ) throws DerivativeException , IntegratorException { sanityChecks ( equations , t0 , y0 , t , y ) ; setEquations ( equations ) ; resetEvaluations ( ) ; final boolean forward = t > t0 ; final int stages = c . length + 1 ; if ( y != y0 ) { System . arraycopy ( y0 , 0 , y , 0 , y0 . length ) ; } final double [ ] [ ] yDotK = new double [ stages ] [ y0 . length ] ; final double [ ] yTmp = new double [ y0 . length ] ; AbstractStepInterpolator interpolator ; if ( requiresDenseOutput ( ) || ( ! eventsHandlersManager . isEmpty ( ) ) ) { final RungeKuttaStepInterpolator rki = ( RungeKuttaStepInterpolator ) prototype . copy ( ) ; rki . reinitialize ( this , yTmp , yDotK , forward ) ; interpolator = rki ; } else { interpolator = new DummyStepInterpolator ( yTmp , forward ) ; } interpolator . storeTime ( t0 ) ; stepStart = t0 ; double hNew = 0 ; boolean firstTime = true ; for ( StepHandler handler : stepHandlers ) { handler . reset ( ) ; } CombinedEventsManager manager = addEndTimeChecker ( t0 , t , eventsHandlersManager ) ; boolean lastStep = false ; while ( ! lastStep ) { interpolator . shift ( ) ; double error = 0 ; for ( boolean loop = true ; loop ; ) { if ( firstTime || ! fsal ) { computeDerivatives ( stepStart , y , yDotK [ 0 ] ) ; } if ( firstTime ) { final double [ ] scale ; if ( vecAbsoluteTolerance == null ) { scale = new double [ y0 . length ] ; java . util . Arrays . fill ( scale , scalAbsoluteTolerance ) ; } else { scale = vecAbsoluteTolerance ; } hNew = initializeStep ( equations , forward , getOrder ( ) , scale , stepStart , y , yDotK [ 0 ] , yTmp , yDotK [ 1 ] ) ; firstTime = false ; } stepSize = hNew ; for ( int k = 1 ; k < stages ; ++ k ) { for ( int j = 0 ; j < y0 . length ; ++ j ) { double sum = a [ k - 1 ] [ 0 ] * yDotK [ 0 ] [ j ] ; for ( int l = 1 ; l < k ; ++ l ) { sum += a [ k - 1 ] [ l ] * yDotK [ l ] [ j ] ; } yTmp [ j ] = y [ j ] + stepSize * sum ; } computeDerivatives ( stepStart + c [ k - 1 ] * stepSize , yTmp , yDotK [ k ] ) ; } for ( int j = 0 ; j < y0 . length ; ++ j ) { double sum = b [ 0 ] * yDotK [ 0 ] [ j ] ; for ( int l = 1 ; l < stages ; ++ l ) { sum += b [ l ] * yDotK [ l ] [ j ] ; } yTmp [ j ] = y [ j ] + stepSize * sum ; } error = estimateError ( yDotK , y , yTmp , stepSize ) ; if ( error <= 1.0 ) { interpolator . storeTime ( stepStart + stepSize ) ; if ( manager . evaluateStep ( interpolator ) ) { final double dt = manager . getEventTime ( ) - stepStart ; if ( Math . abs ( dt ) <= Math . ulp ( stepStart ) ) { loop = false ; } else { hNew = dt ; } } else { loop = false ; } } else { final double factor = Math . min ( maxGrowth , Math . max ( minReduction , safety * Math . pow ( error , exp ) ) ) ; hNew = filterStep ( stepSize * factor , forward , false ) ; } } final double nextStep = stepStart + stepSize ; System . arraycopy ( yTmp , 0 , y , 0 , y0 . length ) ; manager . stepAccepted ( nextStep , y ) ; lastStep = manager . stop ( ) ; interpolator . storeTime ( nextStep ) ; for ( StepHandler handler : stepHandlers ) { handler . handleStep ( interpolator , lastStep ) ; } stepStart = nextStep ; if ( fsal ) { System . arraycopy ( yDotK [ stages - 1 ] , 0 , yDotK [ 0 ] , 0 , y0 . length ) ; } if ( manager . reset ( stepStart , y ) && ! lastStep ) { computeDerivatives ( stepStart , y , yDotK [ 0 ] ) ; } if ( ! lastStep ) { stepSize = filterStep ( stepSize , forward , true ) ; final double factor = Math . min ( maxGrowth , Math . max ( minReduction , safety * Math . pow ( error , exp ) ) ) ; final double scaledH = stepSize * factor ; final double nextT = stepStart + scaledH ; final boolean nextIsLast = forward ? ( nextT >= t ) : ( nextT <= t ) ; hNew = filterStep ( scaledH , forward , nextIsLast ) ; } } final double stopTime = stepStart ; resetInternalState ( ) ; return stopTime ; }",
        "fixed_src": "@Override\n  public double integrate(final FirstOrderDifferentialEquations equations,\n                          final double t0, final double[] y0,\n                          final double t, final double[] y)\n  throws DerivativeException, IntegratorException {\n\n    sanityChecks(equations, t0, y0, t, y);\n    setEquations(equations);\n    resetEvaluations();\n    final boolean forward = t > t0;\n\n    // create some internal working arrays\n    final int stages = c.length + 1;\n    if (y != y0) {\n      System.arraycopy(y0, 0, y, 0, y0.length);\n    }\n    final double[][] yDotK = new double[stages][y0.length];\n    final double[] yTmp = new double[y0.length];\n\n    // set up an interpolator sharing the integrator arrays\n    AbstractStepInterpolator interpolator;\n    if (requiresDenseOutput() || (! eventsHandlersManager.isEmpty())) {\n      final RungeKuttaStepInterpolator rki = (RungeKuttaStepInterpolator) prototype.copy();\n      rki.reinitialize(this, yTmp, yDotK, forward);\n      interpolator = rki;\n    } else {\n      interpolator = new DummyStepInterpolator(yTmp, forward);\n    }\n    interpolator.storeTime(t0);\n\n    // set up integration control objects\n    stepStart         = t0;\n    double  hNew      = 0;\n    boolean firstTime = true;\n    for (StepHandler handler : stepHandlers) {\n        handler.reset();\n    }\n    CombinedEventsManager manager = addEndTimeChecker(t0, t, eventsHandlersManager);\n    boolean lastStep = false;\n\n    // main integration loop\n    while (!lastStep) {\n\n      interpolator.shift();\n\n      double error = 0;\n      for (boolean loop = true; loop;) {\n\n        if (firstTime || !fsal) {\n          // first stage\n          computeDerivatives(stepStart, y, yDotK[0]);\n        }\n\n        if (firstTime) {\n          final double[] scale = new double[y0.length];\n          if (vecAbsoluteTolerance == null) {\n              for (int i = 0; i < scale.length; ++i) {\n                scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * Math.abs(y[i]);\n              }\n            } else {\n              for (int i = 0; i < scale.length; ++i) {\n                scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * Math.abs(y[i]);\n              }\n            }\n          hNew = initializeStep(equations, forward, getOrder(), scale,\n                                stepStart, y, yDotK[0], yTmp, yDotK[1]);\n          firstTime = false;\n        }\n\n        stepSize = hNew;\n\n        // next stages\n        for (int k = 1; k < stages; ++k) {\n\n          for (int j = 0; j < y0.length; ++j) {\n            double sum = a[k-1][0] * yDotK[0][j];\n            for (int l = 1; l < k; ++l) {\n              sum += a[k-1][l] * yDotK[l][j];\n            }\n            yTmp[j] = y[j] + stepSize * sum;\n          }\n\n          computeDerivatives(stepStart + c[k-1] * stepSize, yTmp, yDotK[k]);\n\n        }\n\n        // estimate the state at the end of the step\n        for (int j = 0; j < y0.length; ++j) {\n          double sum    = b[0] * yDotK[0][j];\n          for (int l = 1; l < stages; ++l) {\n            sum    += b[l] * yDotK[l][j];\n          }\n          yTmp[j] = y[j] + stepSize * sum;\n        }\n\n        // estimate the error at the end of the step\n        error = estimateError(yDotK, y, yTmp, stepSize);\n        if (error <= 1.0) {\n\n          // discrete events handling\n          interpolator.storeTime(stepStart + stepSize);\n          if (manager.evaluateStep(interpolator)) {\n              final double dt = manager.getEventTime() - stepStart;\n              if (Math.abs(dt) <= Math.ulp(stepStart)) {\n                  // rejecting the step would lead to a too small next step, we accept it\n                  loop = false;\n              } else {\n                  // reject the step to match exactly the next switch time\n                  hNew = dt;\n              }\n          } else {\n            // accept the step\n            loop = false;\n          }\n\n        } else {\n          // reject the step and attempt to reduce error by stepsize control\n          final double factor =\n              Math.min(maxGrowth,\n                       Math.max(minReduction, safety * Math.pow(error, exp)));\n          hNew = filterStep(stepSize * factor, forward, false);\n        }\n\n      }\n\n      // the step has been accepted\n      final double nextStep = stepStart + stepSize;\n      System.arraycopy(yTmp, 0, y, 0, y0.length);\n      manager.stepAccepted(nextStep, y);\n      lastStep = manager.stop();\n\n      // provide the step data to the step handler\n      interpolator.storeTime(nextStep);\n      for (StepHandler handler : stepHandlers) {\n          handler.handleStep(interpolator, lastStep);\n      }\n      stepStart = nextStep;\n\n      if (fsal) {\n        // save the last evaluation for the next step\n        System.arraycopy(yDotK[stages - 1], 0, yDotK[0], 0, y0.length);\n      }\n\n      if (manager.reset(stepStart, y) && ! lastStep) {\n        // some event handler has triggered changes that\n        // invalidate the derivatives, we need to recompute them\n        computeDerivatives(stepStart, y, yDotK[0]);\n      }\n\n      if (! lastStep) {\n        // in some rare cases we may get here with stepSize = 0, for example\n        // when an event occurs at integration start, reducing the first step\n        // to zero; we have to reset the step to some safe non zero value\n          stepSize = filterStep(stepSize, forward, true);\n\n        // stepsize control for next step\n        final double factor = Math.min(maxGrowth,\n                                       Math.max(minReduction,\n                                                safety * Math.pow(error, exp)));\n        final double  scaledH    = stepSize * factor;\n        final double  nextT      = stepStart + scaledH;\n        final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t);\n        hNew = filterStep(scaledH, forward, nextIsLast);\n      }\n\n    }\n\n    final double stopTime = stepStart;\n    resetInternalState();\n    return stopTime;\n\n  }",
        "fixed_src_wo_comments": "@ Override public double integrate ( final FirstOrderDifferentialEquations equations , final double t0 , final double [ ] y0 , final double t , final double [ ] y ) throws DerivativeException , IntegratorException { sanityChecks ( equations , t0 , y0 , t , y ) ; setEquations ( equations ) ; resetEvaluations ( ) ; final boolean forward = t > t0 ; final int stages = c . length + 1 ; if ( y != y0 ) { System . arraycopy ( y0 , 0 , y , 0 , y0 . length ) ; } final double [ ] [ ] yDotK = new double [ stages ] [ y0 . length ] ; final double [ ] yTmp = new double [ y0 . length ] ; AbstractStepInterpolator interpolator ; if ( requiresDenseOutput ( ) || ( ! eventsHandlersManager . isEmpty ( ) ) ) { final RungeKuttaStepInterpolator rki = ( RungeKuttaStepInterpolator ) prototype . copy ( ) ; rki . reinitialize ( this , yTmp , yDotK , forward ) ; interpolator = rki ; } else { interpolator = new DummyStepInterpolator ( yTmp , forward ) ; } interpolator . storeTime ( t0 ) ; stepStart = t0 ; double hNew = 0 ; boolean firstTime = true ; for ( StepHandler handler : stepHandlers ) { handler . reset ( ) ; } CombinedEventsManager manager = addEndTimeChecker ( t0 , t , eventsHandlersManager ) ; boolean lastStep = false ; while ( ! lastStep ) { interpolator . shift ( ) ; double error = 0 ; for ( boolean loop = true ; loop ; ) { if ( firstTime || ! fsal ) { computeDerivatives ( stepStart , y , yDotK [ 0 ] ) ; } if ( firstTime ) { final double [ ] scale = new double [ y0 . length ] ; if ( vecAbsoluteTolerance == null ) { for ( int i = 0 ; i < scale . length ; ++ i ) { scale [ i ] = scalAbsoluteTolerance + scalRelativeTolerance * Math . abs ( y [ i ] ) ; } } else { for ( int i = 0 ; i < scale . length ; ++ i ) { scale [ i ] = vecAbsoluteTolerance [ i ] + vecRelativeTolerance [ i ] * Math . abs ( y [ i ] ) ; } } hNew = initializeStep ( equations , forward , getOrder ( ) , scale , stepStart , y , yDotK [ 0 ] , yTmp , yDotK [ 1 ] ) ; firstTime = false ; } stepSize = hNew ; for ( int k = 1 ; k < stages ; ++ k ) { for ( int j = 0 ; j < y0 . length ; ++ j ) { double sum = a [ k - 1 ] [ 0 ] * yDotK [ 0 ] [ j ] ; for ( int l = 1 ; l < k ; ++ l ) { sum += a [ k - 1 ] [ l ] * yDotK [ l ] [ j ] ; } yTmp [ j ] = y [ j ] + stepSize * sum ; } computeDerivatives ( stepStart + c [ k - 1 ] * stepSize , yTmp , yDotK [ k ] ) ; } for ( int j = 0 ; j < y0 . length ; ++ j ) { double sum = b [ 0 ] * yDotK [ 0 ] [ j ] ; for ( int l = 1 ; l < stages ; ++ l ) { sum += b [ l ] * yDotK [ l ] [ j ] ; } yTmp [ j ] = y [ j ] + stepSize * sum ; } error = estimateError ( yDotK , y , yTmp , stepSize ) ; if ( error <= 1.0 ) { interpolator . storeTime ( stepStart + stepSize ) ; if ( manager . evaluateStep ( interpolator ) ) { final double dt = manager . getEventTime ( ) - stepStart ; if ( Math . abs ( dt ) <= Math . ulp ( stepStart ) ) { loop = false ; } else { hNew = dt ; } } else { loop = false ; } } else { final double factor = Math . min ( maxGrowth , Math . max ( minReduction , safety * Math . pow ( error , exp ) ) ) ; hNew = filterStep ( stepSize * factor , forward , false ) ; } } final double nextStep = stepStart + stepSize ; System . arraycopy ( yTmp , 0 , y , 0 , y0 . length ) ; manager . stepAccepted ( nextStep , y ) ; lastStep = manager . stop ( ) ; interpolator . storeTime ( nextStep ) ; for ( StepHandler handler : stepHandlers ) { handler . handleStep ( interpolator , lastStep ) ; } stepStart = nextStep ; if ( fsal ) { System . arraycopy ( yDotK [ stages - 1 ] , 0 , yDotK [ 0 ] , 0 , y0 . length ) ; } if ( manager . reset ( stepStart , y ) && ! lastStep ) { computeDerivatives ( stepStart , y , yDotK [ 0 ] ) ; } if ( ! lastStep ) { stepSize = filterStep ( stepSize , forward , true ) ; final double factor = Math . min ( maxGrowth , Math . max ( minReduction , safety * Math . pow ( error , exp ) ) ) ; final double scaledH = stepSize * factor ; final double nextT = stepStart + scaledH ; final boolean nextIsLast = forward ? ( nextT >= t ) : ( nextT <= t ) ; hNew = filterStep ( scaledH , forward , nextIsLast ) ; } } final double stopTime = stepStart ; resetInternalState ( ) ; return stopTime ; }",
        "summary": "Wrong parameter for first step size guess for Embedded Runge Kutta methods",
        "Description": "In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator.\n\nHere, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...)\n\nThe problem comes from the array \"scale\" that is used as a parameter in the call off initializeStep(..)\n\nFollowing the theory described by Hairer in his book \"Solving Ordinary Differential Equations 1 : Nonstiff Problems\", the scaling should be :\n\nsci = Atol i + |y0i| * Rtoli\n\nWhereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli\n\nNote that the Gragg-Bulirsch-Stoer integrator uses the good implementation \"sci = Atol i + |y0i| * Rtoli  \" when he performs the call to the same method initializeStep(..)\n\nIn the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user.\nBut in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...)\n\n\nTo fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator\nFor exemple :\n\n final double[] scale= new double[y0.length];;\n          \n          if (vecAbsoluteTolerance == null) {\n              for (int i = 0; i < scale.length; ++i) {\n                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));\n                scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;\n              }\n            } else {\n              for (int i = 0; i < scale.length; ++i) {\n                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));\n                scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;\n              }\n            }\n          \n          hNew = initializeStep(equations, forward, getOrder(), scale,\n                           stepStart, y, yDotK[0], yTmp, yDotK[1]);\n\n\n\nSorry for the length of this message, looking forward to hearing from you soon\n\nVincent Morand\n\n\n\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-338",
        "comments": [
            "Fixed in subversion repository as of r904112.\nNote that I have changed slightly the fix you proposed: the call to Math.max was not needed because both arguments were the same (in GBS integrator, they are different).\nNote that instead of letting the integrator guess the first step by itself, you can provide it yourself by calling setInitialStepSize. This setting must be done before the call to integrate, which is called by Orekit propagate method if you happen to use Orekit for your application ;-) For such applications, an initial step of the order of magnitude of 1/100 of the keplerian period is a fair bet, it will be adjusted by the integrator if inconsistent with your accuracy settings.\n\nThanks for the report."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of r904112. The fix is slightly different than the one originally proposed, as the call to Math.max was not needed. To avoid having the integrator guess the first step, the user can call setInitialStepSize before the call to integrate. A good initial step size is 1/100 of the keplerian period."
    },
    "JacksonCore_6_src/main/java/com/fasterxml/jackson/core/JsonPointer.java_185_206": {
        "src": "private final static int _parseIndex(String str) {\n        final int len = str.length();\n        // [core#133]: beware of super long indexes; assume we never\n        // have arrays over 2 billion entries so ints are fine.\n        if (len == 0 || len > 10) {\n            return -1;\n        }\n        // [core#176]: no leading zeroes allowed\n        for (int i = 0; i < len; ++i) {\n            char c = str.charAt(i);\n            if (c > '9' || c < '0') {\n                return -1;\n            }\n        }\n        if (len == 10) {\n            long l = NumberInput.parseLong(str);\n            if (l > Integer.MAX_VALUE) {\n                return -1;\n            }\n        }\n        return NumberInput.parseInt(str);\n    }",
        "src_wo_comments": "private final static int _parseIndex ( String str ) { final int len = str . length ( ) ; if ( len == 0 || len > 10 ) { return - 1 ; } for ( int i = 0 ; i < len ; ++ i ) { char c = str . charAt ( i ) ; if ( c > '9' || c < '0' ) { return - 1 ; } } if ( len == 10 ) { long l = NumberInput . parseLong ( str ) ; if ( l > Integer . MAX_VALUE ) { return - 1 ; } } return NumberInput . parseInt ( str ) ; }",
        "fixed_src": "private final static int _parseIndex(String str) {\n        final int len = str.length();\n        // [core#133]: beware of super long indexes; assume we never\n        // have arrays over 2 billion entries so ints are fine.\n        if (len == 0 || len > 10) {\n            return -1;\n        }\n        // [core#176]: no leading zeroes allowed\n        char c = str.charAt(0);\n        if (c <= '0') {\n            return (len == 1 && c == '0') ? 0 : -1;\n        }\n        if (c > '9') {\n            return -1;\n        }\n        for (int i = 1; i < len; ++i) {\n            c = str.charAt(i);\n            if (c > '9' || c < '0') {\n                return -1;\n            }\n        }\n        if (len == 10) {\n            long l = NumberInput.parseLong(str);\n            if (l > Integer.MAX_VALUE) {\n                return -1;\n            }\n        }\n        return NumberInput.parseInt(str);\n    }",
        "fixed_src_wo_comments": "private final static int _parseIndex ( String str ) { final int len = str . length ( ) ; if ( len == 0 || len > 10 ) { return - 1 ; } char c = str . charAt ( 0 ) ; if ( c <= '0' ) { return ( len == 1 && c == '0' ) ? 0 : - 1 ; } if ( c > '9' ) { return - 1 ; } for ( int i = 1 ; i < len ; ++ i ) { c = str . charAt ( i ) ; if ( c > '9' || c < '0' ) { return - 1 ; } } if ( len == 10 ) { long l = NumberInput . parseLong ( str ) ; if ( l > Integer . MAX_VALUE ) { return - 1 ; } } return NumberInput . parseInt ( str ) ; }",
        "summary": "`JsonPointer` should not consider \"00\" to be valid index",
        "Description": "Although `00` can be parsed as `0` in some cases, it is not a valid JSON number; and is also not legal numeric index for JSON Pointer. As such, `JsonPointer` class should ensure it can only match property name \"00\" and not array index.\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug was caused by a missing semicolon in the code. The solution to this bug is to add the missing semicolon to the code."
    },
    "JacksonDatabind_46_src/main/java/com/fasterxml/jackson/databind/type/ReferenceType.java_150_158": {
        "src": "@Override\n    public StringBuilder getGenericSignature(StringBuilder sb)\n    {\n        _classSignature(_class, sb, false);\n        sb.append('<');\n        sb = _referencedType.getGenericSignature(sb);\n        sb.append(';');\n        return sb;\n    }",
        "src_wo_comments": "@ Override public StringBuilder getGenericSignature ( StringBuilder sb ) { _classSignature ( _class , sb , false ) ; sb . append ( '<' ) ; sb = _referencedType . getGenericSignature ( sb ) ; sb . append ( ';' ) ; return sb ; }",
        "fixed_src": "@Override\n    public StringBuilder getGenericSignature(StringBuilder sb)\n    {\n        _classSignature(_class, sb, false);\n        sb.append('<');\n        sb = _referencedType.getGenericSignature(sb);\n        sb.append(\">;\");\n        return sb;\n    }",
        "fixed_src_wo_comments": "@ Override public StringBuilder getGenericSignature ( StringBuilder sb ) { _classSignature ( _class , sb , false ) ; sb . append ( '<' ) ; sb = _referencedType . getGenericSignature ( sb ) ; sb . append ( \">;\" ) ; return sb ; }",
        "summary": "Incorrect signature for generic type via `JavaType.getGenericSignature",
        "Description": "(see https://github.com/FasterXML/jackson-modules-base/issues/8 for background)\n\nIt looks like generic signature generation is missing one closing `>` character to produce:\n\n```\n()Ljava/util/concurrent/atomic/AtomicReference<Ljava/lang/String;;\n```\n\ninstead of expected\n\n```\n()Ljava/util/concurrent/atomic/AtomicReference<Ljava/lang/String;>;\n```\n\nthat is, closing '>' is missing.\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug was caused by a missing semicolon at the end of a line of code. The solution is to add a semicolon at the end of the line of code."
    },
    "Math_23_src/main/java/org/apache/commons/math3/optimization/univariate/BrentOptimizer.java_114_281": {
        "src": "@Override\n    protected UnivariatePointValuePair doOptimize() {\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final double lo = getMin();\n        final double mid = getStartValue();\n        final double hi = getMax();\n\n        // Optional additional convergence criteria.\n        final ConvergenceChecker<UnivariatePointValuePair> checker\n            = getConvergenceChecker();\n\n        double a;\n        double b;\n        if (lo < hi) {\n            a = lo;\n            b = hi;\n        } else {\n            a = hi;\n            b = lo;\n        }\n\n        double x = mid;\n        double v = x;\n        double w = x;\n        double d = 0;\n        double e = 0;\n        double fx = computeObjectiveValue(x);\n        if (!isMinim) {\n            fx = -fx;\n        }\n        double fv = fx;\n        double fw = fx;\n\n        UnivariatePointValuePair previous = null;\n        UnivariatePointValuePair current\n            = new UnivariatePointValuePair(x, isMinim ? fx : -fx);\n        // Best point encountered so far (which is the initial guess).\n\n        int iter = 0;\n        while (true) {\n            final double m = 0.5 * (a + b);\n            final double tol1 = relativeThreshold * FastMath.abs(x) + absoluteThreshold;\n            final double tol2 = 2 * tol1;\n\n            // Default stopping criterion.\n            final boolean stop = FastMath.abs(x - m) <= tol2 - 0.5 * (b - a);\n            if (!stop) {\n                double p = 0;\n                double q = 0;\n                double r = 0;\n                double u = 0;\n\n                if (FastMath.abs(e) > tol1) { // Fit parabola.\n                    r = (x - w) * (fx - fv);\n                    q = (x - v) * (fx - fw);\n                    p = (x - v) * q - (x - w) * r;\n                    q = 2 * (q - r);\n\n                    if (q > 0) {\n                        p = -p;\n                    } else {\n                        q = -q;\n                    }\n\n                    r = e;\n                    e = d;\n\n                    if (p > q * (a - x) &&\n                        p < q * (b - x) &&\n                        FastMath.abs(p) < FastMath.abs(0.5 * q * r)) {\n                        // Parabolic interpolation step.\n                        d = p / q;\n                        u = x + d;\n\n                        // f must not be evaluated too close to a or b.\n                        if (u - a < tol2 || b - u < tol2) {\n                            if (x <= m) {\n                                d = tol1;\n                            } else {\n                                d = -tol1;\n                            }\n                        }\n                    } else {\n                        // Golden section step.\n                        if (x < m) {\n                            e = b - x;\n                        } else {\n                            e = a - x;\n                        }\n                        d = GOLDEN_SECTION * e;\n                    }\n                } else {\n                    // Golden section step.\n                    if (x < m) {\n                        e = b - x;\n                    } else {\n                        e = a - x;\n                    }\n                    d = GOLDEN_SECTION * e;\n                }\n\n                // Update by at least \"tol1\".\n                if (FastMath.abs(d) < tol1) {\n                    if (d >= 0) {\n                        u = x + tol1;\n                    } else {\n                        u = x - tol1;\n                    }\n                } else {\n                    u = x + d;\n                }\n\n                double fu = computeObjectiveValue(u);\n                if (!isMinim) {\n                    fu = -fu;\n                }\n\n                // User-defined convergence checker.\n                previous = current;\n                current = new UnivariatePointValuePair(u, isMinim ? fu : -fu);\n\n                if (checker != null) {\n                    if (checker.converged(iter, previous, current)) {\n                        return best(current, previous, isMinim);\n                    }\n                }\n\n                // Update a, b, v, w and x.\n                if (fu <= fx) {\n                    if (u < x) {\n                        b = x;\n                    } else {\n                        a = x;\n                    }\n                    v = w;\n                    fv = fw;\n                    w = x;\n                    fw = fx;\n                    x = u;\n                    fx = fu;\n                } else {\n                    if (u < x) {\n                        a = u;\n                    } else {\n                        b = u;\n                    }\n                    if (fu <= fw ||\n                        Precision.equals(w, x)) {\n                        v = w;\n                        fv = fw;\n                        w = u;\n                        fw = fu;\n                    } else if (fu <= fv ||\n                               Precision.equals(v, x) ||\n                               Precision.equals(v, w)) {\n                        v = u;\n                        fv = fu;\n                    }\n                }\n            } else { // Default termination (Brent's criterion).\n                return\n                            best(current,\n                                 previous,\n                            isMinim);\n            }\n            ++iter;\n        }\n    }",
        "src_wo_comments": "@ Override protected UnivariatePointValuePair doOptimize ( ) { final boolean isMinim = getGoalType ( ) == GoalType . MINIMIZE ; final double lo = getMin ( ) ; final double mid = getStartValue ( ) ; final double hi = getMax ( ) ; final ConvergenceChecker < UnivariatePointValuePair > checker = getConvergenceChecker ( ) ; double a ; double b ; if ( lo < hi ) { a = lo ; b = hi ; } else { a = hi ; b = lo ; } double x = mid ; double v = x ; double w = x ; double d = 0 ; double e = 0 ; double fx = computeObjectiveValue ( x ) ; if ( ! isMinim ) { fx = - fx ; } double fv = fx ; double fw = fx ; UnivariatePointValuePair previous = null ; UnivariatePointValuePair current = new UnivariatePointValuePair ( x , isMinim ? fx : - fx ) ; int iter = 0 ; while ( true ) { final double m = 0.5 * ( a + b ) ; final double tol1 = relativeThreshold * FastMath . abs ( x ) + absoluteThreshold ; final double tol2 = 2 * tol1 ; final boolean stop = FastMath . abs ( x - m ) <= tol2 - 0.5 * ( b - a ) ; if ( ! stop ) { double p = 0 ; double q = 0 ; double r = 0 ; double u = 0 ; if ( FastMath . abs ( e ) > tol1 ) { r = ( x - w ) * ( fx - fv ) ; q = ( x - v ) * ( fx - fw ) ; p = ( x - v ) * q - ( x - w ) * r ; q = 2 * ( q - r ) ; if ( q > 0 ) { p = - p ; } else { q = - q ; } r = e ; e = d ; if ( p > q * ( a - x ) && p < q * ( b - x ) && FastMath . abs ( p ) < FastMath . abs ( 0.5 * q * r ) ) { d = p / q ; u = x + d ; if ( u - a < tol2 || b - u < tol2 ) { if ( x <= m ) { d = tol1 ; } else { d = - tol1 ; } } } else { if ( x < m ) { e = b - x ; } else { e = a - x ; } d = GOLDEN_SECTION * e ; } } else { if ( x < m ) { e = b - x ; } else { e = a - x ; } d = GOLDEN_SECTION * e ; } if ( FastMath . abs ( d ) < tol1 ) { if ( d >= 0 ) { u = x + tol1 ; } else { u = x - tol1 ; } } else { u = x + d ; } double fu = computeObjectiveValue ( u ) ; if ( ! isMinim ) { fu = - fu ; } previous = current ; current = new UnivariatePointValuePair ( u , isMinim ? fu : - fu ) ; if ( checker != null ) { if ( checker . converged ( iter , previous , current ) ) { return best ( current , previous , isMinim ) ; } } if ( fu <= fx ) { if ( u < x ) { b = x ; } else { a = x ; } v = w ; fv = fw ; w = x ; fw = fx ; x = u ; fx = fu ; } else { if ( u < x ) { a = u ; } else { b = u ; } if ( fu <= fw || Precision . equals ( w , x ) ) { v = w ; fv = fw ; w = u ; fw = fu ; } else if ( fu <= fv || Precision . equals ( v , x ) || Precision . equals ( v , w ) ) { v = u ; fv = fu ; } } } else { return best ( current , previous , isMinim ) ; } ++ iter ; } }",
        "fixed_src": "@Override\n    protected UnivariatePointValuePair doOptimize() {\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final double lo = getMin();\n        final double mid = getStartValue();\n        final double hi = getMax();\n\n        // Optional additional convergence criteria.\n        final ConvergenceChecker<UnivariatePointValuePair> checker\n            = getConvergenceChecker();\n\n        double a;\n        double b;\n        if (lo < hi) {\n            a = lo;\n            b = hi;\n        } else {\n            a = hi;\n            b = lo;\n        }\n\n        double x = mid;\n        double v = x;\n        double w = x;\n        double d = 0;\n        double e = 0;\n        double fx = computeObjectiveValue(x);\n        if (!isMinim) {\n            fx = -fx;\n        }\n        double fv = fx;\n        double fw = fx;\n\n        UnivariatePointValuePair previous = null;\n        UnivariatePointValuePair current\n            = new UnivariatePointValuePair(x, isMinim ? fx : -fx);\n        // Best point encountered so far (which is the initial guess).\n        UnivariatePointValuePair best = current;\n\n        int iter = 0;\n        while (true) {\n            final double m = 0.5 * (a + b);\n            final double tol1 = relativeThreshold * FastMath.abs(x) + absoluteThreshold;\n            final double tol2 = 2 * tol1;\n\n            // Default stopping criterion.\n            final boolean stop = FastMath.abs(x - m) <= tol2 - 0.5 * (b - a);\n            if (!stop) {\n                double p = 0;\n                double q = 0;\n                double r = 0;\n                double u = 0;\n\n                if (FastMath.abs(e) > tol1) { // Fit parabola.\n                    r = (x - w) * (fx - fv);\n                    q = (x - v) * (fx - fw);\n                    p = (x - v) * q - (x - w) * r;\n                    q = 2 * (q - r);\n\n                    if (q > 0) {\n                        p = -p;\n                    } else {\n                        q = -q;\n                    }\n\n                    r = e;\n                    e = d;\n\n                    if (p > q * (a - x) &&\n                        p < q * (b - x) &&\n                        FastMath.abs(p) < FastMath.abs(0.5 * q * r)) {\n                        // Parabolic interpolation step.\n                        d = p / q;\n                        u = x + d;\n\n                        // f must not be evaluated too close to a or b.\n                        if (u - a < tol2 || b - u < tol2) {\n                            if (x <= m) {\n                                d = tol1;\n                            } else {\n                                d = -tol1;\n                            }\n                        }\n                    } else {\n                        // Golden section step.\n                        if (x < m) {\n                            e = b - x;\n                        } else {\n                            e = a - x;\n                        }\n                        d = GOLDEN_SECTION * e;\n                    }\n                } else {\n                    // Golden section step.\n                    if (x < m) {\n                        e = b - x;\n                    } else {\n                        e = a - x;\n                    }\n                    d = GOLDEN_SECTION * e;\n                }\n\n                // Update by at least \"tol1\".\n                if (FastMath.abs(d) < tol1) {\n                    if (d >= 0) {\n                        u = x + tol1;\n                    } else {\n                        u = x - tol1;\n                    }\n                } else {\n                    u = x + d;\n                }\n\n                double fu = computeObjectiveValue(u);\n                if (!isMinim) {\n                    fu = -fu;\n                }\n\n                // User-defined convergence checker.\n                previous = current;\n                current = new UnivariatePointValuePair(u, isMinim ? fu : -fu);\n                best = best(best,\n                            best(current,\n                                 previous,\n                                 isMinim),\n                            isMinim);\n\n                if (checker != null) {\n                    if (checker.converged(iter, previous, current)) {\n                        return best;\n                    }\n                }\n\n                // Update a, b, v, w and x.\n                if (fu <= fx) {\n                    if (u < x) {\n                        b = x;\n                    } else {\n                        a = x;\n                    }\n                    v = w;\n                    fv = fw;\n                    w = x;\n                    fw = fx;\n                    x = u;\n                    fx = fu;\n                } else {\n                    if (u < x) {\n                        a = u;\n                    } else {\n                        b = u;\n                    }\n                    if (fu <= fw ||\n                        Precision.equals(w, x)) {\n                        v = w;\n                        fv = fw;\n                        w = u;\n                        fw = fu;\n                    } else if (fu <= fv ||\n                               Precision.equals(v, x) ||\n                               Precision.equals(v, w)) {\n                        v = u;\n                        fv = fu;\n                    }\n                }\n            } else { // Default termination (Brent's criterion).\n                return best(best,\n                            best(current,\n                                 previous,\n                                 isMinim),\n                            isMinim);\n            }\n            ++iter;\n        }\n    }",
        "fixed_src_wo_comments": "@ Override protected UnivariatePointValuePair doOptimize ( ) { final boolean isMinim = getGoalType ( ) == GoalType . MINIMIZE ; final double lo = getMin ( ) ; final double mid = getStartValue ( ) ; final double hi = getMax ( ) ; final ConvergenceChecker < UnivariatePointValuePair > checker = getConvergenceChecker ( ) ; double a ; double b ; if ( lo < hi ) { a = lo ; b = hi ; } else { a = hi ; b = lo ; } double x = mid ; double v = x ; double w = x ; double d = 0 ; double e = 0 ; double fx = computeObjectiveValue ( x ) ; if ( ! isMinim ) { fx = - fx ; } double fv = fx ; double fw = fx ; UnivariatePointValuePair previous = null ; UnivariatePointValuePair current = new UnivariatePointValuePair ( x , isMinim ? fx : - fx ) ; UnivariatePointValuePair best = current ; int iter = 0 ; while ( true ) { final double m = 0.5 * ( a + b ) ; final double tol1 = relativeThreshold * FastMath . abs ( x ) + absoluteThreshold ; final double tol2 = 2 * tol1 ; final boolean stop = FastMath . abs ( x - m ) <= tol2 - 0.5 * ( b - a ) ; if ( ! stop ) { double p = 0 ; double q = 0 ; double r = 0 ; double u = 0 ; if ( FastMath . abs ( e ) > tol1 ) { r = ( x - w ) * ( fx - fv ) ; q = ( x - v ) * ( fx - fw ) ; p = ( x - v ) * q - ( x - w ) * r ; q = 2 * ( q - r ) ; if ( q > 0 ) { p = - p ; } else { q = - q ; } r = e ; e = d ; if ( p > q * ( a - x ) && p < q * ( b - x ) && FastMath . abs ( p ) < FastMath . abs ( 0.5 * q * r ) ) { d = p / q ; u = x + d ; if ( u - a < tol2 || b - u < tol2 ) { if ( x <= m ) { d = tol1 ; } else { d = - tol1 ; } } } else { if ( x < m ) { e = b - x ; } else { e = a - x ; } d = GOLDEN_SECTION * e ; } } else { if ( x < m ) { e = b - x ; } else { e = a - x ; } d = GOLDEN_SECTION * e ; } if ( FastMath . abs ( d ) < tol1 ) { if ( d >= 0 ) { u = x + tol1 ; } else { u = x - tol1 ; } } else { u = x + d ; } double fu = computeObjectiveValue ( u ) ; if ( ! isMinim ) { fu = - fu ; } previous = current ; current = new UnivariatePointValuePair ( u , isMinim ? fu : - fu ) ; best = best ( best , best ( current , previous , isMinim ) , isMinim ) ; if ( checker != null ) { if ( checker . converged ( iter , previous , current ) ) { return best ; } } if ( fu <= fx ) { if ( u < x ) { b = x ; } else { a = x ; } v = w ; fv = fw ; w = x ; fw = fx ; x = u ; fx = fu ; } else { if ( u < x ) { a = u ; } else { b = u ; } if ( fu <= fw || Precision . equals ( w , x ) ) { v = w ; fv = fw ; w = u ; fw = fu ; } else if ( fu <= fv || Precision . equals ( v , x ) || Precision . equals ( v , w ) ) { v = u ; fv = fu ; } } } else { return best ( best , best ( current , previous , isMinim ) , isMinim ) ; } ++ iter ; } }",
        "summary": "\"BrentOptimizer\" not always reporting the best point",
        "Description": "{{BrentOptimizer}} (package \"o.a.c.m.optimization.univariate\") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-855",
        "comments": [
            "Proposed patch.\n\nThe (somewhat contrived) unit test fails with the current version of the algorithm.\n",
            "OK to apply?\n",
            "Committed in revision 1381195.",
            "That was not it yet; further improvement committed in revision 1382070, together with a Javadoc update explaining some change wrt the original version of the algorithm.\n",
            "Yet another small change in revision 1382441.\n"
        ],
        "summarized_discussion": "\n\nThe bug in the source code was solved by applying a patch, which was committed in three different revisions (1381195, 1382070, and 1382441). The patch included a unit test, a further improvement, and a Javadoc update."
    },
    "JacksonDatabind_102_src/main/java/com/fasterxml/jackson/databind/ser/std/DateTimeSerializerBase.java_61_136": {
        "src": "@Override\n    public JsonSerializer<?> createContextual(SerializerProvider serializers,\n            BeanProperty property) throws JsonMappingException\n    {\n        // Note! Should not skip if `property` null since that'd skip check\n        // for config overrides, in case of root value\n        if (property == null) {\n            return this;\n        }\n        JsonFormat.Value format = findFormatOverrides(serializers, property, handledType());\n        if (format == null) {\n            return this;\n        }\n        // Simple case first: serialize as numeric timestamp?\n        JsonFormat.Shape shape = format.getShape();\n        if (shape.isNumeric()) {\n            return withFormat(Boolean.TRUE, null);\n        }\n\n        // 08-Jun-2017, tatu: With [databind#1648], this gets bit tricky..\n        // First: custom pattern will override things\n        if (format.hasPattern()) {\n            final Locale loc = format.hasLocale()\n                            ? format.getLocale()\n                            : serializers.getLocale();\n            SimpleDateFormat df = new SimpleDateFormat(format.getPattern(), loc);\n            TimeZone tz = format.hasTimeZone() ? format.getTimeZone()\n                    : serializers.getTimeZone();\n            df.setTimeZone(tz);\n            return withFormat(Boolean.FALSE, df);\n        }\n\n        // Otherwise, need one of these changes:\n        final boolean hasLocale = format.hasLocale();\n        final boolean hasTZ = format.hasTimeZone();\n        final boolean asString = (shape == JsonFormat.Shape.STRING);\n\n        if (!hasLocale && !hasTZ && !asString) {\n            return this;\n        }\n\n        DateFormat df0 = serializers.getConfig().getDateFormat();\n        // Jackson's own `StdDateFormat` is quite easy to deal with...\n        if (df0 instanceof StdDateFormat) {\n            StdDateFormat std = (StdDateFormat) df0;\n            if (format.hasLocale()) {\n                std = std.withLocale(format.getLocale());\n            }\n            if (format.hasTimeZone()) {\n                std = std.withTimeZone(format.getTimeZone());\n            }\n            return withFormat(Boolean.FALSE, std);\n        }\n\n        // 08-Jun-2017, tatu: Unfortunately there's no generally usable\n        //    mechanism for changing `DateFormat` instances (or even clone()ing)\n        //    So: require it be `SimpleDateFormat`; can't config other types\n        if (!(df0 instanceof SimpleDateFormat)) {\n            serializers.reportBadDefinition(handledType(), String.format(\n\"Configured `DateFormat` (%s) not a `SimpleDateFormat`; cannot configure `Locale` or `TimeZone`\",\ndf0.getClass().getName()));\n        }\n        SimpleDateFormat df = (SimpleDateFormat) df0;\n        if (hasLocale) {\n            // Ugh. No way to change `Locale`, create copy; must re-crete completely:\n            df = new SimpleDateFormat(df.toPattern(), format.getLocale());\n        } else {\n            df = (SimpleDateFormat) df.clone();\n        }\n        TimeZone newTz = format.getTimeZone();\n        boolean changeTZ = (newTz != null) && !newTz.equals(df.getTimeZone());\n        if (changeTZ) {\n            df.setTimeZone(newTz);\n        }\n        return withFormat(Boolean.FALSE, df);\n    }",
        "src_wo_comments": "@ Override public JsonSerializer < ? > createContextual ( SerializerProvider serializers , BeanProperty property ) throws JsonMappingException { if ( property == null ) { return this ; } JsonFormat . Value format = findFormatOverrides ( serializers , property , handledType ( ) ) ; if ( format == null ) { return this ; } JsonFormat . Shape shape = format . getShape ( ) ; if ( shape . isNumeric ( ) ) { return withFormat ( Boolean . TRUE , null ) ; } if ( format . hasPattern ( ) ) { final Locale loc = format . hasLocale ( ) ? format . getLocale ( ) : serializers . getLocale ( ) ; SimpleDateFormat df = new SimpleDateFormat ( format . getPattern ( ) , loc ) ; TimeZone tz = format . hasTimeZone ( ) ? format . getTimeZone ( ) : serializers . getTimeZone ( ) ; df . setTimeZone ( tz ) ; return withFormat ( Boolean . FALSE , df ) ; } final boolean hasLocale = format . hasLocale ( ) ; final boolean hasTZ = format . hasTimeZone ( ) ; final boolean asString = ( shape == JsonFormat . Shape . STRING ) ; if ( ! hasLocale && ! hasTZ && ! asString ) { return this ; } DateFormat df0 = serializers . getConfig ( ) . getDateFormat ( ) ; if ( df0 instanceof StdDateFormat ) { StdDateFormat std = ( StdDateFormat ) df0 ; if ( format . hasLocale ( ) ) { std = std . withLocale ( format . getLocale ( ) ) ; } if ( format . hasTimeZone ( ) ) { std = std . withTimeZone ( format . getTimeZone ( ) ) ; } return withFormat ( Boolean . FALSE , std ) ; } if ( ! ( df0 instanceof SimpleDateFormat ) ) { serializers . reportBadDefinition ( handledType ( ) , String . format ( \"Configured `DateFormat` (%s) not a `SimpleDateFormat`; cannot configure `Locale` or `TimeZone`\" , df0 . getClass ( ) . getName ( ) ) ) ; } SimpleDateFormat df = ( SimpleDateFormat ) df0 ; if ( hasLocale ) { df = new SimpleDateFormat ( df . toPattern ( ) , format . getLocale ( ) ) ; } else { df = ( SimpleDateFormat ) df . clone ( ) ; } TimeZone newTz = format . getTimeZone ( ) ; boolean changeTZ = ( newTz != null ) && ! newTz . equals ( df . getTimeZone ( ) ) ; if ( changeTZ ) { df . setTimeZone ( newTz ) ; } return withFormat ( Boolean . FALSE , df ) ; }",
        "fixed_src": "@Override\n    public JsonSerializer<?> createContextual(SerializerProvider serializers,\n            BeanProperty property) throws JsonMappingException\n    {\n        // Note! Should not skip if `property` null since that'd skip check\n        // for config overrides, in case of root value\n        JsonFormat.Value format = findFormatOverrides(serializers, property, handledType());\n        if (format == null) {\n            return this;\n        }\n        // Simple case first: serialize as numeric timestamp?\n        JsonFormat.Shape shape = format.getShape();\n        if (shape.isNumeric()) {\n            return withFormat(Boolean.TRUE, null);\n        }\n\n        // 08-Jun-2017, tatu: With [databind#1648], this gets bit tricky..\n        // First: custom pattern will override things\n        if (format.hasPattern()) {\n            final Locale loc = format.hasLocale()\n                            ? format.getLocale()\n                            : serializers.getLocale();\n            SimpleDateFormat df = new SimpleDateFormat(format.getPattern(), loc);\n            TimeZone tz = format.hasTimeZone() ? format.getTimeZone()\n                    : serializers.getTimeZone();\n            df.setTimeZone(tz);\n            return withFormat(Boolean.FALSE, df);\n        }\n\n        // Otherwise, need one of these changes:\n        final boolean hasLocale = format.hasLocale();\n        final boolean hasTZ = format.hasTimeZone();\n        final boolean asString = (shape == JsonFormat.Shape.STRING);\n\n        if (!hasLocale && !hasTZ && !asString) {\n            return this;\n        }\n\n        DateFormat df0 = serializers.getConfig().getDateFormat();\n        // Jackson's own `StdDateFormat` is quite easy to deal with...\n        if (df0 instanceof StdDateFormat) {\n            StdDateFormat std = (StdDateFormat) df0;\n            if (format.hasLocale()) {\n                std = std.withLocale(format.getLocale());\n            }\n            if (format.hasTimeZone()) {\n                std = std.withTimeZone(format.getTimeZone());\n            }\n            return withFormat(Boolean.FALSE, std);\n        }\n\n        // 08-Jun-2017, tatu: Unfortunately there's no generally usable\n        //    mechanism for changing `DateFormat` instances (or even clone()ing)\n        //    So: require it be `SimpleDateFormat`; can't config other types\n        if (!(df0 instanceof SimpleDateFormat)) {\n            serializers.reportBadDefinition(handledType(), String.format(\n\"Configured `DateFormat` (%s) not a `SimpleDateFormat`; cannot configure `Locale` or `TimeZone`\",\ndf0.getClass().getName()));\n        }\n        SimpleDateFormat df = (SimpleDateFormat) df0;\n        if (hasLocale) {\n            // Ugh. No way to change `Locale`, create copy; must re-crete completely:\n            df = new SimpleDateFormat(df.toPattern(), format.getLocale());\n        } else {\n            df = (SimpleDateFormat) df.clone();\n        }\n        TimeZone newTz = format.getTimeZone();\n        boolean changeTZ = (newTz != null) && !newTz.equals(df.getTimeZone());\n        if (changeTZ) {\n            df.setTimeZone(newTz);\n        }\n        return withFormat(Boolean.FALSE, df);\n    }",
        "fixed_src_wo_comments": "@ Override public JsonSerializer < ? > createContextual ( SerializerProvider serializers , BeanProperty property ) throws JsonMappingException { JsonFormat . Value format = findFormatOverrides ( serializers , property , handledType ( ) ) ; if ( format == null ) { return this ; } JsonFormat . Shape shape = format . getShape ( ) ; if ( shape . isNumeric ( ) ) { return withFormat ( Boolean . TRUE , null ) ; } if ( format . hasPattern ( ) ) { final Locale loc = format . hasLocale ( ) ? format . getLocale ( ) : serializers . getLocale ( ) ; SimpleDateFormat df = new SimpleDateFormat ( format . getPattern ( ) , loc ) ; TimeZone tz = format . hasTimeZone ( ) ? format . getTimeZone ( ) : serializers . getTimeZone ( ) ; df . setTimeZone ( tz ) ; return withFormat ( Boolean . FALSE , df ) ; } final boolean hasLocale = format . hasLocale ( ) ; final boolean hasTZ = format . hasTimeZone ( ) ; final boolean asString = ( shape == JsonFormat . Shape . STRING ) ; if ( ! hasLocale && ! hasTZ && ! asString ) { return this ; } DateFormat df0 = serializers . getConfig ( ) . getDateFormat ( ) ; if ( df0 instanceof StdDateFormat ) { StdDateFormat std = ( StdDateFormat ) df0 ; if ( format . hasLocale ( ) ) { std = std . withLocale ( format . getLocale ( ) ) ; } if ( format . hasTimeZone ( ) ) { std = std . withTimeZone ( format . getTimeZone ( ) ) ; } return withFormat ( Boolean . FALSE , std ) ; } if ( ! ( df0 instanceof SimpleDateFormat ) ) { serializers . reportBadDefinition ( handledType ( ) , String . format ( \"Configured `DateFormat` (%s) not a `SimpleDateFormat`; cannot configure `Locale` or `TimeZone`\" , df0 . getClass ( ) . getName ( ) ) ) ; } SimpleDateFormat df = ( SimpleDateFormat ) df0 ; if ( hasLocale ) { df = new SimpleDateFormat ( df . toPattern ( ) , format . getLocale ( ) ) ; } else { df = ( SimpleDateFormat ) df . clone ( ) ; } TimeZone newTz = format . getTimeZone ( ) ; boolean changeTZ = ( newTz != null ) && ! newTz . equals ( df . getTimeZone ( ) ) ; if ( changeTZ ) { df . setTimeZone ( newTz ) ; } return withFormat ( Boolean . FALSE , df ) ; }",
        "summary": "Cannot set custom format for `SqlDateSerializer` globally",
        "Description": "Version: 2.9.5\r\n\r\nAfter https://github.com/FasterXML/jackson-databind/issues/219 was fixed, the default format for `java.sql.Date` serialization switched from string to numeric, following the default value of `WRITE_DATES_AS_TIMESTAMPS`.\r\n\r\nIn order to prevent breaks, I want `java.sql.Date` to serialize as a string, without changing behavior for `java.util.Date` (which has always serialized as a number by default).\r\n\r\nAccording to https://github.com/FasterXML/jackson-databind/issues/219#issuecomment-370690333, I should be able to revert the behavior for `java.sql.Date` only with\r\n```\r\nfinal ObjectMapper mapper = new ObjectMapper();\r\nmapper.configOverride(java.sql.Date.class).setFormat(JsonFormat.Value.forPattern(\"yyyy-MM-dd\"));\r\n```\r\n\r\nThis doesn't seem to do anything, though. Looking at the code, it looks like it's because the custom format isn't actually added to `SqlDateSerializer` except in the `createContextual` method (https://github.com/FasterXML/jackson-databind/blob/master/src/main/java/com/fasterxml/jackson/databind/ser/std/DateTimeSerializerBase.java#L59).\r\n\r\nFor now, I've reverted this behavior with \r\n```\r\nmapper.registerModule(new SimpleModule() {\r\n            {\r\n                addSerializer(\r\n                        java.sql.Date.class,\r\n                        new SqlDateSerializer().withFormat(false, new SimpleDateFormat(\"yyyy-MM-dd\"))\r\n                );\r\n            }\r\n        });\r\n```\r\nbut it seems pretty hacky so I'd prefer the other method if possible. \r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Quick note: timing here is bit unfortunate, as I just released `2.9.6`. But of course, with fixes, better late than never. :)\r\nThank you for reporting the issue.\r\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to release a new version (2.9.6) with the necessary fixes. The release may be late, but it is better than not releasing a fix at all."
    },
    "JacksonDatabind_11_src/main/java/com/fasterxml/jackson/databind/type/TypeFactory.java_889_930": {
        "src": "protected JavaType _fromVariable(TypeVariable<?> type, TypeBindings context)\n    {\n        final String name = type.getName();\n        // 19-Mar-2015: Without context, all we can check are bounds.\n        if (context == null) {\n            // And to prevent infinite loops, now need this:\n            return _unknownType();\n        } else {\n            // Ok: here's where context might come in handy!\n            /* 19-Mar-2015, tatu: As per [databind#609], may need to allow\n             *   unresolved type variables to handle some cases where bounds\n             *   are enough. Let's hope it does not hide real fail cases.\n             */\n            JavaType actualType = context.findType(name);\n            if (actualType != null) {\n                return actualType;\n            }\n        }\n\n        /* 29-Jan-2010, tatu: We used to throw exception here, if type was\n         *   bound: but the problem is that this can occur for generic \"base\"\n         *   method, overridden by sub-class. If so, we will want to ignore\n         *   current type (for method) since it will be masked.\n         */\n        Type[] bounds = type.getBounds();\n\n        // With type variables we must use bound information.\n        // Theoretically this gets tricky, as there may be multiple\n        // bounds (\"... extends A & B\"); and optimally we might\n        // want to choose the best match. Also, bounds are optional;\n        // but here we are lucky in that implicit \"Object\" is\n        // added as bounds if so.\n        // Either way let's just use the first bound, for now, and\n        // worry about better match later on if there is need.\n\n        /* 29-Jan-2010, tatu: One more problem are recursive types\n         *   (T extends Comparable<T>). Need to add \"placeholder\"\n         *   for resolution to catch those.\n         */\n        context._addPlaceholder(name);\n        return _constructType(bounds[0], context);\n    }",
        "src_wo_comments": "protected JavaType _fromVariable ( TypeVariable < ? > type , TypeBindings context ) { final String name = type . getName ( ) ; if ( context == null ) { return _unknownType ( ) ; } else { JavaType actualType = context . findType ( name ) ; if ( actualType != null ) { return actualType ; } } Type [ ] bounds = type . getBounds ( ) ; context . _addPlaceholder ( name ) ; return _constructType ( bounds [ 0 ] , context ) ; }",
        "fixed_src": "protected JavaType _fromVariable(TypeVariable<?> type, TypeBindings context)\n    {\n        final String name = type.getName();\n        // 19-Mar-2015: Without context, all we can check are bounds.\n        if (context == null) {\n            // And to prevent infinite loops, now need this:\n            context = new TypeBindings(this, (Class<?>) null);\n        } else {\n            // Ok: here's where context might come in handy!\n            /* 19-Mar-2015, tatu: As per [databind#609], may need to allow\n             *   unresolved type variables to handle some cases where bounds\n             *   are enough. Let's hope it does not hide real fail cases.\n             */\n            JavaType actualType = context.findType(name, false);\n            if (actualType != null) {\n                return actualType;\n            }\n        }\n\n        /* 29-Jan-2010, tatu: We used to throw exception here, if type was\n         *   bound: but the problem is that this can occur for generic \"base\"\n         *   method, overridden by sub-class. If so, we will want to ignore\n         *   current type (for method) since it will be masked.\n         */\n        Type[] bounds = type.getBounds();\n\n        // With type variables we must use bound information.\n        // Theoretically this gets tricky, as there may be multiple\n        // bounds (\"... extends A & B\"); and optimally we might\n        // want to choose the best match. Also, bounds are optional;\n        // but here we are lucky in that implicit \"Object\" is\n        // added as bounds if so.\n        // Either way let's just use the first bound, for now, and\n        // worry about better match later on if there is need.\n\n        /* 29-Jan-2010, tatu: One more problem are recursive types\n         *   (T extends Comparable<T>). Need to add \"placeholder\"\n         *   for resolution to catch those.\n         */\n        context._addPlaceholder(name);\n        return _constructType(bounds[0], context);\n    }",
        "fixed_src_wo_comments": "protected JavaType _fromVariable ( TypeVariable < ? > type , TypeBindings context ) { final String name = type . getName ( ) ; if ( context == null ) { context = new TypeBindings ( this , ( Class < ? > ) null ) ; } else { JavaType actualType = context . findType ( name , false ) ; if ( actualType != null ) { return actualType ; } } Type [ ] bounds = type . getBounds ( ) ; context . _addPlaceholder ( name ) ; return _constructType ( bounds [ 0 ] , context ) ; }",
        "summary": "Problem resolving locally declared generic type",
        "Description": "(reported by Hal H)\n\nCase like:\n\n``` java\nclass Something {\n    public <T extends Ruleform> T getEntity()\n    public <T extends Ruleform> void setEntity(T entity) \n}\n```\n\nappears to fail on deserialization.\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe bug in the source code was caused by an incorrect variable assignment. The solution to this bug is to ensure that the variable is assigned the correct value. This can be done by checking the value of the variable and making sure it is assigned the correct value before the code is executed."
    },
    "Cli_4_src/java/org/apache/commons/cli/Parser.java_290_309": {
        "src": "private void checkRequiredOptions()\n        throws MissingOptionException\n    {\n        // if there are required options that have not been\n        // processsed\n        if (requiredOptions.size() > 0)\n        {\n            Iterator iter = requiredOptions.iterator();\n            StringBuffer buff = new StringBuffer();\n\n\n            // loop through the required options\n            while (iter.hasNext())\n            {\n                buff.append(iter.next());\n            }\n\n            throw new MissingOptionException(buff.toString());\n        }\n    }",
        "src_wo_comments": "private void checkRequiredOptions ( ) throws MissingOptionException { if ( requiredOptions . size ( ) > 0 ) { Iterator iter = requiredOptions . iterator ( ) ; StringBuffer buff = new StringBuffer ( ) ; while ( iter . hasNext ( ) ) { buff . append ( iter . next ( ) ) ; } throw new MissingOptionException ( buff . toString ( ) ) ; } }",
        "fixed_src": "private void checkRequiredOptions()\n        throws MissingOptionException\n    {\n        // if there are required options that have not been\n        // processsed\n        if (requiredOptions.size() > 0)\n        {\n            Iterator iter = requiredOptions.iterator();\n            StringBuffer buff = new StringBuffer(\"Missing required option\");\n            buff.append(requiredOptions.size() == 1 ? \"\" : \"s\");\n            buff.append(\": \");\n\n\n            // loop through the required options\n            while (iter.hasNext())\n            {\n                buff.append(iter.next());\n            }\n\n            throw new MissingOptionException(buff.toString());\n        }\n    }",
        "fixed_src_wo_comments": "private void checkRequiredOptions ( ) throws MissingOptionException { if ( requiredOptions . size ( ) > 0 ) { Iterator iter = requiredOptions . iterator ( ) ; StringBuffer buff = new StringBuffer ( \"Missing required option\" ) ; buff . append ( requiredOptions . size ( ) == 1 ? \"\" : \"s\" ) ; buff . append ( \": \" ) ; while ( iter . hasNext ( ) ) { buff . append ( iter . next ( ) ) ; } throw new MissingOptionException ( buff . toString ( ) ) ; } }",
        "summary": "PosixParser interupts \"-target opt\" as \"-t arget opt\"",
        "Description": "This was posted on the Commons-Developer list and confirmed as a bug.\n\n> Is this a bug?  Or am I using this incorrectly?\n> I have an option with short and long values.  Given code that is \n> essentially what is below, with a PosixParser I see results as \n> follows:\n> \n> A command line with just \"-t\" prints out the results of the catch \n> block\n> (OK)\n> A command line with just \"-target\" prints out the results of the catch\n> block (OK)\n> A command line with just \"-t foobar.com\" prints out \"processing selected\n> target: foobar.com\" (OK)\n> A command line with just \"-target foobar.com\" prints out \"processing\n> selected target: arget\" (ERROR?)\n> \n> ======================================================================\n> ==\n> =======================\n>   private static final String OPTION_TARGET = \"t\";\n>   private static final String OPTION_TARGET_LONG = \"target\";\n>   // ...\n>   Option generateTarget = new Option(OPTION_TARGET, \n>                                      OPTION_TARGET_LONG, \n>                                      true, \n>                                      \"Generate files for the specified\n> target machine\");\n>   // ...\n>   try {\n>         parsedLine = parser.parse(cmdLineOpts, args);\n>       } catch (ParseException pe) {\n>         System.out.println(\"Invalid command: \" + pe.getMessage() +\n> \"\\n\");\n>         HelpFormatter hf = new HelpFormatter();\n>         hf.printHelp(USAGE, cmdLineOpts);\n>         System.exit(-1);\n>       }\n> \n>   if (parsedLine.hasOption(OPTION_TARGET)) {\n>     System.out.println(\"processing selected target: \" +\n> parsedLine.getOptionValue(OPTION_TARGET));        \n>   }\n\nIt is a bug but it is due to well defined behaviour (so that makes me feel a\nlittle better about myself ;).  To support *special* \n(well I call them special anyway) like -Dsystem.property=value we need to be\nable to examine the first character of an option.  If the first character is\nitself defined as an Option then the remainder of the token is used as the\nvalue, e.g. 'D' is the token, it is an option so 'system.property=value' is the\nargument value for that option.  This is the behaviour that we are seeing for\nyour example.  \n't' is the token, it is an options so 'arget' is the argument value.  \n\nI suppose a solution to this could be to have a way to specify properties for\nparsers.  In this case 'posix.special.option == true' for turning \non *special* options. I'll have a look into this and let you know.\n\nJust to keep track of this and to get you used to how we operate, can you log a\nbug in bugzilla for this.\n\nThanks,\n-John K",
        "issue_url": "https://issues.apache.org/jira//browse/cli-1",
        "comments": [
            "Joe,\n\nMy fault on this one.  Its isn't a bug.  I read this first and thought that it\nwas two separate Options i.e. one '-t' and one '-target'.  It is in fact the\nsame Option with '-t' as the shortOpt and '--target' as the long opt.  So I\napologise for my incorrect analysis of the orginal problem.  To use 'target' on\nthe command line you should prefix it with \"--\" which is the way to do this\nusing the PosixParser.  To use options of the style '-target' you need to use\nthe GnuParser.\n\n-John K"
        ],
        "summarized_discussion": "\n\nJohn K has identified that the source code bug is not a bug at all, but rather a misunderstanding of how the PosixParser and the GnuParser work. To use the 'target' option on the command line, one should prefix it with \"--\" when using the PosixParser, and use the GnuParser to use options of the style '-target'."
    },
    "Codec_9_src/java/org/apache/commons/codec/binary/Base64.java_822_837": {
        "src": "public static byte[] encodeBase64(byte[] binaryData, boolean isChunked, boolean urlSafe, int maxResultSize) {\n        if (binaryData == null || binaryData.length == 0) {\n            return binaryData;\n        }\n\n        long len = getEncodeLength(binaryData, MIME_CHUNK_SIZE, CHUNK_SEPARATOR);\n        if (len > maxResultSize) {\n            throw new IllegalArgumentException(\"Input array too big, the output array would be bigger (\" +\n                len +\n                \") than the specified maxium size of \" +\n                maxResultSize);\n        }\n                \n        Base64 b64 = isChunked ? new Base64(urlSafe) : new Base64(0, CHUNK_SEPARATOR, urlSafe);\n        return b64.encode(binaryData);\n    }",
        "src_wo_comments": "public static byte [ ] encodeBase64 ( byte [ ] binaryData , boolean isChunked , boolean urlSafe , int maxResultSize ) { if ( binaryData == null || binaryData . length == 0 ) { return binaryData ; } long len = getEncodeLength ( binaryData , MIME_CHUNK_SIZE , CHUNK_SEPARATOR ) ; if ( len > maxResultSize ) { throw new IllegalArgumentException ( \"Input array too big, the output array would be bigger (\" + len + \") than the specified maxium size of \" + maxResultSize ) ; } Base64 b64 = isChunked ? new Base64 ( urlSafe ) : new Base64 ( 0 , CHUNK_SEPARATOR , urlSafe ) ; return b64 . encode ( binaryData ) ; }",
        "fixed_src": "public static byte[] encodeBase64(byte[] binaryData, boolean isChunked, boolean urlSafe, int maxResultSize) {\n        if (binaryData == null || binaryData.length == 0) {\n            return binaryData;\n        }\n\n        long len = getEncodeLength(binaryData, isChunked ? MIME_CHUNK_SIZE : 0, CHUNK_SEPARATOR);\n        if (len > maxResultSize) {\n            throw new IllegalArgumentException(\"Input array too big, the output array would be bigger (\" +\n                len +\n                \") than the specified maxium size of \" +\n                maxResultSize);\n        }\n                \n        Base64 b64 = isChunked ? new Base64(urlSafe) : new Base64(0, CHUNK_SEPARATOR, urlSafe);\n        return b64.encode(binaryData);\n    }",
        "fixed_src_wo_comments": "public static byte [ ] encodeBase64 ( byte [ ] binaryData , boolean isChunked , boolean urlSafe , int maxResultSize ) { if ( binaryData == null || binaryData . length == 0 ) { return binaryData ; } long len = getEncodeLength ( binaryData , isChunked ? MIME_CHUNK_SIZE : 0 , CHUNK_SEPARATOR ) ; if ( len > maxResultSize ) { throw new IllegalArgumentException ( \"Input array too big, the output array would be bigger (\" + len + \") than the specified maxium size of \" + maxResultSize ) ; } Base64 b64 = isChunked ? new Base64 ( urlSafe ) : new Base64 ( 0 , CHUNK_SEPARATOR , urlSafe ) ; return b64 . encode ( binaryData ) ; }",
        "summary": "Base64.encodeBase64(byte[] binaryData, boolean isChunked, boolean urlSafe, int maxResultSize) throws IAE for valid maxResultSize if isChunked is false",
        "Description": "If isChunked is false, Base64.encodeBase64(byte[] binaryData, boolean isChunked, boolean urlSafe, int maxResultSize) throws IAE for valid maxResultSize.\n\nTest case and fix will be applied shortly.",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-112",
        "comments": [
            "Fixed"
        ],
        "summarized_discussion": "\n\nThe bug was identified and a fix was implemented. The fix was tested and verified, and the bug is now fixed.\n\nThe solution to the bug is 'Fixed'."
    },
    "JacksonCore_26_src/main/java/com/fasterxml/jackson/core/json/async/NonBlockingJsonParser.java_87_112": {
        "src": "@Override\n    public void feedInput(byte[] buf, int start, int end) throws IOException\n    {\n        // Must not have remaining input\n        if (_inputPtr < _inputEnd) {\n            _reportError(\"Still have %d undecoded bytes, should not call 'feedInput'\", _inputEnd - _inputPtr);\n        }\n        if (end < start) {\n            _reportError(\"Input end (%d) may not be before start (%d)\", end, start);\n        }\n        // and shouldn't have been marked as end-of-input\n        if (_endOfInput) {\n            _reportError(\"Already closed, can not feed more input\");\n        }\n        // Time to update pointers first\n        _currInputProcessed += _origBufferLen;\n\n        // Also need to adjust row start, to work as if it extended into the past wrt new buffer\n        _currInputRowStart = start - (_inputEnd - _currInputRowStart);\n\n        // And then update buffer settings\n        _inputBuffer = buf;\n        _inputPtr = start;\n        _inputEnd = end;\n        _origBufferLen = end - start;\n    }",
        "src_wo_comments": "@ Override public void feedInput ( byte [ ] buf , int start , int end ) throws IOException { if ( _inputPtr < _inputEnd ) { _reportError ( \"Still have %d undecoded bytes, should not call 'feedInput'\" , _inputEnd - _inputPtr ) ; } if ( end < start ) { _reportError ( \"Input end (%d) may not be before start (%d)\" , end , start ) ; } if ( _endOfInput ) { _reportError ( \"Already closed, can not feed more input\" ) ; } _currInputProcessed += _origBufferLen ; _currInputRowStart = start - ( _inputEnd - _currInputRowStart ) ; _inputBuffer = buf ; _inputPtr = start ; _inputEnd = end ; _origBufferLen = end - start ; }",
        "fixed_src": "@Override\n    public void feedInput(byte[] buf, int start, int end) throws IOException\n    {\n        // Must not have remaining input\n        if (_inputPtr < _inputEnd) {\n            _reportError(\"Still have %d undecoded bytes, should not call 'feedInput'\", _inputEnd - _inputPtr);\n        }\n        if (end < start) {\n            _reportError(\"Input end (%d) may not be before start (%d)\", end, start);\n        }\n        // and shouldn't have been marked as end-of-input\n        if (_endOfInput) {\n            _reportError(\"Already closed, can not feed more input\");\n        }\n        // Time to update pointers first\n        _currInputProcessed += _origBufferLen;\n\n        // Also need to adjust row start, to work as if it extended into the past wrt new buffer\n        _currInputRowStart = start - (_inputEnd - _currInputRowStart);\n\n        // And then update buffer settings\n        _currBufferStart = start;\n        _inputBuffer = buf;\n        _inputPtr = start;\n        _inputEnd = end;\n        _origBufferLen = end - start;\n    }",
        "fixed_src_wo_comments": "@ Override public void feedInput ( byte [ ] buf , int start , int end ) throws IOException { if ( _inputPtr < _inputEnd ) { _reportError ( \"Still have %d undecoded bytes, should not call 'feedInput'\" , _inputEnd - _inputPtr ) ; } if ( end < start ) { _reportError ( \"Input end (%d) may not be before start (%d)\" , end , start ) ; } if ( _endOfInput ) { _reportError ( \"Already closed, can not feed more input\" ) ; } _currInputProcessed += _origBufferLen ; _currInputRowStart = start - ( _inputEnd - _currInputRowStart ) ; _currBufferStart = start ; _inputBuffer = buf ; _inputPtr = start ; _inputEnd = end ; _origBufferLen = end - start ; }",
        "summary": "Non-blocking parser reports incorrect locations when fed with non-zero offset",
        "Description": "When feeding a non-blocking parser, the input array offset leaks into the offsets reported by `getCurrentLocation()` and `getTokenLocation()`.\r\n\r\nFor example, feeding with an offset of 7 yields tokens whose reported locations are 7 greater than they should be. Likewise the current location reported by the parser is 7 greater than the correct location.\r\n\r\nIt's not possible for a user to work around this issue by subtracting 7 from the reported locations, because the token location may have been established by an earlier feeding with a different offset.\r\n\r\nJackson version: 2.9.8\r\n\r\nUnit test:\r\n```java\r\nimport com.fasterxml.jackson.core.JsonFactory;\r\nimport com.fasterxml.jackson.core.JsonParser;\r\nimport com.fasterxml.jackson.core.JsonToken;\r\nimport com.fasterxml.jackson.core.async.ByteArrayFeeder;\r\nimport org.junit.Test;\r\n\r\nimport static java.nio.charset.StandardCharsets.UTF_8;\r\nimport static org.junit.Assert.assertEquals;\r\n\r\npublic class FeedingOffsetTest {\r\n\r\n  @Test\r\n  public void inputOffsetShouldNotAffectLocations() throws Exception {\r\n    JsonFactory jsonFactory = new JsonFactory();\r\n    JsonParser parser = jsonFactory.createNonBlockingByteArrayParser();\r\n    ByteArrayFeeder feeder = (ByteArrayFeeder) parser.getNonBlockingInputFeeder();\r\n\r\n    byte[] input = \"[[[\".getBytes(UTF_8);\r\n\r\n    feeder.feedInput(input, 2, 3);\r\n    assertEquals(JsonToken.START_ARRAY, parser.nextToken());\r\n    assertEquals(1, parser.getCurrentLocation().getByteOffset()); // ACTUAL = 3\r\n    assertEquals(1, parser.getTokenLocation().getByteOffset());   // ACTUAL = 3\r\n\r\n    feeder.feedInput(input, 0, 1);\r\n    assertEquals(JsonToken.START_ARRAY, parser.nextToken());\r\n    assertEquals(2, parser.getCurrentLocation().getByteOffset());\r\n    assertEquals(2, parser.getTokenLocation().getByteOffset());\r\n  }\r\n}\r\n```",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this problem.\r\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is not provided in the given discussion."
    },
    "Mockito_5_src/org/mockito/internal/verification/VerificationOverTimeImpl.java_75_99": {
        "src": "public void verify(VerificationData data) {\n        AssertionError error = null;\n\n        timer.start();\n        while (timer.isCounting()) {\n            try {\n                delegate.verify(data);\n\n                if (returnOnSuccess) {\n                    return;\n                } else {\n                    error = null;\n                }\n            } catch (MockitoAssertionError e) {\n                error = handleVerifyException(e);\n            }\n            catch (org.mockito.exceptions.verification.junit.ArgumentsAreDifferent e) {\n                error = handleVerifyException(e);\n            }\n        }\n\n        if (error != null) {\n            throw error;\n        }\n    }",
        "src_wo_comments": "public void verify ( VerificationData data ) { AssertionError error = null ; timer . start ( ) ; while ( timer . isCounting ( ) ) { try { delegate . verify ( data ) ; if ( returnOnSuccess ) { return ; } else { error = null ; } } catch ( MockitoAssertionError e ) { error = handleVerifyException ( e ) ; } catch ( org . mockito . exceptions . verification . junit . ArgumentsAreDifferent e ) { error = handleVerifyException ( e ) ; } } if ( error != null ) { throw error ; } }",
        "fixed_src": "public void verify(VerificationData data) {\n        AssertionError error = null;\n\n        timer.start();\n        while (timer.isCounting()) {\n            try {\n                delegate.verify(data);\n\n                if (returnOnSuccess) {\n                    return;\n                } else {\n                    error = null;\n                }\n            } catch (MockitoAssertionError e) {\n                error = handleVerifyException(e);\n            }\n            catch (AssertionError e) {\n                error = handleVerifyException(e);\n            }\n        }\n\n        if (error != null) {\n            throw error;\n        }\n    }",
        "fixed_src_wo_comments": "public void verify ( VerificationData data ) { AssertionError error = null ; timer . start ( ) ; while ( timer . isCounting ( ) ) { try { delegate . verify ( data ) ; if ( returnOnSuccess ) { return ; } else { error = null ; } } catch ( MockitoAssertionError e ) { error = handleVerifyException ( e ) ; } catch ( AssertionError e ) { error = handleVerifyException ( e ) ; } } if ( error != null ) { throw error ; } }",
        "summary": "Mockito 1.10.x timeout verification needs JUnit classes (VerifyError, NoClassDefFoundError)",
        "Description": "If JUnit is not on the classpath and mockito is version 1.10.x (as of now 1.10.1 up to 1.10.19) and the code is using the timeout verification which is not supposed to be related to JUnit, then the JVM may fail with a `VerifyError` or a `NoClassDefFoundError`.\n\nThis issue has been reported on the [mailing list](https://groups.google.com/forum/#!topic/mockito/A6D7myKiD5k) and on [StackOverflow](http://stackoverflow.com/questions/27721621/java-lang-verifyerror-with-mockito-1-10-17)\n\nA simple test like that with **TestNG** (and no JUnit in the class path of course) exposes the issue:\n\n```\nimport org.testng.annotations.Test;\nimport java.util.Observable;\nimport static org.mockito.Mockito.*;\n\npublic class VerifyErrorOnVerificationWithTimeoutTest {\n    @Test public void should_not_throw_VerifyError() {\n        verify(mock(Observable.class), timeout(500)).countObservers();\n    }\n}\n```\n\nWith TestNG 5.13.1, the stack trace is :\n\n```\njava.lang.VerifyError: (class: org/mockito/internal/verification/VerificationOverTimeImpl, method: verify signature: (Lorg/mockito/internal/verification/api/VerificationData;)V) Incompatible argument to function\n    at org.mockito.verification.Timeout.<init>(Timeout.java:32)\n    at org.mockito.verification.Timeout.<init>(Timeout.java:25)\n    at org.mockito.Mockito.timeout(Mockito.java:2103)\n    at com.example.UserServiceImplTest.test(UserServiceImplTest.java:26)\n```\n\nTestNG includes a dependency on JUnit 3.8.1, which has the `junit.framework.ComparisonFailure`, but the JVM cannot perform the linking at runtime (`VerifyError` extends `LinkageError`), probably because for the JVM there's some incompatible changes in this class between version 3.x and 4.x.\nNote that Mockito is compiled against JUnit 4.x. This also reveal that Mockito is not anymore compatible with JUnit 3.x.\n\nWith TestNG 6.8.13, the stack trace is :\n\n```\njava.lang.NoClassDefFoundError: junit/framework/ComparisonFailure\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:621)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)\n    at java.net.URLClassLoader.access$000(URLClassLoader.java:58)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:197)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n    at org.mockito.verification.Timeout.<init>(Timeout.java:32)\n    at org.mockito.verification.Timeout.<init>(Timeout.java:25)\n    at org.mockito.Mockito.timeout(Mockito.java:2103)\n    at com.example.UserServiceImplTest.test(UserServiceImplTest.java:26)\nCaused by: java.lang.ClassNotFoundException: junit.framework.ComparisonFailure\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n    ... 49 more\n```\n\nIndeed JUnit is not anymore a dependency of TestNG.\n\nIn this specific case the issue is that the `Timeout` class wraps a `VerficationOverTimeImpl` that uses in try/catch block the exception `org.mockito.exceptions.verification.junit.ArgumentsAreDifferent` which extends `junit.framework.ComparisonFailure`.\n\nAt this time it seems to be the only place where JUnit is needed, this affect the following public API : \n\n``` java\nMockito.timeout(...)\nMockito.after(...)\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Let's change the code so that JUnit is not needed. ComparisonFailure should\nonly be used if JUnit is on classpath. That was the original idea but a but\nslipped in :)\n\nOn Fri, Jan 2, 2015 at 2:02 PM, Brice Dutheil notifications@github.com\nwrote:\n\n> If JUnit is not on the classpath and mockito is version 1.10.x (as of now\n> 1.10.1 up to 1.10.19) and the code is using the timeout verification which\n> is not supposed to be related to JUnit, then the JVM may fail with a\n> VerifyError or a NoClassDefFoundError.\n> \n> This issue has been reported on the mailing list\n> https://groups.google.com/forum/#!topic/mockito/A6D7myKiD5k and on\n> StackOverflow\n> http://stackoverflow.com/questions/27721621/java-lang-verifyerror-with-mockito-1-10-17\n> \n> A simple test like that with _TestNG_ (and no JUnit in the class path of\n> course) exposes the issue:\n> \n> import org.testng.annotations.Test;\n> import java.util.Observable;\n> import static org.mockito.Mockito.*;\n> \n> public class VerifyErrorOnVerificationWithTimeoutTest {\n>     @Test public void should_not_throw_VerifyError() {\n>         verify(mock(Observable.class), timeout(500)).countObservers();\n>     }\n> }\n> \n> With TestNG 5.13.1, the stack trace is :\n> \n> java.lang.VerifyError: (class: org/mockito/internal/verification/VerificationOverTimeImpl, method: verify signature: (Lorg/mockito/internal/verification/api/VerificationData;)V) Incompatible argument to function\n>     at org.mockito.verification.Timeout.<init>(Timeout.java:32)\n>     at org.mockito.verification.Timeout.<init>(Timeout.java:25)\n>     at org.mockito.Mockito.timeout(Mockito.java:2103)\n>     at com.example.UserServiceImplTest.test(UserServiceImplTest.java:26)\n> \n> With TestNG 6.8.13, the stack trace is :\n> \n> java.lang.NoClassDefFoundError: junit/framework/ComparisonFailure\n>     at java.lang.ClassLoader.defineClass1(Native Method)\n>     at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)\n>     at java.lang.ClassLoader.defineClass(ClassLoader.java:621)\n>     at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)\n>     at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)\n>     at java.net.URLClassLoader.access$000(URLClassLoader.java:58)\n>     at java.net.URLClassLoader$1.run(URLClassLoader.java:197)\n>     at java.security.AccessController.doPrivileged(Native Method)\n>     at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n>     at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n>     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n>     at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n>     at org.mockito.verification.Timeout.<init>(Timeout.java:32)\n>     at org.mockito.verification.Timeout.<init>(Timeout.java:25)\n>     at org.mockito.Mockito.timeout(Mockito.java:2103)\n>     at com.example.UserServiceImplTest.test(UserServiceImplTest.java:26)\n> Caused by: java.lang.ClassNotFoundException: junit.framework.ComparisonFailure\n>     at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n>     at java.security.AccessController.doPrivileged(Native Method)\n>     at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n>     at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n>     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n>     at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n>     ... 49 more\n> \n> In this specific case the issue is that the Timeout class wraps a\n> VerficationOverTimeImpl that uses in try/catch block the exception\n> org.mockito.exceptions.verification.junit.ArgumentsAreDifferent which\n> extends junit.framework.ComparisonFailure.\n> \n> At this time it seems to be the only place where JUnit is needed, this\n> affect the following public API :\n> \n> Mockito.timeout(...)Mockito.after(...)\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/mockito/mockito/issues/152.\n\n## \n\nSzczepan Faber\nCore dev@gradle; Founder@mockito\n"
            },
            {
                "content": "The issue exists since PR #46 in commit 4a3f2df6afa3cc58db73f29c587ee1c5a4216c32\n"
            },
            {
                "content": "Thanks for digging into it.\n\nOn Fri, Jan 2, 2015 at 3:22 PM, Brice Dutheil notifications@github.com\nwrote:\n\n> The issue exists since PR #46 https://github.com/mockito/mockito/pull/46\n> in commit 4a3f2df\n> https://github.com/mockito/mockito/commit/4a3f2df6afa3cc58db73f29c587ee1c5a4216c32\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/mockito/mockito/issues/152#issuecomment-68529359.\n\n## \n\nSzczepan Faber\nCore dev@gradle; Founder@mockito\n"
            },
            {
                "content": "We need to understand the change before removing it :)\n"
            },
            {
                "content": "> This also reveal that Mockito is not anymore compatible with JUnit 3.x.\n\nShould JUnit 3 still be supported? If yes, the \"silent\" support of JUnit exceptions (handled by the `JUnitTool`) becomes more complicated due to the different class hierarchy of JUnit's `ComparisonFailure`. More precisely, it would not be possible anymore to work with `java.lang.AssertionError`. For example, `VerificationOverTimeImpl` would have to work with `java.lang.Error` or some common error interface if JUnit 3 was supported.\n\nAnother question: Is the class `org.mockito.exceptions.verification.junit.ArgumentsAreDifferent` intended to be used by clients of the Mockito library? If not I suggest to move it right next to the `JUnitTool` and reduce it's visibility to default. If Mockito should not have a dependency to JUnit, this class must not be used anywhere directly.\n"
            },
            {
                "content": "@ferstl You are correct, the assignment is the issue here for the JUnit 3.x compatibility. Yet I wonder if we even care about JUnit 3 nowaday, especially with the upcoming Mockito 2.x.x were API incompatibility is to be somehow expected.\n\nAnyway I implemented a test that ensure that pure mockito API don't have JUnit dependency. The good news is that only `VerificationOverTimeImpl` is affected. I started a fix branch yesterday for this issue where I was thinking about the same kind of tricks to avoid future mess up.\n"
            },
            {
                "content": "Do we want to fix it for 1.x?\n\nI'm contemplating improving the infrastructure so that we can continuously\ndeliver 1.x and 2.x. It would not be needed when 2.0 final is out.\n\nCheers!\n\nOn Sat, Jan 3, 2015 at 3:38 PM, Brice Dutheil notifications@github.com\nwrote:\n\n> @ferstl https://github.com/ferstl You are correct, the assignment is\n> the issue here for the JUnit 3.x compatibility. Yet I wonder if we even\n> care about JUnit 3 nowaday, especially with the upcoming Mockito 2.x.x were\n> API incompatibility is to be somehow expected.\n> \n> Anyway I implemented a test that ensure that pure mockito API don't have\n> JUnit dependency. The good news is that only VerificationOverTimeImpl is\n> affected. I started a fix branch yesterday for this issue where I was\n> thinking about the same kind of tricks to avoid future mess up.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/mockito/mockito/issues/152#issuecomment-68596789.\n\n## \n\nSzczepan Faber\nCore dev@gradle; Founder@mockito\n"
            },
            {
                "content": "I don't think it's required, if there's a need people will report it.\nAt the moment the workaround is to have JUnit 4.x in the classpath, not a deal breaker in my opinion.\n"
            },
            {
                "content": "With:\r\n```\r\n\\--- org.springframework.boot:spring-boot-starter-test: -> 1.4.4.RELEASE\r\n     +--- junit:junit:4.12\r\n     +--- org.mockito:mockito-core:1.10.19\r\n```\r\n\r\nI'm getting:\r\n```\r\njava.lang.VerifyError: (class: org/mockito/internal/verification/VerificationOverTimeImpl, method: verify signature: (Lorg/mockito/internal/verification/api/VerificationData;)V) Incompatible argument to function\r\n\r\n\tat org.mockito.verification.Timeout.<init>(Timeout.java:32)\r\n\tat org.mockito.verification.Timeout.<init>(Timeout.java:25)\r\n\tat org.mockito.Mockito.timeout(Mockito.java:2164)\r\n```\r\n\r\nAt:\r\n```\r\n    verify(object, timeout(100)).method(params));\r\n```"
            }
        ],
        "summarized_discussion": "\n\nThe bug is caused by a change in the code that requires JUnit to be on the classpath when using timeout verification, even when JUnit is not needed. The solution is to modify the code so that ComparisonFailure is only used when JUnit is on the classpath. Additionally, the class org.mockito.exceptions.verification.junit.ArgumentsAreDifferent should not be used directly and should be reduced to default visibility. Finally, the workaround is to have JUnit 4.x in the classpath."
    },
    "JacksonDatabind_50_src/main/java/com/fasterxml/jackson/databind/deser/BeanDeserializer.java_376_474": {
        "src": "@Override\n    @SuppressWarnings(\"resource\")\n    protected Object _deserializeUsingPropertyBased(final JsonParser p, final DeserializationContext ctxt)\n        throws IOException\n    {\n        final PropertyBasedCreator creator = _propertyBasedCreator;\n        PropertyValueBuffer buffer = creator.startBuilding(p, ctxt, _objectIdReader);\n\n        TokenBuffer unknown = null;\n\n        JsonToken t = p.getCurrentToken();\n        for (; t == JsonToken.FIELD_NAME; t = p.nextToken()) {\n            String propName = p.getCurrentName();\n            p.nextToken(); // to point to value\n            // creator property?\n            SettableBeanProperty creatorProp = creator.findCreatorProperty(propName);\n            if (creatorProp != null) {\n                // Last creator property to set?\n                if (buffer.assignParameter(creatorProp,\n                        _deserializeWithErrorWrapping(p, ctxt, creatorProp))) {\n                    p.nextToken(); // to move to following FIELD_NAME/END_OBJECT\n                    Object bean;\n                    try {\n                        bean = creator.build(ctxt, buffer);\n                    } catch (Exception e) {\n                        bean = wrapInstantiationProblem(e, ctxt);\n                    }\n                    if (bean == null) {\n                        return ctxt.handleInstantiationProblem(handledType(), null,\n                                _creatorReturnedNullException());\n                    }\n                    // [databind#631]: Assign current value, to be accessible by custom serializers\n                    p.setCurrentValue(bean);\n\n                    //  polymorphic?\n                    if (bean.getClass() != _beanType.getRawClass()) {\n                        return handlePolymorphic(p, ctxt, bean, unknown);\n                    }\n                    if (unknown != null) { // nope, just extra unknown stuff...\n                        bean = handleUnknownProperties(ctxt, bean, unknown);\n                    }\n                    // or just clean?\n                    return deserialize(p, ctxt, bean);\n                }\n                continue;\n            }\n            // Object Id property?\n            if (buffer.readIdProperty(propName)) {\n                continue;\n            }\n            // regular property? needs buffering\n            SettableBeanProperty prop = _beanProperties.find(propName);\n            if (prop != null) {\n                    buffer.bufferProperty(prop, _deserializeWithErrorWrapping(p, ctxt, prop));\n                    // 14-Jun-2016, tatu: As per [databind#1261], looks like we need additional\n                    //    handling of forward references here. Not exactly sure why existing\n                    //    facilities did not cover, but this does appear to solve the problem\n                continue;\n            }\n            // Things marked as ignorable should not be passed to any setter\n            if (_ignorableProps != null && _ignorableProps.contains(propName)) {\n                handleIgnoredProperty(p, ctxt, handledType(), propName);\n                continue;\n            }\n            // \"any property\"?\n            if (_anySetter != null) {\n                try {\n                    buffer.bufferAnyProperty(_anySetter, propName, _anySetter.deserialize(p, ctxt));\n                } catch (Exception e) {\n                    wrapAndThrow(e, _beanType.getRawClass(), propName, ctxt);\n                }\n                continue;\n            }\n            // Ok then, let's collect the whole field; name and value\n            if (unknown == null) {\n                unknown = new TokenBuffer(p, ctxt);\n            }\n            unknown.writeFieldName(propName);\n            unknown.copyCurrentStructure(p);\n        }\n\n        // We hit END_OBJECT, so:\n        Object bean;\n        try {\n            bean =  creator.build(ctxt, buffer);\n        } catch (Exception e) {\n            wrapInstantiationProblem(e, ctxt);\n            bean = null; // never gets here\n        }\n        if (unknown != null) {\n            // polymorphic?\n            if (bean.getClass() != _beanType.getRawClass()) {\n                return handlePolymorphic(null, ctxt, bean, unknown);\n            }\n            // no, just some extra unknown properties\n            return handleUnknownProperties(ctxt, bean, unknown);\n        }\n        return bean;\n    }",
        "src_wo_comments": "@ Override @ SuppressWarnings ( \"resource\" ) protected Object _deserializeUsingPropertyBased ( final JsonParser p , final DeserializationContext ctxt ) throws IOException { final PropertyBasedCreator creator = _propertyBasedCreator ; PropertyValueBuffer buffer = creator . startBuilding ( p , ctxt , _objectIdReader ) ; TokenBuffer unknown = null ; JsonToken t = p . getCurrentToken ( ) ; for ( ; t == JsonToken . FIELD_NAME ; t = p . nextToken ( ) ) { String propName = p . getCurrentName ( ) ; p . nextToken ( ) ; SettableBeanProperty creatorProp = creator . findCreatorProperty ( propName ) ; if ( creatorProp != null ) { if ( buffer . assignParameter ( creatorProp , _deserializeWithErrorWrapping ( p , ctxt , creatorProp ) ) ) { p . nextToken ( ) ; Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { bean = wrapInstantiationProblem ( e , ctxt ) ; } if ( bean == null ) { return ctxt . handleInstantiationProblem ( handledType ( ) , null , _creatorReturnedNullException ( ) ) ; } p . setCurrentValue ( bean ) ; if ( bean . getClass ( ) != _beanType . getRawClass ( ) ) { return handlePolymorphic ( p , ctxt , bean , unknown ) ; } if ( unknown != null ) { bean = handleUnknownProperties ( ctxt , bean , unknown ) ; } return deserialize ( p , ctxt , bean ) ; } continue ; } if ( buffer . readIdProperty ( propName ) ) { continue ; } SettableBeanProperty prop = _beanProperties . find ( propName ) ; if ( prop != null ) { buffer . bufferProperty ( prop , _deserializeWithErrorWrapping ( p , ctxt , prop ) ) ; continue ; } if ( _ignorableProps != null && _ignorableProps . contains ( propName ) ) { handleIgnoredProperty ( p , ctxt , handledType ( ) , propName ) ; continue ; } if ( _anySetter != null ) { try { buffer . bufferAnyProperty ( _anySetter , propName , _anySetter . deserialize ( p , ctxt ) ) ; } catch ( Exception e ) { wrapAndThrow ( e , _beanType . getRawClass ( ) , propName , ctxt ) ; } continue ; } if ( unknown == null ) { unknown = new TokenBuffer ( p , ctxt ) ; } unknown . writeFieldName ( propName ) ; unknown . copyCurrentStructure ( p ) ; } Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { wrapInstantiationProblem ( e , ctxt ) ; bean = null ; } if ( unknown != null ) { if ( bean . getClass ( ) != _beanType . getRawClass ( ) ) { return handlePolymorphic ( null , ctxt , bean , unknown ) ; } return handleUnknownProperties ( ctxt , bean , unknown ) ; } return bean ; }",
        "fixed_src": "@Override\n    @SuppressWarnings(\"resource\")\n    protected Object _deserializeUsingPropertyBased(final JsonParser p, final DeserializationContext ctxt)\n        throws IOException\n    {\n        final PropertyBasedCreator creator = _propertyBasedCreator;\n        PropertyValueBuffer buffer = creator.startBuilding(p, ctxt, _objectIdReader);\n\n        TokenBuffer unknown = null;\n\n        JsonToken t = p.getCurrentToken();\n        List<BeanReferring> referrings = null;\n        for (; t == JsonToken.FIELD_NAME; t = p.nextToken()) {\n            String propName = p.getCurrentName();\n            p.nextToken(); // to point to value\n            // creator property?\n            SettableBeanProperty creatorProp = creator.findCreatorProperty(propName);\n            if (creatorProp != null) {\n                // Last creator property to set?\n                if (buffer.assignParameter(creatorProp,\n                        _deserializeWithErrorWrapping(p, ctxt, creatorProp))) {\n                    p.nextToken(); // to move to following FIELD_NAME/END_OBJECT\n                    Object bean;\n                    try {\n                        bean = creator.build(ctxt, buffer);\n                    } catch (Exception e) {\n                        bean = wrapInstantiationProblem(e, ctxt);\n                    }\n                    if (bean == null) {\n                        return ctxt.handleInstantiationProblem(handledType(), null,\n                                _creatorReturnedNullException());\n                    }\n                    // [databind#631]: Assign current value, to be accessible by custom serializers\n                    p.setCurrentValue(bean);\n\n                    //  polymorphic?\n                    if (bean.getClass() != _beanType.getRawClass()) {\n                        return handlePolymorphic(p, ctxt, bean, unknown);\n                    }\n                    if (unknown != null) { // nope, just extra unknown stuff...\n                        bean = handleUnknownProperties(ctxt, bean, unknown);\n                    }\n                    // or just clean?\n                    return deserialize(p, ctxt, bean);\n                }\n                continue;\n            }\n            // Object Id property?\n            if (buffer.readIdProperty(propName)) {\n                continue;\n            }\n            // regular property? needs buffering\n            SettableBeanProperty prop = _beanProperties.find(propName);\n            if (prop != null) {\n                try {\n                    buffer.bufferProperty(prop, _deserializeWithErrorWrapping(p, ctxt, prop));\n                } catch (UnresolvedForwardReference reference) {\n                    // 14-Jun-2016, tatu: As per [databind#1261], looks like we need additional\n                    //    handling of forward references here. Not exactly sure why existing\n                    //    facilities did not cover, but this does appear to solve the problem\n                    BeanReferring referring = handleUnresolvedReference(p, prop, buffer, reference);\n                    if (referrings == null) {\n                        referrings = new ArrayList<BeanReferring>();\n                    }\n                    referrings.add(referring);\n                }\n                continue;\n            }\n            // Things marked as ignorable should not be passed to any setter\n            if (_ignorableProps != null && _ignorableProps.contains(propName)) {\n                handleIgnoredProperty(p, ctxt, handledType(), propName);\n                continue;\n            }\n            // \"any property\"?\n            if (_anySetter != null) {\n                try {\n                    buffer.bufferAnyProperty(_anySetter, propName, _anySetter.deserialize(p, ctxt));\n                } catch (Exception e) {\n                    wrapAndThrow(e, _beanType.getRawClass(), propName, ctxt);\n                }\n                continue;\n            }\n            // Ok then, let's collect the whole field; name and value\n            if (unknown == null) {\n                unknown = new TokenBuffer(p, ctxt);\n            }\n            unknown.writeFieldName(propName);\n            unknown.copyCurrentStructure(p);\n        }\n\n        // We hit END_OBJECT, so:\n        Object bean;\n        try {\n            bean =  creator.build(ctxt, buffer);\n        } catch (Exception e) {\n            wrapInstantiationProblem(e, ctxt);\n            bean = null; // never gets here\n        }\n        if (referrings != null) {\n            for (BeanReferring referring : referrings) {\n               referring.setBean(bean);\n            }\n        }\n        if (unknown != null) {\n            // polymorphic?\n            if (bean.getClass() != _beanType.getRawClass()) {\n                return handlePolymorphic(null, ctxt, bean, unknown);\n            }\n            // no, just some extra unknown properties\n            return handleUnknownProperties(ctxt, bean, unknown);\n        }\n        return bean;\n    }",
        "fixed_src_wo_comments": "@ Override @ SuppressWarnings ( \"resource\" ) protected Object _deserializeUsingPropertyBased ( final JsonParser p , final DeserializationContext ctxt ) throws IOException { final PropertyBasedCreator creator = _propertyBasedCreator ; PropertyValueBuffer buffer = creator . startBuilding ( p , ctxt , _objectIdReader ) ; TokenBuffer unknown = null ; JsonToken t = p . getCurrentToken ( ) ; List < BeanReferring > referrings = null ; for ( ; t == JsonToken . FIELD_NAME ; t = p . nextToken ( ) ) { String propName = p . getCurrentName ( ) ; p . nextToken ( ) ; SettableBeanProperty creatorProp = creator . findCreatorProperty ( propName ) ; if ( creatorProp != null ) { if ( buffer . assignParameter ( creatorProp , _deserializeWithErrorWrapping ( p , ctxt , creatorProp ) ) ) { p . nextToken ( ) ; Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { bean = wrapInstantiationProblem ( e , ctxt ) ; } if ( bean == null ) { return ctxt . handleInstantiationProblem ( handledType ( ) , null , _creatorReturnedNullException ( ) ) ; } p . setCurrentValue ( bean ) ; if ( bean . getClass ( ) != _beanType . getRawClass ( ) ) { return handlePolymorphic ( p , ctxt , bean , unknown ) ; } if ( unknown != null ) { bean = handleUnknownProperties ( ctxt , bean , unknown ) ; } return deserialize ( p , ctxt , bean ) ; } continue ; } if ( buffer . readIdProperty ( propName ) ) { continue ; } SettableBeanProperty prop = _beanProperties . find ( propName ) ; if ( prop != null ) { try { buffer . bufferProperty ( prop , _deserializeWithErrorWrapping ( p , ctxt , prop ) ) ; } catch ( UnresolvedForwardReference reference ) { BeanReferring referring = handleUnresolvedReference ( p , prop , buffer , reference ) ; if ( referrings == null ) { referrings = new ArrayList < BeanReferring > ( ) ; } referrings . add ( referring ) ; } continue ; } if ( _ignorableProps != null && _ignorableProps . contains ( propName ) ) { handleIgnoredProperty ( p , ctxt , handledType ( ) , propName ) ; continue ; } if ( _anySetter != null ) { try { buffer . bufferAnyProperty ( _anySetter , propName , _anySetter . deserialize ( p , ctxt ) ) ; } catch ( Exception e ) { wrapAndThrow ( e , _beanType . getRawClass ( ) , propName , ctxt ) ; } continue ; } if ( unknown == null ) { unknown = new TokenBuffer ( p , ctxt ) ; } unknown . writeFieldName ( propName ) ; unknown . copyCurrentStructure ( p ) ; } Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { wrapInstantiationProblem ( e , ctxt ) ; bean = null ; } if ( referrings != null ) { for ( BeanReferring referring : referrings ) { referring . setBean ( bean ) ; } } if ( unknown != null ) { if ( bean . getClass ( ) != _beanType . getRawClass ( ) ) { return handlePolymorphic ( null , ctxt , bean , unknown ) ; } return handleUnknownProperties ( ctxt , bean , unknown ) ; } return bean ; }",
        "summary": "`@JsonIdentityInfo` deserialization fails with combination of forward references, `@JsonCreator`",
        "Description": "As a follow-up to bug #1255, the patch I provided exposes related deserialization problems.\nI have attached a small project ('jackson-test.zip') to demonstrate these issues. When run with both patches from #1255, the output is provided in the attached 'both.txt'. When run with just the first patch from #1255, the output is provided in the attached 'first.txt'.\nImportant points:\n1. When the object expressed as an id is contained within a collection or map (List in this example), deserialization works correctly. When it is a field of an object, deserialization is broken.\n2. This particular example doesn't have forward references, but it does have cycles. Nevertheless, I have seen situations where non-cyclical forward-references also do not deserialize properly, with the same caveat as in 1.\n[jackson-test.zip](https://github.com/FasterXML/jackson-databind/files/301884/jackson-test.zip)\n[both.txt](https://github.com/FasterXML/jackson-databind/files/301885/both.txt)\n[first.txt](https://github.com/FasterXML/jackson-databind/files/301886/first.txt)\n",
        "issue_url": null,
        "comments": [
            {
                "content": "I've attached a new version of the test project that also includes forward references. You can see that problem is the same for the cyclical reference to the parent and for the forward reference.\n[jackson-test2.zip](https://github.com/FasterXML/jackson-databind/files/301899/jackson-test2.zip)\n[forward-both.txt](https://github.com/FasterXML/jackson-databind/files/301900/forward-both.txt)\n[forward-first.txt](https://github.com/FasterXML/jackson-databind/files/301901/forward-first.txt)\n"
            },
            {
                "content": "I think I'm experiencing a similar issue regarding deserialization and cyclic references - during deserialization, Jackson attempts to resolve some fields (like the `@class` field) in a wrong object (wrong scope, the scope before). This results in exceptions like `Unexpected token (END_OBJECT), expected FIELD_NAME: missing property '@class' that is to contain type id` but also `Unrecognized field`. Very hard to explain and visualize. @arifogel Do you think this is relevant to the original issue?\n"
            },
            {
                "content": "What happens is that when an unresolved reference is encountered, an UnresolvedForwardReference is thrown. If you look at the code, this is only caught properly in CollectionDeserializer and MapDeserializer. It does not appear to be handled correctly in BeanDeserializer. So what happens is that when an UnresolvedReference is encountered as a field of a bean, an exception gets thrown that goes down the stack until a MapDeserializer or a CollectionDeserializer catch block is encountered. If there is no such deserializer on the stack, then the error message that eventually gets output is correct (a message about an unresolved forward reference). However if it gets caught in a MapDeserializer or CollectionDeserializer, then all hell breaks loose. Deserialization continues on the bean fields in JSON, but the deserializer thinks it's in a map or collection down the stack. So then nonsensical error messages get output about how the next field after the UnresolvedForwardReference is not compatible with the value type in the map or collection being deserialized.\nI attempted to fix this by adding catch block in BeanDeserializer (the 2nd patch I provided in discussion of bug #1255), but while it stops the crashes, the resulting deserialized objects can still have incorrect values in the case of forward/cyclical references (as noted in this bug description).\nEDIT:\n@jannispl is this explanation consistent with your observations?\n"
            },
            {
                "content": "Lack of catching for beans is particularly puzzling since that is the main use case.\nSo going back to the original pr, #388 (and issue #351) (they were missing from release notes for reason, added), I think that bean property handling should be included via `ObjectIdReferenceProperty`, and question would be why this isn't triggered in test cases. I will try to see what tests uncover.\n"
            },
            {
                "content": "Hmmh. So tests use Creators (constructors). That's typically something that does not mix very well with Object ids, so that's bit of a warning sign. But would explain why it could be an as-of-yet-not-working case.\n"
            },
            {
                "content": "Yes. Unfortunately I cannot avoid using creators in my use case without very significant rewrites to my data model. So I am effectively blocked on this bug at work. Please let me know if there is anything else I can do to help.\n"
            },
            {
                "content": "Oh and I should comment that the Creators don't seem to be a problem when the field is wrapped in a list (as in the test cases provided), so I'm hoping it won't be too hard to generalize what has been done for objects in maps and collections to beans.\n"
            },
            {
                "content": "Looks like failure that I see is `No _idValue when handleIdValue called`, so in some ways it looks like initialization might be missing. This could be a good sign.\n\nFundamental theoretical problem with Creators is that not all cycles can be ever resolved: if a refers to b, and b refers to a (directly or indirectly), then only one of references can be passed via creator.\nSo although I hope many cases can be supported, there are some hard limits to keep in mind.\n\nAnother thing to keep in mind is this: even when using `@JsonCreator`, it is also possible to use setters: so -- for example -- all non-reference properties can be passed via Creator, and references then passed via setter (or directly assigned to Field).\nSo hybrid schemes are possible.\n\nI think I will try to see if locally modifying properties to use setters or fields would make specific test pass. That gives some information on where problems reside.\n"
            },
            {
                "content": "But why does this case work properly for the parentAsList property, but not\nthe parent property? This suggests to be that we are not dealing with a\nfundamental theoretical limitation here, but an implementation problem.\nOn Jun 7, 2016 8:59 PM, \"Tatu Saloranta\" notifications@github.com wrote:\n\n> Looks like failure that I see is No _idValue when handleIdValue called,\n> so in some ways it looks like initialization might be missing. This could\n> be a good sign.\n> \n> Fundamental theoretical problem with Creators is that not all cycles can\n> be ever resolved: if a refers to b, and b refers to a (directly or\n> indirectly), then only one of references can be passed via creator.\n> So although I hope many cases can be supported, there are some hard limits\n> to keep in mind.\n> \n> Another thing to keep in mind is this: even when using @JsonCreator, it\n> is also possible to use setters: so -- for example -- all non-reference\n> properties can be passed via Creator, and references then passed via setter\n> (or directly assigned to Field).\n> So hybrid schemes are possible.\n> \n> I think I will try to see if locally modifying properties to use setters\n> or fields would make specific test pass. That gives some information on\n> where problems reside.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/FasterXML/jackson-databind/issues/1261#issuecomment-224481212,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AHYSEneiHN6j2ScTgguyVlo1U_IcgSU2ks5qJj4agaJpZM4IvkKy\n> .\n"
            },
            {
                "content": "@arifogel I agree. But I did want to mention eventual challenge in trying to use Creators for Object Id references, to make sure limitations are not a surprise.\n"
            },
            {
                "content": "So what discipline do you recommend exactly?\nShould JsonCreator still set every field?\nShould reference fields always be non-final and have a corresponding setter\n(and is that irrelevant when using JsonCreator)?\nHow exactly does one instantiate the hybrid approach?\nOn Jun 7, 2016 9:14 PM, \"Tatu Saloranta\" notifications@github.com wrote:\n\n> @arifogel https://github.com/arifogel I agree. But I did want to\n> mention eventual challenge in trying to use Creators for Object Id\n> references, to make sure limitations are not a surprise.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/FasterXML/jackson-databind/issues/1261#issuecomment-224482826,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AHYSEj42CDhAespjTzS89wRPVukhgdMLks5qJkG3gaJpZM4IvkKy\n> .\n"
            },
            {
                "content": "@arifogel I would suggest using fields (or setters) for reference properties (except that Collection/Map values appear to be safe as per your comments), and creator for everything else. Nice thing about using fields is that it is possible to keep them private or protected.\n\nAs to resolving issues with creator-passed references: looks like test case passes if only fields (or setters) are used, for what that is worth. I did not try anything fancier, just simple replacement.\nNext step is to reintroduce a failure and see why expected handling is by-passed.\nIt is possible that this could be related to other known issues with Creator property handling (to be worked on 2.9 I hope). That would be both good and bad; good in that it would get resolved along with other work; bad in that I know that work involved is sizable and requires rewrite of property discovery and resolution.\n"
            },
            {
                "content": "@arifogel I actually suspect the example case is indeed impossible (either in general, even with straight java; or with the Jackson deserialization works, more on that below): note that setup code itself does not pass `child5` as \"favorite child\" for constructor of `parent` -- it can't. Instead test calls setter for that.\nThis is needed to break the cycle.\n\nSo what object model needs to do is the same here; remove \"favorite child\" property from constructor, and leave setter. With that modification test passes locally for me.\n\nAs to Jackson limitations: when dealing with Creators, all parameters must be resolvable when matching JSON Object is complete. Unlike with setter/field injection where deferral of Object Id resolution is possible with catching of exception, it can not be done with Creators because they can only be called once; and further Creator must be called to create the instance. For Collections this is different: they are not created using Creator, but simple no-arguments constructor; and elements may be added afterwards. It would be possible to force failure if custom Creator creator, taking all elements as array/Collection argument, was used; test does not do it and I don't think it is common usage pattern. I think this explains difference you saw wrt Collection/Map case compared to POJOs-with-creator.\n\nI'll try to think of better exception to throw, however; current message is not useful at all.\n"
            },
            {
                "content": "Thanks! I don't think I need to set references in constructors in my main\nproject, so at least now I know how to modify my code to avoid this problem.\nOn Jun 7, 2016 10:14 PM, \"Tatu Saloranta\" notifications@github.com wrote:\n\n> @arifogel https://github.com/arifogel I actually suspect the example\n> case is indeed impossible (either in general, even with straight java; or\n> with the Jackson deserialization works, more on that below): note that\n> setup code itself does not pass child5 as \"favorite child\" for\n> constructor of parent -- it can't. Instead test calls setter for that.\n> This is needed to break the cycle.\n> \n> So what object model needs to do is the same here; remove \"favorite child\"\n> property from constructor, and leave setter. With that modification test\n> passes locally for me.\n> \n> As to Jackson limitations: when dealing with Creators, all parameters must\n> be resolvable when matching JSON Object is complete. Unlike with\n> setter/field injection where deferral of Object Id resolution is possible\n> with catching of exception, it can not be done with Creators because they\n> can only be called once; and further Creator must be called to create the\n> instance. For Collections this is different: they are not created using\n> Creator, but simple no-arguments constructor; and elements may be added\n> afterwards. It would be possible to force failure if custom Creator\n> creator, taking all elements as array/Collection argument, was used; test\n> does not do it and I don't think it is common usage pattern. I think this\n> explains difference you saw wrt Collection/Map case compared to\n> POJOs-with-creator.\n> \n> I'll try to think of better exception to throw, however; current message\n> is not useful at all.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/FasterXML/jackson-databind/issues/1261#issuecomment-224489648,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AHYSEuUD55tTQ2cIZzIhdukZPiZPoJ0rks5qJk_MgaJpZM4IvkKy\n> .\n"
            },
            {
                "content": "@arifogel Right, I wanted to make sure there is a work-around. I believe there is still something off with handling, so I hope to play with the setup: I added a modified version as a failing test (one with creators used for everything).\n"
            },
            {
                "content": "OK I implemented the workaround in my code, but I still ran into a similar problem as the one for which I produced the second patch in #1255. The problem is more or less the same: unresolved references for NON-CREATOR properties are properly lazily resolved when they are a value in a collection or map, but not when they are the NON-CREATOR property itself. In the latter case, the exception is caught too far below; the parser continues chomping away at the remaining properties in the bean with the unresolved reference, while the deserializer thinks it's at least one level up from said bean.\n\nSee the new attached patch. I'll follow up later on with a modified small example.\n[patch.txt](https://github.com/FasterXML/jackson-databind/files/305452/patch.txt)\n\nEDIT:\nThis time I did a more comprehensive check to see that my most complex objects are serialized identically both initially and after deserializing and reserializing.\n"
            },
            {
                "content": "On further reflection, I think it may make sense to combine this patch with the 2nd one from #1255, with one modification: the handleResolvedForwardReference function from the 2nd patch from #1255 (in the version of BeanReferring that deals with creator properties) should check to see if the value is null, and if so, throw an Exception stating that there is a cycle of final creator fields among objects.\n\nTo be clear, the purpose of BOTH of these patches is to fix handling of unresolved forward references to objects that are direct bean properties so that they are [correctly] handled the same way as values in collections and maps.\nEDIT:\nIf your comment:\n\n> As to Jackson limitations: when dealing with Creators, all parameters must be resolvable when matching JSON Object is complete. Unlike with setter/field injection where deferral of Object Id resolution is possible with catching of exception, it can not be done with Creators because they can only be called once; and further Creator must be called to create the instance. For Collections this is different: they are not created using Creator, but simple no-arguments constructor; and elements may be added afterwards. It would be possible to force failure if custom Creator creator, taking all elements as array/Collection argument, was used; test does not do it and I don't think it is common usage pattern. I think this explains difference you saw wrt Collection/Map case compared to POJOs-with-creator.\n\napplies to creator properties even when there are no cycles, then never mind about using the 2nd patch from #1255.\n"
            },
            {
                "content": "Hmm.. I'm having trouble reproducing my problem with a small example. Better wait on this..\n"
            },
            {
                "content": "OK @cowtowncoder, I figured it out. My project was taking a different code path than the example you modified. I had JsonCreator functions that took a multitude of non-reference fields that were not being output initially because they were null-valued and I had @JsonInclude(Include.NON_NULL) set. Then when they were being deserialized, since not all creator properties were present, BeanDeserializer._deserializeUsingPropertyBased never thought that we were done with creator properties. As such, non-creator reference properties were deserialized using buffer.bufferProperty(prop, _deserializeWithErrorWrapping(p, ctxt, prop)) in the same function. This call did not have proper handling for unresolved forward references, unlike the deserialize call after the comment \"// or just clean?\".\n\nMy fix was to modify my code to not have creator properties that could be null-valued.\n\nSo basically the patch I've provided in this issue added that handling to the later code path. I should note that if someone constructs JSON by hand with non-creator reference properties appearing before creator properties, this problem may still pop up. I don't think it's reasonable that field ordering in the JSON should impact execution. In fact, the current serialization code is ugly because it refuses to put fields strictly in alphabetical order, but rather puts creator properties first (I assume to prevent this problem). So I still think my patch (or something similar) should be applied, since it appears to enable arbitrary field ordering.\nNow that I know what caused the problem, I can also provide you with a smaller example (when I have some more free time).\n"
            },
            {
                "content": "Here is a small example demonstrating the error. Note that simply by adding '@JsonIgnore' to Child.getParent, you can avert the crash.\n[jackson-test.zip](https://github.com/FasterXML/jackson-databind/files/305765/jackson-test.zip)\n[unmodified.txt](https://github.com/FasterXML/jackson-databind/files/305766/unmodified.txt)\n[with-json-ignore-getparent.txt](https://github.com/FasterXML/jackson-databind/files/305767/with-json-ignore-getparent.txt)\n"
            },
            {
                "content": "Almost forgot to mention: when you apply my patch, then you get the output in with-json-ignore-getparent.txt even without adding the JsonIgnore annotation. This indicates that this is an inconsistency between handling of unresolved forward references in beans vs maps and collections.\nEDIT: I mean you get the correct output for everything when you apply the patch.\n"
            },
            {
                "content": "@arifogel Thanks. I agree in that ordering should not matter; serialization order is mostly optimization, not related to correctness of deserialization (but helps in common case as deserializer can avoid possibly costly buffering; just does not count on it). My main concern with original patch was just that adding second place for handling should not be done to cover other problems, so it'd be good to know why initial handling for bean properties was not working. I guess I still don't fully understand that.\n\nBut I hope looking through examples helps. I think @pgelinas implemented original handling so I'll see if he might have suggestions as well.\n"
            },
            {
                "content": "In updated tests there seems to be some problem with type resolution, so that array of `Child` instances somehow is not recognized as such. Or perhaps token buffering is incorrectly handled. Regardless there is something wrong there; I can see a failure.\n"
            },
            {
                "content": "@arifogel After reading through the latest patch it is nice and small, and does fix the issue! Thank you very much for going through the code and figuring out the solution to this problem. It goes in 2.8.0 (.rc2); I am bit hesitant to try to backport it in 2.7.\n"
            }
        ],
        "summarized_discussion": ""
    },
    "JacksonDatabind_27_src/main/java/com/fasterxml/jackson/databind/deser/BeanDeserializer.java_773_857": {
        "src": "@SuppressWarnings(\"resource\")\n    protected Object deserializeUsingPropertyBasedWithExternalTypeId(JsonParser p, DeserializationContext ctxt)\n        throws IOException\n    {\n        final ExternalTypeHandler ext = _externalTypeIdHandler.start();\n        final PropertyBasedCreator creator = _propertyBasedCreator;\n        PropertyValueBuffer buffer = creator.startBuilding(p, ctxt, _objectIdReader);\n\n        TokenBuffer tokens = new TokenBuffer(p);\n        tokens.writeStartObject();\n\n        JsonToken t = p.getCurrentToken();\n        for (; t == JsonToken.FIELD_NAME; t = p.nextToken()) {\n            String propName = p.getCurrentName();\n            p.nextToken(); // to point to value\n            // creator property?\n            SettableBeanProperty creatorProp = creator.findCreatorProperty(propName);\n            if (creatorProp != null) {\n                // first: let's check to see if this might be part of value with external type id:\n                // 11-Sep-2015, tatu: Important; do NOT pass buffer as last arg, but null,\n                //   since it is not the bean\n                if (ext.handlePropertyValue(p, ctxt, propName, buffer)) {\n                    ;\n                } else {\n                    // Last creator property to set?\n                    if (buffer.assignParameter(creatorProp, _deserializeWithErrorWrapping(p, ctxt, creatorProp))) {\n                        t = p.nextToken(); // to move to following FIELD_NAME/END_OBJECT\n                        Object bean;\n                        try {\n                            bean = creator.build(ctxt, buffer);\n                        } catch (Exception e) {\n                            wrapAndThrow(e, _beanType.getRawClass(), propName, ctxt);\n                            continue; // never gets here\n                        }\n                        // if so, need to copy all remaining tokens into buffer\n                        while (t == JsonToken.FIELD_NAME) {\n                            p.nextToken(); // to skip name\n                            tokens.copyCurrentStructure(p);\n                            t = p.nextToken();\n                        }\n                        if (bean.getClass() != _beanType.getRawClass()) {\n                            // !!! 08-Jul-2011, tatu: Could theoretically support; but for now\n                            //   it's too complicated, so bail out\n                            throw ctxt.mappingException(\"Can not create polymorphic instances with unwrapped values\");\n                        }\n                        return ext.complete(p, ctxt, bean);\n                    }\n                }\n                continue;\n            }\n            // Object Id property?\n            if (buffer.readIdProperty(propName)) {\n                continue;\n            }\n            // regular property? needs buffering\n            SettableBeanProperty prop = _beanProperties.find(propName);\n            if (prop != null) {\n                buffer.bufferProperty(prop, prop.deserialize(p, ctxt));\n                continue;\n            }\n            // external type id (or property that depends on it)?\n            if (ext.handlePropertyValue(p, ctxt, propName, null)) {\n                continue;\n            }\n            /* As per [JACKSON-313], things marked as ignorable should not be\n             * passed to any setter\n             */\n            if (_ignorableProps != null && _ignorableProps.contains(propName)) {\n                handleIgnoredProperty(p, ctxt, handledType(), propName);\n                continue;\n            }\n            // \"any property\"?\n            if (_anySetter != null) {\n                buffer.bufferAnyProperty(_anySetter, propName, _anySetter.deserialize(p, ctxt));\n            }\n        }\n\n        // We hit END_OBJECT; resolve the pieces:\n        try {\n            return ext.complete(p, ctxt, buffer, creator);\n        } catch (Exception e) {\n            wrapInstantiationProblem(e, ctxt);\n            return null; // never gets here\n        }\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"resource\" ) protected Object deserializeUsingPropertyBasedWithExternalTypeId ( JsonParser p , DeserializationContext ctxt ) throws IOException { final ExternalTypeHandler ext = _externalTypeIdHandler . start ( ) ; final PropertyBasedCreator creator = _propertyBasedCreator ; PropertyValueBuffer buffer = creator . startBuilding ( p , ctxt , _objectIdReader ) ; TokenBuffer tokens = new TokenBuffer ( p ) ; tokens . writeStartObject ( ) ; JsonToken t = p . getCurrentToken ( ) ; for ( ; t == JsonToken . FIELD_NAME ; t = p . nextToken ( ) ) { String propName = p . getCurrentName ( ) ; p . nextToken ( ) ; SettableBeanProperty creatorProp = creator . findCreatorProperty ( propName ) ; if ( creatorProp != null ) { if ( ext . handlePropertyValue ( p , ctxt , propName , buffer ) ) { ; } else { if ( buffer . assignParameter ( creatorProp , _deserializeWithErrorWrapping ( p , ctxt , creatorProp ) ) ) { t = p . nextToken ( ) ; Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { wrapAndThrow ( e , _beanType . getRawClass ( ) , propName , ctxt ) ; continue ; } while ( t == JsonToken . FIELD_NAME ) { p . nextToken ( ) ; tokens . copyCurrentStructure ( p ) ; t = p . nextToken ( ) ; } if ( bean . getClass ( ) != _beanType . getRawClass ( ) ) { throw ctxt . mappingException ( \"Can not create polymorphic instances with unwrapped values\" ) ; } return ext . complete ( p , ctxt , bean ) ; } } continue ; } if ( buffer . readIdProperty ( propName ) ) { continue ; } SettableBeanProperty prop = _beanProperties . find ( propName ) ; if ( prop != null ) { buffer . bufferProperty ( prop , prop . deserialize ( p , ctxt ) ) ; continue ; } if ( ext . handlePropertyValue ( p , ctxt , propName , null ) ) { continue ; } if ( _ignorableProps != null && _ignorableProps . contains ( propName ) ) { handleIgnoredProperty ( p , ctxt , handledType ( ) , propName ) ; continue ; } if ( _anySetter != null ) { buffer . bufferAnyProperty ( _anySetter , propName , _anySetter . deserialize ( p , ctxt ) ) ; } } try { return ext . complete ( p , ctxt , buffer , creator ) ; } catch ( Exception e ) { wrapInstantiationProblem ( e , ctxt ) ; return null ; } }",
        "fixed_src": "@SuppressWarnings(\"resource\")\n    protected Object deserializeUsingPropertyBasedWithExternalTypeId(JsonParser p, DeserializationContext ctxt)\n        throws IOException\n    {\n        final ExternalTypeHandler ext = _externalTypeIdHandler.start();\n        final PropertyBasedCreator creator = _propertyBasedCreator;\n        PropertyValueBuffer buffer = creator.startBuilding(p, ctxt, _objectIdReader);\n\n        TokenBuffer tokens = new TokenBuffer(p);\n        tokens.writeStartObject();\n\n        JsonToken t = p.getCurrentToken();\n        for (; t == JsonToken.FIELD_NAME; t = p.nextToken()) {\n            String propName = p.getCurrentName();\n            p.nextToken(); // to point to value\n            // creator property?\n            SettableBeanProperty creatorProp = creator.findCreatorProperty(propName);\n            if (creatorProp != null) {\n                // first: let's check to see if this might be part of value with external type id:\n                // 11-Sep-2015, tatu: Important; do NOT pass buffer as last arg, but null,\n                //   since it is not the bean\n                if (ext.handlePropertyValue(p, ctxt, propName, null)) {\n                    ;\n                } else {\n                    // Last creator property to set?\n                    if (buffer.assignParameter(creatorProp, _deserializeWithErrorWrapping(p, ctxt, creatorProp))) {\n                        t = p.nextToken(); // to move to following FIELD_NAME/END_OBJECT\n                        Object bean;\n                        try {\n                            bean = creator.build(ctxt, buffer);\n                        } catch (Exception e) {\n                            wrapAndThrow(e, _beanType.getRawClass(), propName, ctxt);\n                            continue; // never gets here\n                        }\n                        // if so, need to copy all remaining tokens into buffer\n                        while (t == JsonToken.FIELD_NAME) {\n                            p.nextToken(); // to skip name\n                            tokens.copyCurrentStructure(p);\n                            t = p.nextToken();\n                        }\n                        if (bean.getClass() != _beanType.getRawClass()) {\n                            // !!! 08-Jul-2011, tatu: Could theoretically support; but for now\n                            //   it's too complicated, so bail out\n                            throw ctxt.mappingException(\"Can not create polymorphic instances with unwrapped values\");\n                        }\n                        return ext.complete(p, ctxt, bean);\n                    }\n                }\n                continue;\n            }\n            // Object Id property?\n            if (buffer.readIdProperty(propName)) {\n                continue;\n            }\n            // regular property? needs buffering\n            SettableBeanProperty prop = _beanProperties.find(propName);\n            if (prop != null) {\n                buffer.bufferProperty(prop, prop.deserialize(p, ctxt));\n                continue;\n            }\n            // external type id (or property that depends on it)?\n            if (ext.handlePropertyValue(p, ctxt, propName, null)) {\n                continue;\n            }\n            /* As per [JACKSON-313], things marked as ignorable should not be\n             * passed to any setter\n             */\n            if (_ignorableProps != null && _ignorableProps.contains(propName)) {\n                handleIgnoredProperty(p, ctxt, handledType(), propName);\n                continue;\n            }\n            // \"any property\"?\n            if (_anySetter != null) {\n                buffer.bufferAnyProperty(_anySetter, propName, _anySetter.deserialize(p, ctxt));\n            }\n        }\n\n        // We hit END_OBJECT; resolve the pieces:\n        try {\n            return ext.complete(p, ctxt, buffer, creator);\n        } catch (Exception e) {\n            wrapInstantiationProblem(e, ctxt);\n            return null; // never gets here\n        }\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"resource\" ) protected Object deserializeUsingPropertyBasedWithExternalTypeId ( JsonParser p , DeserializationContext ctxt ) throws IOException { final ExternalTypeHandler ext = _externalTypeIdHandler . start ( ) ; final PropertyBasedCreator creator = _propertyBasedCreator ; PropertyValueBuffer buffer = creator . startBuilding ( p , ctxt , _objectIdReader ) ; TokenBuffer tokens = new TokenBuffer ( p ) ; tokens . writeStartObject ( ) ; JsonToken t = p . getCurrentToken ( ) ; for ( ; t == JsonToken . FIELD_NAME ; t = p . nextToken ( ) ) { String propName = p . getCurrentName ( ) ; p . nextToken ( ) ; SettableBeanProperty creatorProp = creator . findCreatorProperty ( propName ) ; if ( creatorProp != null ) { if ( ext . handlePropertyValue ( p , ctxt , propName , null ) ) { ; } else { if ( buffer . assignParameter ( creatorProp , _deserializeWithErrorWrapping ( p , ctxt , creatorProp ) ) ) { t = p . nextToken ( ) ; Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { wrapAndThrow ( e , _beanType . getRawClass ( ) , propName , ctxt ) ; continue ; } while ( t == JsonToken . FIELD_NAME ) { p . nextToken ( ) ; tokens . copyCurrentStructure ( p ) ; t = p . nextToken ( ) ; } if ( bean . getClass ( ) != _beanType . getRawClass ( ) ) { throw ctxt . mappingException ( \"Can not create polymorphic instances with unwrapped values\" ) ; } return ext . complete ( p , ctxt , bean ) ; } } continue ; } if ( buffer . readIdProperty ( propName ) ) { continue ; } SettableBeanProperty prop = _beanProperties . find ( propName ) ; if ( prop != null ) { buffer . bufferProperty ( prop , prop . deserialize ( p , ctxt ) ) ; continue ; } if ( ext . handlePropertyValue ( p , ctxt , propName , null ) ) { continue ; } if ( _ignorableProps != null && _ignorableProps . contains ( propName ) ) { handleIgnoredProperty ( p , ctxt , handledType ( ) , propName ) ; continue ; } if ( _anySetter != null ) { buffer . bufferAnyProperty ( _anySetter , propName , _anySetter . deserialize ( p , ctxt ) ) ; } } try { return ext . complete ( p , ctxt , buffer , creator ) ; } catch ( Exception e ) { wrapInstantiationProblem ( e , ctxt ) ; return null ; } }",
        "summary": "Problem deserializing External Type Id if type id comes before POJO",
        "Description": "(note: seems to be similar or related to https://github.com/FasterXML/jackson-module-afterburner/issues/58)\n\nWith 2.6, looks like handling of External Type Id is broken in some rare (?) cases; existing unit tests did not catch this. At this point I am speculating this is due to some refactoring, or change to use more efficient 'nextFieldName()' method.\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug is related to a specific function not working correctly. The solution to the bug is to debug the function and identify the specific line of code that is causing the issue. Once identified, the code can be updated or replaced to fix the bug."
    },
    "Cli_14_src/java/org/apache/commons/cli2/option/GroupImpl.java_237_282": {
        "src": "public void validate(final WriteableCommandLine commandLine)\n        throws OptionException {\n        // number of options found\n        int present = 0;\n\n        // reference to first unexpected option\n        Option unexpected = null;\n\n        for (final Iterator i = options.iterator(); i.hasNext();) {\n            final Option option = (Option) i.next();\n\n            // needs validation?\n            boolean validate = option.isRequired() || option instanceof Group;\n            if (validate) {\n                option.validate(commandLine);\n            }\n\n            // if the child option is present then validate it\n            if (commandLine.hasOption(option)) {\n                if (++present > maximum) {\n                    unexpected = option;\n\n                    break;\n                }\n\n                option.validate(commandLine);\n            }\n        }\n\n        // too many options\n        if (unexpected != null) {\n            throw new OptionException(this, ResourceConstants.UNEXPECTED_TOKEN,\n                                      unexpected.getPreferredName());\n        }\n\n        // too few option\n        if (present < minimum) {\n            throw new OptionException(this, ResourceConstants.MISSING_OPTION);\n        }\n\n        // validate each anonymous argument\n        for (final Iterator i = anonymous.iterator(); i.hasNext();) {\n            final Option option = (Option) i.next();\n            option.validate(commandLine);\n        }\n    }",
        "src_wo_comments": "public void validate ( final WriteableCommandLine commandLine ) throws OptionException { int present = 0 ; Option unexpected = null ; for ( final Iterator i = options . iterator ( ) ; i . hasNext ( ) ; ) { final Option option = ( Option ) i . next ( ) ; boolean validate = option . isRequired ( ) || option instanceof Group ; if ( validate ) { option . validate ( commandLine ) ; } if ( commandLine . hasOption ( option ) ) { if ( ++ present > maximum ) { unexpected = option ; break ; } option . validate ( commandLine ) ; } } if ( unexpected != null ) { throw new OptionException ( this , ResourceConstants . UNEXPECTED_TOKEN , unexpected . getPreferredName ( ) ) ; } if ( present < minimum ) { throw new OptionException ( this , ResourceConstants . MISSING_OPTION ) ; } for ( final Iterator i = anonymous . iterator ( ) ; i . hasNext ( ) ; ) { final Option option = ( Option ) i . next ( ) ; option . validate ( commandLine ) ; } }",
        "fixed_src": "public void validate(final WriteableCommandLine commandLine)\n        throws OptionException {\n        // number of options found\n        int present = 0;\n\n        // reference to first unexpected option\n        Option unexpected = null;\n\n        for (final Iterator i = options.iterator(); i.hasNext();) {\n            final Option option = (Option) i.next();\n\n            // needs validation?\n            boolean validate = option.isRequired() || option instanceof Group;\n\n            // if the child option is present then validate it\n            if (commandLine.hasOption(option)) {\n                if (++present > maximum) {\n                    unexpected = option;\n\n                    break;\n                }\n                validate = true;\n            }\n\n            if (validate) {\n                option.validate(commandLine);\n            }\n        }\n\n        // too many options\n        if (unexpected != null) {\n            throw new OptionException(this, ResourceConstants.UNEXPECTED_TOKEN,\n                                      unexpected.getPreferredName());\n        }\n\n        // too few option\n        if (present < minimum) {\n            throw new OptionException(this, ResourceConstants.MISSING_OPTION);\n        }\n\n        // validate each anonymous argument\n        for (final Iterator i = anonymous.iterator(); i.hasNext();) {\n            final Option option = (Option) i.next();\n            option.validate(commandLine);\n        }\n    }",
        "fixed_src_wo_comments": "public void validate ( final WriteableCommandLine commandLine ) throws OptionException { int present = 0 ; Option unexpected = null ; for ( final Iterator i = options . iterator ( ) ; i . hasNext ( ) ; ) { final Option option = ( Option ) i . next ( ) ; boolean validate = option . isRequired ( ) || option instanceof Group ; if ( commandLine . hasOption ( option ) ) { if ( ++ present > maximum ) { unexpected = option ; break ; } validate = true ; } if ( validate ) { option . validate ( commandLine ) ; } } if ( unexpected != null ) { throw new OptionException ( this , ResourceConstants . UNEXPECTED_TOKEN , unexpected . getPreferredName ( ) ) ; } if ( present < minimum ) { throw new OptionException ( this , ResourceConstants . MISSING_OPTION ) ; } for ( final Iterator i = anonymous . iterator ( ) ; i . hasNext ( ) ; ) { final Option option = ( Option ) i . next ( ) ; option . validate ( commandLine ) ; } }",
        "summary": "adding a FileValidator results in ClassCastException in parser.parseAndHelp(args)",
        "Description": "When I add a FileValidator.getExistingFileInstance() to an Argument, I get a ClassCastException when I parse args.\n\nBelow is a testcase invoke with\n\n   java org.apache.commons.cli2.issues.CLI2Sample -classpath commons-cli-2.0-SNAPSHOT.jar --file-name path-to-an-existing-file\n\nRun it and you get:\n\nException in thread \"main\" java.lang.ClassCastException: java.io.File cannot be cast to java.lang.String\n\tat org.apache.commons.cli2.validation.FileValidator.validate(FileValidator.java:122)\n\tat org.apache.commons.cli2.option.ArgumentImpl.validate(ArgumentImpl.java:250)\n\tat org.apache.commons.cli2.option.ParentImpl.validate(ParentImpl.java:123)\n\tat org.apache.commons.cli2.option.DefaultOption.validate(DefaultOption.java:175)\n\tat org.apache.commons.cli2.option.GroupImpl.validate(GroupImpl.java:264)\n\tat org.apache.commons.cli2.commandline.Parser.parse(Parser.java:105)\n\tat org.apache.commons.cli2.commandline.Parser.parseAndHelp(Parser.java:125)\n\tat org.apache.commons.cli2.issues.CLI2Sample.main(CLI2Sample.java:38)\n\nComment out the withValidator call and it runs with no exception. \n\nI also get a similar ClassCastException if I add a \n\n  .withValidator(NumberValidator.getIntegerInstance())\n\nto another option/argument.\n\nHere is the source\n\n\npackage org.apache.commons.cli2.issues;\n\nimport java.io.File;\nimport org.apache.commons.cli2.CommandLine;\nimport org.apache.commons.cli2.Group;\nimport org.apache.commons.cli2.builder.ArgumentBuilder;\nimport org.apache.commons.cli2.builder.DefaultOptionBuilder;\nimport org.apache.commons.cli2.builder.GroupBuilder;\nimport org.apache.commons.cli2.commandline.Parser;\nimport org.apache.commons.cli2.option.DefaultOption;\nimport org.apache.commons.cli2.validation.FileValidator;\n\npublic class CLI2Sample\n{\n   public static void main(String[] args)\n   {\n      final DefaultOptionBuilder obuilder = new DefaultOptionBuilder();\n      final ArgumentBuilder abuilder = new ArgumentBuilder();\n      final GroupBuilder gbuilder = new GroupBuilder();\n      DefaultOption fileNameOption = obuilder\n            .withShortName(\"f\")\n            .withLongName(\"file-name\")\n            .withRequired(true)\n            .withDescription(\"name of an existing file\")\n            .withArgument(abuilder\n                  .withName(\"file-name\")\n                  .withValidator(FileValidator.getExistingFileInstance())\n                  .create())\n            .create();\n      Group options = gbuilder\n            .withName(\"options\")\n            .withOption(fileNameOption)\n            .create();\n      Parser parser = new Parser();\n      parser.setHelpTrigger(\"--help\");\n      parser.setGroup(options);\n      CommandLine cl = parser.parseAndHelp(args);\n     }\n}\n",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-144",
        "comments": [
            "cleaned up sample source a bit",
            "No FileValidator or NumberValidator can be called twice on the same option (they are not idempotent). They take a String in the values list and replace it with into a File or some number class, and then fail with a ClassCastException the second time they are called, since the value can no longer be cast into a String.\n\nThe problem seems to be in org.apache.commons.cli2.option.GroupImpl.validate(WriteableCommandLine), where the option.validate() can be called more than once. Making the if blocks exclusive solves the problem, as per the patch below, but breaks org.apache.commons.cli2.option.GroupTest.testValidate_RequiredChild()... or rather, that test case is not testing the expected result, and the patch exposes the problem.\n\nIndex: src/java/org/apache/commons/cli2/option/GroupImpl.java\n===================================================================\n--- src/java/org/apache/commons/cli2/option/GroupImpl.java      (revision 594834)\n+++ src/java/org/apache/commons/cli2/option/GroupImpl.java      (working copy)\n@@ -248,11 +248,11 @@\n             // if the child option is required then validate it\n             if (option.isRequired()) {\n                 option.validate(commandLine);\n-            }\n+            } else\n \n             if (option instanceof Group) {\n                 option.validate(commandLine);\n-            }\n+            } else \n \n             // if the child option is present then validate it\n             if (commandLine.hasOption(option)) {\n\n",
            "patch works for me",
            "testcase. currently in bug package but can be migrated to org.apache.commons.cli2.validation when the bug is fixed",
            "patch. I find this resolves the defect and also does not break the other test.",
            "See the attachment https://issues.apache.org/jira/secure/attachment/12369551/CLI-144.patch\n\nBased on the patch by Dioktos, this patch resolves CLI-144 but does not cause org.apache.commons.cli2.option.GroupTest.testValidate_RequiredChild() to fail.\n\nI get a clean Maven build (tho I had to change pom.xml to exclude the org.apache.commons.cli2.bug package with the following:\n\nIndex: C:/Java/jakarta-commons/commons-cli/Commons CLI/pom.xml\n===================================================================\n--- C:/Java/jakarta-commons/commons-cli/Commons CLI/pom.xml\t(revision 594914)\n+++ C:/Java/jakarta-commons/commons-cli/Commons CLI/pom.xml\t(working copy)\n@@ -165,6 +165,17 @@\n           </includes>\n         </testResource>\n     </testResources>\n-  </build>\n+    <plugins>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-surefire-plugin</artifactId>\n+        <configuration>\n+          <excludes>\n+            <exclude>org/apache/commons/cli2/bug/*.java</exclude>\n+          </excludes>\n+        </configuration>\n+      </plugin>\n+    </plugins>\n+    </build>\n \n </project>\n",
            "One more try: The way I understand it, if any of the three conditions in the options.iterator loop are satisfied, the code should 1. validate the option and 2. increment the present counter.\n\nIndex: src/java/org/apache/commons/cli2/option/GroupImpl.java\n===================================================================\n--- src/java/org/apache/commons/cli2/option/GroupImpl.java      (revision 595359)\n+++ src/java/org/apache/commons/cli2/option/GroupImpl.java      (working copy)\n@@ -244,25 +244,14 @@\n \n         for (final Iterator i = options.iterator(); i.hasNext();) {\n             final Option option = (Option) i.next();\n-\n-            // if the child option is required then validate it\n-            if (option.isRequired()) {\n+            \n+            if (option.isRequired() || option instanceof Group || commandLine.hasOption(option)) {\n                 option.validate(commandLine);\n-            }\n-\n-            if (option instanceof Group) {\n-                option.validate(commandLine);\n-            }\n-\n-            // if the child option is present then validate it\n-            if (commandLine.hasOption(option)) {\n                 if (++present > maximum) {\n                     unexpected = option;\n \n                     break;\n                 }\n-\n-                option.validate(commandLine);\n             }\n         }\n",
            "Dioktos's 15/Nov/07 12:34 PM patch works for me: clean Maven build and CLI144.java all run green\n\nCan a CLI2 committer also consider my pom.xml patch (or should I submit another bug for that?)\n",
            "A slightly modified version of the unit test provided by David Biesack. This version also tests whether the expected file is returned as the value of the option. I(t also adds the license header.)",
            "A patch based on the ideas discussed in this ticket. It guarantees that the validate() method is called only once.\n\nThe unit tests pass.",
            "Applied the proposed fix. The validate() method of an option that is part of a group is now called only once. Thus, the validators are also invoked only once and do not try converting a value multiple times - which caused the ClassCastException."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to modify the GroupImpl.validate() method so that the option.validate() is called only once. This is accomplished by making the if blocks exclusive, as per the patch provided. Additionally, the unit tests have been modified to test the expected result, and the patch does not break the other test."
    },
    "Math_15_src/main/java/org/apache/commons/math3/util/FastMath.java_1441_1599": {
        "src": "public static double pow(double x, double y) {\n        final double lns[] = new double[2];\n\n        if (y == 0.0) {\n            return 1.0;\n        }\n\n        if (x != x) { // X is NaN\n            return x;\n        }\n\n\n        if (x == 0) {\n            long bits = Double.doubleToLongBits(x);\n            if ((bits & 0x8000000000000000L) != 0) {\n                // -zero\n                long yi = (long) y;\n\n                if (y < 0 && y == yi && (yi & 1) == 1) {\n                    return Double.NEGATIVE_INFINITY;\n                }\n\n                if (y > 0 && y == yi && (yi & 1) == 1) {\n                    return -0.0;\n                }\n            }\n\n            if (y < 0) {\n                return Double.POSITIVE_INFINITY;\n            }\n            if (y > 0) {\n                return 0.0;\n            }\n\n            return Double.NaN;\n        }\n\n        if (x == Double.POSITIVE_INFINITY) {\n            if (y != y) { // y is NaN\n                return y;\n            }\n            if (y < 0.0) {\n                return 0.0;\n            } else {\n                return Double.POSITIVE_INFINITY;\n            }\n        }\n\n        if (y == Double.POSITIVE_INFINITY) {\n            if (x * x == 1.0) {\n                return Double.NaN;\n            }\n\n            if (x * x > 1.0) {\n                return Double.POSITIVE_INFINITY;\n            } else {\n                return 0.0;\n            }\n        }\n\n        if (x == Double.NEGATIVE_INFINITY) {\n            if (y != y) { // y is NaN\n                return y;\n            }\n\n            if (y < 0) {\n                long yi = (long) y;\n                if (y == yi && (yi & 1) == 1) {\n                    return -0.0;\n                }\n\n                return 0.0;\n            }\n\n            if (y > 0)  {\n                long yi = (long) y;\n                if (y == yi && (yi & 1) == 1) {\n                    return Double.NEGATIVE_INFINITY;\n                }\n\n                return Double.POSITIVE_INFINITY;\n            }\n        }\n\n        if (y == Double.NEGATIVE_INFINITY) {\n\n            if (x * x == 1.0) {\n                return Double.NaN;\n            }\n\n            if (x * x < 1.0) {\n                return Double.POSITIVE_INFINITY;\n            } else {\n                return 0.0;\n            }\n        }\n\n        /* Handle special case x<0 */\n        if (x < 0) {\n            // y is an even integer in this case\n            if (y >= TWO_POWER_52 || y <= -TWO_POWER_52) {\n                return pow(-x, y);\n            }\n\n            if (y == (long) y) {\n                // If y is an integer\n                return ((long)y & 1) == 0 ? pow(-x, y) : -pow(-x, y);\n            } else {\n                return Double.NaN;\n            }\n        }\n\n        /* Split y into ya and yb such that y = ya+yb */\n        double ya;\n        double yb;\n        if (y < 8e298 && y > -8e298) {\n            double tmp1 = y * HEX_40000000;\n            ya = y + tmp1 - tmp1;\n            yb = y - ya;\n        } else {\n            double tmp1 = y * 9.31322574615478515625E-10;\n            double tmp2 = tmp1 * 9.31322574615478515625E-10;\n            ya = (tmp1 + tmp2 - tmp1) * HEX_40000000 * HEX_40000000;\n            yb = y - ya;\n        }\n\n        /* Compute ln(x) */\n        final double lores = log(x, lns);\n        if (Double.isInfinite(lores)){ // don't allow this to be converted to NaN\n            return lores;\n        }\n\n        double lna = lns[0];\n        double lnb = lns[1];\n\n        /* resplit lns */\n        double tmp1 = lna * HEX_40000000;\n        double tmp2 = lna + tmp1 - tmp1;\n        lnb += lna - tmp2;\n        lna = tmp2;\n\n        // y*ln(x) = (aa+ab)\n        final double aa = lna * ya;\n        final double ab = lna * yb + lnb * ya + lnb * yb;\n\n        lna = aa+ab;\n        lnb = -(lna - aa - ab);\n\n        double z = 1.0 / 120.0;\n        z = z * lnb + (1.0 / 24.0);\n        z = z * lnb + (1.0 / 6.0);\n        z = z * lnb + 0.5;\n        z = z * lnb + 1.0;\n        z = z * lnb;\n\n        final double result = exp(lna, z, null);\n        //result = result + result * z;\n        return result;\n    }",
        "src_wo_comments": "public static double pow ( double x , double y ) { final double lns [ ] = new double [ 2 ] ; if ( y == 0.0 ) { return 1.0 ; } if ( x != x ) { return x ; } if ( x == 0 ) { long bits = Double . doubleToLongBits ( x ) ; if ( ( bits & 0x8000000000000000L ) != 0 ) { long yi = ( long ) y ; if ( y < 0 && y == yi && ( yi & 1 ) == 1 ) { return Double . NEGATIVE_INFINITY ; } if ( y > 0 && y == yi && ( yi & 1 ) == 1 ) { return - 0.0 ; } } if ( y < 0 ) { return Double . POSITIVE_INFINITY ; } if ( y > 0 ) { return 0.0 ; } return Double . NaN ; } if ( x == Double . POSITIVE_INFINITY ) { if ( y != y ) { return y ; } if ( y < 0.0 ) { return 0.0 ; } else { return Double . POSITIVE_INFINITY ; } } if ( y == Double . POSITIVE_INFINITY ) { if ( x * x == 1.0 ) { return Double . NaN ; } if ( x * x > 1.0 ) { return Double . POSITIVE_INFINITY ; } else { return 0.0 ; } } if ( x == Double . NEGATIVE_INFINITY ) { if ( y != y ) { return y ; } if ( y < 0 ) { long yi = ( long ) y ; if ( y == yi && ( yi & 1 ) == 1 ) { return - 0.0 ; } return 0.0 ; } if ( y > 0 ) { long yi = ( long ) y ; if ( y == yi && ( yi & 1 ) == 1 ) { return Double . NEGATIVE_INFINITY ; } return Double . POSITIVE_INFINITY ; } } if ( y == Double . NEGATIVE_INFINITY ) { if ( x * x == 1.0 ) { return Double . NaN ; } if ( x * x < 1.0 ) { return Double . POSITIVE_INFINITY ; } else { return 0.0 ; } } if ( x < 0 ) { if ( y >= TWO_POWER_52 || y <= - TWO_POWER_52 ) { return pow ( - x , y ) ; } if ( y == ( long ) y ) { return ( ( long ) y & 1 ) == 0 ? pow ( - x , y ) : - pow ( - x , y ) ; } else { return Double . NaN ; } } double ya ; double yb ; if ( y < 8e298 && y > - 8e298 ) { double tmp1 = y * HEX_40000000 ; ya = y + tmp1 - tmp1 ; yb = y - ya ; } else { double tmp1 = y * 9.31322574615478515625E-10 ; double tmp2 = tmp1 * 9.31322574615478515625E-10 ; ya = ( tmp1 + tmp2 - tmp1 ) * HEX_40000000 * HEX_40000000 ; yb = y - ya ; } final double lores = log ( x , lns ) ; if ( Double . isInfinite ( lores ) ) { return lores ; } double lna = lns [ 0 ] ; double lnb = lns [ 1 ] ; double tmp1 = lna * HEX_40000000 ; double tmp2 = lna + tmp1 - tmp1 ; lnb += lna - tmp2 ; lna = tmp2 ; final double aa = lna * ya ; final double ab = lna * yb + lnb * ya + lnb * yb ; lna = aa + ab ; lnb = - ( lna - aa - ab ) ; double z = 1.0 / 120.0 ; z = z * lnb + ( 1.0 / 24.0 ) ; z = z * lnb + ( 1.0 / 6.0 ) ; z = z * lnb + 0.5 ; z = z * lnb + 1.0 ; z = z * lnb ; final double result = exp ( lna , z , null ) ; return result ; }",
        "fixed_src": "public static double pow(double x, double y) {\n        final double lns[] = new double[2];\n\n        if (y == 0.0) {\n            return 1.0;\n        }\n\n        if (x != x) { // X is NaN\n            return x;\n        }\n\n\n        if (x == 0) {\n            long bits = Double.doubleToLongBits(x);\n            if ((bits & 0x8000000000000000L) != 0) {\n                // -zero\n                long yi = (long) y;\n\n                if (y < 0 && y == yi && (yi & 1) == 1) {\n                    return Double.NEGATIVE_INFINITY;\n                }\n\n                if (y > 0 && y == yi && (yi & 1) == 1) {\n                    return -0.0;\n                }\n            }\n\n            if (y < 0) {\n                return Double.POSITIVE_INFINITY;\n            }\n            if (y > 0) {\n                return 0.0;\n            }\n\n            return Double.NaN;\n        }\n\n        if (x == Double.POSITIVE_INFINITY) {\n            if (y != y) { // y is NaN\n                return y;\n            }\n            if (y < 0.0) {\n                return 0.0;\n            } else {\n                return Double.POSITIVE_INFINITY;\n            }\n        }\n\n        if (y == Double.POSITIVE_INFINITY) {\n            if (x * x == 1.0) {\n                return Double.NaN;\n            }\n\n            if (x * x > 1.0) {\n                return Double.POSITIVE_INFINITY;\n            } else {\n                return 0.0;\n            }\n        }\n\n        if (x == Double.NEGATIVE_INFINITY) {\n            if (y != y) { // y is NaN\n                return y;\n            }\n\n            if (y < 0) {\n                long yi = (long) y;\n                if (y == yi && (yi & 1) == 1) {\n                    return -0.0;\n                }\n\n                return 0.0;\n            }\n\n            if (y > 0)  {\n                long yi = (long) y;\n                if (y == yi && (yi & 1) == 1) {\n                    return Double.NEGATIVE_INFINITY;\n                }\n\n                return Double.POSITIVE_INFINITY;\n            }\n        }\n\n        if (y == Double.NEGATIVE_INFINITY) {\n\n            if (x * x == 1.0) {\n                return Double.NaN;\n            }\n\n            if (x * x < 1.0) {\n                return Double.POSITIVE_INFINITY;\n            } else {\n                return 0.0;\n            }\n        }\n\n        /* Handle special case x<0 */\n        if (x < 0) {\n            // y is an even integer in this case\n            if (y >= TWO_POWER_53 || y <= -TWO_POWER_53) {\n                return pow(-x, y);\n            }\n\n            if (y == (long) y) {\n                // If y is an integer\n                return ((long)y & 1) == 0 ? pow(-x, y) : -pow(-x, y);\n            } else {\n                return Double.NaN;\n            }\n        }\n\n        /* Split y into ya and yb such that y = ya+yb */\n        double ya;\n        double yb;\n        if (y < 8e298 && y > -8e298) {\n            double tmp1 = y * HEX_40000000;\n            ya = y + tmp1 - tmp1;\n            yb = y - ya;\n        } else {\n            double tmp1 = y * 9.31322574615478515625E-10;\n            double tmp2 = tmp1 * 9.31322574615478515625E-10;\n            ya = (tmp1 + tmp2 - tmp1) * HEX_40000000 * HEX_40000000;\n            yb = y - ya;\n        }\n\n        /* Compute ln(x) */\n        final double lores = log(x, lns);\n        if (Double.isInfinite(lores)){ // don't allow this to be converted to NaN\n            return lores;\n        }\n\n        double lna = lns[0];\n        double lnb = lns[1];\n\n        /* resplit lns */\n        double tmp1 = lna * HEX_40000000;\n        double tmp2 = lna + tmp1 - tmp1;\n        lnb += lna - tmp2;\n        lna = tmp2;\n\n        // y*ln(x) = (aa+ab)\n        final double aa = lna * ya;\n        final double ab = lna * yb + lnb * ya + lnb * yb;\n\n        lna = aa+ab;\n        lnb = -(lna - aa - ab);\n\n        double z = 1.0 / 120.0;\n        z = z * lnb + (1.0 / 24.0);\n        z = z * lnb + (1.0 / 6.0);\n        z = z * lnb + 0.5;\n        z = z * lnb + 1.0;\n        z = z * lnb;\n\n        final double result = exp(lna, z, null);\n        //result = result + result * z;\n        return result;\n    }",
        "fixed_src_wo_comments": "public static double pow ( double x , double y ) { final double lns [ ] = new double [ 2 ] ; if ( y == 0.0 ) { return 1.0 ; } if ( x != x ) { return x ; } if ( x == 0 ) { long bits = Double . doubleToLongBits ( x ) ; if ( ( bits & 0x8000000000000000L ) != 0 ) { long yi = ( long ) y ; if ( y < 0 && y == yi && ( yi & 1 ) == 1 ) { return Double . NEGATIVE_INFINITY ; } if ( y > 0 && y == yi && ( yi & 1 ) == 1 ) { return - 0.0 ; } } if ( y < 0 ) { return Double . POSITIVE_INFINITY ; } if ( y > 0 ) { return 0.0 ; } return Double . NaN ; } if ( x == Double . POSITIVE_INFINITY ) { if ( y != y ) { return y ; } if ( y < 0.0 ) { return 0.0 ; } else { return Double . POSITIVE_INFINITY ; } } if ( y == Double . POSITIVE_INFINITY ) { if ( x * x == 1.0 ) { return Double . NaN ; } if ( x * x > 1.0 ) { return Double . POSITIVE_INFINITY ; } else { return 0.0 ; } } if ( x == Double . NEGATIVE_INFINITY ) { if ( y != y ) { return y ; } if ( y < 0 ) { long yi = ( long ) y ; if ( y == yi && ( yi & 1 ) == 1 ) { return - 0.0 ; } return 0.0 ; } if ( y > 0 ) { long yi = ( long ) y ; if ( y == yi && ( yi & 1 ) == 1 ) { return Double . NEGATIVE_INFINITY ; } return Double . POSITIVE_INFINITY ; } } if ( y == Double . NEGATIVE_INFINITY ) { if ( x * x == 1.0 ) { return Double . NaN ; } if ( x * x < 1.0 ) { return Double . POSITIVE_INFINITY ; } else { return 0.0 ; } } if ( x < 0 ) { if ( y >= TWO_POWER_53 || y <= - TWO_POWER_53 ) { return pow ( - x , y ) ; } if ( y == ( long ) y ) { return ( ( long ) y & 1 ) == 0 ? pow ( - x , y ) : - pow ( - x , y ) ; } else { return Double . NaN ; } } double ya ; double yb ; if ( y < 8e298 && y > - 8e298 ) { double tmp1 = y * HEX_40000000 ; ya = y + tmp1 - tmp1 ; yb = y - ya ; } else { double tmp1 = y * 9.31322574615478515625E-10 ; double tmp2 = tmp1 * 9.31322574615478515625E-10 ; ya = ( tmp1 + tmp2 - tmp1 ) * HEX_40000000 * HEX_40000000 ; yb = y - ya ; } final double lores = log ( x , lns ) ; if ( Double . isInfinite ( lores ) ) { return lores ; } double lna = lns [ 0 ] ; double lnb = lns [ 1 ] ; double tmp1 = lna * HEX_40000000 ; double tmp2 = lna + tmp1 - tmp1 ; lnb += lna - tmp2 ; lna = tmp2 ; final double aa = lna * ya ; final double ab = lna * yb + lnb * ya + lnb * yb ; lna = aa + ab ; lnb = - ( lna - aa - ab ) ; double z = 1.0 / 120.0 ; z = z * lnb + ( 1.0 / 24.0 ) ; z = z * lnb + ( 1.0 / 6.0 ) ; z = z * lnb + 0.5 ; z = z * lnb + 1.0 ; z = z * lnb ; final double result = exp ( lna , z , null ) ; return result ; }",
        "summary": "FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53 ",
        "Description": "As reported by Jeff Hain:\n\npow(double,double):\nMath.pow(-1.0,5.000000000000001E15) = -1.0\nFastMath.pow(-1.0,5.000000000000001E15) = 1.0\n===> This is due to considering that power is an even\ninteger if it is >= 2^52, while you need to test\nthat it is >= 2^53 for it.\n===> replace\n\"if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)\"\nwith\n\"if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)\"\nand that solves it.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-904",
        "comments": [
            "Does anyone know the reason why numbers > 2^53 are assumed to be even?\n\nLooking at the javadoc of Math.pow, this is not mentioned, only the general case:\n\n{noformat}\nIf the first argument is finite and less than zero\n    if the second argument is a finite even integer, the result is equal to the result of raising the absolute value of the first argument to the power of the second argument\n    if the second argument is a finite odd integer, the result is equal to the negative of the result of raising the absolute value of the first argument to the power of the second argument\n    if the second argument is finite and not an integer, then the result is NaN. \n{noformat}",
            "bq. Does anyone know the reason why numbers > 2^53 are assumed to be even?\n\nBecause the mantissa of a double number encoding using IEEE-754 cannot handle a sufficient number of digits. If the Most Significant Bit is large enough, the Least Significant Bit becomes equal to 2.0 (and later when you still increase the MSB, then the LSB will become 4.0, and after that 8.0...\n\nThis is the essence of \"floating\" in floating point numbers. The decimal separator \"floats\", up to the end of the number than it slips out of the number.",
            "ah ok, thanks a lot, I found also an explanation here [http://en.wikipedia.org/wiki/Double-precision_floating-point_format]",
            "Fixed in revision 1413916."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is that numbers greater than 2^53 are assumed to be even due to the mantissa of a double number encoding using IEEE-754 not being able to handle a sufficient number of digits. The decimal separator \"floats\" up to the end of the number until it slips out of the number. The bug was fixed in revision 1413916."
    },
    "JacksonDatabind_70_src/main/java/com/fasterxml/jackson/databind/deser/impl/BeanPropertyMap.java_426_453": {
        "src": "public void remove(SettableBeanProperty propToRm)\n    {\n        ArrayList<SettableBeanProperty> props = new ArrayList<SettableBeanProperty>(_size);\n        String key = getPropertyName(propToRm);\n        boolean found = false;\n\n        for (int i = 1, end = _hashArea.length; i < end; i += 2) {\n            SettableBeanProperty prop = (SettableBeanProperty) _hashArea[i];\n            if (prop == null) {\n                continue;\n            }\n            if (!found) {\n                // 09-Jan-2017, tatu: Important: must check name slot and NOT property name,\n                //   as only former is lower-case in case-insensitive case\n                found = key.equals(prop.getName());\n                if (found) {\n                    // need to leave a hole here\n                    _propsInOrder[_findFromOrdered(prop)] = null;\n                    continue;\n                }\n            }\n            props.add(prop);\n        }\n        if (!found) {\n            throw new NoSuchElementException(\"No entry '\"+propToRm.getName()+\"' found, can't remove\");\n        }\n        init(props);\n    }",
        "src_wo_comments": "public void remove ( SettableBeanProperty propToRm ) { ArrayList < SettableBeanProperty > props = new ArrayList < SettableBeanProperty > ( _size ) ; String key = getPropertyName ( propToRm ) ; boolean found = false ; for ( int i = 1 , end = _hashArea . length ; i < end ; i += 2 ) { SettableBeanProperty prop = ( SettableBeanProperty ) _hashArea [ i ] ; if ( prop == null ) { continue ; } if ( ! found ) { found = key . equals ( prop . getName ( ) ) ; if ( found ) { _propsInOrder [ _findFromOrdered ( prop ) ] = null ; continue ; } } props . add ( prop ) ; } if ( ! found ) { throw new NoSuchElementException ( \"No entry '\" + propToRm . getName ( ) + \"' found, can't remove\" ) ; } init ( props ) ; }",
        "fixed_src": "public void remove(SettableBeanProperty propToRm)\n    {\n        ArrayList<SettableBeanProperty> props = new ArrayList<SettableBeanProperty>(_size);\n        String key = getPropertyName(propToRm);\n        boolean found = false;\n\n        for (int i = 1, end = _hashArea.length; i < end; i += 2) {\n            SettableBeanProperty prop = (SettableBeanProperty) _hashArea[i];\n            if (prop == null) {\n                continue;\n            }\n            if (!found) {\n                // 09-Jan-2017, tatu: Important: must check name slot and NOT property name,\n                //   as only former is lower-case in case-insensitive case\n                found = key.equals(_hashArea[i-1]);\n                if (found) {\n                    // need to leave a hole here\n                    _propsInOrder[_findFromOrdered(prop)] = null;\n                    continue;\n                }\n            }\n            props.add(prop);\n        }\n        if (!found) {\n            throw new NoSuchElementException(\"No entry '\"+propToRm.getName()+\"' found, can't remove\");\n        }\n        init(props);\n    }",
        "fixed_src_wo_comments": "public void remove ( SettableBeanProperty propToRm ) { ArrayList < SettableBeanProperty > props = new ArrayList < SettableBeanProperty > ( _size ) ; String key = getPropertyName ( propToRm ) ; boolean found = false ; for ( int i = 1 , end = _hashArea . length ; i < end ; i += 2 ) { SettableBeanProperty prop = ( SettableBeanProperty ) _hashArea [ i ] ; if ( prop == null ) { continue ; } if ( ! found ) { found = key . equals ( _hashArea [ i - 1 ] ) ; if ( found ) { _propsInOrder [ _findFromOrdered ( prop ) ] = null ; continue ; } } props . add ( prop ) ; } if ( ! found ) { throw new NoSuchElementException ( \"No entry '\" + propToRm . getName ( ) + \"' found, can't remove\" ) ; } init ( props ) ; }",
        "summary": "`ACCEPT_CASE_INSENSITIVE_PROPERTIES` fails with `@JsonUnwrapped`",
        "Description": "(note: moved from https://github.com/FasterXML/jackson-dataformat-csv/issues/133)\r\n\r\nWhen trying to deserialize type like:\r\n\r\n```java\r\npublic class Person {\r\n  @JsonUnwrapped(prefix = \"businessAddress.\")\r\n  public Address businessAddress;\r\n}\r\n\r\npublic class Address {\r\n  public String street;\r\n  public String addon;\r\n  public String zip = \"\";\r\n  public String town;    \r\n  public String country;\r\n}\r\n```\r\n\r\nwith case-insensitive mapper (`mapper.enable(MapperFeature.ACCEPT_CASE_INSENSITIVE_PROPERTIES);`) I get exception:\r\n\r\n```\r\njava.util.NoSuchElementException: No entry 'businessAddress' found, can't remove\r\n\tat com.fasterxml.jackson.databind.deser.impl.BeanPropertyMap.remove(BeanPropertyMap.java:447)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.resolve(BeanDeserializerBase.java:534)\r\n\tat com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:293)\r\n   ...\r\n```\r\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug discussed is a memory leak. The solution to the bug is to identify the source of the leak and fix the code that is causing it. This can be done by using a memory profiler to track memory usage and identify which parts of the code are allocating too much memory. Once the source of the leak is identified, the code can be fixed to prevent the leak from occurring."
    },
    "Math_42_src/main/java/org/apache/commons/math/optimization/linear/SimplexTableau.java_396_425": {
        "src": "protected RealPointValuePair getSolution() {\n      int negativeVarColumn = columnLabels.indexOf(NEGATIVE_VAR_COLUMN_LABEL);\n      Integer negativeVarBasicRow = negativeVarColumn > 0 ? getBasicRow(negativeVarColumn) : null;\n      double mostNegative = negativeVarBasicRow == null ? 0 : getEntry(negativeVarBasicRow, getRhsOffset());\n\n      Set<Integer> basicRows = new HashSet<Integer>();\n      double[] coefficients = new double[getOriginalNumDecisionVariables()];\n      for (int i = 0; i < coefficients.length; i++) {\n          int colIndex = columnLabels.indexOf(\"x\" + i);\n          if (colIndex < 0) {\n            coefficients[i] = 0;\n            continue;\n          }\n          Integer basicRow = getBasicRow(colIndex);\n              // if the basic row is found to be the objective function row\n              // set the coefficient to 0 -> this case handles unconstrained \n              // variables that are still part of the objective function\n          if (basicRows.contains(basicRow)) {\n              // if multiple variables can take a given value\n              // then we choose the first and set the rest equal to 0\n              coefficients[i] = 0 - (restrictToNonNegative ? 0 : mostNegative);\n          } else {\n              basicRows.add(basicRow);\n              coefficients[i] =\n                  (basicRow == null ? 0 : getEntry(basicRow, getRhsOffset())) -\n                  (restrictToNonNegative ? 0 : mostNegative);\n          }\n      }\n      return new RealPointValuePair(coefficients, f.getValue(coefficients));\n    }",
        "src_wo_comments": "protected RealPointValuePair getSolution ( ) { int negativeVarColumn = columnLabels . indexOf ( NEGATIVE_VAR_COLUMN_LABEL ) ; Integer negativeVarBasicRow = negativeVarColumn > 0 ? getBasicRow ( negativeVarColumn ) : null ; double mostNegative = negativeVarBasicRow == null ? 0 : getEntry ( negativeVarBasicRow , getRhsOffset ( ) ) ; Set < Integer > basicRows = new HashSet < Integer > ( ) ; double [ ] coefficients = new double [ getOriginalNumDecisionVariables ( ) ] ; for ( int i = 0 ; i < coefficients . length ; i ++ ) { int colIndex = columnLabels . indexOf ( \"x\" + i ) ; if ( colIndex < 0 ) { coefficients [ i ] = 0 ; continue ; } Integer basicRow = getBasicRow ( colIndex ) ; if ( basicRows . contains ( basicRow ) ) { coefficients [ i ] = 0 - ( restrictToNonNegative ? 0 : mostNegative ) ; } else { basicRows . add ( basicRow ) ; coefficients [ i ] = ( basicRow == null ? 0 : getEntry ( basicRow , getRhsOffset ( ) ) ) - ( restrictToNonNegative ? 0 : mostNegative ) ; } } return new RealPointValuePair ( coefficients , f . getValue ( coefficients ) ) ; }",
        "fixed_src": "protected RealPointValuePair getSolution() {\n      int negativeVarColumn = columnLabels.indexOf(NEGATIVE_VAR_COLUMN_LABEL);\n      Integer negativeVarBasicRow = negativeVarColumn > 0 ? getBasicRow(negativeVarColumn) : null;\n      double mostNegative = negativeVarBasicRow == null ? 0 : getEntry(negativeVarBasicRow, getRhsOffset());\n\n      Set<Integer> basicRows = new HashSet<Integer>();\n      double[] coefficients = new double[getOriginalNumDecisionVariables()];\n      for (int i = 0; i < coefficients.length; i++) {\n          int colIndex = columnLabels.indexOf(\"x\" + i);\n          if (colIndex < 0) {\n            coefficients[i] = 0;\n            continue;\n          }\n          Integer basicRow = getBasicRow(colIndex);\n          if (basicRow != null && basicRow == 0) {\n              // if the basic row is found to be the objective function row\n              // set the coefficient to 0 -> this case handles unconstrained \n              // variables that are still part of the objective function\n              coefficients[i] = 0;\n          } else if (basicRows.contains(basicRow)) {\n              // if multiple variables can take a given value\n              // then we choose the first and set the rest equal to 0\n              coefficients[i] = 0 - (restrictToNonNegative ? 0 : mostNegative);\n          } else {\n              basicRows.add(basicRow);\n              coefficients[i] =\n                  (basicRow == null ? 0 : getEntry(basicRow, getRhsOffset())) -\n                  (restrictToNonNegative ? 0 : mostNegative);\n          }\n      }\n      return new RealPointValuePair(coefficients, f.getValue(coefficients));\n    }",
        "fixed_src_wo_comments": "protected RealPointValuePair getSolution ( ) { int negativeVarColumn = columnLabels . indexOf ( NEGATIVE_VAR_COLUMN_LABEL ) ; Integer negativeVarBasicRow = negativeVarColumn > 0 ? getBasicRow ( negativeVarColumn ) : null ; double mostNegative = negativeVarBasicRow == null ? 0 : getEntry ( negativeVarBasicRow , getRhsOffset ( ) ) ; Set < Integer > basicRows = new HashSet < Integer > ( ) ; double [ ] coefficients = new double [ getOriginalNumDecisionVariables ( ) ] ; for ( int i = 0 ; i < coefficients . length ; i ++ ) { int colIndex = columnLabels . indexOf ( \"x\" + i ) ; if ( colIndex < 0 ) { coefficients [ i ] = 0 ; continue ; } Integer basicRow = getBasicRow ( colIndex ) ; if ( basicRow != null && basicRow == 0 ) { coefficients [ i ] = 0 ; } else if ( basicRows . contains ( basicRow ) ) { coefficients [ i ] = 0 - ( restrictToNonNegative ? 0 : mostNegative ) ; } else { basicRows . add ( basicRow ) ; coefficients [ i ] = ( basicRow == null ? 0 : getEntry ( basicRow , getRhsOffset ( ) ) ) - ( restrictToNonNegative ? 0 : mostNegative ) ; } } return new RealPointValuePair ( coefficients , f . getValue ( coefficients ) ) ; }",
        "summary": "Negative value with restrictNonNegative",
        "Description": "Problem: commons-math-2.2 SimplexSolver.\n\nA variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call:\nSimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);\n\nFunction\n1 * x + 1 * y + 0\n\nConstraints:\n1 * x + 0 * y = 1\n\nResult:\nx = 1; y = -1;\n\nProbably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-713",
        "comments": [
            "Could you please check this against the latest development version from the subversion repository ?\nThere have been several fixes concerning these coefficients in simplex solver since 2.2\n",
            "I'm quite busy at work now... When I'll find some time, I'll try to test it. Of course I'd be glad if one of developers would do it before :)",
            "I have tested this problem with the latest trunk, and the problem still remains.\n\nThe attached patch handles the case of unconstrained variables that still occur in the objective function. I have also added a unit test.",
            "Fixed in subversion repository as of r1207566.\n\nPatch applied directly.\n\nThanks to Michal for the report and thanks to Thomas for the patch."
        ],
        "summarized_discussion": "\n\nThe bug was fixed in the subversion repository as of r1207566 with a patch applied directly. Thanks to Michal for the report and Thomas for the patch."
    },
    "JacksonDatabind_66_src/main/java/com/fasterxml/jackson/databind/deser/std/StdKeyDeserializer.java_306_324": {
        "src": "@SuppressWarnings(\"resource\")\n        @Override\n        public final Object deserializeKey(String key, DeserializationContext ctxt)\n            throws IOException\n        {\n            if (key == null) { // is this even legal call?\n                return null;\n            }\n            try {\n                // Ugh... should not have to give parser which may or may not be correct one...\n                Object result = _delegate.deserialize(ctxt.getParser(), ctxt);\n                if (result != null) {\n                    return result;\n                }\n                return ctxt.handleWeirdKey(_keyClass, key, \"not a valid representation\");\n            } catch (Exception re) {\n                return ctxt.handleWeirdKey(_keyClass, key, \"not a valid representation: %s\", re.getMessage());\n            }\n        }",
        "src_wo_comments": "@ SuppressWarnings ( \"resource\" ) @ Override public final Object deserializeKey ( String key , DeserializationContext ctxt ) throws IOException { if ( key == null ) { return null ; } try { Object result = _delegate . deserialize ( ctxt . getParser ( ) , ctxt ) ; if ( result != null ) { return result ; } return ctxt . handleWeirdKey ( _keyClass , key , \"not a valid representation\" ) ; } catch ( Exception re ) { return ctxt . handleWeirdKey ( _keyClass , key , \"not a valid representation: %s\" , re . getMessage ( ) ) ; } }",
        "fixed_src": "@SuppressWarnings(\"resource\")\n        @Override\n        public final Object deserializeKey(String key, DeserializationContext ctxt)\n            throws IOException\n        {\n            if (key == null) { // is this even legal call?\n                return null;\n            }\n            TokenBuffer tb = new TokenBuffer(ctxt.getParser(), ctxt);\n            tb.writeString(key);\n            try {\n                // Ugh... should not have to give parser which may or may not be correct one...\n                JsonParser p = tb.asParser();\n                p.nextToken();\n                Object result = _delegate.deserialize(p, ctxt);\n                if (result != null) {\n                    return result;\n                }\n                return ctxt.handleWeirdKey(_keyClass, key, \"not a valid representation\");\n            } catch (Exception re) {\n                return ctxt.handleWeirdKey(_keyClass, key, \"not a valid representation: %s\", re.getMessage());\n            }\n        }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"resource\" ) @ Override public final Object deserializeKey ( String key , DeserializationContext ctxt ) throws IOException { if ( key == null ) { return null ; } TokenBuffer tb = new TokenBuffer ( ctxt . getParser ( ) , ctxt ) ; tb . writeString ( key ) ; try { JsonParser p = tb . asParser ( ) ; p . nextToken ( ) ; Object result = _delegate . deserialize ( p , ctxt ) ; if ( result != null ) { return result ; } return ctxt . handleWeirdKey ( _keyClass , key , \"not a valid representation\" ) ; } catch ( Exception re ) { return ctxt . handleWeirdKey ( _keyClass , key , \"not a valid representation: %s\" , re . getMessage ( ) ) ; } }",
        "summary": "Failure with custom Enum key deserializer, polymorphic types",
        "Description": "Normally the `JsonParser` and the `DeserializationContext` is passed to a `Module`'s `JsonDeserializer`. \r\n\r\nHowever, in the `MapDeserializer`, when deserializing a `Map` with an `Enum` key, the `KeyDeserializer` doesn't accept the `JsonParser` as an argument:\r\n\r\nhttps://github.com/FasterXML/jackson-databind/blob/master/src/main/java/com/fasterxml/jackson/databind/deser/std/MapDeserializer.java#L453\r\n    Object key = keyDes.deserializeKey(keyStr, ctxt);\r\n\r\nand the `StdKeyDeserializer.DelegatingKD` uses the context's parser\r\n\r\nhttps://github.com/FasterXML/jackson-databind/blob/master/src/main/java/com/fasterxml/jackson/databind/deser/std/StdKeyDeserializer.java#L315\r\n   Object result = _delegate.deserialize(ctxt.getParser(), ctxt);\r\n\r\nWhen the type info field is missing from the json, the `DeserializationContext`'s `JsonParser`'s token is `END_OBJECT` (presumably because it `nextToken`'d through the object to find type and whiffed). \r\n\r\nThis makes the module fail since the `JsonParser` in the `Module` is wrong, i.e. not the same as the `JsonParser` in the `MapDeserializer`.\r\n\r\nClass:\r\n\r\n    import com.fasterxml.jackson.annotation.JsonTypeInfo;\r\n    \r\n    import java.util.Map;\r\n    \r\n    import static com.fasterxml.jackson.annotation.JsonTypeInfo.Id.NAME;\r\n    \r\n    @JsonTypeInfo(use = NAME, property = \"@type\", defaultImpl = SuperType.class)\r\n    public class SuperType {\r\n        private Map<SuperTypeEnum, String> someMap;\r\n    \r\n        public Map<SuperTypeEnum, String> getSomeMap() {\r\n            return someMap;\r\n        }\r\n    \r\n        public void setSomeMap(Map<SuperTypeEnum, String> someMap) {\r\n            this.someMap = someMap;\r\n        }\r\n    }\r\n\r\nEnum:\r\n\r\n    public enum SuperTypeEnum {\r\n        FOO\r\n    }\r\n\r\n\r\nTest:\r\n\r\n    import com.fasterxml.jackson.core.JsonParser;\r\n    import com.fasterxml.jackson.databind.DeserializationContext;\r\n    import com.fasterxml.jackson.databind.JsonDeserializer;\r\n    import com.fasterxml.jackson.databind.ObjectMapper;\r\n    import com.fasterxml.jackson.databind.module.SimpleModule;\r\n    import org.junit.*;\r\n\r\n    import java.io.IOException;\r\n\r\n    import static org.junit.Assert.assertEquals;\r\n\r\n    public class TestDeserializeType {\r\n\r\n        @Test\r\n        public void testNoTypeShouldDeserialize() throws IOException {\r\n            String json = \"{\\\"someMap\\\": {\\\"FOO\\\": \\\"bar\\\"}}\";\r\n            ObjectMapper mapper = new ObjectMapper();\r\n            SuperType superType = mapper.readValue(json, SuperType.class);\r\n            assertEquals(\"Deserialized someMap.FOO should equal bar\", \"bar\", superType.getSomeMap().get(SuperTypeEnum.FOO));\r\n        }\r\n\r\n        @Test\r\n        public void testNoTypeWithModuleShouldDeserialize() throws IOException {\r\n            String json = \"{\\\"someMap\\\": {\\\"FOO\\\": \\\"bar\\\"}}\";\r\n            ObjectMapper mapper = new ObjectMapper();\r\n            SimpleModule simpleModule = new SimpleModule();\r\n            simpleModule.addDeserializer(SuperTypeEnum.class, new JsonDeserializer<SuperTypeEnum>() {\r\n                @Override\r\n                public SuperTypeEnum deserialize(JsonParser jsonParser, DeserializationContext deserializationContext)\r\n                        throws IOException {\r\n\r\n                    return SuperTypeEnum.valueOf(jsonParser.getText());\r\n                }\r\n            });\r\n            mapper.registerModule(simpleModule);\r\n\r\n            SuperType superType = mapper.readValue(json, SuperType.class);\r\n            assertEquals(\"Deserialized someMap.FOO should equal bar\", \"bar\", superType.getSomeMap().get(SuperTypeEnum.FOO));\r\n        }\r\n    }\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this. It sounds like work-around used to allow some value deserializers to be used as key deserializers has some issues.\n"
            },
            {
                "content": "Wow. Not sure why I implemented delegating key deserializer that way; it's wrong -- should not delegate original parser in general, and in this case parser would be wrong due to buffering needed for type id.\nAnyway: better way is to construct bogus `TokenBuffer` as `JsonParser` and add key (that has already been accessed and is passed as the argument), give that parser; it's safer and necessary here.\n\nThank you for reporting this, providing test case! Fixed for 2.8.5, to be released relatively soon (within next week or two).\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was caused by a work-around used to allow some value deserializers to be used as key deserializers. The solution is to construct a bogus TokenBuffer as a JsonParser and add the key, which has already been accessed and is passed as the argument, and give that parser. This is safer and necessary for the bug to be fixed. The bug has been fixed for 2.8.5 and will be released within the next week or two."
    },
    "Codec_10_src/java/org/apache/commons/codec/language/Caverphone.java_50_142": {
        "src": "public String caverphone(String txt) {\n        // NOTE: Version 1.0 of Caverphone is easily derivable from this code \n        // by commenting out the 2.0 lines and adding in the 1.0 lines\n\n        if( txt == null || txt.length() == 0 ) {\n            return \"1111111111\";\n        }\n\n        // 1. Convert to lowercase\n        txt = txt.toLowerCase(java.util.Locale.ENGLISH);\n\n        // 2. Remove anything not A-Z\n        txt = txt.replaceAll(\"[^a-z]\", \"\");\n\n        // 2.5. Remove final e\n        txt = txt.replaceAll(\"e$\", \"\");             // 2.0 only\n\n        // 3. Handle various start options\n        txt = txt.replaceAll(\"^cough\", \"cou2f\");\n        txt = txt.replaceAll(\"^rough\", \"rou2f\");\n        txt = txt.replaceAll(\"^tough\", \"tou2f\");\n        txt = txt.replaceAll(\"^enough\", \"enou2f\");  // 2.0 only\n        txt = txt.replaceAll(\"^trough\", \"trou2f\");  // 2.0 only - note the spec says ^enough here again, c+p error I assume\n        txt = txt.replaceAll(\"^gn\", \"2n\");\n\n        // End \n        txt = txt.replaceAll(\"^mb\", \"m2\");\n\n        // 4. Handle replacements\n        txt = txt.replaceAll(\"cq\", \"2q\");\n        txt = txt.replaceAll(\"ci\", \"si\");\n        txt = txt.replaceAll(\"ce\", \"se\");\n        txt = txt.replaceAll(\"cy\", \"sy\");\n        txt = txt.replaceAll(\"tch\", \"2ch\");\n        txt = txt.replaceAll(\"c\", \"k\");\n        txt = txt.replaceAll(\"q\", \"k\");\n        txt = txt.replaceAll(\"x\", \"k\");\n        txt = txt.replaceAll(\"v\", \"f\");\n        txt = txt.replaceAll(\"dg\", \"2g\");\n        txt = txt.replaceAll(\"tio\", \"sio\");\n        txt = txt.replaceAll(\"tia\", \"sia\");\n        txt = txt.replaceAll(\"d\", \"t\");\n        txt = txt.replaceAll(\"ph\", \"fh\");\n        txt = txt.replaceAll(\"b\", \"p\");\n        txt = txt.replaceAll(\"sh\", \"s2\");\n        txt = txt.replaceAll(\"z\", \"s\");\n        txt = txt.replaceAll(\"^[aeiou]\", \"A\");\n        txt = txt.replaceAll(\"[aeiou]\", \"3\");\n        txt = txt.replaceAll(\"j\", \"y\");        // 2.0 only\n        txt = txt.replaceAll(\"^y3\", \"Y3\");     // 2.0 only\n        txt = txt.replaceAll(\"^y\", \"A\");       // 2.0 only\n        txt = txt.replaceAll(\"y\", \"3\");        // 2.0 only\n        txt = txt.replaceAll(\"3gh3\", \"3kh3\");\n        txt = txt.replaceAll(\"gh\", \"22\");\n        txt = txt.replaceAll(\"g\", \"k\");\n        txt = txt.replaceAll(\"s+\", \"S\");\n        txt = txt.replaceAll(\"t+\", \"T\");\n        txt = txt.replaceAll(\"p+\", \"P\");\n        txt = txt.replaceAll(\"k+\", \"K\");\n        txt = txt.replaceAll(\"f+\", \"F\");\n        txt = txt.replaceAll(\"m+\", \"M\");\n        txt = txt.replaceAll(\"n+\", \"N\");\n        txt = txt.replaceAll(\"w3\", \"W3\");\n        //txt = txt.replaceAll(\"wy\", \"Wy\");    // 1.0 only\n        txt = txt.replaceAll(\"wh3\", \"Wh3\");\n        txt = txt.replaceAll(\"w$\", \"3\");       // 2.0 only\n        //txt = txt.replaceAll(\"why\", \"Why\");  // 1.0 only\n        txt = txt.replaceAll(\"w\", \"2\");\n        txt = txt.replaceAll(\"^h\", \"A\");\n        txt = txt.replaceAll(\"h\", \"2\");\n        txt = txt.replaceAll(\"r3\", \"R3\");\n        txt = txt.replaceAll(\"r$\", \"3\");       // 2.0 only\n        //txt = txt.replaceAll(\"ry\", \"Ry\");    // 1.0 only\n        txt = txt.replaceAll(\"r\", \"2\");\n        txt = txt.replaceAll(\"l3\", \"L3\");\n        txt = txt.replaceAll(\"l$\", \"3\");       // 2.0 only\n        //txt = txt.replaceAll(\"ly\", \"Ly\");    // 1.0 only\n        txt = txt.replaceAll(\"l\", \"2\");\n        //txt = txt.replaceAll(\"j\", \"y\");      // 1.0 only\n        //txt = txt.replaceAll(\"y3\", \"Y3\");    // 1.0 only\n        //txt = txt.replaceAll(\"y\", \"2\");      // 1.0 only\n\n        // 5. Handle removals\n        txt = txt.replaceAll(\"2\", \"\");\n        txt = txt.replaceAll(\"3$\", \"A\");       // 2.0 only\n        txt = txt.replaceAll(\"3\", \"\");\n\n        // 6. put ten 1s on the end\n        txt = txt + \"111111\" + \"1111\";        // 1.0 only has 6 1s\n\n        // 7. take the first six characters as the code\n        return txt.substring(0, 10);          // 1.0 truncates to 6\n    }",
        "src_wo_comments": "public String caverphone ( String txt ) { if ( txt == null || txt . length ( ) == 0 ) { return \"1111111111\" ; } txt = txt . toLowerCase ( java . util . Locale . ENGLISH ) ; txt = txt . replaceAll ( \"[^a-z]\" , \"\" ) ; txt = txt . replaceAll ( \"e$\" , \"\" ) ; txt = txt . replaceAll ( \"^cough\" , \"cou2f\" ) ; txt = txt . replaceAll ( \"^rough\" , \"rou2f\" ) ; txt = txt . replaceAll ( \"^tough\" , \"tou2f\" ) ; txt = txt . replaceAll ( \"^enough\" , \"enou2f\" ) ; txt = txt . replaceAll ( \"^trough\" , \"trou2f\" ) ; txt = txt . replaceAll ( \"^gn\" , \"2n\" ) ; txt = txt . replaceAll ( \"^mb\" , \"m2\" ) ; txt = txt . replaceAll ( \"cq\" , \"2q\" ) ; txt = txt . replaceAll ( \"ci\" , \"si\" ) ; txt = txt . replaceAll ( \"ce\" , \"se\" ) ; txt = txt . replaceAll ( \"cy\" , \"sy\" ) ; txt = txt . replaceAll ( \"tch\" , \"2ch\" ) ; txt = txt . replaceAll ( \"c\" , \"k\" ) ; txt = txt . replaceAll ( \"q\" , \"k\" ) ; txt = txt . replaceAll ( \"x\" , \"k\" ) ; txt = txt . replaceAll ( \"v\" , \"f\" ) ; txt = txt . replaceAll ( \"dg\" , \"2g\" ) ; txt = txt . replaceAll ( \"tio\" , \"sio\" ) ; txt = txt . replaceAll ( \"tia\" , \"sia\" ) ; txt = txt . replaceAll ( \"d\" , \"t\" ) ; txt = txt . replaceAll ( \"ph\" , \"fh\" ) ; txt = txt . replaceAll ( \"b\" , \"p\" ) ; txt = txt . replaceAll ( \"sh\" , \"s2\" ) ; txt = txt . replaceAll ( \"z\" , \"s\" ) ; txt = txt . replaceAll ( \"^[aeiou]\" , \"A\" ) ; txt = txt . replaceAll ( \"[aeiou]\" , \"3\" ) ; txt = txt . replaceAll ( \"j\" , \"y\" ) ; txt = txt . replaceAll ( \"^y3\" , \"Y3\" ) ; txt = txt . replaceAll ( \"^y\" , \"A\" ) ; txt = txt . replaceAll ( \"y\" , \"3\" ) ; txt = txt . replaceAll ( \"3gh3\" , \"3kh3\" ) ; txt = txt . replaceAll ( \"gh\" , \"22\" ) ; txt = txt . replaceAll ( \"g\" , \"k\" ) ; txt = txt . replaceAll ( \"s+\" , \"S\" ) ; txt = txt . replaceAll ( \"t+\" , \"T\" ) ; txt = txt . replaceAll ( \"p+\" , \"P\" ) ; txt = txt . replaceAll ( \"k+\" , \"K\" ) ; txt = txt . replaceAll ( \"f+\" , \"F\" ) ; txt = txt . replaceAll ( \"m+\" , \"M\" ) ; txt = txt . replaceAll ( \"n+\" , \"N\" ) ; txt = txt . replaceAll ( \"w3\" , \"W3\" ) ; txt = txt . replaceAll ( \"wh3\" , \"Wh3\" ) ; txt = txt . replaceAll ( \"w$\" , \"3\" ) ; txt = txt . replaceAll ( \"w\" , \"2\" ) ; txt = txt . replaceAll ( \"^h\" , \"A\" ) ; txt = txt . replaceAll ( \"h\" , \"2\" ) ; txt = txt . replaceAll ( \"r3\" , \"R3\" ) ; txt = txt . replaceAll ( \"r$\" , \"3\" ) ; txt = txt . replaceAll ( \"r\" , \"2\" ) ; txt = txt . replaceAll ( \"l3\" , \"L3\" ) ; txt = txt . replaceAll ( \"l$\" , \"3\" ) ; txt = txt . replaceAll ( \"l\" , \"2\" ) ; txt = txt . replaceAll ( \"2\" , \"\" ) ; txt = txt . replaceAll ( \"3$\" , \"A\" ) ; txt = txt . replaceAll ( \"3\" , \"\" ) ; txt = txt + \"111111\" + \"1111\" ; return txt . substring ( 0 , 10 ) ; }",
        "fixed_src": "public String caverphone(String txt) {\n        // NOTE: Version 1.0 of Caverphone is easily derivable from this code \n        // by commenting out the 2.0 lines and adding in the 1.0 lines\n\n        if( txt == null || txt.length() == 0 ) {\n            return \"1111111111\";\n        }\n\n        // 1. Convert to lowercase\n        txt = txt.toLowerCase(java.util.Locale.ENGLISH);\n\n        // 2. Remove anything not A-Z\n        txt = txt.replaceAll(\"[^a-z]\", \"\");\n\n        // 2.5. Remove final e\n        txt = txt.replaceAll(\"e$\", \"\");             // 2.0 only\n\n        // 3. Handle various start options\n        txt = txt.replaceAll(\"^cough\", \"cou2f\");\n        txt = txt.replaceAll(\"^rough\", \"rou2f\");\n        txt = txt.replaceAll(\"^tough\", \"tou2f\");\n        txt = txt.replaceAll(\"^enough\", \"enou2f\");  // 2.0 only\n        txt = txt.replaceAll(\"^trough\", \"trou2f\");  // 2.0 only - note the spec says ^enough here again, c+p error I assume\n        txt = txt.replaceAll(\"^gn\", \"2n\");\n\n        // End \n        txt = txt.replaceAll(\"mb$\", \"m2\");\n\n        // 4. Handle replacements\n        txt = txt.replaceAll(\"cq\", \"2q\");\n        txt = txt.replaceAll(\"ci\", \"si\");\n        txt = txt.replaceAll(\"ce\", \"se\");\n        txt = txt.replaceAll(\"cy\", \"sy\");\n        txt = txt.replaceAll(\"tch\", \"2ch\");\n        txt = txt.replaceAll(\"c\", \"k\");\n        txt = txt.replaceAll(\"q\", \"k\");\n        txt = txt.replaceAll(\"x\", \"k\");\n        txt = txt.replaceAll(\"v\", \"f\");\n        txt = txt.replaceAll(\"dg\", \"2g\");\n        txt = txt.replaceAll(\"tio\", \"sio\");\n        txt = txt.replaceAll(\"tia\", \"sia\");\n        txt = txt.replaceAll(\"d\", \"t\");\n        txt = txt.replaceAll(\"ph\", \"fh\");\n        txt = txt.replaceAll(\"b\", \"p\");\n        txt = txt.replaceAll(\"sh\", \"s2\");\n        txt = txt.replaceAll(\"z\", \"s\");\n        txt = txt.replaceAll(\"^[aeiou]\", \"A\");\n        txt = txt.replaceAll(\"[aeiou]\", \"3\");\n        txt = txt.replaceAll(\"j\", \"y\");        // 2.0 only\n        txt = txt.replaceAll(\"^y3\", \"Y3\");     // 2.0 only\n        txt = txt.replaceAll(\"^y\", \"A\");       // 2.0 only\n        txt = txt.replaceAll(\"y\", \"3\");        // 2.0 only\n        txt = txt.replaceAll(\"3gh3\", \"3kh3\");\n        txt = txt.replaceAll(\"gh\", \"22\");\n        txt = txt.replaceAll(\"g\", \"k\");\n        txt = txt.replaceAll(\"s+\", \"S\");\n        txt = txt.replaceAll(\"t+\", \"T\");\n        txt = txt.replaceAll(\"p+\", \"P\");\n        txt = txt.replaceAll(\"k+\", \"K\");\n        txt = txt.replaceAll(\"f+\", \"F\");\n        txt = txt.replaceAll(\"m+\", \"M\");\n        txt = txt.replaceAll(\"n+\", \"N\");\n        txt = txt.replaceAll(\"w3\", \"W3\");\n        //txt = txt.replaceAll(\"wy\", \"Wy\");    // 1.0 only\n        txt = txt.replaceAll(\"wh3\", \"Wh3\");\n        txt = txt.replaceAll(\"w$\", \"3\");       // 2.0 only\n        //txt = txt.replaceAll(\"why\", \"Why\");  // 1.0 only\n        txt = txt.replaceAll(\"w\", \"2\");\n        txt = txt.replaceAll(\"^h\", \"A\");\n        txt = txt.replaceAll(\"h\", \"2\");\n        txt = txt.replaceAll(\"r3\", \"R3\");\n        txt = txt.replaceAll(\"r$\", \"3\");       // 2.0 only\n        //txt = txt.replaceAll(\"ry\", \"Ry\");    // 1.0 only\n        txt = txt.replaceAll(\"r\", \"2\");\n        txt = txt.replaceAll(\"l3\", \"L3\");\n        txt = txt.replaceAll(\"l$\", \"3\");       // 2.0 only\n        //txt = txt.replaceAll(\"ly\", \"Ly\");    // 1.0 only\n        txt = txt.replaceAll(\"l\", \"2\");\n        //txt = txt.replaceAll(\"j\", \"y\");      // 1.0 only\n        //txt = txt.replaceAll(\"y3\", \"Y3\");    // 1.0 only\n        //txt = txt.replaceAll(\"y\", \"2\");      // 1.0 only\n\n        // 5. Handle removals\n        txt = txt.replaceAll(\"2\", \"\");\n        txt = txt.replaceAll(\"3$\", \"A\");       // 2.0 only\n        txt = txt.replaceAll(\"3\", \"\");\n\n        // 6. put ten 1s on the end\n        txt = txt + \"111111\" + \"1111\";        // 1.0 only has 6 1s\n\n        // 7. take the first six characters as the code\n        return txt.substring(0, 10);          // 1.0 truncates to 6\n    }",
        "fixed_src_wo_comments": "public String caverphone ( String txt ) { if ( txt == null || txt . length ( ) == 0 ) { return \"1111111111\" ; } txt = txt . toLowerCase ( java . util . Locale . ENGLISH ) ; txt = txt . replaceAll ( \"[^a-z]\" , \"\" ) ; txt = txt . replaceAll ( \"e$\" , \"\" ) ; txt = txt . replaceAll ( \"^cough\" , \"cou2f\" ) ; txt = txt . replaceAll ( \"^rough\" , \"rou2f\" ) ; txt = txt . replaceAll ( \"^tough\" , \"tou2f\" ) ; txt = txt . replaceAll ( \"^enough\" , \"enou2f\" ) ; txt = txt . replaceAll ( \"^trough\" , \"trou2f\" ) ; txt = txt . replaceAll ( \"^gn\" , \"2n\" ) ; txt = txt . replaceAll ( \"mb$\" , \"m2\" ) ; txt = txt . replaceAll ( \"cq\" , \"2q\" ) ; txt = txt . replaceAll ( \"ci\" , \"si\" ) ; txt = txt . replaceAll ( \"ce\" , \"se\" ) ; txt = txt . replaceAll ( \"cy\" , \"sy\" ) ; txt = txt . replaceAll ( \"tch\" , \"2ch\" ) ; txt = txt . replaceAll ( \"c\" , \"k\" ) ; txt = txt . replaceAll ( \"q\" , \"k\" ) ; txt = txt . replaceAll ( \"x\" , \"k\" ) ; txt = txt . replaceAll ( \"v\" , \"f\" ) ; txt = txt . replaceAll ( \"dg\" , \"2g\" ) ; txt = txt . replaceAll ( \"tio\" , \"sio\" ) ; txt = txt . replaceAll ( \"tia\" , \"sia\" ) ; txt = txt . replaceAll ( \"d\" , \"t\" ) ; txt = txt . replaceAll ( \"ph\" , \"fh\" ) ; txt = txt . replaceAll ( \"b\" , \"p\" ) ; txt = txt . replaceAll ( \"sh\" , \"s2\" ) ; txt = txt . replaceAll ( \"z\" , \"s\" ) ; txt = txt . replaceAll ( \"^[aeiou]\" , \"A\" ) ; txt = txt . replaceAll ( \"[aeiou]\" , \"3\" ) ; txt = txt . replaceAll ( \"j\" , \"y\" ) ; txt = txt . replaceAll ( \"^y3\" , \"Y3\" ) ; txt = txt . replaceAll ( \"^y\" , \"A\" ) ; txt = txt . replaceAll ( \"y\" , \"3\" ) ; txt = txt . replaceAll ( \"3gh3\" , \"3kh3\" ) ; txt = txt . replaceAll ( \"gh\" , \"22\" ) ; txt = txt . replaceAll ( \"g\" , \"k\" ) ; txt = txt . replaceAll ( \"s+\" , \"S\" ) ; txt = txt . replaceAll ( \"t+\" , \"T\" ) ; txt = txt . replaceAll ( \"p+\" , \"P\" ) ; txt = txt . replaceAll ( \"k+\" , \"K\" ) ; txt = txt . replaceAll ( \"f+\" , \"F\" ) ; txt = txt . replaceAll ( \"m+\" , \"M\" ) ; txt = txt . replaceAll ( \"n+\" , \"N\" ) ; txt = txt . replaceAll ( \"w3\" , \"W3\" ) ; txt = txt . replaceAll ( \"wh3\" , \"Wh3\" ) ; txt = txt . replaceAll ( \"w$\" , \"3\" ) ; txt = txt . replaceAll ( \"w\" , \"2\" ) ; txt = txt . replaceAll ( \"^h\" , \"A\" ) ; txt = txt . replaceAll ( \"h\" , \"2\" ) ; txt = txt . replaceAll ( \"r3\" , \"R3\" ) ; txt = txt . replaceAll ( \"r$\" , \"3\" ) ; txt = txt . replaceAll ( \"r\" , \"2\" ) ; txt = txt . replaceAll ( \"l3\" , \"L3\" ) ; txt = txt . replaceAll ( \"l$\" , \"3\" ) ; txt = txt . replaceAll ( \"l\" , \"2\" ) ; txt = txt . replaceAll ( \"2\" , \"\" ) ; txt = txt . replaceAll ( \"3$\" , \"A\" ) ; txt = txt . replaceAll ( \"3\" , \"\" ) ; txt = txt + \"111111\" + \"1111\" ; return txt . substring ( 0 , 10 ) ; }",
        "summary": "Caverphone encodes names starting and ending with \"mb\" incorrectly.",
        "Description": "Caverphone encode names starting and ending with \"mb\" incorrectly.\n\nAccording to the spec:\n\"If the name ends with mb make it m2\".\n\nThis has been coded as:\n\"If the name _starts_ with mb make it m2\".",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-117",
        "comments": [
            "Forgot to check in this issue."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to check in the issue."
    },
    "Math_102_src/java/org/apache/commons/math/stat/inference/ChiSquareTestImpl.java_64_81": {
        "src": "public double chiSquare(double[] expected, long[] observed)\n        throws IllegalArgumentException {\n        if ((expected.length < 2) || (expected.length != observed.length)) {\n            throw new IllegalArgumentException(\n                    \"observed, expected array lengths incorrect\");\n        }\n        if (!isPositive(expected) || !isNonNegative(observed)) {\n            throw new IllegalArgumentException(\n                \"observed counts must be non-negative and expected counts must be postive\");\n        }\n        double sumSq = 0.0d;\n        double dev = 0.0d;\n        for (int i = 0; i < observed.length; i++) {\n                dev = ((double) observed[i] - expected[i]);\n                sumSq += dev * dev / expected[i];\n        }\n        return sumSq;\n    }",
        "src_wo_comments": "public double chiSquare ( double [ ] expected , long [ ] observed ) throws IllegalArgumentException { if ( ( expected . length < 2 ) || ( expected . length != observed . length ) ) { throw new IllegalArgumentException ( \"observed, expected array lengths incorrect\" ) ; } if ( ! isPositive ( expected ) || ! isNonNegative ( observed ) ) { throw new IllegalArgumentException ( \"observed counts must be non-negative and expected counts must be postive\" ) ; } double sumSq = 0.0d ; double dev = 0.0d ; for ( int i = 0 ; i < observed . length ; i ++ ) { dev = ( ( double ) observed [ i ] - expected [ i ] ) ; sumSq += dev * dev / expected [ i ] ; } return sumSq ; }",
        "fixed_src": "public double chiSquare(double[] expected, long[] observed)\n        throws IllegalArgumentException {\n        if ((expected.length < 2) || (expected.length != observed.length)) {\n            throw new IllegalArgumentException(\n                    \"observed, expected array lengths incorrect\");\n        }\n        if (!isPositive(expected) || !isNonNegative(observed)) {\n            throw new IllegalArgumentException(\n                \"observed counts must be non-negative and expected counts must be postive\");\n        }\n        double sumExpected = 0d;\n        double sumObserved = 0d;\n        for (int i = 0; i < observed.length; i++) {\n            sumExpected += expected[i];\n            sumObserved += observed[i];\n        }\n        double ratio = 1.0d;\n        boolean rescale = false;\n        if (Math.abs(sumExpected - sumObserved) > 10E-6) {\n            ratio = sumObserved / sumExpected;\n            rescale = true;\n        }\n        double sumSq = 0.0d;\n        double dev = 0.0d;\n        for (int i = 0; i < observed.length; i++) {\n            if (rescale) {\n                dev = ((double) observed[i] - ratio * expected[i]);\n                sumSq += dev * dev / (ratio * expected[i]);\n            } else {\n                dev = ((double) observed[i] - expected[i]);\n                sumSq += dev * dev / expected[i];\n            }\n        }\n        return sumSq;\n    }",
        "fixed_src_wo_comments": "public double chiSquare ( double [ ] expected , long [ ] observed ) throws IllegalArgumentException { if ( ( expected . length < 2 ) || ( expected . length != observed . length ) ) { throw new IllegalArgumentException ( \"observed, expected array lengths incorrect\" ) ; } if ( ! isPositive ( expected ) || ! isNonNegative ( observed ) ) { throw new IllegalArgumentException ( \"observed counts must be non-negative and expected counts must be postive\" ) ; } double sumExpected = 0d ; double sumObserved = 0d ; for ( int i = 0 ; i < observed . length ; i ++ ) { sumExpected += expected [ i ] ; sumObserved += observed [ i ] ; } double ratio = 1.0d ; boolean rescale = false ; if ( Math . abs ( sumExpected - sumObserved ) > 10E-6 ) { ratio = sumObserved / sumExpected ; rescale = true ; } double sumSq = 0.0d ; double dev = 0.0d ; for ( int i = 0 ; i < observed . length ; i ++ ) { if ( rescale ) { dev = ( ( double ) observed [ i ] - ratio * expected [ i ] ) ; sumSq += dev * dev / ( ratio * expected [ i ] ) ; } else { dev = ( ( double ) observed [ i ] - expected [ i ] ) ; sumSq += dev * dev / expected [ i ] ; } } return sumSq ; }",
        "summary": "chiSquare(double[] expected, long[] observed) is returning incorrect test statistic",
        "Description": "ChiSquareTestImpl is returning incorrect chi-squared value. An implicit assumption of public double chiSquare(double[] expected, long[] observed) is that the sum of expected and observed are equal. That is, in the code:\nfor (int i = 0; i < observed.length; i++) {\n            dev = ((double) observed[i] - expected[i]);\n            sumSq += dev * dev / expected[i];\n        }\nthis calculation is only correct if sum(observed)==sum(expected). When they are not equal then one must rescale the expected value by sum(observed) / sum(expected) so that they are.\nIronically, it is an example in the unit test ChiSquareTestTest that highlights the error:\n\nlong[] observed1 = { 500, 623, 72, 70, 31 };\n        double[] expected1 = { 485, 541, 82, 61, 37 };\n        assertEquals( \"chi-square test statistic\", 16.4131070362, testStatistic.chiSquare(expected1, observed1), 1E-10);\n        assertEquals(\"chi-square p-value\", 0.002512096, testStatistic.chiSquareTest(expected1, observed1), 1E-9);\n\n16.413 is not correct because the expected values do not make sense, they should be: 521.19403 581.37313  88.11940  65.55224  39.76119 so that the sum of expected equals 1296 which is the sum of observed.\n\nHere is some R code (r-project.org) which proves it:\n> o1\n[1] 500 623  72  70  31\n> e1\n[1] 485 541  82  61  37\n> chisq.test(o1,p=e1,rescale.p=TRUE)\n\n        Chi-squared test for given probabilities\n\ndata:  o1 \nX-squared = 9.0233, df = 4, p-value = 0.06052\n\n> chisq.test(o1,p=e1,rescale.p=TRUE)$observed\n[1] 500 623  72  70  31\n> chisq.test(o1,p=e1,rescale.p=TRUE)$expected\n[1] 521.19403 581.37313  88.11940  65.55224  39.76119\n\n\n\n\n\n ",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-175",
        "comments": [
            "here is the calculations currently in the code, and below the correct calculations",
            "Thank you for reporting this.  I agree that if the count sums are unequal, the test is not meaningful (at least I can't see a meaningful interpretation).  So the question is, do we throw IllegalArgumentException in this case or assume expected should be rescaled? ",
            "Hi Phil,\n\nI coded a rescaling, as below, but I have to admit that I spent a long\ntime puzzling over why results from Java differed from those with R\nbecause neither threw an exception or any warning that the argument sums\ndiffered. It just didn't occur to me at first that this was an issue.\n\nCarl\n\n\npackage com.archimedesmodel.automation.stats;\n\nimport org.apache.commons.math.stat.inference.ChiSquareTestImpl;\n\npublic class ArchiChiSquared extends ChiSquareTestImpl {\n\n\tpublic double chiSquare(double[] expected, long[] observed)\n\t\t\tthrows IllegalArgumentException {\n\t\tdouble sumSq = 0.0d;\n\t\tdouble dev = 0.0d;\n\t\tif ((expected.length < 2) || (expected.length !=\nobserved.length)) {\n\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\"observed, expected array\nlengths incorrect\");\n\t\t}\n\n\t\tdouble sumObs = 0;\n\t\tfor (int i = 0; i < observed.length; i++) {\n\t\t\tsumObs += observed[i];\n\t\t\tif (observed[i] < 0) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\t\"observed counts must be\nnon-negative\");\n\t\t\t}\n\t\t}\n\n\t\tdouble sumExp = 0;\n\t\tfor (int i = 0; i < expected.length; i++) {\n\t\t\tsumExp += expected[i];\n\t\t\tif (expected[i] <= 0) {\n\t\t\t\tthrow new IllegalArgumentException(\n\t\t\t\t\t\t\"expected counts must be\npostive\");\n\t\t\t}\n\t\t}\n\n\t\tdouble ratio = 1.0;\n\t\tif (Double.compare(sumObs, sumExp) != 0) {\n\t\t\t//log some warning?\n\t\t\tratio = sumObs / sumExp;\n\t\t}\n\n\t\tfor (int i = 0; i < observed.length; i++) {\n\t\t\tdev = ((double) observed[i] - ratio *\nexpected[i]);\n\t\t\tsumSq += dev * dev / (ratio * expected[i]);\n\t\t}\n\t\treturn sumSq;\n\t}\n\n}\n\n\n\n\n\n\n\n",
            "Automatic rescaling fix committed in r610274. "
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to rescale the expected values if the count sums are unequal, and this was committed in r610274."
    },
    "Math_78_src/main/java/org/apache/commons/math/ode/events/EventState.java_167_263": {
        "src": "public boolean evaluateStep(final StepInterpolator interpolator)\n        throws DerivativeException, EventException, ConvergenceException {\n\n        try {\n\n            forward = interpolator.isForward();\n            final double t1 = interpolator.getCurrentTime();\n            final int    n  = Math.max(1, (int) Math.ceil(Math.abs(t1 - t0) / maxCheckInterval));\n            final double h  = (t1 - t0) / n;\n\n            double ta = t0;\n            double ga = g0;\n            double tb = t0 + (interpolator.isForward() ? convergence : -convergence);\n            for (int i = 0; i < n; ++i) {\n\n                // evaluate handler value at the end of the substep\n                tb += h;\n                interpolator.setInterpolatedTime(tb);\n                final double gb = handler.g(tb, interpolator.getInterpolatedState());\n\n                // check events occurrence\n                if (g0Positive ^ (gb >= 0)) {\n                    // there is a sign change: an event is expected during this step\n\n                        // this is a corner case:\n                        // - there was an event near ta,\n                        // - there is another event between ta and tb\n                        // - when ta was computed, convergence was reached on the \"wrong side\" of the interval\n                        // this implies that the real sign of ga is the same as gb, so we need to slightly\n                        // shift ta to make sure ga and gb get opposite signs and the solver won't complain\n                        // about bracketing\n                            // this should never happen\n                         \n                    // variation direction, with respect to the integration direction\n                    increasing = gb >= ga;\n\n                    final UnivariateRealFunction f = new UnivariateRealFunction() {\n                        public double value(final double t) throws FunctionEvaluationException {\n                            try {\n                                interpolator.setInterpolatedTime(t);\n                                return handler.g(t, interpolator.getInterpolatedState());\n                            } catch (DerivativeException e) {\n                                throw new FunctionEvaluationException(e, t);\n                            } catch (EventException e) {\n                                throw new FunctionEvaluationException(e, t);\n                            }\n                        }\n                    };\n                    final BrentSolver solver = new BrentSolver();\n                    solver.setAbsoluteAccuracy(convergence);\n                    solver.setMaximalIterationCount(maxIterationCount);\n                    final double root = (ta <= tb) ? solver.solve(f, ta, tb) : solver.solve(f, tb, ta);\n                    if ((Math.abs(root - ta) <= convergence) &&\n                         (Math.abs(root - previousEventTime) <= convergence)) {\n                        // we have either found nothing or found (again ?) a past event, we simply ignore it\n                        ta = tb;\n                        ga = gb;\n                    } else if (Double.isNaN(previousEventTime) ||\n                               (Math.abs(previousEventTime - root) > convergence)) {\n                        pendingEventTime = root;\n                        if (pendingEvent && (Math.abs(t1 - pendingEventTime) <= convergence)) {\n                            // we were already waiting for this event which was\n                            // found during a previous call for a step that was\n                            // rejected, this step must now be accepted since it\n                            // properly ends exactly at the event occurrence\n                            return false;\n                        }\n                        // either we were not waiting for the event or it has\n                        // moved in such a way the step cannot be accepted\n                        pendingEvent = true;\n                        return true;\n                    }\n\n                } else {\n                    // no sign change: there is no event for now\n                    ta = tb;\n                    ga = gb;\n                }\n\n            }\n\n            // no event during the whole step\n            pendingEvent     = false;\n            pendingEventTime = Double.NaN;\n            return false;\n\n        } catch (FunctionEvaluationException e) {\n            final Throwable cause = e.getCause();\n            if ((cause != null) && (cause instanceof DerivativeException)) {\n                throw (DerivativeException) cause;\n            } else if ((cause != null) && (cause instanceof EventException)) {\n                throw (EventException) cause;\n            }\n            throw new EventException(e);\n        }\n\n    }",
        "src_wo_comments": "public boolean evaluateStep ( final StepInterpolator interpolator ) throws DerivativeException , EventException , ConvergenceException { try { forward = interpolator . isForward ( ) ; final double t1 = interpolator . getCurrentTime ( ) ; final int n = Math . max ( 1 , ( int ) Math . ceil ( Math . abs ( t1 - t0 ) / maxCheckInterval ) ) ; final double h = ( t1 - t0 ) / n ; double ta = t0 ; double ga = g0 ; double tb = t0 + ( interpolator . isForward ( ) ? convergence : - convergence ) ; for ( int i = 0 ; i < n ; ++ i ) { tb += h ; interpolator . setInterpolatedTime ( tb ) ; final double gb = handler . g ( tb , interpolator . getInterpolatedState ( ) ) ; if ( g0Positive ^ ( gb >= 0 ) ) { increasing = gb >= ga ; final UnivariateRealFunction f = new UnivariateRealFunction ( ) { public double value ( final double t ) throws FunctionEvaluationException { try { interpolator . setInterpolatedTime ( t ) ; return handler . g ( t , interpolator . getInterpolatedState ( ) ) ; } catch ( DerivativeException e ) { throw new FunctionEvaluationException ( e , t ) ; } catch ( EventException e ) { throw new FunctionEvaluationException ( e , t ) ; } } } ; final BrentSolver solver = new BrentSolver ( ) ; solver . setAbsoluteAccuracy ( convergence ) ; solver . setMaximalIterationCount ( maxIterationCount ) ; final double root = ( ta <= tb ) ? solver . solve ( f , ta , tb ) : solver . solve ( f , tb , ta ) ; if ( ( Math . abs ( root - ta ) <= convergence ) && ( Math . abs ( root - previousEventTime ) <= convergence ) ) { ta = tb ; ga = gb ; } else if ( Double . isNaN ( previousEventTime ) || ( Math . abs ( previousEventTime - root ) > convergence ) ) { pendingEventTime = root ; if ( pendingEvent && ( Math . abs ( t1 - pendingEventTime ) <= convergence ) ) { return false ; } pendingEvent = true ; return true ; } } else { ta = tb ; ga = gb ; } } pendingEvent = false ; pendingEventTime = Double . NaN ; return false ; } catch ( FunctionEvaluationException e ) { final Throwable cause = e . getCause ( ) ; if ( ( cause != null ) && ( cause instanceof DerivativeException ) ) { throw ( DerivativeException ) cause ; } else if ( ( cause != null ) && ( cause instanceof EventException ) ) { throw ( EventException ) cause ; } throw new EventException ( e ) ; } }",
        "fixed_src": "public boolean evaluateStep(final StepInterpolator interpolator)\n        throws DerivativeException, EventException, ConvergenceException {\n\n        try {\n\n            forward = interpolator.isForward();\n            final double t1 = interpolator.getCurrentTime();\n            final int    n  = Math.max(1, (int) Math.ceil(Math.abs(t1 - t0) / maxCheckInterval));\n            final double h  = (t1 - t0) / n;\n\n            double ta = t0;\n            double ga = g0;\n            double tb = t0 + (interpolator.isForward() ? convergence : -convergence);\n            for (int i = 0; i < n; ++i) {\n\n                // evaluate handler value at the end of the substep\n                tb += h;\n                interpolator.setInterpolatedTime(tb);\n                final double gb = handler.g(tb, interpolator.getInterpolatedState());\n\n                // check events occurrence\n                if (g0Positive ^ (gb >= 0)) {\n                    // there is a sign change: an event is expected during this step\n\n                    if (ga * gb > 0) {\n                        // this is a corner case:\n                        // - there was an event near ta,\n                        // - there is another event between ta and tb\n                        // - when ta was computed, convergence was reached on the \"wrong side\" of the interval\n                        // this implies that the real sign of ga is the same as gb, so we need to slightly\n                        // shift ta to make sure ga and gb get opposite signs and the solver won't complain\n                        // about bracketing\n                        final double epsilon = (forward ? 0.25 : -0.25) * convergence;\n                        for (int k = 0; (k < 4) && (ga * gb > 0); ++k) {\n                            ta += epsilon;\n                            interpolator.setInterpolatedTime(ta);\n                            ga = handler.g(ta, interpolator.getInterpolatedState());\n                        }\n                        if (ga * gb > 0) {\n                            // this should never happen\n                            throw MathRuntimeException.createInternalError(null);\n                        }\n                    }\n                         \n                    // variation direction, with respect to the integration direction\n                    increasing = gb >= ga;\n\n                    final UnivariateRealFunction f = new UnivariateRealFunction() {\n                        public double value(final double t) throws FunctionEvaluationException {\n                            try {\n                                interpolator.setInterpolatedTime(t);\n                                return handler.g(t, interpolator.getInterpolatedState());\n                            } catch (DerivativeException e) {\n                                throw new FunctionEvaluationException(e, t);\n                            } catch (EventException e) {\n                                throw new FunctionEvaluationException(e, t);\n                            }\n                        }\n                    };\n                    final BrentSolver solver = new BrentSolver();\n                    solver.setAbsoluteAccuracy(convergence);\n                    solver.setMaximalIterationCount(maxIterationCount);\n                    final double root = (ta <= tb) ? solver.solve(f, ta, tb) : solver.solve(f, tb, ta);\n                    if ((Math.abs(root - ta) <= convergence) &&\n                         (Math.abs(root - previousEventTime) <= convergence)) {\n                        // we have either found nothing or found (again ?) a past event, we simply ignore it\n                        ta = tb;\n                        ga = gb;\n                    } else if (Double.isNaN(previousEventTime) ||\n                               (Math.abs(previousEventTime - root) > convergence)) {\n                        pendingEventTime = root;\n                        if (pendingEvent && (Math.abs(t1 - pendingEventTime) <= convergence)) {\n                            // we were already waiting for this event which was\n                            // found during a previous call for a step that was\n                            // rejected, this step must now be accepted since it\n                            // properly ends exactly at the event occurrence\n                            return false;\n                        }\n                        // either we were not waiting for the event or it has\n                        // moved in such a way the step cannot be accepted\n                        pendingEvent = true;\n                        return true;\n                    }\n\n                } else {\n                    // no sign change: there is no event for now\n                    ta = tb;\n                    ga = gb;\n                }\n\n            }\n\n            // no event during the whole step\n            pendingEvent     = false;\n            pendingEventTime = Double.NaN;\n            return false;\n\n        } catch (FunctionEvaluationException e) {\n            final Throwable cause = e.getCause();\n            if ((cause != null) && (cause instanceof DerivativeException)) {\n                throw (DerivativeException) cause;\n            } else if ((cause != null) && (cause instanceof EventException)) {\n                throw (EventException) cause;\n            }\n            throw new EventException(e);\n        }\n\n    }",
        "fixed_src_wo_comments": "public boolean evaluateStep ( final StepInterpolator interpolator ) throws DerivativeException , EventException , ConvergenceException { try { forward = interpolator . isForward ( ) ; final double t1 = interpolator . getCurrentTime ( ) ; final int n = Math . max ( 1 , ( int ) Math . ceil ( Math . abs ( t1 - t0 ) / maxCheckInterval ) ) ; final double h = ( t1 - t0 ) / n ; double ta = t0 ; double ga = g0 ; double tb = t0 + ( interpolator . isForward ( ) ? convergence : - convergence ) ; for ( int i = 0 ; i < n ; ++ i ) { tb += h ; interpolator . setInterpolatedTime ( tb ) ; final double gb = handler . g ( tb , interpolator . getInterpolatedState ( ) ) ; if ( g0Positive ^ ( gb >= 0 ) ) { if ( ga * gb > 0 ) { final double epsilon = ( forward ? 0.25 : - 0.25 ) * convergence ; for ( int k = 0 ; ( k < 4 ) && ( ga * gb > 0 ) ; ++ k ) { ta += epsilon ; interpolator . setInterpolatedTime ( ta ) ; ga = handler . g ( ta , interpolator . getInterpolatedState ( ) ) ; } if ( ga * gb > 0 ) { throw MathRuntimeException . createInternalError ( null ) ; } } increasing = gb >= ga ; final UnivariateRealFunction f = new UnivariateRealFunction ( ) { public double value ( final double t ) throws FunctionEvaluationException { try { interpolator . setInterpolatedTime ( t ) ; return handler . g ( t , interpolator . getInterpolatedState ( ) ) ; } catch ( DerivativeException e ) { throw new FunctionEvaluationException ( e , t ) ; } catch ( EventException e ) { throw new FunctionEvaluationException ( e , t ) ; } } } ; final BrentSolver solver = new BrentSolver ( ) ; solver . setAbsoluteAccuracy ( convergence ) ; solver . setMaximalIterationCount ( maxIterationCount ) ; final double root = ( ta <= tb ) ? solver . solve ( f , ta , tb ) : solver . solve ( f , tb , ta ) ; if ( ( Math . abs ( root - ta ) <= convergence ) && ( Math . abs ( root - previousEventTime ) <= convergence ) ) { ta = tb ; ga = gb ; } else if ( Double . isNaN ( previousEventTime ) || ( Math . abs ( previousEventTime - root ) > convergence ) ) { pendingEventTime = root ; if ( pendingEvent && ( Math . abs ( t1 - pendingEventTime ) <= convergence ) ) { return false ; } pendingEvent = true ; return true ; } } else { ta = tb ; ga = gb ; } } pendingEvent = false ; pendingEventTime = Double . NaN ; return false ; } catch ( FunctionEvaluationException e ) { final Throwable cause = e . getCause ( ) ; if ( ( cause != null ) && ( cause instanceof DerivativeException ) ) { throw ( DerivativeException ) cause ; } else if ( ( cause != null ) && ( cause instanceof EventException ) ) { throw ( EventException ) cause ; } throw new EventException ( e ) ; } }",
        "summary": "during ODE integration, the last event in a pair of very close event may not be detected",
        "Description": "When an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let's say this step spans from 90.0 to 153.0. The switching function switches once again in this step.\n\nIf the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative.\n\nThis bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-322",
        "comments": [
            "fixed in subversion repository as of r887794"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the Subversion repository as of revision 887794."
    },
    "Math_97_src/java/org/apache/commons/math/analysis/BrentSolver.java_125_152": {
        "src": "public double solve(double min, double max) throws MaxIterationsExceededException, \n        FunctionEvaluationException {\n        \n        clearResult();\n        verifyInterval(min, max);\n        \n        double ret = Double.NaN;\n        \n        double yMin = f.value(min);\n        double yMax = f.value(max);\n        \n        // Verify bracketing\n        double sign = yMin * yMax;\n        if (sign >= 0) {\n            // check if either value is close to a zero\n                // neither value is close to zero and min and max do not bracket root.\n                throw new IllegalArgumentException\n                (\"Function values at endpoints do not have different signs.\" +\n                        \"  Endpoints: [\" + min + \",\" + max + \"]\" + \n                        \"  Values: [\" + yMin + \",\" + yMax + \"]\");\n        } else {\n            // solve using only the first endpoint as initial guess\n            ret = solve(min, yMin, max, yMax, min, yMin);\n            // either min or max is a root\n        }\n\n        return ret;\n    }",
        "src_wo_comments": "public double solve ( double min , double max ) throws MaxIterationsExceededException , FunctionEvaluationException { clearResult ( ) ; verifyInterval ( min , max ) ; double ret = Double . NaN ; double yMin = f . value ( min ) ; double yMax = f . value ( max ) ; double sign = yMin * yMax ; if ( sign >= 0 ) { throw new IllegalArgumentException ( \"Function values at endpoints do not have different signs.\" + \"  Endpoints: [\" + min + \",\" + max + \"]\" + \"  Values: [\" + yMin + \",\" + yMax + \"]\" ) ; } else { ret = solve ( min , yMin , max , yMax , min , yMin ) ; } return ret ; }",
        "fixed_src": "public double solve(double min, double max) throws MaxIterationsExceededException, \n        FunctionEvaluationException {\n        \n        clearResult();\n        verifyInterval(min, max);\n        \n        double ret = Double.NaN;\n        \n        double yMin = f.value(min);\n        double yMax = f.value(max);\n        \n        // Verify bracketing\n        double sign = yMin * yMax;\n        if (sign > 0) {\n            // check if either value is close to a zero\n            if (Math.abs(yMin) <= functionValueAccuracy) {\n                setResult(min, 0);\n                ret = min;\n            } else if (Math.abs(yMax) <= functionValueAccuracy) {\n                setResult(max, 0);\n                ret = max;\n            } else {\n                // neither value is close to zero and min and max do not bracket root.\n                throw new IllegalArgumentException\n                (\"Function values at endpoints do not have different signs.\" +\n                        \"  Endpoints: [\" + min + \",\" + max + \"]\" + \n                        \"  Values: [\" + yMin + \",\" + yMax + \"]\");\n            }\n        } else if (sign < 0){\n            // solve using only the first endpoint as initial guess\n            ret = solve(min, yMin, max, yMax, min, yMin);\n        } else {\n            // either min or max is a root\n            if (yMin == 0.0) {\n                ret = min;\n            } else {\n                ret = max;\n            }\n        }\n\n        return ret;\n    }",
        "fixed_src_wo_comments": "public double solve ( double min , double max ) throws MaxIterationsExceededException , FunctionEvaluationException { clearResult ( ) ; verifyInterval ( min , max ) ; double ret = Double . NaN ; double yMin = f . value ( min ) ; double yMax = f . value ( max ) ; double sign = yMin * yMax ; if ( sign > 0 ) { if ( Math . abs ( yMin ) <= functionValueAccuracy ) { setResult ( min , 0 ) ; ret = min ; } else if ( Math . abs ( yMax ) <= functionValueAccuracy ) { setResult ( max , 0 ) ; ret = max ; } else { throw new IllegalArgumentException ( \"Function values at endpoints do not have different signs.\" + \"  Endpoints: [\" + min + \",\" + max + \"]\" + \"  Values: [\" + yMin + \",\" + yMax + \"]\" ) ; } } else if ( sign < 0 ) { ret = solve ( min , yMin , max , yMax , min , yMin ) ; } else { if ( yMin == 0.0 ) { ret = min ; } else { ret = max ; } } return ret ; }",
        "summary": "BrentSolver throws IllegalArgumentException ",
        "Description": "I am getting this exception:\n\njava.lang.IllegalArgumentException: Function values at endpoints do not have different signs.  Endpoints: [-100000.0,1.7976931348623157E308]  Values: [0.0,-101945.04630982173]\nat org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:99)\nat org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:62)\n\nThe exception should not be thrown with values  [0.0,-101945.04630982173] because 0.0 is positive.\nAccording to Brent Worden, the algorithm should stop and return 0 as the root instead of throwing an exception.\n\nThe problem comes from this method:\n    public double solve(double min, double max) throws MaxIterationsExceededException, \n        FunctionEvaluationException {\n        \n        clearResult();\n        verifyInterval(min, max);\n        \n        double yMin = f.value(min);\n        double yMax = f.value(max);\n        \n        // Verify bracketing\n        if (yMin * yMax >= 0) {\n            throw new IllegalArgumentException\n            (\"Function values at endpoints do not have different signs.\" +\n                    \"  Endpoints: [\" + min + \",\" + max + \"]\" + \n                    \"  Values: [\" + yMin + \",\" + yMax + \"]\");       \n        }\n\n        // solve using only the first endpoint as initial guess\n        return solve(min, yMin, max, yMax, min, yMin);\n\n    }\n\nOne way to fix it would be to add this code after the assignment of yMin and yMax:\n        if (yMin ==0 || yMax == 0) {\n        \treturn 0;\n       \t}\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-204",
        "comments": [
            "SVN 654100.  added root checks for the endpoints."
        ],
        "summarized_discussion": "\n\nThe solution to bug SVN 654100 is to add root checks for the endpoints."
    },
    "Cli_8_src/java/org/apache/commons/cli/HelpFormatter.java_792_823": {
        "src": "protected StringBuffer renderWrappedText(StringBuffer sb, int width, \n                                             int nextLineTabStop, String text)\n    {\n        int pos = findWrapPos(text, width, 0);\n\n        if (pos == -1)\n        {\n            sb.append(rtrim(text));\n\n            return sb;\n        }\n        sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n\n        // all following lines must be padded with nextLineTabStop space \n        // characters\n        final String padding = createPadding(nextLineTabStop);\n\n        while (true)\n        {\n            text = padding + text.substring(pos).trim();\n            pos = findWrapPos(text, width, nextLineTabStop);\n\n            if (pos == -1)\n            {\n                sb.append(text);\n\n                return sb;\n            }\n\n            sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n        }\n    }",
        "src_wo_comments": "protected StringBuffer renderWrappedText ( StringBuffer sb , int width , int nextLineTabStop , String text ) { int pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( rtrim ( text ) ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; final String padding = createPadding ( nextLineTabStop ) ; while ( true ) { text = padding + text . substring ( pos ) . trim ( ) ; pos = findWrapPos ( text , width , nextLineTabStop ) ; if ( pos == - 1 ) { sb . append ( text ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; } }",
        "fixed_src": "protected StringBuffer renderWrappedText(StringBuffer sb, int width, \n                                             int nextLineTabStop, String text)\n    {\n        int pos = findWrapPos(text, width, 0);\n\n        if (pos == -1)\n        {\n            sb.append(rtrim(text));\n\n            return sb;\n        }\n        sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n\n        // all following lines must be padded with nextLineTabStop space \n        // characters\n        final String padding = createPadding(nextLineTabStop);\n\n        while (true)\n        {\n            text = padding + text.substring(pos).trim();\n            pos = findWrapPos(text, width, 0);\n\n            if (pos == -1)\n            {\n                sb.append(text);\n\n                return sb;\n            }\n\n            sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n        }\n    }",
        "fixed_src_wo_comments": "protected StringBuffer renderWrappedText ( StringBuffer sb , int width , int nextLineTabStop , String text ) { int pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( rtrim ( text ) ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; final String padding = createPadding ( nextLineTabStop ) ; while ( true ) { text = padding + text . substring ( pos ) . trim ( ) ; pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( text ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; } }",
        "summary": "HelpFormatter wraps incorrectly on every line beyond the first",
        "Description": "The method findWrapPos(...) in the HelpFormatter is a couple of bugs in the way that it deals with the \"startPos\" variable.  This causes it to format every line beyond the first line by \"startPos\" to many characters, beyond the specified width.  \n\nTo see this, create an option with a long description, and then use the help formatter to print it.  The first line will be the correct length.  The 2nd, 3rd, etc lines will all be too long.\n\nI don't have a patch (sorry) - but here is a corrected version of the method.\n\nI fixed it in two places - both were using \"width + startPos\" when they should have been using width.\n\n{code}\n protected int findWrapPos(String text, int width, int startPos)\n    {\n        int pos = -1;\n\n        // the line ends before the max wrap pos or a new line char found\n        if (((pos = text.indexOf('\\n', startPos)) != -1 && pos <= width)\n            || ((pos = text.indexOf('\\t', startPos)) != -1 && pos <= width))\n        {\n            return pos+1;\n        }\n        else if ((width) >= text.length())\n        {\n            return -1;\n        }\n\n\n        // look for the last whitespace character before startPos+width\n        pos = width;\n\n        char c;\n\n        while ((pos >= startPos) && ((c = text.charAt(pos)) != ' ')\n               && (c != '\\n') && (c != '\\r'))\n        {\n            --pos;\n        }\n\n        // if we found it - just return\n        if (pos > startPos)\n        {\n            return pos;\n        }\n        \n        // must look for the first whitespace chearacter after startPos \n        // + width\n        pos = startPos + width;\n\n        while ((pos <= text.length()) && ((c = text.charAt(pos)) != ' ')\n               && (c != '\\n') && (c != '\\r'))\n        {\n            ++pos;\n        }\n\n        return (pos == text.length())        ? (-1) : pos;\n    }\n{code}",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-151",
        "comments": [
            "I have observed this same problem.\n\nI'm not sure the suggested fix to {{findWrapPos}} is correct.  When I tried it, the {{testFindWrapPos}} test case failed with\n\n{noformat}\n[junit] wrap position 2 expected:<-1> but was:<16>\n[junit] junit.framework.AssertionFailedError:\n[junit]     wrap position 2 expected:<-1> but was:<16>\n[junit]   at org.apache.commons.cli.HelpFormatterTest.testFindWrapPos(\n[junit]     HelpFormatterTest.java:62)\n{noformat}\n\nI think the problem is actually in {{HelpFormatter.renderWrappedText}} ({{HelpFormatter.java}} line 812).  It calls {{findWrapPos}} with a start position of {{nextLineTabStop}}, but that's not right; it should be a start position of 0.\n\nThe line wrapping works for me after making this correction.  I will attach a patch containing this fix as well as an addition to the test cases to validate it.",
            "Line wrapping fix.",
            "Looks good to me - thanks for the report and the patch.\n\n---\n\nsvn ci -m \"Applying J. Lewis Muir's patch from CLI-151 fixing HelpFormatter so it wraps properly on multiple lines\"\n\nSending        src/java/org/apache/commons/cli/HelpFormatter.java\nSending        src/test/org/apache/commons/cli/HelpFormatterTest.java\nTransmitting file data ..\nCommitted revision 654428.",
            "This caused OutOfMemoryExceptions in some cases, so reopening.",
            "I've sat and looked at the code fresh, and I agree with the 0 as a fix here. Digging into the new failing test, I think the errors are in other code - so closing this out again."
        ],
        "summarized_discussion": "\n\nThe bug was solved by changing the start position of {{findWrapPos}} to 0 in {{HelpFormatter.renderWrappedText}}. This fix was applied and tested, and the line wrapping works correctly. OutOfMemoryExceptions were observed in some cases, so the issue was reopened and looked at again. After looking at the code, it was agreed that the 0 start position was the correct fix, and the issue was closed again."
    },
    "Cli_38_src/main/java/org/apache/commons/cli/DefaultParser.java_299_312": {
        "src": "private boolean isShortOption(String token)\n    {\n        // short options (-S, -SV, -S=V, -SV1=V2, -S1S2)\n        if (!token.startsWith(\"-\") || token.length() == 1)\n        {\n            return false;\n        }\n\n        // remove leading \"-\" and \"=value\"\n        int pos = token.indexOf(\"=\");\n        String optName = pos == -1 ? token.substring(1) : token.substring(1, pos);\n        return options.hasShortOption(optName);\n        // check for several concatenated short options\n    }",
        "src_wo_comments": "private boolean isShortOption ( String token ) { if ( ! token . startsWith ( \"-\" ) || token . length ( ) == 1 ) { return false ; } int pos = token . indexOf ( \"=\" ) ; String optName = pos == - 1 ? token . substring ( 1 ) : token . substring ( 1 , pos ) ; return options . hasShortOption ( optName ) ; }",
        "fixed_src": "private boolean isShortOption(String token)\n    {\n        // short options (-S, -SV, -S=V, -SV1=V2, -S1S2)\n        if (!token.startsWith(\"-\") || token.length() == 1)\n        {\n            return false;\n        }\n\n        // remove leading \"-\" and \"=value\"\n        int pos = token.indexOf(\"=\");\n        String optName = pos == -1 ? token.substring(1) : token.substring(1, pos);\n        if (options.hasShortOption(optName))\n        {\n            return true;\n        }\n        // check for several concatenated short options\n        return optName.length() > 0 && options.hasShortOption(String.valueOf(optName.charAt(0)));\n    }",
        "fixed_src_wo_comments": "private boolean isShortOption ( String token ) { if ( ! token . startsWith ( \"-\" ) || token . length ( ) == 1 ) { return false ; } int pos = token . indexOf ( \"=\" ) ; String optName = pos == - 1 ? token . substring ( 1 ) : token . substring ( 1 , pos ) ; if ( options . hasShortOption ( optName ) ) { return true ; } return optName . length ( ) > 0 && options . hasShortOption ( String . valueOf ( optName . charAt ( 0 ) ) ) ; }",
        "summary": "Optional argument picking up next regular option as its argument",
        "Description": null,
        "issue_url": "https://issues.apache.org/jira//browse/CLI-265",
        "comments": [
            "I have recently migrated a project from CLI 1.2 to 1.3.1 and have encountered what may be a bug or difference in the way optional arguments are being processed.\n\nI have a command that opens several different kinds of databases by type, or alternately, the last opened database of that type.\n\nOption TYPE1 = Option.builder(\"t1\").hasArg().numberOfArgs(1).optionalArg(true).argName(\"t1_path\").build();\nOption TYPE2 = Option.builder(\"t2\").hasArg().numberOfArgs(1).optionalArg(true).argName(\"t2_path\").build();\nOption LAST  =  Option.builder(\"last\").hasArg(false).build();\n\nCommands then look like \"open -t1 path/to/my/db\" or \"open -t1 -last\"\n\nIf I use the now deprecated GnuParser, both commands work as expected.  However, if I use the new DefaultParser, for the 2nd example, it thinks \"-last\" is the argument for -t1 rather than an option in its own right.\n\nI added the numberOfArgs(1) after reading a post on StackOverflow, but it made no difference in the behavior.  Only switching back to the GnuParser seemed to work.\n\n",
            "See also this question/answer on Stackoverflow for a slightly different symptom and potential fix.\n\nhttp://stackoverflow.com/questions/38964866/defaultparser-in-commons-cli-doesnt-behave-like-the-deprecated-parsers/38965293#38965293",
            "Seeing the exact same behavior as described here:\n\nhttp://markmail.org/thread/ozeke3q7ni572psi#query:+page:1+mid:i6gssbpyvxozt633+state:results\n",
            "{code}\n~/w/a/c/cli > svn ci -m \"CLI-265: Optional argument picking up next regular option as its argument. Thank you to Lynn Henderson, Martin Sandiford and Veit Guna for providing reproductions.\"\nSending        src/changes/changes.xml\nSending        src/main/java/org/apache/commons/cli/DefaultParser.java\nAdding         src/test/java/org/apache/commons/cli/bug/BugCLI265Test.java\nTransmitting file data ...done\nCommitting transaction...\nCommitted revision 1759695.\n{code}\n\nThank you!",
            "Hey Benedikt,  thanks for looking at this quickly.\n\nI'm not sure if this is a complete fix.  It seems to miss the case where short options are concatenated after an option that takes an optional argument.\n\nA failing test case for this would be to modify {{setUp()}} in BugCLI265Test.java to include short options \"a\" and \"b\":\n\n{code:java}\n    @Before\n    public void setUp() throws Exception {\n        parser = new DefaultParser();\n\n        Option TYPE1 = Option.builder(\"t1\").hasArg().numberOfArgs(1).optionalArg(true).argName(\"t1_path\").build();\n        Option OPTION_A = Option.builder(\"a\").hasArg(false).build();\n        Option OPTION_B = Option.builder(\"b\").hasArg(false).build();\n        Option LAST = Option.builder(\"last\").hasArg(false).build();\n\n        options = new Options().addOption(TYPE1).addOption(OPTION_A).addOption(OPTION_B).addOption(LAST);\n    }\n{code}\n\nAdd add a test for the concatenated options following an option with optional argument case:\n\n{code:java}\n    @Test\n    public void shouldParseConcatenatedShortOptions() throws Exception {\n      String[] concatenatedShortOptions = new String[] { \"-t1\", \"-ab\" };\n\n      final CommandLine commandLine = parser.parse(options, concatenatedShortOptions);\n\n      assertTrue(commandLine.hasOption(\"t1\"));\n      assertEquals(null, commandLine.getOptionValue(\"t1\"));\n      assertTrue(commandLine.hasOption(\"a\"));\n      assertTrue(commandLine.hasOption(\"b\"));\n      assertFalse(commandLine.hasOption(\"last\"));\n    }\n{code}\n\nOne possible fix is to check that at least the first character of the option is a short option if all the other cases fail in {{isShortOption(...)}} like so:\n\n{code:java}\n    private boolean isShortOption(String token)\n    {\n        // short options (-S, -SV, -S=V, -SV1=V2, -S1S2)\n        if (!token.startsWith(\"-\") || token.length() == 1)\n        {\n            return false;\n        }\n\n        // remove leading \"-\" and \"=value\"\n        int pos = token.indexOf(\"=\");\n        String optName = pos == -1 ? token.substring(1) : token.substring(1, pos);\n        if (options.hasShortOption(optName))\n        {\n            return true;\n        }\n        return optName.length() > 0 && options.hasShortOption(String.valueOf(optName.charAt(0)));\n    }\n{code}",
            "Reopen issue to address the problem identified by Martin Sandiford.",
            "See http://svn.apache.org/r1759745",
            "Sorry it's been a few years, but I'm in the process of updating some libraries for an existing project, including CLI 1.4 (downloaded today), and I notice the bug I reported before still seems to be present in 1.4.\r\n\r\nIf I have an option that takes an optional argument but don't include one, it's picking up the next option as if it were the argument.\r\n\r\n\u00a0Option DB = Option\r\n .builder(\"db\").hasArg().optionalArg(true).numberOfArgs(1).build();\r\n\r\nOption LAST = Option.builder(\"last\").hasArg(false)\r\n .desc(\"Open the most recently opened database\")\r\n .required(false).build();\r\n\r\nCommands are of the form \"open -db /path/to/db\" or \"open -db -last\".\r\n\r\nExecuting \"open -db -last\" with 1.4 using the DefaultParser still gives me this error:\r\n ERROR Database \"-last\" not found (i.e., it thinks it's the arg to option -db).\r\n\r\nAs with 1.2 and 1.3.1, if I use the now deprecated GnuParser, it works as expected.\r\n\r\nIn 1.4, reversing the order of the options works with DefaultParser, but this shouldn't be required: \"open -last -db\"",
            "Turns out I had a dependent project that was still using 1.3.1, so that ending up taking precedence in my classpath instead of 1.4.\u00a0 Apologies for the false alarm."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to check that at least the first character of the option is a short option if all the other cases fail in isShortOption() in DefaultParser.java. This was committed in revision 1759745."
    },
    "Compress_36_src/main/java/org/apache/commons/compress/archivers/sevenz/SevenZFile.java_901_916": {
        "src": "private InputStream getCurrentStream() throws IOException {\n        if (deferredBlockStreams.isEmpty()) {\n            throw new IllegalStateException(\"No current 7z entry (call getNextEntry() first).\");\n        }\n        \n        while (deferredBlockStreams.size() > 1) {\n            // In solid compression mode we need to decompress all leading folder'\n            // streams to get access to an entry. We defer this until really needed\n            // so that entire blocks can be skipped without wasting time for decompression.\n            final InputStream stream = deferredBlockStreams.remove(0);\n            IOUtils.skip(stream, Long.MAX_VALUE);\n            stream.close();\n        }\n\n        return deferredBlockStreams.get(0);\n    }",
        "src_wo_comments": "private InputStream getCurrentStream ( ) throws IOException { if ( deferredBlockStreams . isEmpty ( ) ) { throw new IllegalStateException ( \"No current 7z entry (call getNextEntry() first).\" ) ; } while ( deferredBlockStreams . size ( ) > 1 ) { final InputStream stream = deferredBlockStreams . remove ( 0 ) ; IOUtils . skip ( stream , Long . MAX_VALUE ) ; stream . close ( ) ; } return deferredBlockStreams . get ( 0 ) ; }",
        "fixed_src": "private InputStream getCurrentStream() throws IOException {\n        if (archive.files[currentEntryIndex].getSize() == 0) {\n            return new ByteArrayInputStream(new byte[0]);\n        }\n        if (deferredBlockStreams.isEmpty()) {\n            throw new IllegalStateException(\"No current 7z entry (call getNextEntry() first).\");\n        }\n        \n        while (deferredBlockStreams.size() > 1) {\n            // In solid compression mode we need to decompress all leading folder'\n            // streams to get access to an entry. We defer this until really needed\n            // so that entire blocks can be skipped without wasting time for decompression.\n            final InputStream stream = deferredBlockStreams.remove(0);\n            IOUtils.skip(stream, Long.MAX_VALUE);\n            stream.close();\n        }\n\n        return deferredBlockStreams.get(0);\n    }",
        "fixed_src_wo_comments": "private InputStream getCurrentStream ( ) throws IOException { if ( archive . files [ currentEntryIndex ] . getSize ( ) == 0 ) { return new ByteArrayInputStream ( new byte [ 0 ] ) ; } if ( deferredBlockStreams . isEmpty ( ) ) { throw new IllegalStateException ( \"No current 7z entry (call getNextEntry() first).\" ) ; } while ( deferredBlockStreams . size ( ) > 1 ) { final InputStream stream = deferredBlockStreams . remove ( 0 ) ; IOUtils . skip ( stream , Long . MAX_VALUE ) ; stream . close ( ) ; } return deferredBlockStreams . get ( 0 ) ; }",
        "summary": "Calling SevenZFile.read() on empty SevenZArchiveEntry throws IllegalStateException",
        "Description": "I'm pretty sure COMPRESS-340 breaks reading empty archive entries. When calling getNextEntry() and that entry has no content, the code jumps into the first block at line 830 (SevenZFile.class), clearing the deferredBlockStreams. When calling entry.read(...) afterwards an IllegalStateException (\"No current 7z entry (call getNextEntry() first).\") is thrown. IMHO, there should be another check for entry.getSize() == 0.\n\nThis worked correctly up until 1.10.\n",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-348",
        "comments": [
            "Thank you for creating a new issue for this.\n\nI had seen your comments in COMPRESS-340 but wanted to create a testcase before opening a new issue - unfortunately I still need to find a time slot to do this.",
            "You are correct, I see what you describe. At least for my simplistic archive the situation is fixed with git comit d050157 - it would be good if you could confirm this with your own archives."
        ],
        "summarized_discussion": "\n\nThe solution to the bug appears to be git commit d050157, and it would be beneficial to confirm this with the user's own archives."
    },
    "Math_39_src/main/java/org/apache/commons/math/ode/nonstiff/EmbeddedRungeKuttaIntegrator.java_190_328": {
        "src": "@Override\n  public void integrate(final ExpandableStatefulODE equations, final double t)\n      throws MathIllegalStateException, MathIllegalArgumentException {\n\n    sanityChecks(equations, t);\n    setEquations(equations);\n    final boolean forward = t > equations.getTime();\n\n    // create some internal working arrays\n    final double[] y0  = equations.getCompleteState();\n    final double[] y = y0.clone();\n    final int stages = c.length + 1;\n    final double[][] yDotK = new double[stages][y.length];\n    final double[] yTmp    = y0.clone();\n    final double[] yDotTmp = new double[y.length];\n\n    // set up an interpolator sharing the integrator arrays\n    final RungeKuttaStepInterpolator interpolator = (RungeKuttaStepInterpolator) prototype.copy();\n    interpolator.reinitialize(this, yTmp, yDotK, forward,\n                              equations.getPrimaryMapper(), equations.getSecondaryMappers());\n    interpolator.storeTime(equations.getTime());\n\n    // set up integration control objects\n    stepStart         = equations.getTime();\n    double  hNew      = 0;\n    boolean firstTime = true;\n    initIntegration(equations.getTime(), y0, t);\n\n    // main integration loop\n    isLastStep = false;\n    do {\n\n      interpolator.shift();\n\n      // iterate over step size, ensuring local normalized error is smaller than 1\n      double error = 10;\n      while (error >= 1.0) {\n\n        if (firstTime || !fsal) {\n          // first stage\n          computeDerivatives(stepStart, y, yDotK[0]);\n        }\n\n        if (firstTime) {\n          final double[] scale = new double[mainSetDimension];\n          if (vecAbsoluteTolerance == null) {\n              for (int i = 0; i < scale.length; ++i) {\n                scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * FastMath.abs(y[i]);\n              }\n          } else {\n              for (int i = 0; i < scale.length; ++i) {\n                scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * FastMath.abs(y[i]);\n              }\n          }\n          hNew = initializeStep(forward, getOrder(), scale,\n                                stepStart, y, yDotK[0], yTmp, yDotK[1]);\n          firstTime = false;\n        }\n\n        stepSize = hNew;\n\n        // next stages\n        for (int k = 1; k < stages; ++k) {\n\n          for (int j = 0; j < y0.length; ++j) {\n            double sum = a[k-1][0] * yDotK[0][j];\n            for (int l = 1; l < k; ++l) {\n              sum += a[k-1][l] * yDotK[l][j];\n            }\n            yTmp[j] = y[j] + stepSize * sum;\n          }\n\n          computeDerivatives(stepStart + c[k-1] * stepSize, yTmp, yDotK[k]);\n\n        }\n\n        // estimate the state at the end of the step\n        for (int j = 0; j < y0.length; ++j) {\n          double sum    = b[0] * yDotK[0][j];\n          for (int l = 1; l < stages; ++l) {\n            sum    += b[l] * yDotK[l][j];\n          }\n          yTmp[j] = y[j] + stepSize * sum;\n        }\n\n        // estimate the error at the end of the step\n        error = estimateError(yDotK, y, yTmp, stepSize);\n        if (error >= 1.0) {\n          // reject the step and attempt to reduce error by stepsize control\n          final double factor =\n              FastMath.min(maxGrowth,\n                           FastMath.max(minReduction, safety * FastMath.pow(error, exp)));\n          hNew = filterStep(stepSize * factor, forward, false);\n        }\n\n      }\n\n      // local error is small enough: accept the step, trigger events and step handlers\n      interpolator.storeTime(stepStart + stepSize);\n      System.arraycopy(yTmp, 0, y, 0, y0.length);\n      System.arraycopy(yDotK[stages - 1], 0, yDotTmp, 0, y0.length);\n      stepStart = acceptStep(interpolator, y, yDotTmp, t);\n      System.arraycopy(y, 0, yTmp, 0, y.length);\n\n      if (!isLastStep) {\n\n          // prepare next step\n          interpolator.storeTime(stepStart);\n\n          if (fsal) {\n              // save the last evaluation for the next step\n              System.arraycopy(yDotTmp, 0, yDotK[0], 0, y0.length);\n          }\n\n          // stepsize control for next step\n          final double factor =\n              FastMath.min(maxGrowth, FastMath.max(minReduction, safety * FastMath.pow(error, exp)));\n          final double  scaledH    = stepSize * factor;\n          final double  nextT      = stepStart + scaledH;\n          final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t);\n          hNew = filterStep(scaledH, forward, nextIsLast);\n\n          final double  filteredNextT      = stepStart + hNew;\n          final boolean filteredNextIsLast = forward ? (filteredNextT >= t) : (filteredNextT <= t);\n          if (filteredNextIsLast) {\n              hNew = t - stepStart;\n          }\n\n      }\n\n    } while (!isLastStep);\n\n    // dispatch results\n    equations.setTime(stepStart);\n    equations.setCompleteState(y);\n\n    resetInternalState();\n\n  }",
        "src_wo_comments": "@ Override public void integrate ( final ExpandableStatefulODE equations , final double t ) throws MathIllegalStateException , MathIllegalArgumentException { sanityChecks ( equations , t ) ; setEquations ( equations ) ; final boolean forward = t > equations . getTime ( ) ; final double [ ] y0 = equations . getCompleteState ( ) ; final double [ ] y = y0 . clone ( ) ; final int stages = c . length + 1 ; final double [ ] [ ] yDotK = new double [ stages ] [ y . length ] ; final double [ ] yTmp = y0 . clone ( ) ; final double [ ] yDotTmp = new double [ y . length ] ; final RungeKuttaStepInterpolator interpolator = ( RungeKuttaStepInterpolator ) prototype . copy ( ) ; interpolator . reinitialize ( this , yTmp , yDotK , forward , equations . getPrimaryMapper ( ) , equations . getSecondaryMappers ( ) ) ; interpolator . storeTime ( equations . getTime ( ) ) ; stepStart = equations . getTime ( ) ; double hNew = 0 ; boolean firstTime = true ; initIntegration ( equations . getTime ( ) , y0 , t ) ; isLastStep = false ; do { interpolator . shift ( ) ; double error = 10 ; while ( error >= 1.0 ) { if ( firstTime || ! fsal ) { computeDerivatives ( stepStart , y , yDotK [ 0 ] ) ; } if ( firstTime ) { final double [ ] scale = new double [ mainSetDimension ] ; if ( vecAbsoluteTolerance == null ) { for ( int i = 0 ; i < scale . length ; ++ i ) { scale [ i ] = scalAbsoluteTolerance + scalRelativeTolerance * FastMath . abs ( y [ i ] ) ; } } else { for ( int i = 0 ; i < scale . length ; ++ i ) { scale [ i ] = vecAbsoluteTolerance [ i ] + vecRelativeTolerance [ i ] * FastMath . abs ( y [ i ] ) ; } } hNew = initializeStep ( forward , getOrder ( ) , scale , stepStart , y , yDotK [ 0 ] , yTmp , yDotK [ 1 ] ) ; firstTime = false ; } stepSize = hNew ; for ( int k = 1 ; k < stages ; ++ k ) { for ( int j = 0 ; j < y0 . length ; ++ j ) { double sum = a [ k - 1 ] [ 0 ] * yDotK [ 0 ] [ j ] ; for ( int l = 1 ; l < k ; ++ l ) { sum += a [ k - 1 ] [ l ] * yDotK [ l ] [ j ] ; } yTmp [ j ] = y [ j ] + stepSize * sum ; } computeDerivatives ( stepStart + c [ k - 1 ] * stepSize , yTmp , yDotK [ k ] ) ; } for ( int j = 0 ; j < y0 . length ; ++ j ) { double sum = b [ 0 ] * yDotK [ 0 ] [ j ] ; for ( int l = 1 ; l < stages ; ++ l ) { sum += b [ l ] * yDotK [ l ] [ j ] ; } yTmp [ j ] = y [ j ] + stepSize * sum ; } error = estimateError ( yDotK , y , yTmp , stepSize ) ; if ( error >= 1.0 ) { final double factor = FastMath . min ( maxGrowth , FastMath . max ( minReduction , safety * FastMath . pow ( error , exp ) ) ) ; hNew = filterStep ( stepSize * factor , forward , false ) ; } } interpolator . storeTime ( stepStart + stepSize ) ; System . arraycopy ( yTmp , 0 , y , 0 , y0 . length ) ; System . arraycopy ( yDotK [ stages - 1 ] , 0 , yDotTmp , 0 , y0 . length ) ; stepStart = acceptStep ( interpolator , y , yDotTmp , t ) ; System . arraycopy ( y , 0 , yTmp , 0 , y . length ) ; if ( ! isLastStep ) { interpolator . storeTime ( stepStart ) ; if ( fsal ) { System . arraycopy ( yDotTmp , 0 , yDotK [ 0 ] , 0 , y0 . length ) ; } final double factor = FastMath . min ( maxGrowth , FastMath . max ( minReduction , safety * FastMath . pow ( error , exp ) ) ) ; final double scaledH = stepSize * factor ; final double nextT = stepStart + scaledH ; final boolean nextIsLast = forward ? ( nextT >= t ) : ( nextT <= t ) ; hNew = filterStep ( scaledH , forward , nextIsLast ) ; final double filteredNextT = stepStart + hNew ; final boolean filteredNextIsLast = forward ? ( filteredNextT >= t ) : ( filteredNextT <= t ) ; if ( filteredNextIsLast ) { hNew = t - stepStart ; } } } while ( ! isLastStep ) ; equations . setTime ( stepStart ) ; equations . setCompleteState ( y ) ; resetInternalState ( ) ; }",
        "fixed_src": "@Override\n  public void integrate(final ExpandableStatefulODE equations, final double t)\n      throws MathIllegalStateException, MathIllegalArgumentException {\n\n    sanityChecks(equations, t);\n    setEquations(equations);\n    final boolean forward = t > equations.getTime();\n\n    // create some internal working arrays\n    final double[] y0  = equations.getCompleteState();\n    final double[] y = y0.clone();\n    final int stages = c.length + 1;\n    final double[][] yDotK = new double[stages][y.length];\n    final double[] yTmp    = y0.clone();\n    final double[] yDotTmp = new double[y.length];\n\n    // set up an interpolator sharing the integrator arrays\n    final RungeKuttaStepInterpolator interpolator = (RungeKuttaStepInterpolator) prototype.copy();\n    interpolator.reinitialize(this, yTmp, yDotK, forward,\n                              equations.getPrimaryMapper(), equations.getSecondaryMappers());\n    interpolator.storeTime(equations.getTime());\n\n    // set up integration control objects\n    stepStart         = equations.getTime();\n    double  hNew      = 0;\n    boolean firstTime = true;\n    initIntegration(equations.getTime(), y0, t);\n\n    // main integration loop\n    isLastStep = false;\n    do {\n\n      interpolator.shift();\n\n      // iterate over step size, ensuring local normalized error is smaller than 1\n      double error = 10;\n      while (error >= 1.0) {\n\n        if (firstTime || !fsal) {\n          // first stage\n          computeDerivatives(stepStart, y, yDotK[0]);\n        }\n\n        if (firstTime) {\n          final double[] scale = new double[mainSetDimension];\n          if (vecAbsoluteTolerance == null) {\n              for (int i = 0; i < scale.length; ++i) {\n                scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * FastMath.abs(y[i]);\n              }\n          } else {\n              for (int i = 0; i < scale.length; ++i) {\n                scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * FastMath.abs(y[i]);\n              }\n          }\n          hNew = initializeStep(forward, getOrder(), scale,\n                                stepStart, y, yDotK[0], yTmp, yDotK[1]);\n          firstTime = false;\n        }\n\n        stepSize = hNew;\n        if (forward) {\n            if (stepStart + stepSize >= t) {\n                stepSize = t - stepStart;\n            }\n        } else {\n            if (stepStart + stepSize <= t) {\n                stepSize = t - stepStart;\n            }\n        }\n\n        // next stages\n        for (int k = 1; k < stages; ++k) {\n\n          for (int j = 0; j < y0.length; ++j) {\n            double sum = a[k-1][0] * yDotK[0][j];\n            for (int l = 1; l < k; ++l) {\n              sum += a[k-1][l] * yDotK[l][j];\n            }\n            yTmp[j] = y[j] + stepSize * sum;\n          }\n\n          computeDerivatives(stepStart + c[k-1] * stepSize, yTmp, yDotK[k]);\n\n        }\n\n        // estimate the state at the end of the step\n        for (int j = 0; j < y0.length; ++j) {\n          double sum    = b[0] * yDotK[0][j];\n          for (int l = 1; l < stages; ++l) {\n            sum    += b[l] * yDotK[l][j];\n          }\n          yTmp[j] = y[j] + stepSize * sum;\n        }\n\n        // estimate the error at the end of the step\n        error = estimateError(yDotK, y, yTmp, stepSize);\n        if (error >= 1.0) {\n          // reject the step and attempt to reduce error by stepsize control\n          final double factor =\n              FastMath.min(maxGrowth,\n                           FastMath.max(minReduction, safety * FastMath.pow(error, exp)));\n          hNew = filterStep(stepSize * factor, forward, false);\n        }\n\n      }\n\n      // local error is small enough: accept the step, trigger events and step handlers\n      interpolator.storeTime(stepStart + stepSize);\n      System.arraycopy(yTmp, 0, y, 0, y0.length);\n      System.arraycopy(yDotK[stages - 1], 0, yDotTmp, 0, y0.length);\n      stepStart = acceptStep(interpolator, y, yDotTmp, t);\n      System.arraycopy(y, 0, yTmp, 0, y.length);\n\n      if (!isLastStep) {\n\n          // prepare next step\n          interpolator.storeTime(stepStart);\n\n          if (fsal) {\n              // save the last evaluation for the next step\n              System.arraycopy(yDotTmp, 0, yDotK[0], 0, y0.length);\n          }\n\n          // stepsize control for next step\n          final double factor =\n              FastMath.min(maxGrowth, FastMath.max(minReduction, safety * FastMath.pow(error, exp)));\n          final double  scaledH    = stepSize * factor;\n          final double  nextT      = stepStart + scaledH;\n          final boolean nextIsLast = forward ? (nextT >= t) : (nextT <= t);\n          hNew = filterStep(scaledH, forward, nextIsLast);\n\n          final double  filteredNextT      = stepStart + hNew;\n          final boolean filteredNextIsLast = forward ? (filteredNextT >= t) : (filteredNextT <= t);\n          if (filteredNextIsLast) {\n              hNew = t - stepStart;\n          }\n\n      }\n\n    } while (!isLastStep);\n\n    // dispatch results\n    equations.setTime(stepStart);\n    equations.setCompleteState(y);\n\n    resetInternalState();\n\n  }",
        "fixed_src_wo_comments": "@ Override public void integrate ( final ExpandableStatefulODE equations , final double t ) throws MathIllegalStateException , MathIllegalArgumentException { sanityChecks ( equations , t ) ; setEquations ( equations ) ; final boolean forward = t > equations . getTime ( ) ; final double [ ] y0 = equations . getCompleteState ( ) ; final double [ ] y = y0 . clone ( ) ; final int stages = c . length + 1 ; final double [ ] [ ] yDotK = new double [ stages ] [ y . length ] ; final double [ ] yTmp = y0 . clone ( ) ; final double [ ] yDotTmp = new double [ y . length ] ; final RungeKuttaStepInterpolator interpolator = ( RungeKuttaStepInterpolator ) prototype . copy ( ) ; interpolator . reinitialize ( this , yTmp , yDotK , forward , equations . getPrimaryMapper ( ) , equations . getSecondaryMappers ( ) ) ; interpolator . storeTime ( equations . getTime ( ) ) ; stepStart = equations . getTime ( ) ; double hNew = 0 ; boolean firstTime = true ; initIntegration ( equations . getTime ( ) , y0 , t ) ; isLastStep = false ; do { interpolator . shift ( ) ; double error = 10 ; while ( error >= 1.0 ) { if ( firstTime || ! fsal ) { computeDerivatives ( stepStart , y , yDotK [ 0 ] ) ; } if ( firstTime ) { final double [ ] scale = new double [ mainSetDimension ] ; if ( vecAbsoluteTolerance == null ) { for ( int i = 0 ; i < scale . length ; ++ i ) { scale [ i ] = scalAbsoluteTolerance + scalRelativeTolerance * FastMath . abs ( y [ i ] ) ; } } else { for ( int i = 0 ; i < scale . length ; ++ i ) { scale [ i ] = vecAbsoluteTolerance [ i ] + vecRelativeTolerance [ i ] * FastMath . abs ( y [ i ] ) ; } } hNew = initializeStep ( forward , getOrder ( ) , scale , stepStart , y , yDotK [ 0 ] , yTmp , yDotK [ 1 ] ) ; firstTime = false ; } stepSize = hNew ; if ( forward ) { if ( stepStart + stepSize >= t ) { stepSize = t - stepStart ; } } else { if ( stepStart + stepSize <= t ) { stepSize = t - stepStart ; } } for ( int k = 1 ; k < stages ; ++ k ) { for ( int j = 0 ; j < y0 . length ; ++ j ) { double sum = a [ k - 1 ] [ 0 ] * yDotK [ 0 ] [ j ] ; for ( int l = 1 ; l < k ; ++ l ) { sum += a [ k - 1 ] [ l ] * yDotK [ l ] [ j ] ; } yTmp [ j ] = y [ j ] + stepSize * sum ; } computeDerivatives ( stepStart + c [ k - 1 ] * stepSize , yTmp , yDotK [ k ] ) ; } for ( int j = 0 ; j < y0 . length ; ++ j ) { double sum = b [ 0 ] * yDotK [ 0 ] [ j ] ; for ( int l = 1 ; l < stages ; ++ l ) { sum += b [ l ] * yDotK [ l ] [ j ] ; } yTmp [ j ] = y [ j ] + stepSize * sum ; } error = estimateError ( yDotK , y , yTmp , stepSize ) ; if ( error >= 1.0 ) { final double factor = FastMath . min ( maxGrowth , FastMath . max ( minReduction , safety * FastMath . pow ( error , exp ) ) ) ; hNew = filterStep ( stepSize * factor , forward , false ) ; } } interpolator . storeTime ( stepStart + stepSize ) ; System . arraycopy ( yTmp , 0 , y , 0 , y0 . length ) ; System . arraycopy ( yDotK [ stages - 1 ] , 0 , yDotTmp , 0 , y0 . length ) ; stepStart = acceptStep ( interpolator , y , yDotTmp , t ) ; System . arraycopy ( y , 0 , yTmp , 0 , y . length ) ; if ( ! isLastStep ) { interpolator . storeTime ( stepStart ) ; if ( fsal ) { System . arraycopy ( yDotTmp , 0 , yDotK [ 0 ] , 0 , y0 . length ) ; } final double factor = FastMath . min ( maxGrowth , FastMath . max ( minReduction , safety * FastMath . pow ( error , exp ) ) ) ; final double scaledH = stepSize * factor ; final double nextT = stepStart + scaledH ; final boolean nextIsLast = forward ? ( nextT >= t ) : ( nextT <= t ) ; hNew = filterStep ( scaledH , forward , nextIsLast ) ; final double filteredNextT = stepStart + hNew ; final boolean filteredNextIsLast = forward ? ( filteredNextT >= t ) : ( filteredNextT <= t ) ; if ( filteredNextIsLast ) { hNew = t - stepStart ; } } } while ( ! isLastStep ) ; equations . setTime ( stepStart ) ; equations . setCompleteState ( y ) ; resetInternalState ( ) ; }",
        "summary": "too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)",
        "Description": "Adaptive step size integrators compute the first step size by themselves if it is not provided.\nFor embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-727",
        "comments": [
            "Fixed in subversion repository as of r1215524."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of revision 1215524."
    },
    "Codec_5_src/java/org/apache/commons/codec/binary/Base64.java_550_599": {
        "src": "void decode(byte[] in, int inPos, int inAvail) {\n        if (eof) {\n            return;\n        }\n        if (inAvail < 0) {\n            eof = true;\n        }\n        for (int i = 0; i < inAvail; i++) {\n            if (buffer == null || buffer.length - pos < decodeSize) {\n                resizeBuffer();\n            }\n            byte b = in[inPos++];\n            if (b == PAD) {\n                // We're done.\n                eof = true;\n                break;\n            } else {\n                if (b >= 0 && b < DECODE_TABLE.length) {\n                    int result = DECODE_TABLE[b];\n                    if (result >= 0) {\n                        modulus = (++modulus) % 4;\n                        x = (x << 6) + result;\n                        if (modulus == 0) {\n                            buffer[pos++] = (byte) ((x >> 16) & MASK_8BITS);\n                            buffer[pos++] = (byte) ((x >> 8) & MASK_8BITS);\n                            buffer[pos++] = (byte) (x & MASK_8BITS);\n                        }\n                    }\n                }\n            }\n        }\n\n        // Two forms of EOF as far as base64 decoder is concerned: actual\n        // EOF (-1) and first time '=' character is encountered in stream.\n        // This approach makes the '=' padding characters completely optional.\n        if (eof && modulus != 0) {\n            \n            x = x << 6;\n            switch (modulus) {\n                case 2 :\n                    x = x << 6;\n                    buffer[pos++] = (byte) ((x >> 16) & MASK_8BITS);\n                    break;\n                case 3 :\n                    buffer[pos++] = (byte) ((x >> 16) & MASK_8BITS);\n                    buffer[pos++] = (byte) ((x >> 8) & MASK_8BITS);\n                    break;\n            }\n        }\n    }",
        "src_wo_comments": "void decode ( byte [ ] in , int inPos , int inAvail ) { if ( eof ) { return ; } if ( inAvail < 0 ) { eof = true ; } for ( int i = 0 ; i < inAvail ; i ++ ) { if ( buffer == null || buffer . length - pos < decodeSize ) { resizeBuffer ( ) ; } byte b = in [ inPos ++ ] ; if ( b == PAD ) { eof = true ; break ; } else { if ( b >= 0 && b < DECODE_TABLE . length ) { int result = DECODE_TABLE [ b ] ; if ( result >= 0 ) { modulus = ( ++ modulus ) % 4 ; x = ( x << 6 ) + result ; if ( modulus == 0 ) { buffer [ pos ++ ] = ( byte ) ( ( x >> 16 ) & MASK_8BITS ) ; buffer [ pos ++ ] = ( byte ) ( ( x >> 8 ) & MASK_8BITS ) ; buffer [ pos ++ ] = ( byte ) ( x & MASK_8BITS ) ; } } } } } if ( eof && modulus != 0 ) { x = x << 6 ; switch ( modulus ) { case 2 : x = x << 6 ; buffer [ pos ++ ] = ( byte ) ( ( x >> 16 ) & MASK_8BITS ) ; break ; case 3 : buffer [ pos ++ ] = ( byte ) ( ( x >> 16 ) & MASK_8BITS ) ; buffer [ pos ++ ] = ( byte ) ( ( x >> 8 ) & MASK_8BITS ) ; break ; } } }",
        "fixed_src": "void decode(byte[] in, int inPos, int inAvail) {\n        if (eof) {\n            return;\n        }\n        if (inAvail < 0) {\n            eof = true;\n        }\n        for (int i = 0; i < inAvail; i++) {\n            if (buffer == null || buffer.length - pos < decodeSize) {\n                resizeBuffer();\n            }\n            byte b = in[inPos++];\n            if (b == PAD) {\n                // We're done.\n                eof = true;\n                break;\n            } else {\n                if (b >= 0 && b < DECODE_TABLE.length) {\n                    int result = DECODE_TABLE[b];\n                    if (result >= 0) {\n                        modulus = (++modulus) % 4;\n                        x = (x << 6) + result;\n                        if (modulus == 0) {\n                            buffer[pos++] = (byte) ((x >> 16) & MASK_8BITS);\n                            buffer[pos++] = (byte) ((x >> 8) & MASK_8BITS);\n                            buffer[pos++] = (byte) (x & MASK_8BITS);\n                        }\n                    }\n                }\n            }\n        }\n\n        // Two forms of EOF as far as base64 decoder is concerned: actual\n        // EOF (-1) and first time '=' character is encountered in stream.\n        // This approach makes the '=' padding characters completely optional.\n        if (eof && modulus != 0) {\n            if (buffer == null || buffer.length - pos < decodeSize) {\n                resizeBuffer();\n            }\n            \n            x = x << 6;\n            switch (modulus) {\n                case 2 :\n                    x = x << 6;\n                    buffer[pos++] = (byte) ((x >> 16) & MASK_8BITS);\n                    break;\n                case 3 :\n                    buffer[pos++] = (byte) ((x >> 16) & MASK_8BITS);\n                    buffer[pos++] = (byte) ((x >> 8) & MASK_8BITS);\n                    break;\n            }\n        }\n    }",
        "fixed_src_wo_comments": "void decode ( byte [ ] in , int inPos , int inAvail ) { if ( eof ) { return ; } if ( inAvail < 0 ) { eof = true ; } for ( int i = 0 ; i < inAvail ; i ++ ) { if ( buffer == null || buffer . length - pos < decodeSize ) { resizeBuffer ( ) ; } byte b = in [ inPos ++ ] ; if ( b == PAD ) { eof = true ; break ; } else { if ( b >= 0 && b < DECODE_TABLE . length ) { int result = DECODE_TABLE [ b ] ; if ( result >= 0 ) { modulus = ( ++ modulus ) % 4 ; x = ( x << 6 ) + result ; if ( modulus == 0 ) { buffer [ pos ++ ] = ( byte ) ( ( x >> 16 ) & MASK_8BITS ) ; buffer [ pos ++ ] = ( byte ) ( ( x >> 8 ) & MASK_8BITS ) ; buffer [ pos ++ ] = ( byte ) ( x & MASK_8BITS ) ; } } } } } if ( eof && modulus != 0 ) { if ( buffer == null || buffer . length - pos < decodeSize ) { resizeBuffer ( ) ; } x = x << 6 ; switch ( modulus ) { case 2 : x = x << 6 ; buffer [ pos ++ ] = ( byte ) ( ( x >> 16 ) & MASK_8BITS ) ; break ; case 3 : buffer [ pos ++ ] = ( byte ) ( ( x >> 16 ) & MASK_8BITS ) ; buffer [ pos ++ ] = ( byte ) ( ( x >> 8 ) & MASK_8BITS ) ; break ; } } }",
        "summary": "Base64InputStream causes NullPointerException on some input",
        "Description": "Certain (malformed?) input to {{Base64InputStream}} causes a {{NullPointerException}} in {{Base64.decode}}.\n\nThe exception occurs when {{Base64.decode}} is entered with the following conditions:\n\n* {{buffer}} is {{null}}\n* {{modulus}} is {{3}} from a previous entry.\n* {{inAvail}} is {{-1}} because {{Base64InputStream.read}} reached EOF on line 150.\n\nUnder these conditions, {{Base64.decode}} reaches line 581 with {{buffer}} still {{null}} and throws a {{NullPointerException}}.\n\nHere is some input data that will trigger it:\n\n{noformat}\nH4sIAAAAAAAAAFvzloG1uIhBKiuxLFGvODW5tCizpFIvODM9LzXFPykrNbmE8//eDC2bq/+ZGJij\nGdiT8/NKUvNKShiYop2iGTiLgQoTS0qLUgsZ6hgYfRh4SjJSE3PS84GmZOSWMAj5gMzVz0nMS9cP\nLinKzEu3rigoLQJpXvNZ/AcbR8gDJgaGigIGBqbLayAuMUxNKdVLTyxJTc7QS07WSyzKLC7JL8lJ\n1StJLErMKynNSdTLyUxOzStO1fOB0AwQwMjEwOrJwJMbn+mSWFkclpiTmeID4joml2SWpYZk5qaW\nMEj45Bel62flpyTqlwAF9F2A9oBkrMEqnYtSoXyob1hy4z1dShgEIL4oLcnM0Q8N9XQBqubKjYfa\nDjTV1AfoZn2Im/WTk/XhbtaHu1kf6mZ9T5g2YED8BwKgj8WAbtIDuUkP5CY9mJt22FSkZEXf/QkK\noCIGeVRFSYlA/zsBCZjq//9/PvSP1VvMxMDkxcCe6ZuZk5NZ7MPAnemcUZSfl5+Tn15ZwiCF5n2E\nnDUoDhjVfhrpNABdpI5qWTJYmZ5nsD9Cg0pwSWnSyhOCaYXmAerMoDgsxnAkzG1R+XmpYPXL9Bln\n1RhJPQarL+dgYNM1MLUyMKioKAYFOCvIBb8vl8qCOFxA4/jAiRIU7HqgYN8zk/n7jNxWfbAXeXJS\nE4tLgOnUKbOk2IuBOzcfzqso6M1QmrzKkedPzcYO3QZu129As4xITlZI6QqYFNhz44v9EkFpCGua\nLmEQdkktS83JL8gF5g4FqBGlIJ+wAI1gKJtZEvTws/j3FluPu4lcr7ra9OfHKXIZNTa4FPd8n33J\nQXPFLte9AZe5uBaJvGrKVl+rbrTaXDZO6NwU7gnHOVgzzsmnGX2Y5GDqrst8wcTear0Ab1yj6PrD\nF977vL/5iUMg773My5qLLK8OVAu6Tz7Xcyjy9Uym02Z/+xY7m85nYo/t4E93FXFKOf9/a3X78neS\njE5Tu066K3Mdf17m66mbpXN9y34ZZ3ErRobfn+RfzVBIWj0vc82vY7YPvM5eLHHOulV77M6CoB4h\nxb/FjHWHRR+ldb6QmSP1ROGwGs+nx2quwitN7+mIpsRFhU37JPRoZe2ZjiX/70j7CS1tz51YP/3W\n/xfnV2i/4rAoYeAN9nA0NTQqBxYMQcGOAG5\n{noformat}\n\nSay this is read from file with a {{byte[]}} of size {{1024}} using {{Base64InputStream.read(byte[])}}.  In the first iteration, all {{1190}} bytes get read into {{buf}}, then it enters {{Base64.setInitialBuffer}} and assigns the {{byte[1024]}} to {{buffer}} and does a round of decoding.  When it then enters {{Base64.readResults}} on line {{162}} in {{Base64InputStream}}, it sets {{buffer}} to {{null}}, {{modulus}} has the left-over value {{3}}, and the NPE occurs the next iteration.\n\n{{Base64InputStream}} could avoid this by returning right away on EOF ({{-1}}), but I think the real fix needs to happen in {{Base64}} since it this same situation could be created by direct use.  My guess is either more needs to happen in the body of the {{if}} on line {{542}} (set {{modulus}} to {{0}}?) or the condition on line {{573}} is flawed and needs adjusting.\n",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-98",
        "comments": [
            "Removed 'malformed' from summary since I'm not sure this is actually the case.",
            "Thank you for the report. Can you please provide a unit test that demonstrate the issue?",
            "Here's one with a shorter input.  Basically, you can encode something to Base64 and then knock a character or two off the end of a line and it will usually be a triggering input.  However, if you use the input below with everything from \"//\" to the end removed, it doesn't trigger the NPE.\n\n\\\\\n{code:title=Base64NPETest.java}\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.io.UnsupportedEncodingException;\n\nimport org.apache.commons.codec.binary.Base64InputStream;\nimport org.junit.Test;\n\npublic class Base64NPETest\n{\n\tpublic static final String INPUT =\n\t\t\"H4sIAAAAAAAAAFvzloG1uIhBKiuxLFGvODW5tCizpFIvODM9LzXFPykrNbmE8//eDC2bq/+ZGJi\";\n\n\t@Test\n\tpublic void testCodec98() throws UnsupportedEncodingException\n\t{\n\t\tByteArrayInputStream data = new ByteArrayInputStream(INPUT.getBytes(\"UTF-8\"));\n\t\ttry\n\t\t{\n\t\t\tBase64InputStream stream = new Base64InputStream(data);\n\t\t\tbyte[] buf = new byte[1024];\n\t\t\tint read = 0;\n\t\t\twhile( read != -1 )\n\t\t\t\tread = stream.read(buf);\n\t\t\t// success if no NPE by this point\n\t\t}\n\t\tcatch(IOException ignore) \n\t\t{ \n\t\t\tSystem.err.println(\"Ignoring IOException\");\n\t\t}\n\t}\n}\n{code}",
            "So what should the behavior be on bad input? Garbage in, garbage out, no NPE? It's not complete garbage out in the case you describe where the end of the transmission is lopped off and causes an NPE.",
            "From {{Base64InputStream}}, {{IOException}} or a more specific subclass thereof would be appropriate.  Client code handling the stream will have to be expecting a possible {{IOException}}, but a NPE when {{read}}'s argument is known to be non-{{null}} is not expected and unlikely to be caught in a helpful place (if it gets caught at all).\n\nFor {{Base64}}, {{DecoderException}} could be thrown by {{Base64.decode}}.  Although this is adding a checked exception, the method is package-private so you should have some flexibility.  It looks like the public methods already throw this.  {{Base64InputStream}} could then wrap the {{DecoderException}} in an {{IOException}}.\n\nIf someone had a use-case for wanting the portion that was decoded successfully despite knowing an error occurred, you could modify the exception to allow partial results to be attached.  Personally, I don't consider this necessary but others may disagree.",
            "The attached patch file fixes it!  I'll upload unit tests within next 2 hours.\n\nI'm using my own test case:\nYWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXpBQkNERUZHSElKS0xNTk9QUVJTVFVWV1hZWjAxMjM\n\nNotice that Base64.decodeBase64() is able to deal with it:\nabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123\n\nBut Base64InputStream is not (with buffer of size 1024).  So this isn't an example of garbage-in/garbage-out.  This is valid input blowing up inside Base64InputStream!\n",
            "\nHere are some JUnits to trigger the NPE.  I managed to trigger it with Base64OutputStream as well.\n\n",
            "Julius: \n\nYour patch https://issues.apache.org/jira/secure/attachment/12439927/codec-98-fix.patch is not licensed for Apache, there is no Apache feather next to the patch link in this ticket.\n\nCan remove and resubmit? Thank you.",
            "Sorry, accidentially clicked \" Attachment not intended for inclusion\" radio last time.\n\nJust to be clear, this 3 line patch _IS_ granted to ASF for inclusion in ASF works.\n\n\n\n",
            "Committed to trunk: http://svn.apache.org/viewvc?view=revision&revision=950267"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to modify the exception to allow partial results to be attached, and to throw an IOException or a more specific subclass thereof when read's argument is known to be non-null. The patch file submitted to https://issues.apache.org/jira/secure/attachment/12439927/codec-98-fix.patch is licensed for Apache and has been committed to the trunk."
    },
    "JxPath_5_src/java/org/apache/commons/jxpath/ri/model/NodePointer.java_642_675": {
        "src": "private int compareNodePointers(\n        NodePointer p1,\n        int depth1,\n        NodePointer p2,\n        int depth2) \n    {\n        if (depth1 < depth2) {\n            int r = compareNodePointers(p1, depth1, p2.parent, depth2 - 1);\n            return r == 0 ? -1 : r;\n        }\n        if (depth1 > depth2) {\n            int r = compareNodePointers(p1.parent, depth1 - 1, p2, depth2);\n            return r == 0 ? 1 : r;\n        }\n        if (p1 == null && p2 == null) {\n            return 0;\n        }\n\n        if (p1 != null && p1.equals(p2)) {\n            return 0;\n        }\n\n        if (depth1 == 1) {\n            throw new JXPathException(\n                    \"Cannot compare pointers that do not belong to the same tree: '\"\n                            + p1 + \"' and '\" + p2 + \"'\");\n        }\n        int r = compareNodePointers(p1.parent, depth1 - 1, p2.parent, depth2 - 1);\n        if (r != 0) {\n            return r;\n        }\n\n        return p1.parent.compareChildNodePointers(p1, p2);\n    }",
        "src_wo_comments": "private int compareNodePointers ( NodePointer p1 , int depth1 , NodePointer p2 , int depth2 ) { if ( depth1 < depth2 ) { int r = compareNodePointers ( p1 , depth1 , p2 . parent , depth2 - 1 ) ; return r == 0 ? - 1 : r ; } if ( depth1 > depth2 ) { int r = compareNodePointers ( p1 . parent , depth1 - 1 , p2 , depth2 ) ; return r == 0 ? 1 : r ; } if ( p1 == null && p2 == null ) { return 0 ; } if ( p1 != null && p1 . equals ( p2 ) ) { return 0 ; } if ( depth1 == 1 ) { throw new JXPathException ( \"Cannot compare pointers that do not belong to the same tree: '\" + p1 + \"' and '\" + p2 + \"'\" ) ; } int r = compareNodePointers ( p1 . parent , depth1 - 1 , p2 . parent , depth2 - 1 ) ; if ( r != 0 ) { return r ; } return p1 . parent . compareChildNodePointers ( p1 , p2 ) ; }",
        "fixed_src": "private int compareNodePointers(\n        NodePointer p1,\n        int depth1,\n        NodePointer p2,\n        int depth2) \n    {\n        if (depth1 < depth2) {\n            int r = compareNodePointers(p1, depth1, p2.parent, depth2 - 1);\n            return r == 0 ? -1 : r;\n        }\n        if (depth1 > depth2) {\n            int r = compareNodePointers(p1.parent, depth1 - 1, p2, depth2);\n            return r == 0 ? 1 : r;\n        }\n        if (p1 == null && p2 == null) {\n            return 0;\n        }\n\n        if (p1 != null && p1.equals(p2)) {\n            return 0;\n        }\n\n        if (depth1 == 1) {\n            return 0;\n        }\n        int r = compareNodePointers(p1.parent, depth1 - 1, p2.parent, depth2 - 1);\n        if (r != 0) {\n            return r;\n        }\n\n        return p1.parent.compareChildNodePointers(p1, p2);\n    }",
        "fixed_src_wo_comments": "private int compareNodePointers ( NodePointer p1 , int depth1 , NodePointer p2 , int depth2 ) { if ( depth1 < depth2 ) { int r = compareNodePointers ( p1 , depth1 , p2 . parent , depth2 - 1 ) ; return r == 0 ? - 1 : r ; } if ( depth1 > depth2 ) { int r = compareNodePointers ( p1 . parent , depth1 - 1 , p2 , depth2 ) ; return r == 0 ? 1 : r ; } if ( p1 == null && p2 == null ) { return 0 ; } if ( p1 != null && p1 . equals ( p2 ) ) { return 0 ; } if ( depth1 == 1 ) { return 0 ; } int r = compareNodePointers ( p1 . parent , depth1 - 1 , p2 . parent , depth2 - 1 ) ; if ( r != 0 ) { return r ; } return p1 . parent . compareChildNodePointers ( p1 , p2 ) ; }",
        "summary": "Cannot compare pointers that do not belong to the same tree",
        "Description": "For XPath \"$var | /MAIN/A\" exception is thrown:\n\norg.apache.commons.jxpath.JXPathException: Cannot compare pointers that do not belong to the same tree: '$var' and ''\n\tat org.apache.commons.jxpath.ri.model.NodePointer.compareNodePointers(NodePointer.java:665)\n\tat org.apache.commons.jxpath.ri.model.NodePointer.compareNodePointers(NodePointer.java:649)\n\tat org.apache.commons.jxpath.ri.model.NodePointer.compareNodePointers(NodePointer.java:649)\n\tat org.apache.commons.jxpath.ri.model.NodePointer.compareTo(NodePointer.java:639)\n\tat java.util.Arrays.mergeSort(Arrays.java:1152)\n\tat java.util.Arrays.sort(Arrays.java:1079)\n\tat java.util.Collections.sort(Collections.java:113)\n\tat org.apache.commons.jxpath.ri.EvalContext.constructIterator(EvalContext.java:176)\n\tat org.apache.commons.jxpath.ri.EvalContext.hasNext(EvalContext.java:100)\n\tat org.apache.commons.jxpath.JXPathContext.selectNodes(JXPathContext.java:648)\n\tat org.apache.commons.jxpath.ri.model.VariablePointerTestCase.testUnionOfVariableAndNode(VariablePointerTestCase.java:76)",
        "issue_url": "https://issues.apache.org/jira//browse/JXPATH-89",
        "comments": [
            "Test case",
            "fixed in trunk, rev. 552591.  Thanks for the report!",
            "Matt,\n\nIs it possible to fix it without relaxing restriction?\n\nI.e. is there a mechanism to put some part of tree to variable, and use this variable later as part of the tree, not as part of another tree.\n\nRemoving exceptions looks like just a workaround. Sorting is not working anyway.",
            "Pretending a variable is a child of some random parent seems like the workaround to me.  I removed the exception because I saw the problem differently:  if two nodes have no common parents, then their comparison is undefined and thus irrelevant.  I'm not sure what you're referring to when you say sorting is not working.  The existing testcases that relied on the sort did not break, so I can only assume you're talking about sorting things that formerly broke in the manner of your original report.  Can you provide an example and explain what expectations you have that are not being met?",
            "Matt,\n\nThe example is in original test case. I extract several variables from original tree and want to select them (and sort) in one XPath expression.\n\nI can't do this right now, because variables are from different trees... but is it possible to \"remember\" that variables are from original tree?",
            "But that's just the problem:  a variable isn't part of the object graph represented by the JXPathContext; it exists parallel to that graph.  So how would you expect the results of such a query to sort?",
            "For example, Variables interface should allow to return not the object itself, but node pointer, so value will be part of original tree.\n\nThen at \"sorting stage\" variable is unpacked and original node pointer (if such exists) is used.",
            "I'm afraid I still don't follow you.  The variables exist as an externally-injected dataset to use in conjunction with the JXPathContext.  If in your own code you know that the variable's value is somewhere in the graph accessible from a given parent object, you may be able to do some custom work to search it out.   My current opinion is that the route by which a value is reached _defines_ its location, because multiple routes may lead to the same object.  This means that a variable rightly has no contextual information by which it can correctly be compared to true path nodes.",
            "Let me try to implement my idea and discuss this later :)",
            "\"Hack\" patch is included. This patch only illustates the idea, it should not be included in project without validation.",
            "Updates test case. See the second testUnionOfVariableByPointerAndNode() test.\n\nMain idea - to return nodePointer instead of variable value in getVariable() method.",
            "Matt,\n\nplease, look at the updated test case testUnionOfVariableByPointerAndNode(). \"Hach\" patch should be applied to jxpath/src for test case to work."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to return a node pointer instead of a variable value in the getVariable() method, and to apply a \"hack\" patch to the jxpath/src for the test case to work."
    },
    "Chart_17_source/org/jfree/data/time/TimeSeries.java_856_859": {
        "src": "public Object clone() throws CloneNotSupportedException {\n        Object clone = createCopy(0, getItemCount() - 1);\n        return clone;\n    }",
        "src_wo_comments": "public Object clone ( ) throws CloneNotSupportedException { Object clone = createCopy ( 0 , getItemCount ( ) - 1 ) ; return clone ; }",
        "fixed_src": "public Object clone() throws CloneNotSupportedException {\n        TimeSeries clone = (TimeSeries) super.clone();\n        clone.data = (List) ObjectUtilities.deepClone(this.data);\n        return clone;\n    }",
        "fixed_src_wo_comments": "public Object clone ( ) throws CloneNotSupportedException { TimeSeries clone = ( TimeSeries ) super . clone ( ) ; clone . data = ( List ) ObjectUtilities . deepClone ( this . data ) ; return clone ; }",
        "summary": "cloning of TimeSeries",
        "Description": "It's just a minor bug!\n\nWhen I clone a TimeSeries which has no items, I get an IllegalArgumentException (\"Requires start <= end\").\nBut I don't think the user should be responsible for checking whether the TimeSeries has any items or not.",
        "issue_url": "https://sourceforge.net/p/jfreechart/bugs/803/",
        "comments": [
            {
                "content": "labels: --> General\nassigned_to: nobody --> mungady\nstatus: open --> closed-fixed"
            },
            {
                "content": "Logged In: YES\nuser_id=112975\nOriginator: NO\n\nThanks for the report. I've committed a fix to Subversion, for inclusion in the 1.0.8 release."
            }
        ],
        "summarized_discussion": "\n\nThe bug has been assigned to mungady, the status has been changed to closed-fixed, and a fix has been committed to Subversion for inclusion in the 1.0.8 release."
    },
    "Mockito_9_src/org/mockito/internal/stubbing/answers/CallsRealMethods.java_35_37": {
        "src": "public Object answer(InvocationOnMock invocation) throws Throwable {\n        return invocation.callRealMethod();\n    }",
        "src_wo_comments": "public Object answer ( InvocationOnMock invocation ) throws Throwable { return invocation . callRealMethod ( ) ; }",
        "fixed_src": "public Object answer(InvocationOnMock invocation) throws Throwable {\n    \tif (Modifier.isAbstract(invocation.getMethod().getModifiers())) {\n    \t\treturn new GloballyConfiguredAnswer().answer(invocation);\n    \t}\n        return invocation.callRealMethod();\n    }",
        "fixed_src_wo_comments": "public Object answer ( InvocationOnMock invocation ) throws Throwable { if ( Modifier . isAbstract ( invocation . getMethod ( ) . getModifiers ( ) ) ) { return new GloballyConfiguredAnswer ( ) . answer ( invocation ) ; } return invocation . callRealMethod ( ) ; }",
        "summary": "Problem spying on abstract classes",
        "Description": "There's a problem with spying on abstract classes when the real implementation calls out to the abstract method. More details: #121 \n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug was related to a function that was not correctly returning a value. The solution was to edit the function to make sure it returns the correct value."
    },
    "Mockito_8_src/org/mockito/internal/util/reflection/GenericMetadataSupport.java_66_84": {
        "src": "protected void registerTypeVariablesOn(Type classType) {\n        if (!(classType instanceof ParameterizedType)) {\n            return;\n        }\n        ParameterizedType parameterizedType = (ParameterizedType) classType;\n        TypeVariable[] typeParameters = ((Class<?>) parameterizedType.getRawType()).getTypeParameters();\n        Type[] actualTypeArguments = parameterizedType.getActualTypeArguments();\n        for (int i = 0; i < actualTypeArguments.length; i++) {\n            TypeVariable typeParameter = typeParameters[i];\n            Type actualTypeArgument = actualTypeArguments[i];\n\n            if (actualTypeArgument instanceof WildcardType) {\n                contextualActualTypeParameters.put(typeParameter, boundsOf((WildcardType) actualTypeArgument));\n            } else {\n                contextualActualTypeParameters.put(typeParameter, actualTypeArgument);\n            }\n            // logger.log(\"For '\" + parameterizedType + \"' found type variable : { '\" + typeParameter + \"(\" + System.identityHashCode(typeParameter) + \")\" + \"' : '\" + actualTypeArgument + \"(\" + System.identityHashCode(typeParameter) + \")\" + \"' }\");\n        }\n    }",
        "src_wo_comments": "protected void registerTypeVariablesOn ( Type classType ) { if ( ! ( classType instanceof ParameterizedType ) ) { return ; } ParameterizedType parameterizedType = ( ParameterizedType ) classType ; TypeVariable [ ] typeParameters = ( ( Class < ? > ) parameterizedType . getRawType ( ) ) . getTypeParameters ( ) ; Type [ ] actualTypeArguments = parameterizedType . getActualTypeArguments ( ) ; for ( int i = 0 ; i < actualTypeArguments . length ; i ++ ) { TypeVariable typeParameter = typeParameters [ i ] ; Type actualTypeArgument = actualTypeArguments [ i ] ; if ( actualTypeArgument instanceof WildcardType ) { contextualActualTypeParameters . put ( typeParameter , boundsOf ( ( WildcardType ) actualTypeArgument ) ) ; } else { contextualActualTypeParameters . put ( typeParameter , actualTypeArgument ) ; } } }",
        "fixed_src": "protected void registerTypeVariablesOn(Type classType) {\n        if (!(classType instanceof ParameterizedType)) {\n            return;\n        }\n        ParameterizedType parameterizedType = (ParameterizedType) classType;\n        TypeVariable[] typeParameters = ((Class<?>) parameterizedType.getRawType()).getTypeParameters();\n        Type[] actualTypeArguments = parameterizedType.getActualTypeArguments();\n        for (int i = 0; i < actualTypeArguments.length; i++) {\n            TypeVariable typeParameter = typeParameters[i];\n            Type actualTypeArgument = actualTypeArguments[i];\n\n            if (actualTypeArgument instanceof WildcardType) {\n                contextualActualTypeParameters.put(typeParameter, boundsOf((WildcardType) actualTypeArgument));\n            } else if (typeParameter != actualTypeArgument) {\n                contextualActualTypeParameters.put(typeParameter, actualTypeArgument);\n            }\n            // logger.log(\"For '\" + parameterizedType + \"' found type variable : { '\" + typeParameter + \"(\" + System.identityHashCode(typeParameter) + \")\" + \"' : '\" + actualTypeArgument + \"(\" + System.identityHashCode(typeParameter) + \")\" + \"' }\");\n        }\n    }",
        "fixed_src_wo_comments": "protected void registerTypeVariablesOn ( Type classType ) { if ( ! ( classType instanceof ParameterizedType ) ) { return ; } ParameterizedType parameterizedType = ( ParameterizedType ) classType ; TypeVariable [ ] typeParameters = ( ( Class < ? > ) parameterizedType . getRawType ( ) ) . getTypeParameters ( ) ; Type [ ] actualTypeArguments = parameterizedType . getActualTypeArguments ( ) ; for ( int i = 0 ; i < actualTypeArguments . length ; i ++ ) { TypeVariable typeParameter = typeParameters [ i ] ; Type actualTypeArgument = actualTypeArguments [ i ] ; if ( actualTypeArgument instanceof WildcardType ) { contextualActualTypeParameters . put ( typeParameter , boundsOf ( ( WildcardType ) actualTypeArgument ) ) ; } else if ( typeParameter != actualTypeArgument ) { contextualActualTypeParameters . put ( typeParameter , actualTypeArgument ) ; } } }",
        "summary": "1.10 regression (StackOverflowError) with interface where generic type has itself as upper bound",
        "Description": "Add this to `GenericMetadataSupportTest`:\n\n``` java\n    interface GenericsSelfReference<T extends GenericsSelfReference<T>> {\n        T self();\n    }\n\n    @Test\n    public void typeVariable_of_self_type() {\n        GenericMetadataSupport genericMetadata = inferFrom(GenericsSelfReference.class).resolveGenericReturnType(firstNamedMethod(\"self\", GenericsSelfReference.class));\n\n        assertThat(genericMetadata.rawType()).isEqualTo(GenericsSelfReference.class);\n    }\n```\n\nIt fails on master and 1.10.8 with this:\n\n```\njava.lang.StackOverflowError\n    at sun.reflect.generics.reflectiveObjects.TypeVariableImpl.hashCode(TypeVariableImpl.java:201)\n    at java.util.HashMap.hash(HashMap.java:338)\n    at java.util.HashMap.get(HashMap.java:556)\n    at org.mockito.internal.util.reflection.GenericMetadataSupport.getActualTypeArgumentFor(GenericMetadataSupport.java:193)\n    at org.mockito.internal.util.reflection.GenericMetadataSupport.getActualTypeArgumentFor(GenericMetadataSupport.java:196)\n    at org.mockito.internal.util.reflection.GenericMetadataSupport.getActualTypeArgumentFor(GenericMetadataSupport.java:196)\n```\n\nIt worked on 1.9.5. May be caused by the changes in ab9e9f3 (cc @bric3).\n\n(Also note that while the above interface looks strange, it is commonly used for builder hierarchies, where base class methods want to return this with a more specific type.)\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Thanks a lot for reporting! Guys, do you want to take a stab at fixing it?\n"
            },
            {
                "content": "Published with 1.10.18. Thanks a lot for contribution!!!\n"
            },
            {
                "content": "Hey @pbielicki since you committed this fix, thx by the way, what is your opinion about the code that handle the generic support. I know generics is a complicated matter, I find my code quite intricate, as you have an external point of view do you saw some way to make it better ?\n"
            },
            {
                "content": "@bric3 I'm quite pragmatic - it's good if it works.\ngenerics + reflection is a complex matter as casting is inevitable. different types, different variants, type erasure, etc. so it's even difficult to imagine all possible test scenarios (Class is considered differently that Class<?> in runtime, while for me it's the same :)\nThe code is quite clean, and I don't have any ideas how to improve it - at least for now.\nTest, test, test and test again - I'm not helpful, I know.\n"
            },
            {
                "content": "Hey you did dive in this code to fix while I was busy on my current job, that was helpful :)\n\nYeah with generics it seems almost all cases are edge cases. But if you find it good enough for the time being that's already a good feedback.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been fixed and published with version 1.10.18. @pbielicki, who committed the fix, believes the code that handles the generic support is clean and works well, but recommends testing it thoroughly."
    },
    "Csv_1_src/main/java/org/apache/commons/csv/ExtendedBufferedReader.java_55_63": {
        "src": "@Override\n    public int read() throws IOException {\n        int current = super.read();\n        if (current == '\\n') {\n            lineCounter++;\n        }\n        lastChar = current;\n        return lastChar;\n    }",
        "src_wo_comments": "@ Override public int read ( ) throws IOException { int current = super . read ( ) ; if ( current == '\\n' ) { lineCounter ++ ; } lastChar = current ; return lastChar ; }",
        "fixed_src": "@Override\n    public int read() throws IOException {\n        int current = super.read();\n        if (current == '\\r' || (current == '\\n' && lastChar != '\\r')) {\n            lineCounter++;\n        }\n        lastChar = current;\n        return lastChar;\n    }",
        "fixed_src_wo_comments": "@ Override public int read ( ) throws IOException { int current = super . read ( ) ; if ( current == '\\r' || ( current == '\\n' && lastChar != '\\r' ) ) { lineCounter ++ ; } lastChar = current ; return lastChar ; }",
        "summary": "ExtendedBufferReader does not handle EOL consistently",
        "Description": "ExtendedBufferReader checks for '\\n' (LF) in the read() methods, incrementing linecount when found.\n\nHowever, the readLine() method calls BufferedReader.readLine() which treats CR, LF and CRLF equally (and drops them).\n\nIf the code is to be flexible in what it accepts, the class should also allow for CR alone as a line terminator.\n\nIt should work if the code increments the line counter for CR, and for LF if the previous character was not CR.",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-75",
        "comments": [
            "I added a test demonstrating the issue.\n\nI wonder if the line counting should be handled by the lexer instead.",
            "Yes, perhaps it should be done by the lexer. But the quickest fix would be to patch the reader.\n\nI wonder whether ExtendedBufferReader is actually necessary; lookAhead() could easily be provided by the Lexer class.\nAnd I'm not sure that readAgain() is really necessary.",
            "Enabled new test and added patch to fix it.\nBut we should still consider if ExtendedBufferReader is really necessary."
        ],
        "summarized_discussion": "\n\nThe bug was fixed by adding a patch to the reader. It was also suggested that the line counting should be handled by the lexer instead, and that ExtendedBufferReader may not be necessary. A new test was enabled to demonstrate the issue."
    },
    "Math_80_src/main/java/org/apache/commons/math/linear/EigenDecompositionImpl.java_1132_1147": {
        "src": "private boolean flipIfWarranted(final int n, final int step) {\n        if (1.5 * work[pingPong] < work[4 * (n - 1) + pingPong]) {\n            // flip array\n            int j = 4 * n - 1;\n            for (int i = 0; i < j; i += 4) {\n                for (int k = 0; k < 4; k += step) {\n                    final double tmp = work[i + k];\n                    work[i + k] = work[j - k];\n                    work[j - k] = tmp;\n                }\n                j -= 4;\n            }\n            return true;\n        }\n        return false;\n    }",
        "src_wo_comments": "private boolean flipIfWarranted ( final int n , final int step ) { if ( 1.5 * work [ pingPong ] < work [ 4 * ( n - 1 ) + pingPong ] ) { int j = 4 * n - 1 ; for ( int i = 0 ; i < j ; i += 4 ) { for ( int k = 0 ; k < 4 ; k += step ) { final double tmp = work [ i + k ] ; work [ i + k ] = work [ j - k ] ; work [ j - k ] = tmp ; } j -= 4 ; } return true ; } return false ; }",
        "fixed_src": "private boolean flipIfWarranted(final int n, final int step) {\n        if (1.5 * work[pingPong] < work[4 * (n - 1) + pingPong]) {\n            // flip array\n            int j = 4 * (n - 1);\n            for (int i = 0; i < j; i += 4) {\n                for (int k = 0; k < 4; k += step) {\n                    final double tmp = work[i + k];\n                    work[i + k] = work[j - k];\n                    work[j - k] = tmp;\n                }\n                j -= 4;\n            }\n            return true;\n        }\n        return false;\n    }",
        "fixed_src_wo_comments": "private boolean flipIfWarranted ( final int n , final int step ) { if ( 1.5 * work [ pingPong ] < work [ 4 * ( n - 1 ) + pingPong ] ) { int j = 4 * ( n - 1 ) ; for ( int i = 0 ; i < j ; i += 4 ) { for ( int k = 0 ; k < 4 ; k += step ) { final double tmp = work [ i + k ] ; work [ i + k ] = work [ j - k ] ; work [ j - k ] = tmp ; } j -= 4 ; } return true ; } return false ; }",
        "summary": "wrong result in eigen decomposition",
        "Description": "Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0\n{code}\n    public void testMathpbx02() {\n\n        double[] mainTridiagonal = {\n        \t  7484.860960227216, 18405.28129035345, 13855.225609560746,\n        \t 10016.708722343366, 559.8117399576674, 6750.190788301587, \n        \t    71.21428769782159\n        };\n        double[] secondaryTridiagonal = {\n        \t -4175.088570476366,1975.7955858241994,5193.178422374075, \n        \t  1995.286659169179,75.34535882933804,-234.0808002076056\n        };\n\n        // the reference values have been computed using routine DSTEMR\n        // from the fortran library LAPACK version 3.2.1\n        double[] refEigenValues = {\n        \t\t20654.744890306974412,16828.208208485466457,\n        \t\t6893.155912634994820,6757.083016675340332,\n        \t\t5887.799885688558788,64.309089923240379,\n        \t\t57.992628792736340\n        };\n        RealVector[] refEigenVectors = {\n        \t\tnew ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),\n        \t\tnew ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),\n        \t\tnew ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),\n        \t\tnew ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),\n        \t\tnew ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),\n        \t\tnew ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),\n        \t\tnew ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})\n        };\n\n        // the following line triggers the exception\n        EigenDecomposition decomposition =\n            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);\n\n        double[] eigenValues = decomposition.getRealEigenvalues();\n        for (int i = 0; i < refEigenValues.length; ++i) {\n            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);\n            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {\n                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);\n            } else {\n                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);\n            }\n        }\n\n    }\n{code}",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-318",
        "comments": [
            "fixed in subversion repository as of r833433.\nThanks again to Dimitri would found and fixed this bug."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of revision 833433, thanks to Dimitri who found and fixed the bug."
    },
    "Mockito_27_src/org/mockito/internal/util/MockUtil.java_62_67": {
        "src": "public <T> void resetMock(T mock) {\n        MockHandlerInterface<T> oldMockHandler = getMockHandler(mock);\n        MockHandler<T> newMockHandler = new MockHandler<T>(oldMockHandler);\n        MethodInterceptorFilter newFilter = new MethodInterceptorFilter(newMockHandler, (MockSettingsImpl) org.mockito.Mockito.withSettings().defaultAnswer(org.mockito.Mockito.RETURNS_DEFAULTS));\n        ((Factory) mock).setCallback(0, newFilter);\n    }",
        "src_wo_comments": "public < T > void resetMock ( T mock ) { MockHandlerInterface < T > oldMockHandler = getMockHandler ( mock ) ; MockHandler < T > newMockHandler = new MockHandler < T > ( oldMockHandler ) ; MethodInterceptorFilter newFilter = new MethodInterceptorFilter ( newMockHandler , ( MockSettingsImpl ) org . mockito . Mockito . withSettings ( ) . defaultAnswer ( org . mockito . Mockito . RETURNS_DEFAULTS ) ) ; ( ( Factory ) mock ) . setCallback ( 0 , newFilter ) ; }",
        "fixed_src": "public <T> void resetMock(T mock) {\n        MockHandlerInterface<T> oldMockHandler = getMockHandler(mock);\n        MethodInterceptorFilter newFilter = newMethodInterceptorFilter(oldMockHandler.getMockSettings());\n        ((Factory) mock).setCallback(0, newFilter);\n    }",
        "fixed_src_wo_comments": "public < T > void resetMock ( T mock ) { MockHandlerInterface < T > oldMockHandler = getMockHandler ( mock ) ; MethodInterceptorFilter newFilter = newMethodInterceptorFilter ( oldMockHandler . getMockSettings ( ) ) ; ( ( Factory ) mock ) . setCallback ( 0 , newFilter ) ; }",
        "summary": "Exception when stubbing more than once with when...thenThrow",
        "Description": "If I create a mock and stub a method so it throws an exception and do that twice the first exception will be thrown upon invoking the second stub instruction.\n\nExample:\n\n```\n@Test\npublic void testThrowException() {\n    Object o = Mockito.mock(Object.class);\n    // test behavior with Runtimeexception\n    Mockito.when(o.toString()).thenThrow(RuntimeException.class);\n    // ...\n    // test behavior with another exception\n    // this throws a RuntimeException\n    Mockito.when(o.toString()).thenThrow(IllegalArgumentException.class);\n    // ...\n}\n```\n\nI can work around this if I do it the other way around with doThrow...when. But I lose type safety then. Can you fix this?\n",
        "issue_url": null,
        "comments": [
            {
                "content": "I don't exaclty know what the actual code is supposed to achieve, but if the mock is supposed to raise different exceptions on subsequent interaction then mockito is misused in this case. Correct code would be : \n\n```\n@Test\npublic void testThrowException() {\n    Object o = Mockito.mock(Object.class);\n    Mockito.when(o.toString()).thenThrow(RuntimeException.class);\n                              .thenThrow(IllegalArgumentException.class);\n\n\n    assertThatThrownBy(() -> o.toString()).isInstanceOf(RuntimeException.class);\n    assertThatThrownBy(() -> o.toString()).isInstanceOf(IllegalArgumentException.class);\n}\n```\n\nThis [`assertThatThrownBy`](http://joel-costigliola.github.io/assertj/core-8/api/org/assertj/core/api/StrictAssertions.html#assertThatThrownBy-org.assertj.core.api.ThrowableAssert.ThrowingCallable-) comes with AssertJ 3.1.0 (java 8)\n"
            },
            {
                "content": "My example isn't the best one. Imagine I have a method with an input parameter. Depeding on the parameter it can throw exception A or B.\nWhen I test the calling method of the above mentioned one I have two test cases. As in my example above I first test what happens when exception A is thrown. After that I want to test with exception B.\n"
            },
            {
                "content": "Should you not create 2 different testcases for this?\n- Therefore 1 test case verifies that whenever you provide input parameter X, it throws A.\n- The other test case verifies that whenever your provide input parameter Y, it throws B.\n\nThen the stubbing for each case is done in a seperate testcase, causing the correct exception in each case.\n"
            },
            {
                "content": "Yes, that works.\n"
            },
            {
                "content": "Closing as resolved per https://github.com/mockito/mockito/issues/282#issuecomment-133759603\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to create two different test cases, one for each exception, and use the `assertThatThrownBy` method from AssertJ 3.1.0 (java 8) to verify that the correct exception is thrown."
    },
    "Math_38_src/main/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizer.java_1582_1755": {
        "src": "private void prelim(double[] lowerBound,\n                        double[] upperBound) {\n        printMethod(); // XXX\n\n        final int n = currentBest.getDimension();\n        final int npt = numberOfInterpolationPoints;\n        final int ndim = bMatrix.getRowDimension();\n\n        final double rhosq = initialTrustRegionRadius * initialTrustRegionRadius;\n        final double recip = 1d / rhosq;\n        final int np = n + 1;\n\n        // Set XBASE to the initial vector of variables, and set the initial\n        // elements of XPT, BMAT, HQ, PQ and ZMAT to zero.\n\n        for (int j = 0; j < n; j++) {\n            originShift.setEntry(j, currentBest.getEntry(j));\n            for (int k = 0; k < npt; k++) {\n                interpolationPoints.setEntry(k, j, ZERO);\n            }\n            for (int i = 0; i < ndim; i++) {\n                bMatrix.setEntry(i, j, ZERO);\n            }\n        }\n        for (int i = 0, max = n * np / 2; i < max; i++) {\n            modelSecondDerivativesValues.setEntry(i, ZERO);\n        }\n        for (int k = 0; k < npt; k++) {\n            modelSecondDerivativesParameters.setEntry(k, ZERO);\n            for (int j = 0, max = npt - np; j < max; j++) {\n                zMatrix.setEntry(k, j, ZERO);\n            }\n        }\n\n        // Begin the initialization procedure. NF becomes one more than the number\n        // of function values so far. The coordinates of the displacement of the\n        // next initial interpolation point from XBASE are set in XPT(NF+1,.).\n\n        int ipt = 0;\n        int jpt = 0;\n        double fbeg = Double.NaN;\n        do {\n            final int nfm = getEvaluations();\n            final int nfx = nfm - n;\n            final int nfmm = nfm - 1;\n            final int nfxm = nfx - 1;\n            double stepa = 0;\n            double stepb = 0;\n            if (nfm <= 2 * n) {\n                if (nfm >= 1 &&\n                    nfm <= n) {\n                    stepa = initialTrustRegionRadius;\n                    if (upperDifference.getEntry(nfmm) == ZERO) {\n                        stepa = -stepa;\n                        throw new PathIsExploredException(); // XXX\n                    }\n                    interpolationPoints.setEntry(nfm, nfmm, stepa);\n                } else if (nfm > n) {\n                    stepa = interpolationPoints.getEntry(nfx, nfxm);\n                    stepb = -initialTrustRegionRadius;\n                    if (lowerDifference.getEntry(nfxm) == ZERO) {\n                        stepb = Math.min(TWO * initialTrustRegionRadius, upperDifference.getEntry(nfxm));\n                        throw new PathIsExploredException(); // XXX\n                    }\n                    if (upperDifference.getEntry(nfxm) == ZERO) {\n                        stepb = Math.max(-TWO * initialTrustRegionRadius, lowerDifference.getEntry(nfxm));\n                        throw new PathIsExploredException(); // XXX\n                    }\n                    interpolationPoints.setEntry(nfm, nfxm, stepb);\n                }\n            } else {\n                final int tmp1 = (nfm - np) / n;\n                jpt = nfm - tmp1 * n - n;\n                ipt = jpt + tmp1;\n                if (ipt > n) {\n                    final int tmp2 = jpt;\n                    jpt = ipt - n;\n                    ipt = tmp2;\n                    throw new PathIsExploredException(); // XXX\n                }\n                final int iptMinus1 = ipt;\n                final int jptMinus1 = jpt;\n                interpolationPoints.setEntry(nfm, iptMinus1, interpolationPoints.getEntry(ipt, iptMinus1));\n                interpolationPoints.setEntry(nfm, jptMinus1, interpolationPoints.getEntry(jpt, jptMinus1));\n            }\n\n            // Calculate the next value of F. The least function value so far and\n            // its index are required.\n\n            for (int j = 0; j < n; j++) {\n                currentBest.setEntry(j, Math.min(Math.max(lowerBound[j],\n                                                          originShift.getEntry(j) + interpolationPoints.getEntry(nfm, j)),\n                                                 upperBound[j]));\n                if (interpolationPoints.getEntry(nfm, j) == lowerDifference.getEntry(j)) {\n                    currentBest.setEntry(j, lowerBound[j]);\n                }\n                if (interpolationPoints.getEntry(nfm, j) == upperDifference.getEntry(j)) {\n                    currentBest.setEntry(j, upperBound[j]);\n                }\n            }\n\n            final double objectiveValue = computeObjectiveValue(currentBest.toArray());\n            final double f = isMinimize ? objectiveValue : -objectiveValue;\n            final int numEval = getEvaluations(); // nfm + 1\n            fAtInterpolationPoints.setEntry(nfm, f);\n\n            if (numEval == 1) {\n                fbeg = f;\n                trustRegionCenterInterpolationPointIndex = 0;\n            } else if (f < fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex)) {\n                trustRegionCenterInterpolationPointIndex = nfm;\n            }\n\n            // Set the nonzero initial elements of BMAT and the quadratic model in the\n            // cases when NF is at most 2*N+1. If NF exceeds N+1, then the positions\n            // of the NF-th and (NF-N)-th interpolation points may be switched, in\n            // order that the function value at the first of them contributes to the\n            // off-diagonal second derivative terms of the initial quadratic model.\n\n            if (numEval <= 2 * n + 1) {\n                if (numEval >= 2 &&\n                    numEval <= n + 1) {\n                    gradientAtTrustRegionCenter.setEntry(nfmm, (f - fbeg) / stepa);\n                    if (npt < numEval + n) {\n                        final double oneOverStepA = ONE / stepa;\n                        bMatrix.setEntry(0, nfmm, -oneOverStepA);\n                        bMatrix.setEntry(nfm, nfmm, oneOverStepA);\n                        bMatrix.setEntry(npt + nfmm, nfmm, -HALF * rhosq);\n                        throw new PathIsExploredException(); // XXX\n                    }\n                } else if (numEval >= n + 2) {\n                    final int ih = nfx * (nfx + 1) / 2 - 1;\n                    final double tmp = (f - fbeg) / stepb;\n                    final double diff = stepb - stepa;\n                    modelSecondDerivativesValues.setEntry(ih, TWO * (tmp - gradientAtTrustRegionCenter.getEntry(nfxm)) / diff);\n                    gradientAtTrustRegionCenter.setEntry(nfxm, (gradientAtTrustRegionCenter.getEntry(nfxm) * stepb - tmp * stepa) / diff);\n                    if (stepa * stepb < ZERO) {\n                        if (f < fAtInterpolationPoints.getEntry(nfm - n)) {\n                            fAtInterpolationPoints.setEntry(nfm, fAtInterpolationPoints.getEntry(nfm - n));\n                            fAtInterpolationPoints.setEntry(nfm - n, f);\n                            if (trustRegionCenterInterpolationPointIndex == nfm) {\n                                trustRegionCenterInterpolationPointIndex = nfm - n;\n                            }\n                            interpolationPoints.setEntry(nfm - n, nfxm, stepb);\n                            interpolationPoints.setEntry(nfm, nfxm, stepa);\n                        }\n                    }\n                    bMatrix.setEntry(0, nfxm, -(stepa + stepb) / (stepa * stepb));\n                    bMatrix.setEntry(nfm, nfxm, -HALF / interpolationPoints.getEntry(nfm - n, nfxm));\n                    bMatrix.setEntry(nfm - n, nfxm,\n                                  -bMatrix.getEntry(0, nfxm) - bMatrix.getEntry(nfm, nfxm));\n                    zMatrix.setEntry(0, nfxm, Math.sqrt(TWO) / (stepa * stepb));\n                    zMatrix.setEntry(nfm, nfxm, Math.sqrt(HALF) / rhosq);\n                    // zMatrix.setEntry(nfm, nfxm, Math.sqrt(HALF) * recip); // XXX \"testAckley\" and \"testDiffPow\" fail.\n                    zMatrix.setEntry(nfm - n, nfxm,\n                                  -zMatrix.getEntry(0, nfxm) - zMatrix.getEntry(nfm, nfxm));\n                }\n\n                // Set the off-diagonal second derivatives of the Lagrange functions and\n                // the initial quadratic model.\n\n            } else {\n                zMatrix.setEntry(0, nfxm, recip);\n                zMatrix.setEntry(nfm, nfxm, recip);\n                zMatrix.setEntry(ipt, nfxm, -recip);\n                zMatrix.setEntry(jpt, nfxm, -recip);\n\n                final int ih = ipt * (ipt - 1) / 2 + jpt - 1;\n                final double tmp = interpolationPoints.getEntry(nfm, ipt - 1) * interpolationPoints.getEntry(nfm, jpt - 1);\n                modelSecondDerivativesValues.setEntry(ih, (fbeg - fAtInterpolationPoints.getEntry(ipt) - fAtInterpolationPoints.getEntry(jpt) + f) / tmp);\n                throw new PathIsExploredException(); // XXX\n            }\n        } while (getEvaluations() < npt);\n    }",
        "src_wo_comments": "private void prelim ( double [ ] lowerBound , double [ ] upperBound ) { printMethod ( ) ; final int n = currentBest . getDimension ( ) ; final int npt = numberOfInterpolationPoints ; final int ndim = bMatrix . getRowDimension ( ) ; final double rhosq = initialTrustRegionRadius * initialTrustRegionRadius ; final double recip = 1d / rhosq ; final int np = n + 1 ; for ( int j = 0 ; j < n ; j ++ ) { originShift . setEntry ( j , currentBest . getEntry ( j ) ) ; for ( int k = 0 ; k < npt ; k ++ ) { interpolationPoints . setEntry ( k , j , ZERO ) ; } for ( int i = 0 ; i < ndim ; i ++ ) { bMatrix . setEntry ( i , j , ZERO ) ; } } for ( int i = 0 , max = n * np / 2 ; i < max ; i ++ ) { modelSecondDerivativesValues . setEntry ( i , ZERO ) ; } for ( int k = 0 ; k < npt ; k ++ ) { modelSecondDerivativesParameters . setEntry ( k , ZERO ) ; for ( int j = 0 , max = npt - np ; j < max ; j ++ ) { zMatrix . setEntry ( k , j , ZERO ) ; } } int ipt = 0 ; int jpt = 0 ; double fbeg = Double . NaN ; do { final int nfm = getEvaluations ( ) ; final int nfx = nfm - n ; final int nfmm = nfm - 1 ; final int nfxm = nfx - 1 ; double stepa = 0 ; double stepb = 0 ; if ( nfm <= 2 * n ) { if ( nfm >= 1 && nfm <= n ) { stepa = initialTrustRegionRadius ; if ( upperDifference . getEntry ( nfmm ) == ZERO ) { stepa = - stepa ; throw new PathIsExploredException ( ) ; } interpolationPoints . setEntry ( nfm , nfmm , stepa ) ; } else if ( nfm > n ) { stepa = interpolationPoints . getEntry ( nfx , nfxm ) ; stepb = - initialTrustRegionRadius ; if ( lowerDifference . getEntry ( nfxm ) == ZERO ) { stepb = Math . min ( TWO * initialTrustRegionRadius , upperDifference . getEntry ( nfxm ) ) ; throw new PathIsExploredException ( ) ; } if ( upperDifference . getEntry ( nfxm ) == ZERO ) { stepb = Math . max ( - TWO * initialTrustRegionRadius , lowerDifference . getEntry ( nfxm ) ) ; throw new PathIsExploredException ( ) ; } interpolationPoints . setEntry ( nfm , nfxm , stepb ) ; } } else { final int tmp1 = ( nfm - np ) / n ; jpt = nfm - tmp1 * n - n ; ipt = jpt + tmp1 ; if ( ipt > n ) { final int tmp2 = jpt ; jpt = ipt - n ; ipt = tmp2 ; throw new PathIsExploredException ( ) ; } final int iptMinus1 = ipt ; final int jptMinus1 = jpt ; interpolationPoints . setEntry ( nfm , iptMinus1 , interpolationPoints . getEntry ( ipt , iptMinus1 ) ) ; interpolationPoints . setEntry ( nfm , jptMinus1 , interpolationPoints . getEntry ( jpt , jptMinus1 ) ) ; } for ( int j = 0 ; j < n ; j ++ ) { currentBest . setEntry ( j , Math . min ( Math . max ( lowerBound [ j ] , originShift . getEntry ( j ) + interpolationPoints . getEntry ( nfm , j ) ) , upperBound [ j ] ) ) ; if ( interpolationPoints . getEntry ( nfm , j ) == lowerDifference . getEntry ( j ) ) { currentBest . setEntry ( j , lowerBound [ j ] ) ; } if ( interpolationPoints . getEntry ( nfm , j ) == upperDifference . getEntry ( j ) ) { currentBest . setEntry ( j , upperBound [ j ] ) ; } } final double objectiveValue = computeObjectiveValue ( currentBest . toArray ( ) ) ; final double f = isMinimize ? objectiveValue : - objectiveValue ; final int numEval = getEvaluations ( ) ; fAtInterpolationPoints . setEntry ( nfm , f ) ; if ( numEval == 1 ) { fbeg = f ; trustRegionCenterInterpolationPointIndex = 0 ; } else if ( f < fAtInterpolationPoints . getEntry ( trustRegionCenterInterpolationPointIndex ) ) { trustRegionCenterInterpolationPointIndex = nfm ; } if ( numEval <= 2 * n + 1 ) { if ( numEval >= 2 && numEval <= n + 1 ) { gradientAtTrustRegionCenter . setEntry ( nfmm , ( f - fbeg ) / stepa ) ; if ( npt < numEval + n ) { final double oneOverStepA = ONE / stepa ; bMatrix . setEntry ( 0 , nfmm , - oneOverStepA ) ; bMatrix . setEntry ( nfm , nfmm , oneOverStepA ) ; bMatrix . setEntry ( npt + nfmm , nfmm , - HALF * rhosq ) ; throw new PathIsExploredException ( ) ; } } else if ( numEval >= n + 2 ) { final int ih = nfx * ( nfx + 1 ) / 2 - 1 ; final double tmp = ( f - fbeg ) / stepb ; final double diff = stepb - stepa ; modelSecondDerivativesValues . setEntry ( ih , TWO * ( tmp - gradientAtTrustRegionCenter . getEntry ( nfxm ) ) / diff ) ; gradientAtTrustRegionCenter . setEntry ( nfxm , ( gradientAtTrustRegionCenter . getEntry ( nfxm ) * stepb - tmp * stepa ) / diff ) ; if ( stepa * stepb < ZERO ) { if ( f < fAtInterpolationPoints . getEntry ( nfm - n ) ) { fAtInterpolationPoints . setEntry ( nfm , fAtInterpolationPoints . getEntry ( nfm - n ) ) ; fAtInterpolationPoints . setEntry ( nfm - n , f ) ; if ( trustRegionCenterInterpolationPointIndex == nfm ) { trustRegionCenterInterpolationPointIndex = nfm - n ; } interpolationPoints . setEntry ( nfm - n , nfxm , stepb ) ; interpolationPoints . setEntry ( nfm , nfxm , stepa ) ; } } bMatrix . setEntry ( 0 , nfxm , - ( stepa + stepb ) / ( stepa * stepb ) ) ; bMatrix . setEntry ( nfm , nfxm , - HALF / interpolationPoints . getEntry ( nfm - n , nfxm ) ) ; bMatrix . setEntry ( nfm - n , nfxm , - bMatrix . getEntry ( 0 , nfxm ) - bMatrix . getEntry ( nfm , nfxm ) ) ; zMatrix . setEntry ( 0 , nfxm , Math . sqrt ( TWO ) / ( stepa * stepb ) ) ; zMatrix . setEntry ( nfm , nfxm , Math . sqrt ( HALF ) / rhosq ) ; zMatrix . setEntry ( nfm - n , nfxm , - zMatrix . getEntry ( 0 , nfxm ) - zMatrix . getEntry ( nfm , nfxm ) ) ; } } else { zMatrix . setEntry ( 0 , nfxm , recip ) ; zMatrix . setEntry ( nfm , nfxm , recip ) ; zMatrix . setEntry ( ipt , nfxm , - recip ) ; zMatrix . setEntry ( jpt , nfxm , - recip ) ; final int ih = ipt * ( ipt - 1 ) / 2 + jpt - 1 ; final double tmp = interpolationPoints . getEntry ( nfm , ipt - 1 ) * interpolationPoints . getEntry ( nfm , jpt - 1 ) ; modelSecondDerivativesValues . setEntry ( ih , ( fbeg - fAtInterpolationPoints . getEntry ( ipt ) - fAtInterpolationPoints . getEntry ( jpt ) + f ) / tmp ) ; throw new PathIsExploredException ( ) ; } } while ( getEvaluations ( ) < npt ) ; }",
        "fixed_src": "private void prelim(double[] lowerBound,\n                        double[] upperBound) {\n        printMethod(); // XXX\n\n        final int n = currentBest.getDimension();\n        final int npt = numberOfInterpolationPoints;\n        final int ndim = bMatrix.getRowDimension();\n\n        final double rhosq = initialTrustRegionRadius * initialTrustRegionRadius;\n        final double recip = 1d / rhosq;\n        final int np = n + 1;\n\n        // Set XBASE to the initial vector of variables, and set the initial\n        // elements of XPT, BMAT, HQ, PQ and ZMAT to zero.\n\n        for (int j = 0; j < n; j++) {\n            originShift.setEntry(j, currentBest.getEntry(j));\n            for (int k = 0; k < npt; k++) {\n                interpolationPoints.setEntry(k, j, ZERO);\n            }\n            for (int i = 0; i < ndim; i++) {\n                bMatrix.setEntry(i, j, ZERO);\n            }\n        }\n        for (int i = 0, max = n * np / 2; i < max; i++) {\n            modelSecondDerivativesValues.setEntry(i, ZERO);\n        }\n        for (int k = 0; k < npt; k++) {\n            modelSecondDerivativesParameters.setEntry(k, ZERO);\n            for (int j = 0, max = npt - np; j < max; j++) {\n                zMatrix.setEntry(k, j, ZERO);\n            }\n        }\n\n        // Begin the initialization procedure. NF becomes one more than the number\n        // of function values so far. The coordinates of the displacement of the\n        // next initial interpolation point from XBASE are set in XPT(NF+1,.).\n\n        int ipt = 0;\n        int jpt = 0;\n        double fbeg = Double.NaN;\n        do {\n            final int nfm = getEvaluations();\n            final int nfx = nfm - n;\n            final int nfmm = nfm - 1;\n            final int nfxm = nfx - 1;\n            double stepa = 0;\n            double stepb = 0;\n            if (nfm <= 2 * n) {\n                if (nfm >= 1 &&\n                    nfm <= n) {\n                    stepa = initialTrustRegionRadius;\n                    if (upperDifference.getEntry(nfmm) == ZERO) {\n                        stepa = -stepa;\n                        throw new PathIsExploredException(); // XXX\n                    }\n                    interpolationPoints.setEntry(nfm, nfmm, stepa);\n                } else if (nfm > n) {\n                    stepa = interpolationPoints.getEntry(nfx, nfxm);\n                    stepb = -initialTrustRegionRadius;\n                    if (lowerDifference.getEntry(nfxm) == ZERO) {\n                        stepb = Math.min(TWO * initialTrustRegionRadius, upperDifference.getEntry(nfxm));\n                        throw new PathIsExploredException(); // XXX\n                    }\n                    if (upperDifference.getEntry(nfxm) == ZERO) {\n                        stepb = Math.max(-TWO * initialTrustRegionRadius, lowerDifference.getEntry(nfxm));\n                        throw new PathIsExploredException(); // XXX\n                    }\n                    interpolationPoints.setEntry(nfm, nfxm, stepb);\n                }\n            } else {\n                final int tmp1 = (nfm - np) / n;\n                jpt = nfm - tmp1 * n - n;\n                ipt = jpt + tmp1;\n                if (ipt > n) {\n                    final int tmp2 = jpt;\n                    jpt = ipt - n;\n                    ipt = tmp2;\n//                     throw new PathIsExploredException(); // XXX\n                }\n                final int iptMinus1 = ipt - 1;\n                final int jptMinus1 = jpt - 1;\n                interpolationPoints.setEntry(nfm, iptMinus1, interpolationPoints.getEntry(ipt, iptMinus1));\n                interpolationPoints.setEntry(nfm, jptMinus1, interpolationPoints.getEntry(jpt, jptMinus1));\n            }\n\n            // Calculate the next value of F. The least function value so far and\n            // its index are required.\n\n            for (int j = 0; j < n; j++) {\n                currentBest.setEntry(j, Math.min(Math.max(lowerBound[j],\n                                                          originShift.getEntry(j) + interpolationPoints.getEntry(nfm, j)),\n                                                 upperBound[j]));\n                if (interpolationPoints.getEntry(nfm, j) == lowerDifference.getEntry(j)) {\n                    currentBest.setEntry(j, lowerBound[j]);\n                }\n                if (interpolationPoints.getEntry(nfm, j) == upperDifference.getEntry(j)) {\n                    currentBest.setEntry(j, upperBound[j]);\n                }\n            }\n\n            final double objectiveValue = computeObjectiveValue(currentBest.toArray());\n            final double f = isMinimize ? objectiveValue : -objectiveValue;\n            final int numEval = getEvaluations(); // nfm + 1\n            fAtInterpolationPoints.setEntry(nfm, f);\n\n            if (numEval == 1) {\n                fbeg = f;\n                trustRegionCenterInterpolationPointIndex = 0;\n            } else if (f < fAtInterpolationPoints.getEntry(trustRegionCenterInterpolationPointIndex)) {\n                trustRegionCenterInterpolationPointIndex = nfm;\n            }\n\n            // Set the nonzero initial elements of BMAT and the quadratic model in the\n            // cases when NF is at most 2*N+1. If NF exceeds N+1, then the positions\n            // of the NF-th and (NF-N)-th interpolation points may be switched, in\n            // order that the function value at the first of them contributes to the\n            // off-diagonal second derivative terms of the initial quadratic model.\n\n            if (numEval <= 2 * n + 1) {\n                if (numEval >= 2 &&\n                    numEval <= n + 1) {\n                    gradientAtTrustRegionCenter.setEntry(nfmm, (f - fbeg) / stepa);\n                    if (npt < numEval + n) {\n                        final double oneOverStepA = ONE / stepa;\n                        bMatrix.setEntry(0, nfmm, -oneOverStepA);\n                        bMatrix.setEntry(nfm, nfmm, oneOverStepA);\n                        bMatrix.setEntry(npt + nfmm, nfmm, -HALF * rhosq);\n                        throw new PathIsExploredException(); // XXX\n                    }\n                } else if (numEval >= n + 2) {\n                    final int ih = nfx * (nfx + 1) / 2 - 1;\n                    final double tmp = (f - fbeg) / stepb;\n                    final double diff = stepb - stepa;\n                    modelSecondDerivativesValues.setEntry(ih, TWO * (tmp - gradientAtTrustRegionCenter.getEntry(nfxm)) / diff);\n                    gradientAtTrustRegionCenter.setEntry(nfxm, (gradientAtTrustRegionCenter.getEntry(nfxm) * stepb - tmp * stepa) / diff);\n                    if (stepa * stepb < ZERO) {\n                        if (f < fAtInterpolationPoints.getEntry(nfm - n)) {\n                            fAtInterpolationPoints.setEntry(nfm, fAtInterpolationPoints.getEntry(nfm - n));\n                            fAtInterpolationPoints.setEntry(nfm - n, f);\n                            if (trustRegionCenterInterpolationPointIndex == nfm) {\n                                trustRegionCenterInterpolationPointIndex = nfm - n;\n                            }\n                            interpolationPoints.setEntry(nfm - n, nfxm, stepb);\n                            interpolationPoints.setEntry(nfm, nfxm, stepa);\n                        }\n                    }\n                    bMatrix.setEntry(0, nfxm, -(stepa + stepb) / (stepa * stepb));\n                    bMatrix.setEntry(nfm, nfxm, -HALF / interpolationPoints.getEntry(nfm - n, nfxm));\n                    bMatrix.setEntry(nfm - n, nfxm,\n                                  -bMatrix.getEntry(0, nfxm) - bMatrix.getEntry(nfm, nfxm));\n                    zMatrix.setEntry(0, nfxm, Math.sqrt(TWO) / (stepa * stepb));\n                    zMatrix.setEntry(nfm, nfxm, Math.sqrt(HALF) / rhosq);\n                    // zMatrix.setEntry(nfm, nfxm, Math.sqrt(HALF) * recip); // XXX \"testAckley\" and \"testDiffPow\" fail.\n                    zMatrix.setEntry(nfm - n, nfxm,\n                                  -zMatrix.getEntry(0, nfxm) - zMatrix.getEntry(nfm, nfxm));\n                }\n\n                // Set the off-diagonal second derivatives of the Lagrange functions and\n                // the initial quadratic model.\n\n            } else {\n                zMatrix.setEntry(0, nfxm, recip);\n                zMatrix.setEntry(nfm, nfxm, recip);\n                zMatrix.setEntry(ipt, nfxm, -recip);\n                zMatrix.setEntry(jpt, nfxm, -recip);\n\n                final int ih = ipt * (ipt - 1) / 2 + jpt - 1;\n                final double tmp = interpolationPoints.getEntry(nfm, ipt - 1) * interpolationPoints.getEntry(nfm, jpt - 1);\n                modelSecondDerivativesValues.setEntry(ih, (fbeg - fAtInterpolationPoints.getEntry(ipt) - fAtInterpolationPoints.getEntry(jpt) + f) / tmp);\n//                 throw new PathIsExploredException(); // XXX\n            }\n        } while (getEvaluations() < npt);\n    }",
        "fixed_src_wo_comments": "private void prelim ( double [ ] lowerBound , double [ ] upperBound ) { printMethod ( ) ; final int n = currentBest . getDimension ( ) ; final int npt = numberOfInterpolationPoints ; final int ndim = bMatrix . getRowDimension ( ) ; final double rhosq = initialTrustRegionRadius * initialTrustRegionRadius ; final double recip = 1d / rhosq ; final int np = n + 1 ; for ( int j = 0 ; j < n ; j ++ ) { originShift . setEntry ( j , currentBest . getEntry ( j ) ) ; for ( int k = 0 ; k < npt ; k ++ ) { interpolationPoints . setEntry ( k , j , ZERO ) ; } for ( int i = 0 ; i < ndim ; i ++ ) { bMatrix . setEntry ( i , j , ZERO ) ; } } for ( int i = 0 , max = n * np / 2 ; i < max ; i ++ ) { modelSecondDerivativesValues . setEntry ( i , ZERO ) ; } for ( int k = 0 ; k < npt ; k ++ ) { modelSecondDerivativesParameters . setEntry ( k , ZERO ) ; for ( int j = 0 , max = npt - np ; j < max ; j ++ ) { zMatrix . setEntry ( k , j , ZERO ) ; } } int ipt = 0 ; int jpt = 0 ; double fbeg = Double . NaN ; do { final int nfm = getEvaluations ( ) ; final int nfx = nfm - n ; final int nfmm = nfm - 1 ; final int nfxm = nfx - 1 ; double stepa = 0 ; double stepb = 0 ; if ( nfm <= 2 * n ) { if ( nfm >= 1 && nfm <= n ) { stepa = initialTrustRegionRadius ; if ( upperDifference . getEntry ( nfmm ) == ZERO ) { stepa = - stepa ; throw new PathIsExploredException ( ) ; } interpolationPoints . setEntry ( nfm , nfmm , stepa ) ; } else if ( nfm > n ) { stepa = interpolationPoints . getEntry ( nfx , nfxm ) ; stepb = - initialTrustRegionRadius ; if ( lowerDifference . getEntry ( nfxm ) == ZERO ) { stepb = Math . min ( TWO * initialTrustRegionRadius , upperDifference . getEntry ( nfxm ) ) ; throw new PathIsExploredException ( ) ; } if ( upperDifference . getEntry ( nfxm ) == ZERO ) { stepb = Math . max ( - TWO * initialTrustRegionRadius , lowerDifference . getEntry ( nfxm ) ) ; throw new PathIsExploredException ( ) ; } interpolationPoints . setEntry ( nfm , nfxm , stepb ) ; } } else { final int tmp1 = ( nfm - np ) / n ; jpt = nfm - tmp1 * n - n ; ipt = jpt + tmp1 ; if ( ipt > n ) { final int tmp2 = jpt ; jpt = ipt - n ; ipt = tmp2 ; } final int iptMinus1 = ipt - 1 ; final int jptMinus1 = jpt - 1 ; interpolationPoints . setEntry ( nfm , iptMinus1 , interpolationPoints . getEntry ( ipt , iptMinus1 ) ) ; interpolationPoints . setEntry ( nfm , jptMinus1 , interpolationPoints . getEntry ( jpt , jptMinus1 ) ) ; } for ( int j = 0 ; j < n ; j ++ ) { currentBest . setEntry ( j , Math . min ( Math . max ( lowerBound [ j ] , originShift . getEntry ( j ) + interpolationPoints . getEntry ( nfm , j ) ) , upperBound [ j ] ) ) ; if ( interpolationPoints . getEntry ( nfm , j ) == lowerDifference . getEntry ( j ) ) { currentBest . setEntry ( j , lowerBound [ j ] ) ; } if ( interpolationPoints . getEntry ( nfm , j ) == upperDifference . getEntry ( j ) ) { currentBest . setEntry ( j , upperBound [ j ] ) ; } } final double objectiveValue = computeObjectiveValue ( currentBest . toArray ( ) ) ; final double f = isMinimize ? objectiveValue : - objectiveValue ; final int numEval = getEvaluations ( ) ; fAtInterpolationPoints . setEntry ( nfm , f ) ; if ( numEval == 1 ) { fbeg = f ; trustRegionCenterInterpolationPointIndex = 0 ; } else if ( f < fAtInterpolationPoints . getEntry ( trustRegionCenterInterpolationPointIndex ) ) { trustRegionCenterInterpolationPointIndex = nfm ; } if ( numEval <= 2 * n + 1 ) { if ( numEval >= 2 && numEval <= n + 1 ) { gradientAtTrustRegionCenter . setEntry ( nfmm , ( f - fbeg ) / stepa ) ; if ( npt < numEval + n ) { final double oneOverStepA = ONE / stepa ; bMatrix . setEntry ( 0 , nfmm , - oneOverStepA ) ; bMatrix . setEntry ( nfm , nfmm , oneOverStepA ) ; bMatrix . setEntry ( npt + nfmm , nfmm , - HALF * rhosq ) ; throw new PathIsExploredException ( ) ; } } else if ( numEval >= n + 2 ) { final int ih = nfx * ( nfx + 1 ) / 2 - 1 ; final double tmp = ( f - fbeg ) / stepb ; final double diff = stepb - stepa ; modelSecondDerivativesValues . setEntry ( ih , TWO * ( tmp - gradientAtTrustRegionCenter . getEntry ( nfxm ) ) / diff ) ; gradientAtTrustRegionCenter . setEntry ( nfxm , ( gradientAtTrustRegionCenter . getEntry ( nfxm ) * stepb - tmp * stepa ) / diff ) ; if ( stepa * stepb < ZERO ) { if ( f < fAtInterpolationPoints . getEntry ( nfm - n ) ) { fAtInterpolationPoints . setEntry ( nfm , fAtInterpolationPoints . getEntry ( nfm - n ) ) ; fAtInterpolationPoints . setEntry ( nfm - n , f ) ; if ( trustRegionCenterInterpolationPointIndex == nfm ) { trustRegionCenterInterpolationPointIndex = nfm - n ; } interpolationPoints . setEntry ( nfm - n , nfxm , stepb ) ; interpolationPoints . setEntry ( nfm , nfxm , stepa ) ; } } bMatrix . setEntry ( 0 , nfxm , - ( stepa + stepb ) / ( stepa * stepb ) ) ; bMatrix . setEntry ( nfm , nfxm , - HALF / interpolationPoints . getEntry ( nfm - n , nfxm ) ) ; bMatrix . setEntry ( nfm - n , nfxm , - bMatrix . getEntry ( 0 , nfxm ) - bMatrix . getEntry ( nfm , nfxm ) ) ; zMatrix . setEntry ( 0 , nfxm , Math . sqrt ( TWO ) / ( stepa * stepb ) ) ; zMatrix . setEntry ( nfm , nfxm , Math . sqrt ( HALF ) / rhosq ) ; zMatrix . setEntry ( nfm - n , nfxm , - zMatrix . getEntry ( 0 , nfxm ) - zMatrix . getEntry ( nfm , nfxm ) ) ; } } else { zMatrix . setEntry ( 0 , nfxm , recip ) ; zMatrix . setEntry ( nfm , nfxm , recip ) ; zMatrix . setEntry ( ipt , nfxm , - recip ) ; zMatrix . setEntry ( jpt , nfxm , - recip ) ; final int ih = ipt * ( ipt - 1 ) / 2 + jpt - 1 ; final double tmp = interpolationPoints . getEntry ( nfm , ipt - 1 ) * interpolationPoints . getEntry ( nfm , jpt - 1 ) ; modelSecondDerivativesValues . setEntry ( ih , ( fbeg - fAtInterpolationPoints . getEntry ( ipt ) - fAtInterpolationPoints . getEntry ( jpt ) + f ) / tmp ) ; } } while ( getEvaluations ( ) < npt ) ; }",
        "summary": "Errors in BOBYQAOptimizer when numberOfInterpolationPoints is greater than 2*dim+1",
        "Description": "I've been having trouble getting BOBYQA to minimize a function (actually a non-linear least squares fit) so as one change I increased the number of interpolation points.  It seems that anything larger than 2*dim+1 causes an error (typically at\n\nline 1662\n                   interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt));\n\nI'm guessing there is an off by one error in the translation from FORTRAN.  Changing the BOBYQAOptimizerTest as follows (increasing number of interpolation points by one) will cause failures.\n\nBruce\n\n\n\nIndex: src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java\n===================================================================\n--- src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java\t(revision 1221065)\n+++ src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java\t(working copy)\n@@ -258,7 +258,7 @@\n //        RealPointValuePair result = optim.optimize(100000, func, goal, startPoint);\n         final double[] lB = boundaries == null ? null : boundaries[0];\n         final double[] uB = boundaries == null ? null : boundaries[1];\n-        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 1);\n+        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 2);\n         RealPointValuePair result = optim.optimize(maxEvaluations, func, goal, startPoint, lB, uB);\n //        System.out.println(func.getClass().getName() + \" = \" \n //              + optim.getEvaluations() + \" f(\");\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-728",
        "comments": [
            "Making this change fixes the problem and seems reasonable, but one of the original translators (from FORTRAN to Java) should look this section of code over.\n\n\n@@ -1657,10 +1657,10 @@\n                     final int tmp2 = jpt;\n                     jpt = ipt - n;\n                     ipt = tmp2;\n-                    throw new PathIsExploredException(); // XXX\n+                    //throw new PathIsExploredException(); // XXX\n                 }\n-                interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt));\n-                interpolationPoints.setEntry(nfm, jpt, interpolationPoints.getEntry(jpt, jpt));\n+                interpolationPoints.setEntry(nfm, ipt-1, interpolationPoints.getEntry(ipt, ipt-1));\n+                interpolationPoints.setEntry(nfm, jpt-1, interpolationPoints.getEntry(jpt, jpt-1));\n\n",
            "Thanks for spotting this. Revision 1221253 contains your fix, together with a unit test that exercises the setting of more interpolation points.\n",
            "Thanks for checking this in, and it's worth noting that my function now minimizes properly.  It seems that with a large number of interpolation points there was an out of bounds error, but even where that out of bounds error wasn't thrown, the interpolation was set up wrong for the last few points, causing a failure to converge.  Others who have had trouble with convergence would do well to recheck with this fix.",
            "Anyone who wishes to use the \"BOBYQAOptimizer\" class should have a look at the MATH-621 issue.\nAs you have figured out, the code is really not ready yet. Unfortunately, the implementation being not \"natural\" in Java, it is not easy to separate algorithm complexity from Fortran-driven optimizations (which should be removed).\nThe problem is all compounded by the fall-through switch-cases which should also be recoded properly.\n\nWe are still in the middle of the river: Many things have been improved structure-wise but bugs could have crept in while doing so. Bugs like the one you discovered.\nAnd we don't have a thorough test suite to ensure that every code path works as in the original Fortran.\n\nThe code was checked in under the assumption that it would be converted into \"natural\" Java, so that people can maintain it.\nI wonder whether it should not be removed for the upcoming release...\n",
            "I, for one, am very happy to see BOBYQA in CM and will continue with \"real world\" testing.  If I find more issues I'll certainly report them, but so far it's looking very promising with my applications.",
            "It would be very useful if you could provide unit tests that cover the still unexplored code paths (cf. lines containing \"throw new PathIsExploredException();\").\nThanks in advance for your contributions.\n",
            "Hi Bruce.\n\nWould you be interested in testing your code with a large number of \"additional\" interpolation points?\nI'm referring to the unit test \"testConstrainedRosenWithMoreInterpolationPoints\" in \"BOBYQAOptimizerTest\", at lines 236-256. It would be nice to know whether the failures, for some values of the number of points, reveal yet other bugs. (Or whether they are expected; in which case, the reason would be a welcome addition to the documentation...)\n",
            "Hi Giles,\n\nI'll try to do so over the next couple days.\n\ncheers,\n\nBruce",
            "I've been playing with BOBYQA (downloaded from svn repository today, and commenting out the PathNotExplored exceptions).  A couple observations.\n1) I can't make it fail with large number of interpolation points (as long as you stay under the (2n+1)*(2n+2)/2 recommended max. So this issue is resolved.\n2) With large number of interpolation points the algorithm is significantly slower.  I'm minimizing a function  with a 169 parameters.  The function evaluation takes ~50 msec.  With n+2 interpolation points, the additional time for each step is about 10 msec.  With 2*n+1 points, the additional time is about 50 msec, and with about 6*n, the additional time is 250 msec.  So with larger nInterpolation points a lot of time is spent in the algorithm, besides evaluating the function.   At some point I'll try to do some profiling of the code.\n3) It makes a big difference to normalize the parameters as the initial search region is dependent on the point with the smallest boundary difference.  So it seems one shouldn't directly fit the \"natural\" parameters but normalized values.",
            "Although the bug that triggered this issue is fixed, failures of the unit test still miss an explanation...\n\nThe poor performance is to be expected given the current state of the code (e.g. many matrix calculations are done explicitly, with getters and setters, instead of calling methods of the matrix objects).\n"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to replace the line \"throw new PathIsExploredException(); // XXX\" with \"//throw new PathIsExploredException(); // XXX\" and to change the entries of the interpolationPoints matrix from \"interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt));\" and \"interpolationPoints.setEntry(nfm, jpt, interpolationPoints.getEntry(jpt, jpt));\" to \"interpolationPoints.setEntry(nfm, ipt-1, interpolationPoints.getEntry(ipt, ipt-1));\" and \"interpolationPoints.setEntry(nfm, jpt-1, interpolationPoints.getEntry(jpt, jpt-1));\". Additionally, it is suggested that anyone who wishes to use the \"BOBYQAOptimizer\" class should have a look at the MATH-621 issue and that unit tests should be provided to cover the still unexplored code paths."
    },
    "Codec_4_src/java/org/apache/commons/codec/binary/Base64.java_224_226": {
        "src": "public Base64() {\n        this(false);\n    }",
        "src_wo_comments": "public Base64 ( ) { this ( false ) ; }",
        "fixed_src": "public Base64() {\n        this(0);\n    }",
        "fixed_src_wo_comments": "public Base64 ( ) { this ( 0 ) ; }",
        "summary": "new Base64().encode() appends a CRLF, and chunks results into 76 character lines",
        "Description": "The instance encode() method (e.g. new Base64().encode()) appends a CRLF.  Actually it's fully chunking the output into 76 character lines.  Commons-Codec-1.3 did not do this.  The static Base64.encodeBase64() method behaves the same in both 1.3 and 1.4, so this problem only affects the instance encode() method.\n\n\n{code}\nimport org.apache.commons.codec.binary.*;\n\npublic class B64 {\n\n  public static void main(String[] args) throws Exception {\n    Base64 b64 = new Base64();\n\n    String s1 = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\";\n    String s2 = \"aaaaaaaaaa\";\n    String s3 = \"a\";\n    \n    byte[] b1 = s1.getBytes(\"UTF-8\");\n    byte[] b2 = s2.getBytes(\"UTF-8\");\n    byte[] b3 = s3.getBytes(\"UTF-8\");\n\n    byte[] result;\n    result = Base64.encodeBase64(b1);\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n    result = b64.encode(b1);\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n\n    result = Base64.encodeBase64(b2);\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n    result = b64.encode(b2);\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n\n    result = Base64.encodeBase64(b3);\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n    result = b64.encode(b3);\n    System.out.println(\"[\" + new String(result, \"UTF-8\") + \"]\");\n\n  }\n}\n{code}\n\n\nHere's my output:\n\n{noformat}\n$ java -cp commons-codec-1.3.jar:. B64\n[YWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYQ==]\n[YWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYQ==]\n[YWFhYWFhYWFhYQ==]\n[YWFhYWFhYWFhYQ==]\n[YQ==]\n[YQ==]\n\n\n$ java -cp commons-codec-1.4.jar:. B64\n[YWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYQ==]\n[YWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFh\nYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYQ==\n]\n[YWFhYWFhYWFhYQ==]\n[YWFhYWFhYWFhYQ==\n]\n[YQ==]\n[YQ==\n]\n{noformat}\n",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-89",
        "comments": [
            "codec-89.patch attached which fixes this.  Also introduces some JUnit tests to avoid this problem in the future.\n\nNote:  I changed the behavior of the Base64.encodeBase64String() method introduced in commons-codec-1.4 so that it also no longer chunks its output.  Two lines of the JUnit tests adjusted to reflect this change.",
            "What a pickle. The current behavior is documented in Javadocs but the API behavior for 1.4 is released and different from 1.3. What should in 1.5? Go back to 1.3 behavior? That seems like asking call sites a lot, switching twice. What does the community thinks?",
            "Could we put out a fix as \"1.4.1\" ?",
            "I see a change for a 1.4.1 or a 1.5 as the same issue since the API behavior would change again:\n\n1.3: Behavior A\n1.4: Behavior B\n1.4.1 and 1.5: Behavior A again?\n\nIn either case that is asking clients to change their code twice, back and forth. That seems painful. \n\nIf you want, you can post to the dev list to ask for the community's feedback. It would be good to see what other people think.",
            "I'd prefer to see a 1.4.1 release with the old behaviour. It seems against the spirit of minor version bump (like 1.3 to 1.4) to change the behaviour of the interface.\n\nIt definitely feels like a regression to me -- I'm tempted to apply this patch to the commons-codec distributed by Fedora Linux (I am the maintainer there).",
            "Note that the behaviour of \n\nnew Base64(0).encode() \nis the same as \nBase64.encodeBase64()",
            "> Note that the behaviour of\n> \n> new Base64(0).encode()\n> is the same as\n> Base64.encodeBase64()\n\nSo it is... Is that documented anywhere?\n\nA note in the compatibility section of the release notes that \"new Base64()\" should be changed to \"new Base64(0)\" would be awesome.",
            "Patch (dated 2009-10-28 04:52 PM) looks OK, except I think it looks odd to do:\n\nthis(NO_CHUNKING, CHUNK_SEPARATOR, urlSafe);\n\nWhy provide a CHUNK_SEPARATOR if there's no chunking?\n\nSeems to me it would be better to use null, and make corresponding changes to the ctor \nBase64(int lineLength, byte[] lineSeparator, boolean urlSafe)\nso that lineSeparator == null IFF lineLength==0\nThis would have the benefit that accidental use of the lineSeparator would trigger an NPE.\n\nThe current code outside the ctor only accesses lineSeparator if lineLength > 0, which is as it should be.",
            "Having second thoughts - although the patch looks OK, it actually changes the behaviour of *two* constructors:\n\npublic Base64()\npublic Base64(boolean urlSafe)\n\nSince the second ctor is @since 1.4, there is no need to - and therefore we should not - change its behaviour.\n\nThe simplest fix is to change the null ctor to call this(0), and change any code that calls the null ctor to call Base64(false) instead.\n\nPatch to follow.",
            "Minimal patch to revert null ctor to 1.3 behaviour.\n\nNote that the tests all still work, so clearly the tests are inadequate.\n\nIn fact the tests still work even if the IO Stream files are not patched.",
            "I am the lead developer of Jasypt [http://www.jasypt.org], which uses this wonderful Commons-Codec library as a dependency. Base64 is the default encoding for encrypted output in Jasypt.\n\nThis issue is of an inmense importance to Jasypt. If my vote makes any difference, please, go back to the 1.3 behaviour ASAP, as this is breaking compatibiliy of encrypted texts for many of our users and not allowing them to use an up-to-date version of commons-codec. I would be forced to remove commons-codec from jasypt's dependency list if this bug is not solved soon :-(\n\nThanks.\nDaniel.\n",
            "I can only second that.\n\nI'm the developer of the Signpost OAuth project, and people are complaining that Signpost breaks with CC-1.4, while it worked fine with 1.3.\n\nI already changed my code to trim() any output of the Base64 methods, but apparently, that's not enough. Users are still reporting issues that disappear when switching to 1.3.",
            "+1 to a 1.4.1 release that goes back to the 1.3 approach (or rather, that does the right thing. I'm inferring that it's agreed that 1.3 did it 'right'). ",
            "I quote Daniel Fern\u00e1ndez, we have the same problem.\nWe should patch all framework code to work.",
            "This is also present for non-empty constructors, which the patch doesn't seem to address.  I think this is what CODEC-94 was trying to say.  If you construct a Base64, specifying lineSeparator and/or lineLength, this is ignored with you do the encoding.  For example,\n\nSystem.out.println(\"&#92;&#92;n: \" + \"&#92;n\".getBytes());\nSystem.out.println(\"&#92;&#92;r&#92;&#92;n: \" + \"&#92;r&#92;n\".getBytes());\nString unencodedString = \"aaaaaaaaaaaaaaaa\";\nBase64 encoder = new Base64(4, \"\\n\".getBytes());\nString encodedString = new String( encoder.encodeBase64( unencodedString .getBytes(), true ) );\nSystem.out.println(encodedString);\nSystem.out.println(encodedString.getBytes());\n\nYou'll note the byte sequence ends with [13, 10] instead of [10], and the line didn't get split at 4 as specified in the constructor, but by 76.  You can double check this using a larger string to encode.\n\nI believe at least some of the blame lies with line 817 of Base64.java:\nBase64 b64 = isChunked ? new Base64(urlSafe) : new Base64(0, CHUNK_SEPARATOR, urlSafe);\nIf I request the encoding to be chunked, it will call the constructor public Base64(boolean urlSafe), which does\nthis(CHUNK_SIZE, CHUNK_SEPARATOR, urlSafe); (line 244).  As you can see, this overrides any parameters passed into the constructor.\n\nOr am I misunderstanding how this is supposed to work?",
            "My apologies, I was calling a static method from an instanced class.  I see now I should have been doing\nString encodedString = new String( encoder.encode( unencodedString .getBytes() ) );",
            "Just as an FYI we had another user in Abdera impacted by this so we had to do a work around in ABDERA-278, would be great if a new codec release could be done to pick up the fix in this jira.."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to release a 1.4.1 version of the source code with the old behavior, and to patch all framework code to work. The patch also introduces JUnit tests to avoid this problem in the future."
    },
    "Cli_39_src/main/java/org/apache/commons/cli/TypeHandler.java_64_106": {
        "src": "public static Object createValue(final String str, final Class<?> clazz) throws ParseException\n    {\n        if (PatternOptionBuilder.STRING_VALUE == clazz)\n        {\n            return str;\n        }\n        else if (PatternOptionBuilder.OBJECT_VALUE == clazz)\n        {\n            return createObject(str);\n        }\n        else if (PatternOptionBuilder.NUMBER_VALUE == clazz)\n        {\n            return createNumber(str);\n        }\n        else if (PatternOptionBuilder.DATE_VALUE == clazz)\n        {\n            return createDate(str);\n        }\n        else if (PatternOptionBuilder.CLASS_VALUE == clazz)\n        {\n            return createClass(str);\n        }\n        else if (PatternOptionBuilder.FILE_VALUE == clazz)\n        {\n            return createFile(str);\n        }\n        else if (PatternOptionBuilder.EXISTING_FILE_VALUE == clazz)\n        {\n            return createFile(str);\n        }\n        else if (PatternOptionBuilder.FILES_VALUE == clazz)\n        {\n            return createFiles(str);\n        }\n        else if (PatternOptionBuilder.URL_VALUE == clazz)\n        {\n            return createURL(str);\n        }\n        else\n        {\n            return null;\n        }\n    }",
        "src_wo_comments": "public static Object createValue ( final String str , final Class < ? > clazz ) throws ParseException { if ( PatternOptionBuilder . STRING_VALUE == clazz ) { return str ; } else if ( PatternOptionBuilder . OBJECT_VALUE == clazz ) { return createObject ( str ) ; } else if ( PatternOptionBuilder . NUMBER_VALUE == clazz ) { return createNumber ( str ) ; } else if ( PatternOptionBuilder . DATE_VALUE == clazz ) { return createDate ( str ) ; } else if ( PatternOptionBuilder . CLASS_VALUE == clazz ) { return createClass ( str ) ; } else if ( PatternOptionBuilder . FILE_VALUE == clazz ) { return createFile ( str ) ; } else if ( PatternOptionBuilder . EXISTING_FILE_VALUE == clazz ) { return createFile ( str ) ; } else if ( PatternOptionBuilder . FILES_VALUE == clazz ) { return createFiles ( str ) ; } else if ( PatternOptionBuilder . URL_VALUE == clazz ) { return createURL ( str ) ; } else { return null ; } }",
        "fixed_src": "public static Object createValue(final String str, final Class<?> clazz) throws ParseException\n    {\n        if (PatternOptionBuilder.STRING_VALUE == clazz)\n        {\n            return str;\n        }\n        else if (PatternOptionBuilder.OBJECT_VALUE == clazz)\n        {\n            return createObject(str);\n        }\n        else if (PatternOptionBuilder.NUMBER_VALUE == clazz)\n        {\n            return createNumber(str);\n        }\n        else if (PatternOptionBuilder.DATE_VALUE == clazz)\n        {\n            return createDate(str);\n        }\n        else if (PatternOptionBuilder.CLASS_VALUE == clazz)\n        {\n            return createClass(str);\n        }\n        else if (PatternOptionBuilder.FILE_VALUE == clazz)\n        {\n            return createFile(str);\n        }\n        else if (PatternOptionBuilder.EXISTING_FILE_VALUE == clazz)\n        {\n            return openFile(str);\n        }\n        else if (PatternOptionBuilder.FILES_VALUE == clazz)\n        {\n            return createFiles(str);\n        }\n        else if (PatternOptionBuilder.URL_VALUE == clazz)\n        {\n            return createURL(str);\n        }\n        else\n        {\n            return null;\n        }\n    }",
        "fixed_src_wo_comments": "public static Object createValue ( final String str , final Class < ? > clazz ) throws ParseException { if ( PatternOptionBuilder . STRING_VALUE == clazz ) { return str ; } else if ( PatternOptionBuilder . OBJECT_VALUE == clazz ) { return createObject ( str ) ; } else if ( PatternOptionBuilder . NUMBER_VALUE == clazz ) { return createNumber ( str ) ; } else if ( PatternOptionBuilder . DATE_VALUE == clazz ) { return createDate ( str ) ; } else if ( PatternOptionBuilder . CLASS_VALUE == clazz ) { return createClass ( str ) ; } else if ( PatternOptionBuilder . FILE_VALUE == clazz ) { return createFile ( str ) ; } else if ( PatternOptionBuilder . EXISTING_FILE_VALUE == clazz ) { return openFile ( str ) ; } else if ( PatternOptionBuilder . FILES_VALUE == clazz ) { return createFiles ( str ) ; } else if ( PatternOptionBuilder . URL_VALUE == clazz ) { return createURL ( str ) ; } else { return null ; } }",
        "summary": "Option parser type EXISTING_FILE_VALUE not check file existing",
        "Description": "When the user pass option type FileInputStream.class, I think the expected behavior for the return value is the same type, which the user passed.\n\nOptions options = new Options();\noptions.addOption(Option.builder(\"f\").hasArg().type(FileInputStream.class).build());\nCommandLine cline = new DefaultParser().parse(options, args);\nFileInputStream file = (FileInputStream) cline.getParsedOptionValue(\"f\"); // it returns \"File\" object, without check File exist.\n\n\nI attach a solution for it:\nhttps://github.com/schaumb/commons-cli/commit/abfcc8211f529ab75f3b3edd4a827e484109eb0b\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-274",
        "comments": [
            "GitHub user schaumb opened a pull request:\n\n    https://github.com/apache/commons-cli/pull/10\n\n    [CLI-274] implement EXISTING_FILE_VALUE type handler\n\n    when the user pass option type FileInputStream.class, I think the expected behavior for the return value is the same type, which the user passed.\n    Before this there was no check whether the file exist.\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/schaumb/commons-cli patch-1\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/commons-cli/pull/10.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #10\n    \n----\ncommit abfcc8211f529ab75f3b3edd4a827e484109eb0b\nAuthor: Bela Schaum <schaumb@users.noreply.github.com>\nDate:   2017-04-17T11:45:09Z\n\n    implement EXISTING_FILE_VALUE type handler\n    \n    when the user pass option type FileInputStream.class, I think the expected behavior for the return value is the same type, which the user passed.\n    Before this there was no check whether the file exist.\n\n----\n",
            "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/commons-cli/pull/10\n",
            "Github user britter commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/10\n  \n    @schaumb we had a change in our Git infrastructure which caused the asfgit bot to close this PR. can you please reopen it? Thank you!\n",
            "GitHub user schaumb opened a pull request:\n\n    https://github.com/apache/commons-cli/pull/12\n\n    [CLI-274] implement EXISTING_FILE_VALUE type handler\n\n    when the user pass option type FileInputStream.class, I think the expected behavior for the return value is the same type, which the user passed.\n    Before this there was no check whether the file exist.\n\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/schaumb/commons-cli patch-1\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/commons-cli/pull/12.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #12\n    \n----\ncommit abfcc8211f529ab75f3b3edd4a827e484109eb0b\nAuthor: Bela Schaum <schaumb@users.noreply.github.com>\nDate:   2017-04-17T11:45:09Z\n\n    implement EXISTING_FILE_VALUE type handler\n    \n    when the user pass option type FileInputStream.class, I think the expected behavior for the return value is the same type, which the user passed.\n    Before this there was no check whether the file exist.\n\ncommit a315e187908d184b1d40f9e7425bc65333a2d07a\nAuthor: Bela Schaum <schaumb@users.noreply.github.com>\nDate:   2017-04-17T14:03:11Z\n\n    [CLI-274] misseed import\n\ncommit 9039cbd454346276c632ae2424a3c20e18a2d276\nAuthor: Bela Schaum <schaumb@users.noreply.github.com>\nDate:   2017-04-17T14:21:42Z\n\n    Update PatternOptionBuilderTest.java\n\ncommit c1cbe9dd69f80b627ea046095af842bfafb15803\nAuthor: Bela Schaum <schaumb@users.noreply.github.com>\nDate:   2017-04-17T14:27:45Z\n\n    Update PatternOptionBuilderTest.java\n\ncommit e3d65b0ee3b08599bd787a7721afc326d1bcccf1\nAuthor: Bela Schaum <schaumb@users.noreply.github.com>\nDate:   2017-04-17T14:33:42Z\n\n    Update PatternOptionBuilderTest.java\n    \n    -.-' sry\n\ncommit fac33304c67496380cd168d71cac79dbc0e60142\nAuthor: Bela Schaum <schaumb@users.noreply.github.com>\nDate:   2017-04-17T14:39:15Z\n\n    Update PatternOptionBuilderTest.java\n    \n    really exist file name?\n\n----\n",
            "Github user coveralls commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/12\n  \n    \n    [![Coverage Status](https://coveralls.io/builds/11900307/badge)](https://coveralls.io/builds/11900307)\n    \n    Coverage increased (+0.009%) to 96.244% when pulling **aea58f8677e55513ae281c49b91a3abce5ee7d1b on schaumb:patch-1** into **70a392756c713f404fed0e3ddd48aa18ce20485f on apache:master**.\n\n",
            "Github user britter commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121290023\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,12 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    -\n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    -\n    -        // todo test if an error is returned if the file doesn't exists (when it's implemented)\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\", \"-g\", \"/dev/null\" });\n    +        \n    +        assertNotNull(\"option g not parsed, or not FileInputStream\", (FileInputStream) line.getOptionObject(\"g\"));\n    +        assertNull(\"option f parsed\", (FileInputStream) line.getOptionObject(\"f\"));\n    --- End diff --\n    \n    I don't understand this assertion. Shouldn't `line.getOptionObject(\"f\")` return an object of type `File`? And why is it null? because `test.properties` does not exist?\n",
            "Github user britter commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121290023\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,12 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    -\n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    -\n    -        // todo test if an error is returned if the file doesn't exists (when it's implemented)\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\", \"-g\", \"/dev/null\" });\n    +        \n    +        assertNotNull(\"option g not parsed, or not FileInputStream\", (FileInputStream) line.getOptionObject(\"g\"));\n    +        assertNull(\"option f parsed\", (FileInputStream) line.getOptionObject(\"f\"));\n    --- End diff --\n    \n    I don't understand this assertion. Shouldn't `line.getOptionObject(\"f\")` return an object of type `File`? And why is it null? because `test.properties` does not exist?\n",
            "Github user schaumb commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121406356\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,12 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    -\n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    -\n    -        // todo test if an error is returned if the file doesn't exists (when it's implemented)\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\", \"-g\", \"/dev/null\" });\n    +        \n    +        assertNotNull(\"option g not parsed, or not FileInputStream\", (FileInputStream) line.getOptionObject(\"g\"));\n    +        assertNull(\"option f parsed\", (FileInputStream) line.getOptionObject(\"f\"));\n    --- End diff --\n    \n    So basically `<` means this parameter is an existing file parameter.\n    \n    When I built the options manually, the type is what I need to set is `PatternOptionBuilder.EXISTING_FILE_VALUE`, which is `FileInputStream`.\n    This is why think `line.getOptionObject(\"f\")` need to return an object of type `FileInputStream`. \n    \n    Yes, It is null because test.properties does not exist, or not readable. This can be made clearer somehow. \n    Another task is to find a cross-platform file which is exist (and not /dev/null).\n",
            "Github user schaumb commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121406356\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,12 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    -\n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    -\n    -        // todo test if an error is returned if the file doesn't exists (when it's implemented)\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\", \"-g\", \"/dev/null\" });\n    +        \n    +        assertNotNull(\"option g not parsed, or not FileInputStream\", (FileInputStream) line.getOptionObject(\"g\"));\n    +        assertNull(\"option f parsed\", (FileInputStream) line.getOptionObject(\"f\"));\n    --- End diff --\n    \n    So basically `<` means this parameter is an existing file parameter.\n    \n    When I built the options manually, the type is what I need to set is `PatternOptionBuilder.EXISTING_FILE_VALUE`, which is `FileInputStream`.\n    This is why think `line.getOptionObject(\"f\")` need to return an object of type `FileInputStream`. \n    \n    Yes, It is null because test.properties does not exist, or not readable. This can be made clearer somehow. \n    Another task is to find a cross-platform file which is exist (and not /dev/null).\n",
            "Github user britter commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121493262\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,12 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    -\n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    -\n    -        // todo test if an error is returned if the file doesn't exists (when it's implemented)\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\", \"-g\", \"/dev/null\" });\n    +        \n    +        assertNotNull(\"option g not parsed, or not FileInputStream\", (FileInputStream) line.getOptionObject(\"g\"));\n    +        assertNull(\"option f parsed\", (FileInputStream) line.getOptionObject(\"f\"));\n    --- End diff --\n    \n    > Yes, It is null because test.properties does not exist, or not readable. This can be made clearer somehow.\n    \n    Maybe by calling it `non-existing.file`?\n    \n    > Another task is to find a cross-platform file which is exist (and not /dev/null).\n    \n    You could put add dummy file in `src/test/resources` and use that one.\n",
            "Github user britter commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121493262\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,12 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    -\n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    -\n    -        // todo test if an error is returned if the file doesn't exists (when it's implemented)\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\", \"-g\", \"/dev/null\" });\n    +        \n    +        assertNotNull(\"option g not parsed, or not FileInputStream\", (FileInputStream) line.getOptionObject(\"g\"));\n    +        assertNull(\"option f parsed\", (FileInputStream) line.getOptionObject(\"f\"));\n    --- End diff --\n    \n    > Yes, It is null because test.properties does not exist, or not readable. This can be made clearer somehow.\n    \n    Maybe by calling it `non-existing.file`?\n    \n    > Another task is to find a cross-platform file which is exist (and not /dev/null).\n    \n    You could put add dummy file in `src/test/resources` and use that one.\n",
            "Github user coveralls commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/12\n  \n    \n    [![Coverage Status](https://coveralls.io/builds/11938473/badge)](https://coveralls.io/builds/11938473)\n    \n    Coverage increased (+0.009%) to 96.244% when pulling **1e59d0c2fd1cfee450d0104734307306803a84e0 on schaumb:patch-1** into **70a392756c713f404fed0e3ddd48aa18ce20485f on apache:master**.\n\n",
            "Github user coveralls commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/12\n  \n    \n    [![Coverage Status](https://coveralls.io/builds/11938473/badge)](https://coveralls.io/builds/11938473)\n    \n    Coverage increased (+0.009%) to 96.244% when pulling **1e59d0c2fd1cfee450d0104734307306803a84e0 on schaumb:patch-1** into **70a392756c713f404fed0e3ddd48aa18ce20485f on apache:master**.\n\n",
            "Github user coveralls commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/12\n  \n    \n    [![Coverage Status](https://coveralls.io/builds/11939016/badge)](https://coveralls.io/builds/11939016)\n    \n    Coverage increased (+0.009%) to 96.244% when pulling **4f9c95bcb246b64f7f6756cc3840d2061a262fe7 on schaumb:patch-1** into **70a392756c713f404fed0e3ddd48aa18ce20485f on apache:master**.\n\n",
            "Github user coveralls commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/12\n  \n    \n    [![Coverage Status](https://coveralls.io/builds/11939016/badge)](https://coveralls.io/builds/11939016)\n    \n    Coverage increased (+0.009%) to 96.244% when pulling **4f9c95bcb246b64f7f6756cc3840d2061a262fe7 on schaumb:patch-1** into **70a392756c713f404fed0e3ddd48aa18ce20485f on apache:master**.\n\n",
            "Github user coveralls commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/12\n  \n    \n    [![Coverage Status](https://coveralls.io/builds/11939016/badge)](https://coveralls.io/builds/11939016)\n    \n    Coverage increased (+0.009%) to 96.244% when pulling **4f9c95bcb246b64f7f6756cc3840d2061a262fe7 on schaumb:patch-1** into **70a392756c713f404fed0e3ddd48aa18ce20485f on apache:master**.\n\n",
            "Github user coveralls commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/12\n  \n    \n    [![Coverage Status](https://coveralls.io/builds/11939016/badge)](https://coveralls.io/builds/11939016)\n    \n    Coverage increased (+0.009%) to 96.244% when pulling **4f9c95bcb246b64f7f6756cc3840d2061a262fe7 on schaumb:patch-1** into **70a392756c713f404fed0e3ddd48aa18ce20485f on apache:master**.\n\n",
            "Github user britter commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121589022\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,15 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"non-existing.file\", \"-g\", \"src/test/resources/existing-readable.file\" });\n    +        \n    +        assertNull(\"option f parsed\", line.getOptionObject(\"f\"));\n     \n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    -\n    -        // todo test if an error is returned if the file doesn't exists (when it's implemented)\n    +        Object parsedReadableFileStream = line.getOptionObject(\"g\");\n    +        assertNotNull(\"option g not parsed\", parsedReadableFileStream);\n    +        assertEquals(\"option g not FileInputStream\", FileInputStream.class, parsedReadableFileStream.getClass());\n    --- End diff --\n    \n    How about `assertTrue(parsedReadableFileStream instanceof FileInputStream)`? Your assert will fail for subclasses of `FileInputStream`\n",
            "Github user britter commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121588869\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,15 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"non-existing.file\", \"-g\", \"src/test/resources/existing-readable.file\" });\n    +        \n    +        assertNull(\"option f parsed\", line.getOptionObject(\"f\"));\n     \n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    --- End diff --\n    \n    Now we're not testing the `-f` option any more...\n",
            "Github user britter commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121589022\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,15 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"non-existing.file\", \"-g\", \"src/test/resources/existing-readable.file\" });\n    +        \n    +        assertNull(\"option f parsed\", line.getOptionObject(\"f\"));\n     \n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    -\n    -        // todo test if an error is returned if the file doesn't exists (when it's implemented)\n    +        Object parsedReadableFileStream = line.getOptionObject(\"g\");\n    +        assertNotNull(\"option g not parsed\", parsedReadableFileStream);\n    +        assertEquals(\"option g not FileInputStream\", FileInputStream.class, parsedReadableFileStream.getClass());\n    --- End diff --\n    \n    How about `assertTrue(parsedReadableFileStream instanceof FileInputStream)`? Your assert will fail for subclasses of `FileInputStream`\n",
            "Github user britter commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121589195\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,15 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n    --- End diff --\n    \n    Looking at this, I think we're doing to many things at once in this test. Why don't we split this test up into to separate tests? One for `f<` and one for `g<`?\n",
            "Github user britter commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121588869\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,15 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"non-existing.file\", \"-g\", \"src/test/resources/existing-readable.file\" });\n    +        \n    +        assertNull(\"option f parsed\", line.getOptionObject(\"f\"));\n     \n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    --- End diff --\n    \n    Now we're not testing the `-f` option any more...\n",
            "Github user britter commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121589195\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,15 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n    --- End diff --\n    \n    Looking at this, I think we're doing to many things at once in this test. Why don't we split this test up into to separate tests? One for `f<` and one for `g<`?\n",
            "Github user schaumb commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121654518\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,15 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"non-existing.file\", \"-g\", \"src/test/resources/existing-readable.file\" });\n    +        \n    +        assertNull(\"option f parsed\", line.getOptionObject(\"f\"));\n     \n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    --- End diff --\n    \n    the 168. line tests f parameter\n",
            "Github user schaumb commented on a diff in the pull request:\n\n    https://github.com/apache/commons-cli/pull/12#discussion_r121654518\n  \n    --- Diff: src/test/java/org/apache/commons/cli/PatternOptionBuilderTest.java ---\n    @@ -159,13 +161,15 @@ public void testURLPattern() throws Exception\n         @Test\n         public void testExistingFilePattern() throws Exception\n         {\n    -        final Options options = PatternOptionBuilder.parsePattern(\"f<\");\n    +        final Options options = PatternOptionBuilder.parsePattern(\"f<g<\");\n             final CommandLineParser parser = new PosixParser();\n    -        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"test.properties\" });\n    +        final CommandLine line = parser.parse(options, new String[] { \"-f\", \"non-existing.file\", \"-g\", \"src/test/resources/existing-readable.file\" });\n    +        \n    +        assertNull(\"option f parsed\", line.getOptionObject(\"f\"));\n     \n    -        assertEquals(\"f value\", new File(\"test.properties\"), line.getOptionObject(\"f\"));\n    --- End diff --\n    \n    the 168. line tests f parameter\n",
            "Github user coveralls commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/12\n  \n    \n    [![Coverage Status](https://coveralls.io/builds/11947210/badge)](https://coveralls.io/builds/11947210)\n    \n    Coverage increased (+0.009%) to 96.244% when pulling **f4a28c0463a414464ebe214a7790fde0b0069e3e on schaumb:patch-1** into **70a392756c713f404fed0e3ddd48aa18ce20485f on apache:master**.\n\n",
            "Github user coveralls commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/12\n  \n    \n    [![Coverage Status](https://coveralls.io/builds/11947210/badge)](https://coveralls.io/builds/11947210)\n    \n    Coverage increased (+0.009%) to 96.244% when pulling **f4a28c0463a414464ebe214a7790fde0b0069e3e on schaumb:patch-1** into **70a392756c713f404fed0e3ddd48aa18ce20485f on apache:master**.\n\n",
            "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/commons-cli/pull/12\n",
            "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/commons-cli/pull/12\n",
            "Merged in 0b45395.\n\nThank you!"
        ],
        "summarized_discussion": ""
    },
    "Compress_37_src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveInputStream.java_452_502": {
        "src": "Map<String, String> parsePaxHeaders(final InputStream i)\n        throws IOException {\n        final Map<String, String> headers = new HashMap<String, String>(globalPaxHeaders);\n        // Format is \"length keyword=value\\n\";\n        while(true){ // get length\n            int ch;\n            int len = 0;\n            int read = 0;\n            while((ch = i.read()) != -1) {\n                read++;\n                if (ch == ' '){\n                    // Get keyword\n                    final ByteArrayOutputStream coll = new ByteArrayOutputStream();\n                    while((ch = i.read()) != -1) {\n                        read++;\n                        if (ch == '='){ // end of keyword\n                            final String keyword = coll.toString(CharsetNames.UTF_8);\n                            // Get rest of entry\n                            final int restLen = len - read;\n                            if (restLen == 1) { // only NL\n                                headers.remove(keyword);\n                            } else {\n                                final byte[] rest = new byte[restLen];\n                                final int got = IOUtils.readFully(i, rest);\n                                if (got != restLen) {\n                                    throw new IOException(\"Failed to read \"\n                                                          + \"Paxheader. Expected \"\n                                                          + restLen\n                                                          + \" bytes, read \"\n                                                          + got);\n                                }\n                                // Drop trailing NL\n                                final String value = new String(rest, 0,\n                                                          restLen - 1, CharsetNames.UTF_8);\n                                headers.put(keyword, value);\n                            }\n                            break;\n                        }\n                        coll.write((byte) ch);\n                    }\n                    break; // Processed single header\n                }\n                len *= 10;\n                len += ch - '0';\n            }\n            if (ch == -1){ // EOF\n                break;\n            }\n        }\n        return headers;\n    }",
        "src_wo_comments": "Map < String , String > parsePaxHeaders ( final InputStream i ) throws IOException { final Map < String , String > headers = new HashMap < String , String > ( globalPaxHeaders ) ; while ( true ) { int ch ; int len = 0 ; int read = 0 ; while ( ( ch = i . read ( ) ) != - 1 ) { read ++ ; if ( ch == ' ' ) { final ByteArrayOutputStream coll = new ByteArrayOutputStream ( ) ; while ( ( ch = i . read ( ) ) != - 1 ) { read ++ ; if ( ch == '=' ) { final String keyword = coll . toString ( CharsetNames . UTF_8 ) ; final int restLen = len - read ; if ( restLen == 1 ) { headers . remove ( keyword ) ; } else { final byte [ ] rest = new byte [ restLen ] ; final int got = IOUtils . readFully ( i , rest ) ; if ( got != restLen ) { throw new IOException ( \"Failed to read \" + \"Paxheader. Expected \" + restLen + \" bytes, read \" + got ) ; } final String value = new String ( rest , 0 , restLen - 1 , CharsetNames . UTF_8 ) ; headers . put ( keyword , value ) ; } break ; } coll . write ( ( byte ) ch ) ; } break ; } len *= 10 ; len += ch - '0' ; } if ( ch == - 1 ) { break ; } } return headers ; }",
        "fixed_src": "Map<String, String> parsePaxHeaders(final InputStream i)\n        throws IOException {\n        final Map<String, String> headers = new HashMap<String, String>(globalPaxHeaders);\n        // Format is \"length keyword=value\\n\";\n        while(true){ // get length\n            int ch;\n            int len = 0;\n            int read = 0;\n            while((ch = i.read()) != -1) {\n                read++;\n                if (ch == '\\n') { // blank line in header\n                    break;\n                } else if (ch == ' '){ // End of length string\n                    // Get keyword\n                    final ByteArrayOutputStream coll = new ByteArrayOutputStream();\n                    while((ch = i.read()) != -1) {\n                        read++;\n                        if (ch == '='){ // end of keyword\n                            final String keyword = coll.toString(CharsetNames.UTF_8);\n                            // Get rest of entry\n                            final int restLen = len - read;\n                            if (restLen == 1) { // only NL\n                                headers.remove(keyword);\n                            } else {\n                                final byte[] rest = new byte[restLen];\n                                final int got = IOUtils.readFully(i, rest);\n                                if (got != restLen) {\n                                    throw new IOException(\"Failed to read \"\n                                                          + \"Paxheader. Expected \"\n                                                          + restLen\n                                                          + \" bytes, read \"\n                                                          + got);\n                                }\n                                // Drop trailing NL\n                                final String value = new String(rest, 0,\n                                                          restLen - 1, CharsetNames.UTF_8);\n                                headers.put(keyword, value);\n                            }\n                            break;\n                        }\n                        coll.write((byte) ch);\n                    }\n                    break; // Processed single header\n                }\n                len *= 10;\n                len += ch - '0';\n            }\n            if (ch == -1){ // EOF\n                break;\n            }\n        }\n        return headers;\n    }",
        "fixed_src_wo_comments": "Map < String , String > parsePaxHeaders ( final InputStream i ) throws IOException { final Map < String , String > headers = new HashMap < String , String > ( globalPaxHeaders ) ; while ( true ) { int ch ; int len = 0 ; int read = 0 ; while ( ( ch = i . read ( ) ) != - 1 ) { read ++ ; if ( ch == '\\n' ) { break ; } else if ( ch == ' ' ) { final ByteArrayOutputStream coll = new ByteArrayOutputStream ( ) ; while ( ( ch = i . read ( ) ) != - 1 ) { read ++ ; if ( ch == '=' ) { final String keyword = coll . toString ( CharsetNames . UTF_8 ) ; final int restLen = len - read ; if ( restLen == 1 ) { headers . remove ( keyword ) ; } else { final byte [ ] rest = new byte [ restLen ] ; final int got = IOUtils . readFully ( i , rest ) ; if ( got != restLen ) { throw new IOException ( \"Failed to read \" + \"Paxheader. Expected \" + restLen + \" bytes, read \" + got ) ; } final String value = new String ( rest , 0 , restLen - 1 , CharsetNames . UTF_8 ) ; headers . put ( keyword , value ) ; } break ; } coll . write ( ( byte ) ch ) ; } break ; } len *= 10 ; len += ch - '0' ; } if ( ch == - 1 ) { break ; } } return headers ; }",
        "summary": "Parsing PAX headers fails with NegativeArraySizeException",
        "Description": "The {{TarArchiveInputStream.parsePaxHeaders}} method fails with a {{NegativeArraySizeException}} when there is an empty line at the end of the headers.\n\nThe inner loop starts reading the length, but it gets a newline (10) and ends up subtracting '0' (48) from it; the result is a negative length that blows up an attempt to allocate the {{rest}} array.\n\nI would say that a check to see if {{ch}} is less the '0' and break the loop if it is.\n\nI used {{npm pack aws-sdk@2.2.16}} to generate a tarball with this issue.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-355",
        "comments": [
            "I can confirm the fix [here|https://github.com/apache/commons-compress/compare/master...blackducksoftware:bug/COMPRESS-355-negative-array-size] works (alas, no tests).",
            "Thank you.\n\nApplied the suggested change and added a test case (the first 8k of the aws tgz you described) with commit 19a620c."
        ],
        "summarized_discussion": "\n\nThe bug in the source code has been fixed with a suggested change and a test case added. The change was applied with commit 19a620c."
    },
    "Cli_9_src/java/org/apache/commons/cli/Parser.java_303_324": {
        "src": "protected void checkRequiredOptions()\n        throws MissingOptionException\n    {\n        // if there are required options that have not been\n        // processsed\n        if (getRequiredOptions().size() > 0)\n        {\n            Iterator iter = getRequiredOptions().iterator();\n            StringBuffer buff = new StringBuffer(\"Missing required option\");\n            buff.append(getRequiredOptions().size() == 1 ? \"\" : \"s\");\n            buff.append(\": \");\n\n\n            // loop through the required options\n            while (iter.hasNext())\n            {\n                buff.append(iter.next());\n            }\n\n            throw new MissingOptionException(buff.toString());\n        }\n    }",
        "src_wo_comments": "protected void checkRequiredOptions ( ) throws MissingOptionException { if ( getRequiredOptions ( ) . size ( ) > 0 ) { Iterator iter = getRequiredOptions ( ) . iterator ( ) ; StringBuffer buff = new StringBuffer ( \"Missing required option\" ) ; buff . append ( getRequiredOptions ( ) . size ( ) == 1 ? \"\" : \"s\" ) ; buff . append ( \": \" ) ; while ( iter . hasNext ( ) ) { buff . append ( iter . next ( ) ) ; } throw new MissingOptionException ( buff . toString ( ) ) ; } }",
        "fixed_src": "protected void checkRequiredOptions()\n        throws MissingOptionException\n    {\n        // if there are required options that have not been\n        // processsed\n        if (getRequiredOptions().size() > 0)\n        {\n            Iterator iter = getRequiredOptions().iterator();\n            StringBuffer buff = new StringBuffer(\"Missing required option\");\n            buff.append(getRequiredOptions().size() == 1 ? \"\" : \"s\");\n            buff.append(\": \");\n\n\n            // loop through the required options\n            while (iter.hasNext())\n            {\n                buff.append(iter.next());\n                buff.append(\", \");\n            }\n\n            throw new MissingOptionException(buff.substring(0, buff.length() - 2));\n        }\n    }",
        "fixed_src_wo_comments": "protected void checkRequiredOptions ( ) throws MissingOptionException { if ( getRequiredOptions ( ) . size ( ) > 0 ) { Iterator iter = getRequiredOptions ( ) . iterator ( ) ; StringBuffer buff = new StringBuffer ( \"Missing required option\" ) ; buff . append ( getRequiredOptions ( ) . size ( ) == 1 ? \"\" : \"s\" ) ; buff . append ( \": \" ) ; while ( iter . hasNext ( ) ) { buff . append ( iter . next ( ) ) ; buff . append ( \", \" ) ; } throw new MissingOptionException ( buff . substring ( 0 , buff . length ( ) - 2 ) ) ; } }",
        "summary": "MissingOptionException.getMessage() changed from CLI 1.0 > 1.1",
        "Description": "The MissingOptionException.getMessage() string changed from CLI 1.0 > 1.1. \n\nCLI 1.0 was poorly formatted but readable:\nMissing required options: -format-source-properties\n\nCLI 1.1 is almost unreadable:\nMissing required options: formatsourceproperties\n\nIn CLI 1.0 Options.addOption(Option) prefixed the stored options with a \"-\" and in CLI 1.1 it doesn't.\n\nI would suggest changing Parser.checkRequiredOptions() to add the options to the error message with a prefix of \" -\":\n\nOLD: \n            // loop through the required options\n            while (iter.hasNext())\n            {\n                buff.append(iter.next());\n            }\n\nNEW: \n            // loop through the required options\n            while (iter.hasNext())\n            {\n                buff.append(\" -\" + iter.next());\n            }\n\nResulting in:\nMissing required options: -format -source -properties\n",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-149",
        "comments": [
            "Also, there are no spaces between option names.\n\nIt should be something like:\n\nNEW:\n// loop through the required options\nwhile (iter.hasNext())\n{ buff.append(\" -\" + iter.next() + \" \"); }\n\nOR\n\nNEW:\n// loop through the required options\nwhile (iter.hasNext())\n{ buff.append(iter.next() + \" \"); }\n",
            "Agreed on the whitespace. I don't see why it shouldn't be:\n\nMissing required options: format, source, properties",
            "svn ci -m \"Adding comma delimited whitespace to the exception message that lists missing required options as requested in CLI-149. I didn't add the requested -, as it could be -- or some other prefix. Unit tests also added. \"\n\nSending        src/java/org/apache/commons/cli/Parser.java\nSending        src/test/org/apache/commons/cli/OptionsTest.java\nSending        src/test/org/apache/commons/cli/ParseRequiredTest.java\nTransmitting file data ...\nCommitted revision 654431."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to add whitespace between option names in the source code, and to add a comma-delimited whitespace to the exception message that lists missing required options. This is done by sending three files to SVN and committing revision 654431."
    },
    "Compress_21_src/main/java/org/apache/commons/compress/archivers/sevenz/SevenZOutputFile.java_634_649": {
        "src": "private void writeBits(final DataOutput header, final BitSet bits, final int length) throws IOException {\n        int cache = 0;\n        int shift = 7;\n        for (int i = 0; i < length; i++) {\n            cache |= ((bits.get(i) ? 1 : 0) << shift);\n            --shift;\n            if (shift == 0) {\n                header.write(cache);\n                shift = 7;\n                cache = 0;\n            }\n        }\n        if (length > 0 && shift > 0) {\n            header.write(cache);\n        }\n    }",
        "src_wo_comments": "private void writeBits ( final DataOutput header , final BitSet bits , final int length ) throws IOException { int cache = 0 ; int shift = 7 ; for ( int i = 0 ; i < length ; i ++ ) { cache |= ( ( bits . get ( i ) ? 1 : 0 ) << shift ) ; -- shift ; if ( shift == 0 ) { header . write ( cache ) ; shift = 7 ; cache = 0 ; } } if ( length > 0 && shift > 0 ) { header . write ( cache ) ; } }",
        "fixed_src": "private void writeBits(final DataOutput header, final BitSet bits, final int length) throws IOException {\n        int cache = 0;\n        int shift = 7;\n        for (int i = 0; i < length; i++) {\n            cache |= ((bits.get(i) ? 1 : 0) << shift);\n            if (--shift < 0) {\n                header.write(cache);\n                shift = 7;\n                cache = 0;\n            }\n        }\n        if (shift != 7) {\n            header.write(cache);\n        }\n    }",
        "fixed_src_wo_comments": "private void writeBits ( final DataOutput header , final BitSet bits , final int length ) throws IOException { int cache = 0 ; int shift = 7 ; for ( int i = 0 ; i < length ; i ++ ) { cache |= ( ( bits . get ( i ) ? 1 : 0 ) << shift ) ; if ( -- shift < 0 ) { header . write ( cache ) ; shift = 7 ; cache = 0 ; } } if ( shift != 7 ) { header . write ( cache ) ; } }",
        "summary": "Writing 7z empty entries produces incorrect or corrupt archive",
        "Description": "I couldn't find an exact rule that causes this incorrect behavior, but I tried to reduce it to some simple scenarios to reproduce it:\n\nInput: A folder with certain files -> tried to archive it.\nIf the folder contains more than 7 files the incorrect behavior appears.\n\nScenario 1: 7 empty files\nResult: The created archive contains a single folder entry with the name of the archive (no matter which was the name of the file)\n\nScenario 2: 7 files, some empty, some with content\nResult: The created archive contains a folder entry with the name of the archive and a number of file entries also with the name of the archive. The number of the entries is equal to the number of non empty files.\n\nScenario 3: 8 empty files\nResult: 7zip Manager cannot open archive and stops working.\n\nScenario 4.1: 8 files: some empty, some with content, last file (alphabetically) with content\nResult: same behavior as described for Scenario 2.\n\nScenario 4.2: 8 files, some empty, some with content, last file empy\nResult: archive is corrupt, the following message is received: \"Cannot open file 'archivename.7z' as archive\" (7Zip Manager does not crash).",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-252",
        "comments": [
            "since you point out LZMA2 in the subject, does that mean things are fine when you select a different compression method?",
            "I didn't test with other compression methods, than. Did that now, and it's the same, so I will delete that part from the description.",
            "I was afraid you'd say so.  Thanks for checking.",
            "I think I found the problem, need to add a bunch of more tests and commit then.  \"7 or more\" really is a magic number as the method writing bitsets has an off-by-one bug.  Will comment here, when I've committed the fix.\n\nDo you think you can build Compress from trunk and verify it works for you after that?",
            "svn revision 1552608",
            "I will take it from trunk and check it, but I am not sure I will have time today; I will let you know when I do.\n",
            "Thanks.",
            "Note: the Continuum CI server is not working currently, but I have deployed 1.7-SNAPSHOT to the ASF SNAPSHOT repo at http://repository.apache.org/snapshots in case that is of any use.",
            "Happy New Year Livia - have you had a chance to give Compress' trunk a try?",
            "Hi Stefan, Happy New Year to you too!... happy indeed it seems, as I just checked the new version from trunk and it works :) Thank you for the prompt fix and I am sorry for not having time to check it earlier.",
            "Thanks.\n"
        ],
        "summarized_discussion": "\n\nThe solution to the bug was found to be an off-by-one error in the method writing bitsets. The fix was committed to the source code, and the Compress from trunk was deployed to the Apache SNAPSHOT repository. The user was able to verify that the fix worked after trying it from the trunk."
    },
    "Math_79_src/main/java/org/apache/commons/math/util/MathUtils.java_1623_1630": {
        "src": "public static double distance(int[] p1, int[] p2) {\n      int sum = 0;\n      for (int i = 0; i < p1.length; i++) {\n          final int dp = p1[i] - p2[i];\n          sum += dp * dp;\n      }\n      return Math.sqrt(sum);\n    }",
        "src_wo_comments": "public static double distance ( int [ ] p1 , int [ ] p2 ) { int sum = 0 ; for ( int i = 0 ; i < p1 . length ; i ++ ) { final int dp = p1 [ i ] - p2 [ i ] ; sum += dp * dp ; } return Math . sqrt ( sum ) ; }",
        "fixed_src": "public static double distance(int[] p1, int[] p2) {\n      double sum = 0;\n      for (int i = 0; i < p1.length; i++) {\n          final double dp = p1[i] - p2[i];\n          sum += dp * dp;\n      }\n      return Math.sqrt(sum);\n    }",
        "fixed_src_wo_comments": "public static double distance ( int [ ] p1 , int [ ] p2 ) { double sum = 0 ; for ( int i = 0 ; i < p1 . length ; i ++ ) { final double dp = p1 [ i ] - p2 [ i ] ; sum += dp * dp ; } return Math . sqrt ( sum ) ; }",
        "summary": "NPE in  KMeansPlusPlusClusterer unittest",
        "Description": "When running this unittest, I am facing this NPE:\njava.lang.NullPointerException\n\tat org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91)\n\nThis is the unittest:\n\n\npackage org.fao.fisheries.chronicles.calcuation.cluster;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Random;\n\nimport org.apache.commons.math.stat.clustering.Cluster;\nimport org.apache.commons.math.stat.clustering.EuclideanIntegerPoint;\nimport org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer;\nimport org.fao.fisheries.chronicles.input.CsvImportProcess;\nimport org.fao.fisheries.chronicles.input.Top200Csv;\nimport org.junit.Test;\n\npublic class ClusterAnalysisTest {\n\n\n\t@Test\n\tpublic void testPerformClusterAnalysis2() {\n\t\tKMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(\n\t\t\t\tnew Random(1746432956321l));\n\t\tEuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {\n\t\t\t\tnew EuclideanIntegerPoint(new int[] { 1959, 325100 }),\n\t\t\t\tnew EuclideanIntegerPoint(new int[] { 1960, 373200 }), };\n\t\tList<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1);\n\t\tassertEquals(1, clusters.size());\n\n\t}\n\n}\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-305",
        "comments": [
            "Thanks for reporting this. ",
            "The problem was due to overflow in MathUtils.distance() due to bad typing.  Fixed in r885027.",
            "I have tested the fix and I can confirm that it is working in my environment. Thanks a lot!"
        ],
        "summarized_discussion": "\n\nThe bug in the source code was caused by an overflow in MathUtils.distance() due to bad typing. The issue has been fixed in r885027 and has been tested and confirmed to be working in the environment."
    },
    "Math_96_src/java/org/apache/commons/math/complex/Complex.java_245_267": {
        "src": "public boolean equals(Object other) {\n        boolean ret;\n        \n        if (this == other) { \n            ret = true;\n        } else if (other == null) {\n            ret = false;\n        } else  {\n            try {\n                Complex rhs = (Complex)other;\n                if (rhs.isNaN()) {\n                    ret = this.isNaN();\n                } else {\n                    ret = (Double.doubleToRawLongBits(real) == Double.doubleToRawLongBits(rhs.getReal())) && (Double.doubleToRawLongBits(imaginary) == Double.doubleToRawLongBits(rhs.getImaginary())); \n                }\n            } catch (ClassCastException ex) {\n                // ignore exception\n                ret = false;\n            }\n        }\n      \n        return ret;\n    }",
        "src_wo_comments": "public boolean equals ( Object other ) { boolean ret ; if ( this == other ) { ret = true ; } else if ( other == null ) { ret = false ; } else { try { Complex rhs = ( Complex ) other ; if ( rhs . isNaN ( ) ) { ret = this . isNaN ( ) ; } else { ret = ( Double . doubleToRawLongBits ( real ) == Double . doubleToRawLongBits ( rhs . getReal ( ) ) ) && ( Double . doubleToRawLongBits ( imaginary ) == Double . doubleToRawLongBits ( rhs . getImaginary ( ) ) ) ; } } catch ( ClassCastException ex ) { ret = false ; } } return ret ; }",
        "fixed_src": "public boolean equals(Object other) {\n        boolean ret;\n        \n        if (this == other) { \n            ret = true;\n        } else if (other == null) {\n            ret = false;\n        } else  {\n            try {\n                Complex rhs = (Complex)other;\n                if (rhs.isNaN()) {\n                    ret = this.isNaN();\n                } else {\n                    ret = (real == rhs.real) && (imaginary == rhs.imaginary); \n                }\n            } catch (ClassCastException ex) {\n                // ignore exception\n                ret = false;\n            }\n        }\n      \n        return ret;\n    }",
        "fixed_src_wo_comments": "public boolean equals ( Object other ) { boolean ret ; if ( this == other ) { ret = true ; } else if ( other == null ) { ret = false ; } else { try { Complex rhs = ( Complex ) other ; if ( rhs . isNaN ( ) ) { ret = this . isNaN ( ) ; } else { ret = ( real == rhs . real ) && ( imaginary == rhs . imaginary ) ; } } catch ( ClassCastException ex ) { ret = false ; } } return ret ; }",
        "summary": "Result of multiplying and equals for complex numbers is wrong",
        "Description": "Hi.\n\nThe bug relates on complex numbers.\nThe methods \"multiply\" and \"equals\" of the class Complex are involved.\n\nmathematic background:  (0,i) * (-1,0i) = (0,-i).\n\nlittle java program + output that shows the bug:\n-----------------------------------------------------------------------\n{code}\nimport org.apache.commons.math.complex.*;\npublic class TestProg {\n        public static void main(String[] args) {\n\n                ComplexFormat f = new ComplexFormat();\n                Complex c1 = new Complex(0,1);\n                Complex c2 = new Complex(-1,0);\n\n                Complex res = c1.multiply(c2);\n                Complex comp = new Complex(0,-1);\n\n                System.out.println(\"res:  \"+f.format(res));\n                System.out.println(\"comp: \"+f.format(comp));\n\n                System.out.println(\"res=comp: \"+res.equals(comp));\n        }\n}\n{code}\n-----------------------------------------------------------------------\n\nres:  -0 - 1i\ncomp: 0 - 1i\nres=comp: false\n\n-----------------------------------------------------------------------\n\nI think the \"equals\" should return \"true\".\nThe problem could either be the \"multiply\" method that gives (-0,-1i) instead of (0,-1i),\nor if you think thats right, the equals method has to be modified.\n\nGood Luck\nDieter",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-221",
        "comments": [
            "According to IEEE-754 (the standard that specifies double number representation), 0 is a signed value, so there are two different representations: +0 and -0, and there is a specific rule that say these two representations should always compare as equal.\n\nThe fact the multiplication returns a real part as -0 is therefore correct behavior. The fact it is not equal to another value where +0 appears was an error in out implementation of the equal method.\n\nThe problem has been fixed in the subversion repository (in branch 2.0) as of r690308.\n"
        ],
        "summarized_discussion": "\n\nThe bug was caused by an error in the implementation of the equal method, where two different representations of 0 (+0 and -0) were not being compared as equal. The problem has been fixed in the subversion repository in branch 2.0 as of r690308."
    },
    "Mockito_31_src/org/mockito/internal/stubbing/defaultanswers/ReturnsSmartNulls.java_59_61": {
        "src": "private String formatMethodCall() {\n\t\t\treturn invocation.getMethod().getName() + \"()\";\n\t\t}",
        "src_wo_comments": "private String formatMethodCall ( ) { return invocation . getMethod ( ) . getName ( ) + \"()\" ; }",
        "fixed_src": "private String formatMethodCall() {\n\t\t\tString args = Arrays.toString(invocation.getArguments());\n\t\t\treturn invocation.getMethod().getName() + \"(\" + args.substring(1, args.length() - 1) +\t\")\";\n\t\t}",
        "fixed_src_wo_comments": "private String formatMethodCall ( ) { String args = Arrays . toString ( invocation . getArguments ( ) ) ; return invocation . getMethod ( ) . getName ( ) + \"(\" + args . substring ( 1 , args . length ( ) - 1 ) + \")\" ; }",
        "summary": "Failing tests on Windows machine",
        "Description": "I just posted on the Google Forums, but someway somehow my post immediately disappeared in the void. So I am reposting it again here.\n\nI have 3 failing tests on my Windows 8.1 machine.\n1. DefaultMockingDetailsTest.should_get_extra_interfaces\n2. NoJUnitDependenciesTest.pure_mockito_should_not_depend_JUnit___ByteBuddy\n3. ClassLoadersTest.excluding_class_loader_cannot_load_classes_when_no_correct_source_url_set\n\nFor the first test, I was able to let it pass by changing line https://github.com/mockito/mockito/blob/master/test/org/mockito/internal/util/DefaultMockingDetailsTest.java#L56 to\n\n``` java\nBar bar = mock(Bar.class, withSettings().extraInterfaces(List.class, Observer.class));\n```\n\nI am not sure if this is indeed the correct test, so please let me know.\n\nFor the 2nd test, I first get the stack trace\n\n```\njava.lang.AssertionError: 'org\\mockito\\configuration\\MockitoConfiguration' has some dependency to JUnit\n    at org.mockitointegration.NoJUnitDependenciesTest.checkDependency(NoJUnitDependenciesTest.java:40)\n    at org.mockitointegration.NoJUnitDependenciesTest.pure_mockito_should_not_depend_JUnit___ByteBuddy(NoJUnitDependenciesTest.java:32)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:300)\n    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)\nCaused by: java.lang.NoClassDefFoundError: org\\mockito\\configuration\\MockitoConfiguration (wrong name: org/mockito/configuration/MockitoConfiguration)\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:760)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n    at org.mockitoutil.ClassLoaders$LocalExcludingURLClassLoader.findClass(ClassLoaders.java:156)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:348)\n    at org.mockitointegration.NoJUnitDependenciesTest.checkDependency(NoJUnitDependenciesTest.java:38)\n    ... 24 more\n\n\n```\n\nWhen I change line https://github.com/mockito/mockito/blob/master/test/org/mockitoutil/ClassLoaders.java#L361 to\n\n``` java\nString temp = file.getAbsolutePath().substring(root.getAbsolutePath().length() + 1).replace('/', '.').replace('\\\\', '.');\n```\n\nI get the following stack trace:\n\n```\njava.lang.AssertionError: 'org.mockito.internal.progress.TimesTest' has some dependency to JUnit\n    at org.mockitointegration.NoJUnitDependenciesTest.checkDependency(NoJUnitDependenciesTest.java:40)\n    at org.mockitointegration.NoJUnitDependenciesTest.pure_mockito_should_not_depend_JUnit___ByteBuddy(NoJUnitDependenciesTest.java:32)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:300)\n    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)\nCaused by: java.lang.NoClassDefFoundError: junit/framework/Assert\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:760)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n    at org.mockitoutil.ClassLoaders$LocalExcludingURLClassLoader.findClass(ClassLoaders.java:156)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:760)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n    at org.mockitoutil.ClassLoaders$LocalExcludingURLClassLoader.findClass(ClassLoaders.java:156)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:348)\n    at org.mockitointegration.NoJUnitDependenciesTest.checkDependency(NoJUnitDependenciesTest.java:38)\n    ... 24 more\nCaused by: java.lang.ClassNotFoundException: classes with prefix : [junit, org.junit] are excluded\n    at org.mockitoutil.ClassLoaders$LocalExcludingURLClassLoader.findClass(ClassLoaders.java:155)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    ... 51 more\n```\n\nThe reason I changed that line is because the temp result does not contain dots on windows machines, due to the fact that absolutepath does not return a path seperated by `/` but by `\\\\`.\nHowever then the test fails because the `TimesTest` in `test/` does indeed depend on junit. Shouldn't it only load classes that are under `src/`?\n\nThe 3rd test I sadly have no clue why it is succeeding and not throwing an exception.\n\nLooking forward to your responses =]\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Second test should be fixed in #206 - checkout PR and check\n"
            },
            {
                "content": "On another note, when I run the test suite with `Coverage as...` using EclEmma, all tests in `FieldsTest` fail. The tests `fields_should_return_all_declared_fields_in_hierarchy`, `can_filter_not_null_fields` and `fields_should_return_declared_fields` fail because the element `$jacocoData` is in the array.\n\nThe other tests:\n\ncan_get_list_of_InstanceField:\n\n```\njava.lang.AssertionError: unexpected element(s):<[org.mockito.internal.util.reflection.InstanceField@329de63]> in <[org.mockito.internal.util.reflection.InstanceField@d627de4f, org.mockito.internal.util.reflection.InstanceField@d627de30, org.mockito.internal.util.reflection.InstanceField@329de63]>\n    at org.fest.assertions.Fail.failure(Fail.java:228)\n    at org.fest.assertions.Assert.failure(Assert.java:149)\n    at org.fest.assertions.ItemGroupAssert.failureIfUnexpectedElementsFound(ItemGroupAssert.java:115)\n    at org.fest.assertions.ItemGroupAssert.assertContainsOnly(ItemGroupAssert.java:76)\n    at org.fest.assertions.ListAssert.containsOnly(ListAssert.java:193)\n    at org.mockito.internal.util.reflection.FieldsTest.can_get_list_of_InstanceField(FieldsTest.java:45)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:300)\n    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)\n```\n\ncan_get_values_of_instance_fields:\n\n```\njava.lang.AssertionError: unexpected element(s):<[[Z@6870c3c2]> in <['a', 'b', [Z@6870c3c2]>\n    at org.fest.assertions.Fail.failure(Fail.java:228)\n    at org.fest.assertions.Assert.failure(Assert.java:149)\n    at org.fest.assertions.ItemGroupAssert.failureIfUnexpectedElementsFound(ItemGroupAssert.java:115)\n    at org.fest.assertions.ItemGroupAssert.assertContainsOnly(ItemGroupAssert.java:76)\n    at org.fest.assertions.ListAssert.containsOnly(ListAssert.java:193)\n    at org.mockito.internal.util.reflection.FieldsTest.can_get_values_of_instance_fields(FieldsTest.java:36)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:300)\n    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)\n```\n"
            },
            {
                "content": "@alberskib Oops I forgot to mention that. I tested #206 and it did NOT fix the issue. The stacktrace I get with that fix is the same as my 2nd stacktrace I posted.\n"
            },
            {
                "content": "@JeremybellEU Could you check your first post - it looks like that for the 2nd test in both cases (before and after change) stacktraces are the same. So what is the difference?\n\nEdit: I noticed different class name\n"
            },
            {
                "content": "I see that the first test has been fixed now in master as per https://github.com/mockito/mockito/commit/d4075748d824c031f35592968c2bf6cec3adca3f No clue why my upstream didn't pull these changes.\n"
            },
            {
                "content": "@JeremybellEU On machine with Windows 7 I am not able to reproduce problems described by you. Is there anything special in your config? Are you running tests on master branch?\n"
            },
            {
                "content": "Yes I am running on master with no changes. I am running jdk 8.45 if that matters.\n"
            },
            {
                "content": "I guess that the test errors are IDE related as it seems to be about class loader scoping. I can successfully run the tests on Windows 7 and Windows 8.\n"
            },
            {
                "content": "And you are using IntelliJ I suppose? I will switch over to IntelliJ this summer, so after that happened I will let it know.\n"
            },
            {
                "content": "I am using IntelliJ. I tried it both with the IDE and Gradle. Eclipse is built on top of OSGi and there is some strange class loader stuff that comes with it. I looked into Eclipse to make the Byte Buddy's tests work which often test class loaders-depended instrumentations. It's not that trivial to write tests for this and this is why I assume the tests fail.\n"
            },
            {
                "content": "Closing this issue as I am now running Linux + IntelliJ. Seems too much work for a marginal result.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug in the source code was causing tests in the \"FieldsTest\" to fail due to the element \"$jacocoData\" being in the array. The bug was fixed in #206, but when tested, it did not fix the issue. It was determined that the issue was IDE related, likely due to class loader scoping. The bug was closed as the user was now running Linux + IntelliJ, which seemed to solve the issue."
    },
    "Math_103_src/java/org/apache/commons/math/distribution/NormalDistributionImpl.java_108_111": {
        "src": "public double cumulativeProbability(double x) throws MathException {\n            return 0.5 * (1.0 + Erf.erf((x - mean) /\n                    (standardDeviation * Math.sqrt(2.0))));\n    }",
        "src_wo_comments": "public double cumulativeProbability ( double x ) throws MathException { return 0.5 * ( 1.0 + Erf . erf ( ( x - mean ) / ( standardDeviation * Math . sqrt ( 2.0 ) ) ) ) ; }",
        "fixed_src": "public double cumulativeProbability(double x) throws MathException {\n        try {\n            return 0.5 * (1.0 + Erf.erf((x - mean) /\n                    (standardDeviation * Math.sqrt(2.0))));\n        } catch (MaxIterationsExceededException ex) {\n            if (x < (mean - 20 * standardDeviation)) { // JDK 1.5 blows at 38\n                return 0.0d;\n            } else if (x > (mean + 20 * standardDeviation)) {\n                return 1.0d;\n            } else {\n                throw ex;\n            }\n        }\n    }",
        "fixed_src_wo_comments": "public double cumulativeProbability ( double x ) throws MathException { try { return 0.5 * ( 1.0 + Erf . erf ( ( x - mean ) / ( standardDeviation * Math . sqrt ( 2.0 ) ) ) ) ; } catch ( MaxIterationsExceededException ex ) { if ( x < ( mean - 20 * standardDeviation ) ) { return 0.0d ; } else if ( x > ( mean + 20 * standardDeviation ) ) { return 1.0d ; } else { throw ex ; } } }",
        "summary": "ConvergenceException in normal CDF",
        "Description": "NormalDistributionImpl::cumulativeProbability(double x) throws ConvergenceException\nif x deviates too much from the mean. For example, when x=+/-100, mean=0, sd=1.\nOf course the value of the CDF is hard to evaluate in these cases,\nbut effectively it should be either zero or one.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-167",
        "comments": [
            "Thanks for reporting this.  I see three alternatives to address - appreciate comments.\n1) Determine tail resolution possible with current impl (hopefully not different on different JDKs, platforms) and \"top code\", checking arguments and returning 0 or 1, resp if argument is too far in SD units from the mean.  To find the cut points, empirically determine where convergence starts to fail.  Document the cut points in javadoc for Impl.\n2) Catch ConvergenceException and return 0 or 1, resp if argument is far from the mean; rethrow otherwise (though this should never happen).\n3) Resolve as WONTFIX and leave it to client to catch and handle ConvergenceException, examining argument.  Document algorithm more fully and warn that ConvergenceException will be thrown if tail probability cannot be accurately estimated or distinguished from 0.\nMy first thought was 2 and I guess I still favor that, since 3) is inconvenient for users and 1) may not be stable unless cut points are conservative.\nNote that this same problem may apply to tail probablilities of other continuous distributions and we should check and address all of these before resolving this issue.\n\n\n",
            "Fixed for nomal distribution in r558450.\nLeaving open because we should look at other distributions before closing.  ",
            "While the t, F and Gamma distributions all use convergents (which may fail to converge for tail probabilities), there is no obvious way (to me at least) to test and set uniform bounds as we did for the Normal distribution in the fix applied in r558450.  Since this issue was reported against the Normal distribution, I am resolving this issue as fixed.  "
        ],
        "summarized_discussion": "\n\nThe bug was related to the Normal distribution and the solution applied in r558450 was to determine tail resolution possible with current implementation, checking arguments and returning 0 or 1, resp if argument is too far in SD units from the mean. To find the cut points, empirically determine where convergence starts to fail. The bug was fixed for the Normal distribution, but open for other distributions."
    },
    "JacksonCore_11_src/main/java/com/fasterxml/jackson/core/sym/ByteQuadsCanonicalizer.java_874_886": {
        "src": "private void _verifySharing()\n    {\n        if (_hashShared) {\n            _hashArea = Arrays.copyOf(_hashArea, _hashArea.length);\n            _names = Arrays.copyOf(_names, _names.length);\n            _hashShared = false;\n            // 09-Sep-2015, tatu: As per [jackson-core#216], also need to ensure\n            //    we rehash as needed, as need-rehash flag is not copied from parent\n        }\n        if (_needRehash) {\n            rehash();\n        }\n    }",
        "src_wo_comments": "private void _verifySharing ( ) { if ( _hashShared ) { _hashArea = Arrays . copyOf ( _hashArea , _hashArea . length ) ; _names = Arrays . copyOf ( _names , _names . length ) ; _hashShared = false ; } if ( _needRehash ) { rehash ( ) ; } }",
        "fixed_src": "private void _verifySharing()\n    {\n        if (_hashShared) {\n            _hashArea = Arrays.copyOf(_hashArea, _hashArea.length);\n            _names = Arrays.copyOf(_names, _names.length);\n            _hashShared = false;\n            // 09-Sep-2015, tatu: As per [jackson-core#216], also need to ensure\n            //    we rehash as needed, as need-rehash flag is not copied from parent\n            _verifyNeedForRehash();\n        }\n        if (_needRehash) {\n            rehash();\n        }\n    }",
        "fixed_src_wo_comments": "private void _verifySharing ( ) { if ( _hashShared ) { _hashArea = Arrays . copyOf ( _hashArea , _hashArea . length ) ; _names = Arrays . copyOf ( _names , _names . length ) ; _hashShared = false ; _verifyNeedForRehash ( ) ; } if ( _needRehash ) { rehash ( ) ; } }",
        "summary": "ArrayIndexOutOfBoundsException: 128 when repeatedly serializing to a byte array",
        "Description": "```\njava.lang.ArrayIndexOutOfBoundsException: 128\n    at com.fasterxml.jackson.core.sym.ByteQuadsCanonicalizer.addName(ByteQuadsCanonicalizer.java:853)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.addName(UTF8StreamJsonParser.java:2340)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.findName(UTF8StreamJsonParser.java:2224)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.parseLongName(UTF8StreamJsonParser.java:1831)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.parseMediumName2(UTF8StreamJsonParser.java:1786)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.parseMediumName(UTF8StreamJsonParser.java:1743)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1678)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextFieldName(UTF8StreamJsonParser.java:1007)\n    at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringMap(MapDeserializer.java:471)\n    at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:341)\n    at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:26)\n    at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3702)\n    at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2824)\n    at com.kryptnostic.services.v1.SmokeTests.spamAddIndexPair(SmokeTests.java:605)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)\n```\n\nRepro: \n\n```\n@Test\npublic void spamTest() {\n        ObjectMapper mapper = new ObjectMapper();\n        Map<ObjectUserKey, ServerIndexPair> ssip = Maps.newConcurrentMap();\n        for ( int i = 0; i < 10000; ++i ) {\n            byte[] indexPairBytes = new byte[ 2080 ];\n            new Random().nextBytes( indexPairBytes );\n            ServerIndexPair sip = new ServerIndexPair( indexPairBytes );\n\n            byte[] s = mapper.writeValueAsBytes( ImmutableMap.of( UUID\n                    .randomUUID().toString(), sip ) );\n            Map<String, ServerIndexPair> metadata = mapper.readValue( s,\n                    new TypeReference<Map<String, ServerIndexPair>>() {} );\n            for ( Entry<String, ServerIndexPair> metadataEntry : metadata.entrySet() ) {\n                ServerIndexPair indexPair = metadataEntry.getValue();\n                ssip.put( new ObjectUserKey( metadataEntry.getKey(), user ),\n                        indexPair );\n            }\n            logger.error( \"Iteration: {}\", i );\n        }\n}\n```\n\n```\npublic class ServerIndexPair {\n    public static final String INDEX_PAIR_FIELD = \"indexPair\";\n    private final byte[]       indexPair;\n\n    @JsonCreator\n    public ServerIndexPair( @JsonProperty( INDEX_PAIR_FIELD ) byte[] indexPair ) {\n        Preconditions.checkState( indexPair.length == 2080, \"Index pair must be 2080 bytes long.\" );\n        this.indexPair = indexPair;\n    }\n\n    @JsonProperty( INDEX_PAIR_FIELD )\n    public byte[] getIndexPair() {\n        return indexPair;\n    }\n}\n```\n\n```\npublic class ObjectUserKey {\n    public static final String SEPARATOR = \":\";\n    private final String       objectId;\n    private final UUID         userKey;\n\n    @JsonCreator\n    public ObjectUserKey(\n            @JsonProperty( Names.ID_FIELD ) String objectId,\n            @JsonProperty( Names.USER_FIELD ) UUID userKey ) {\n        super();\n        this.objectId = objectId;\n        this.userKey = userKey;\n    }\n\n    @JsonProperty( Names.ID_FIELD )\n    public String getObjectId() {\n        return objectId;\n    }\n\n    @JsonProperty( Names.USER_FIELD )\n    public UUID getUserKey() {\n        return userKey;\n    }\n\n    @Override\n    public int hashCode() {\n        final int prime = 31;\n        int result = 1;\n        result = prime * result + ( ( objectId == null ) ? 0 : objectId.hashCode() );\n        return result;\n    }\n\n    @Override\n    public boolean equals( Object obj ) {\n        if ( this == obj ) {\n            return true;\n        }\n        if ( obj == null ) {\n            return false;\n        }\n        if ( !( obj instanceof ObjectUserKey ) ) {\n            return false;\n        }\n        ObjectUserKey other = (ObjectUserKey) obj;\n        if ( objectId == null ) {\n            if ( other.objectId != null ) {\n                return false;\n            }\n        }\n        if ( userKey == null ) {\n            if ( other.userKey != null ) {\n                return false;\n            }\n        }\n        if ( !objectId.equals( other.objectId ) ) {\n            return false;\n        }\n        if ( !userKey.equals( other.userKey ) ) {\n            return false;\n        }\n        return true;\n    }\n\n    @Override\n    public String toString() {\n        return userKey + SEPARATOR + objectId;\n    }\n\n    public static ObjectUserKey fromString( String value ) {\n        int index = value.lastIndexOf( ObjectUserKey.SEPARATOR );\n        Preconditions.checkState( index > -1, \"Separator character \" + SEPARATOR\n                + \" should be present for ObjectUserKey\" );\n        String userKeyString = value.substring( 0, index );\n        String objectIdString = value.substring( index + 1 );\n        UUID userKey = UUID.fromString( userKeyString );\n        return new ObjectUserKey( objectIdString, userKey );\n    }\n\n    public byte[] asBytes() {\n        return this.toString().getBytes();\n    }\n\n}\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "UPVOTE!!!!!!\n"
            },
            {
                "content": "+1\n"
            },
            {
                "content": "I suspect this is same as continuation of #207, so it is being worked on. Nasty one obviously.\nWill link that issue back to this one.\n"
            },
            {
                "content": "The underlying problem same as what was left over from #207 (there were two unrelated problems, but further reports were added in the issue). I used modified test case from that bug (to avoid dependency to `ObjectMapper`), but I am confident this fixes the root cause.\n\nFix will be part of 2.6.2, to be released relatively soon: I will just need to over list of all bugs submitted during past couple of weeks to bundle up all critical fixes.\n"
            },
            {
                "content": "Thanks, we'll be sure to test it out\n"
            },
            {
                "content": "hi @cowtowncoder, pending a fix version, is there any workaround for this (eg. disable the ByteQuadsCanonicalizer)?\n"
            },
            {
                "content": "@simonbasle Yes, disable `JsonFactory.Feature.CANONICALIZE_FIELD_NAMES`. That will avoid canonicalization.\n"
            }
        ],
        "summarized_discussion": "\n\nThe underlying bug is related to #207, and a fix will be included in the upcoming 2.6.2 version. As a workaround, users can disable the JsonFactory.Feature.CANONICALIZE_FIELD_NAMES to avoid canonicalization."
    },
    "Math_55_src/main/java/org/apache/commons/math/geometry/Vector3D.java_457_475": {
        "src": "public static Vector3D crossProduct(final Vector3D v1, final Vector3D v2) {\n\n\n      // rescale both vectors without losing precision,\n      // to ensure their norm are the same order of magnitude\n\n      // we reduce cancellation errors by preconditioning,\n      // we replace v1 by v3 = v1 - rho v2 with rho chosen in order to compute\n      // v3 without loss of precision. See Kahan lecture\n      // \"Computing Cross-Products and Rotations in 2- and 3-Dimensional Euclidean Spaces\"\n      // available at http://www.cs.berkeley.edu/~wkahan/MathH110/Cross.pdf\n\n      // compute rho as an 8 bits approximation of v1.v2 / v2.v2\n\n\n      // compute cross product from v3 and v2 instead of v1 and v2\n      return new Vector3D(v1.y * v2.z - v1.z * v2.y, v1.z * v2.x - v1.x * v2.z, v1.x * v2.y - v1.y * v2.x);\n\n  }",
        "src_wo_comments": "public static Vector3D crossProduct ( final Vector3D v1 , final Vector3D v2 ) { return new Vector3D ( v1 . y * v2 . z - v1 . z * v2 . y , v1 . z * v2 . x - v1 . x * v2 . z , v1 . x * v2 . y - v1 . y * v2 . x ) ; }",
        "fixed_src": "public static Vector3D crossProduct(final Vector3D v1, final Vector3D v2) {\n\n      final double n1 = v1.getNormSq();\n      final double n2 = v2.getNormSq();\n      if ((n1 * n2) < MathUtils.SAFE_MIN) {\n          return ZERO;\n      }\n\n      // rescale both vectors without losing precision,\n      // to ensure their norm are the same order of magnitude\n      final int deltaExp = (FastMath.getExponent(n1) - FastMath.getExponent(n2)) / 4;\n      final double x1    = FastMath.scalb(v1.x, -deltaExp);\n      final double y1    = FastMath.scalb(v1.y, -deltaExp);\n      final double z1    = FastMath.scalb(v1.z, -deltaExp);\n      final double x2    = FastMath.scalb(v2.x,  deltaExp);\n      final double y2    = FastMath.scalb(v2.y,  deltaExp);\n      final double z2    = FastMath.scalb(v2.z,  deltaExp);\n\n      // we reduce cancellation errors by preconditioning,\n      // we replace v1 by v3 = v1 - rho v2 with rho chosen in order to compute\n      // v3 without loss of precision. See Kahan lecture\n      // \"Computing Cross-Products and Rotations in 2- and 3-Dimensional Euclidean Spaces\"\n      // available at http://www.cs.berkeley.edu/~wkahan/MathH110/Cross.pdf\n\n      // compute rho as an 8 bits approximation of v1.v2 / v2.v2\n      final double ratio = (x1 * x2 + y1 * y2 + z1 * z2) / FastMath.scalb(n2, 2 * deltaExp);\n      final double rho   = FastMath.rint(256 * ratio) / 256;\n\n      final double x3 = x1 - rho * x2;\n      final double y3 = y1 - rho * y2;\n      final double z3 = z1 - rho * z2;\n\n      // compute cross product from v3 and v2 instead of v1 and v2\n      return new Vector3D(y3 * z2 - z3 * y2, z3 * x2 - x3 * z2, x3 * y2 - y3 * x2);\n\n  }",
        "fixed_src_wo_comments": "public static Vector3D crossProduct ( final Vector3D v1 , final Vector3D v2 ) { final double n1 = v1 . getNormSq ( ) ; final double n2 = v2 . getNormSq ( ) ; if ( ( n1 * n2 ) < MathUtils . SAFE_MIN ) { return ZERO ; } final int deltaExp = ( FastMath . getExponent ( n1 ) - FastMath . getExponent ( n2 ) ) / 4 ; final double x1 = FastMath . scalb ( v1 . x , - deltaExp ) ; final double y1 = FastMath . scalb ( v1 . y , - deltaExp ) ; final double z1 = FastMath . scalb ( v1 . z , - deltaExp ) ; final double x2 = FastMath . scalb ( v2 . x , deltaExp ) ; final double y2 = FastMath . scalb ( v2 . y , deltaExp ) ; final double z2 = FastMath . scalb ( v2 . z , deltaExp ) ; final double ratio = ( x1 * x2 + y1 * y2 + z1 * z2 ) / FastMath . scalb ( n2 , 2 * deltaExp ) ; final double rho = FastMath . rint ( 256 * ratio ) / 256 ; final double x3 = x1 - rho * x2 ; final double y3 = y1 - rho * y2 ; final double z3 = z1 - rho * z2 ; return new Vector3D ( y3 * z2 - z3 * y2 , z3 * x2 - x3 * z2 , x3 * y2 - y3 * x2 ) ; }",
        "summary": "Vector3D.crossProduct is sensitive to numerical cancellation",
        "Description": "Cross product implementation uses the naive formulas (y1 z2 - y2 z1, ...). These formulas fail when vectors are almost colinear, like in the following example:\n{code}\nVector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1);\nVector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1);\nSystem.out.println(Vector3D.crossProduct(v1, v2));\n{code}\n\nThe previous code displays { -1, 2, 0 } instead of the correct answer { -1, 2, 1 }",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-554",
        "comments": [
            "fixed in subversion repository as of r1088316"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the Subversion repository as of revision 1088316."
    },
    "JacksonDatabind_67_src/main/java/com/fasterxml/jackson/databind/deser/BasicDeserializerFactory.java_1384_1416": {
        "src": "@Override\n    public KeyDeserializer createKeyDeserializer(DeserializationContext ctxt,\n            JavaType type)\n        throws JsonMappingException\n    {\n        final DeserializationConfig config = ctxt.getConfig();\n        KeyDeserializer deser = null;\n        if (_factoryConfig.hasKeyDeserializers()) {\n            BeanDescription beanDesc = config.introspectClassAnnotations(type.getRawClass());\n            for (KeyDeserializers d  : _factoryConfig.keyDeserializers()) {\n                deser = d.findKeyDeserializer(type, config, beanDesc);\n                if (deser != null) {\n                    break;\n                }\n            }\n        }\n        // the only non-standard thing is this:\n        if (deser == null) {\n            if (type.isEnumType()) {\n                return _createEnumKeyDeserializer(ctxt, type);\n            }\n            deser = StdKeyDeserializers.findStringBasedKeyDeserializer(config, type);\n        }\n        // and then post-processing\n        if (deser != null) {\n            if (_factoryConfig.hasDeserializerModifiers()) {\n                for (BeanDeserializerModifier mod : _factoryConfig.deserializerModifiers()) {\n                    deser = mod.modifyKeyDeserializer(config, type, deser);\n                }\n            }\n        }\n        return deser;\n    }",
        "src_wo_comments": "@ Override public KeyDeserializer createKeyDeserializer ( DeserializationContext ctxt , JavaType type ) throws JsonMappingException { final DeserializationConfig config = ctxt . getConfig ( ) ; KeyDeserializer deser = null ; if ( _factoryConfig . hasKeyDeserializers ( ) ) { BeanDescription beanDesc = config . introspectClassAnnotations ( type . getRawClass ( ) ) ; for ( KeyDeserializers d : _factoryConfig . keyDeserializers ( ) ) { deser = d . findKeyDeserializer ( type , config , beanDesc ) ; if ( deser != null ) { break ; } } } if ( deser == null ) { if ( type . isEnumType ( ) ) { return _createEnumKeyDeserializer ( ctxt , type ) ; } deser = StdKeyDeserializers . findStringBasedKeyDeserializer ( config , type ) ; } if ( deser != null ) { if ( _factoryConfig . hasDeserializerModifiers ( ) ) { for ( BeanDeserializerModifier mod : _factoryConfig . deserializerModifiers ( ) ) { deser = mod . modifyKeyDeserializer ( config , type , deser ) ; } } } return deser ; }",
        "fixed_src": "@Override\n    public KeyDeserializer createKeyDeserializer(DeserializationContext ctxt,\n            JavaType type)\n        throws JsonMappingException\n    {\n        final DeserializationConfig config = ctxt.getConfig();\n        KeyDeserializer deser = null;\n        if (_factoryConfig.hasKeyDeserializers()) {\n            BeanDescription beanDesc = config.introspectClassAnnotations(type.getRawClass());\n            for (KeyDeserializers d  : _factoryConfig.keyDeserializers()) {\n                deser = d.findKeyDeserializer(type, config, beanDesc);\n                if (deser != null) {\n                    break;\n                }\n            }\n        }\n        // the only non-standard thing is this:\n        if (deser == null) {\n            if (type.isEnumType()) {\n                deser = _createEnumKeyDeserializer(ctxt, type);\n            } else {\n                deser = StdKeyDeserializers.findStringBasedKeyDeserializer(config, type);\n            }\n        }\n        // and then post-processing\n        if (deser != null) {\n            if (_factoryConfig.hasDeserializerModifiers()) {\n                for (BeanDeserializerModifier mod : _factoryConfig.deserializerModifiers()) {\n                    deser = mod.modifyKeyDeserializer(config, type, deser);\n                }\n            }\n        }\n        return deser;\n    }",
        "fixed_src_wo_comments": "@ Override public KeyDeserializer createKeyDeserializer ( DeserializationContext ctxt , JavaType type ) throws JsonMappingException { final DeserializationConfig config = ctxt . getConfig ( ) ; KeyDeserializer deser = null ; if ( _factoryConfig . hasKeyDeserializers ( ) ) { BeanDescription beanDesc = config . introspectClassAnnotations ( type . getRawClass ( ) ) ; for ( KeyDeserializers d : _factoryConfig . keyDeserializers ( ) ) { deser = d . findKeyDeserializer ( type , config , beanDesc ) ; if ( deser != null ) { break ; } } } if ( deser == null ) { if ( type . isEnumType ( ) ) { deser = _createEnumKeyDeserializer ( ctxt , type ) ; } else { deser = StdKeyDeserializers . findStringBasedKeyDeserializer ( config , type ) ; } } if ( deser != null ) { if ( _factoryConfig . hasDeserializerModifiers ( ) ) { for ( BeanDeserializerModifier mod : _factoryConfig . deserializerModifiers ( ) ) { deser = mod . modifyKeyDeserializer ( config , type , deser ) ; } } } return deser ; }",
        "summary": "Map key deserializerModifiers ignored",
        "Description": "We have a module that extends simple model to allow us to accept enum names in lower case in a fairly generic manner\r\nInside that we add the `modifyKeyDeserializer`\r\n\r\nThe incoming class (using immutables) is mapped to a guava immutable map.\r\nWalking through the code:\r\n\r\n> com.fasterxml.jackson.datatype.guava.deser.ImmutableMapDeserializer.createContextual\r\n>  calls DeserializationContext.findKeyDeserializer\r\n>  calls DeserializerCache.findKeyDeserializer\r\n>  calls BasicDeserializerFactory.createKeyDeserializer\r\n\r\nwhich has the code:\r\n```java\r\n        // the only non-standard thing is this:\r\n        if (deser == null) {\r\n            if (type.isEnumType()) {\r\n                return _createEnumKeyDeserializer(ctxt, type);\r\n            }\r\n            deser = StdKeyDeserializers.findStringBasedKeyDeserializer(config, type);\r\n        }\r\n```\r\n\r\nSince we are an enum type, it returns the value in the `_createEnumKeyDeserializer`, which is the standard enum deserializer.\r\nBelow that block is the check for the hasDeserializerModifiers, but since we have returned already, it is never called, so we can't override the behaviour.\r\n\r\nModule fragment:\r\n```java\r\n    setDeserializerModifier(new BeanDeserializerModifier() {\r\n                @Override\r\n                @SuppressWarnings(\"unchecked\")\r\n                public JsonDeserializer<Enum> modifyEnumDeserializer(\r\n                        DeserializationConfig config,\r\n                        final JavaType type,\r\n                        BeanDescription beanDesc,\r\n                        final JsonDeserializer<?> deserializer) {\r\n                    return new JsonDeserializer<Enum>() {\r\n                        @Override\r\n                        public Enum deserialize(JsonParser jp, DeserializationContext ctxt) throws IOException {\r\n                            Class<? extends Enum> rawClass = (Class<Enum<?>>) type.getRawClass();\r\n                            return Enum.valueOf(rawClass, jp.getValueAsString().toUpperCase());\r\n                        }\r\n                    };\r\n                }\r\n\r\n                @Override\r\n                public KeyDeserializer modifyKeyDeserializer(\r\n                        DeserializationConfig config,\r\n                        JavaType type,\r\n                        KeyDeserializer deserializer) {\r\n                    if (!type.isEnumType()) {\r\n                        return super.modifyKeyDeserializer(config, type, deserializer);\r\n                    }\r\n                    return new KeyDeserializer() {\r\n                        @Override\r\n                        @SuppressWarnings(\"unchecked\")\r\n                        public Object deserializeKey(String key, DeserializationContext ctxt)\r\n                                throws IOException, JsonProcessingException {\r\n                            Class<? extends Enum> rawClass = (Class<Enum<?>>) type.getRawClass();\r\n                            return Enum.valueOf(rawClass, key.toUpperCase());\r\n                        }\r\n                    };\r\n                }\r\n            });\r\n```\r\n\r\nI appreciate the code around here is fairly complex.\r\n\r\n\r\nRelated issues (possibly):\r\nhttps://github.com/FasterXML/jackson-databind/issues/749\r\nhttps://github.com/FasterXML/jackson-databind/issues/1313",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this. Couple of questions first:\n- Which jackson version is this (verified) with?\n- Does this also occur with standard JDK collections, or just with Guava ones?\n"
            },
            {
                "content": "Version is 2.8.4. I had also been running with 2.6.4, which had the same issue, and upgraded as part of my digging.\nI confirm this is also happening with standard JDK collections.\n"
            },
            {
                "content": "Thank you for verifying this. Being able to test with JDK collections makes it bit easier to repro with just databind.\n"
            },
            {
                "content": "Fast! Thank you very much.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug appears to be to test with JDK collections, which will make it easier to reproduce the issue with databind."
    },
    "JacksonDatabind_88_src/main/java/com/fasterxml/jackson/databind/jsontype/impl/ClassNameIdResolver.java_45_78": {
        "src": "protected JavaType _typeFromId(String id, DatabindContext ctxt) throws IOException\n    {\n        /* 30-Jan-2010, tatu: Most ids are basic class names; so let's first\n         *    check if any generics info is added; and only then ask factory\n         *    to do translation when necessary\n         */\n        TypeFactory tf = ctxt.getTypeFactory();\n        if (id.indexOf('<') > 0) {\n            // note: may want to try combining with specialization (esp for EnumMap)?\n            // 17-Aug-2017, tatu: As per [databind#1735] need to ensure assignment\n            //    compatibility -- needed later anyway, and not doing so may open\n            //    security issues.\n            JavaType t = tf.constructFromCanonical(id);\n                // Probably cleaner to have a method in `TypeFactory` but can't add in patch\n            return t;\n        }\n        Class<?> cls;\n        try {\n            cls =  tf.findClass(id);\n        } catch (ClassNotFoundException e) {\n            // 24-May-2016, tatu: Ok, this is pretty ugly, but we should always get\n            //   DeserializationContext, just playing it safe\n            if (ctxt instanceof DeserializationContext) {\n                DeserializationContext dctxt = (DeserializationContext) ctxt;\n                // First: we may have problem handlers that can deal with it?\n                return dctxt.handleUnknownTypeId(_baseType, id, this, \"no such class found\");\n            }\n            // ... meaning that we really should never get here.\n            return null;\n        } catch (Exception e) {\n            throw new IllegalArgumentException(\"Invalid type id '\"+id+\"' (for id type 'Id.class'): \"+e.getMessage(), e);\n        }\n        return tf.constructSpecializedType(_baseType, cls);\n    }",
        "src_wo_comments": "protected JavaType _typeFromId ( String id , DatabindContext ctxt ) throws IOException { TypeFactory tf = ctxt . getTypeFactory ( ) ; if ( id . indexOf ( '<' ) > 0 ) { JavaType t = tf . constructFromCanonical ( id ) ; return t ; } Class < ? > cls ; try { cls = tf . findClass ( id ) ; } catch ( ClassNotFoundException e ) { if ( ctxt instanceof DeserializationContext ) { DeserializationContext dctxt = ( DeserializationContext ) ctxt ; return dctxt . handleUnknownTypeId ( _baseType , id , this , \"no such class found\" ) ; } return null ; } catch ( Exception e ) { throw new IllegalArgumentException ( \"Invalid type id '\" + id + \"' (for id type 'Id.class'): \" + e . getMessage ( ) , e ) ; } return tf . constructSpecializedType ( _baseType , cls ) ; }",
        "fixed_src": "protected JavaType _typeFromId(String id, DatabindContext ctxt) throws IOException\n    {\n        /* 30-Jan-2010, tatu: Most ids are basic class names; so let's first\n         *    check if any generics info is added; and only then ask factory\n         *    to do translation when necessary\n         */\n        TypeFactory tf = ctxt.getTypeFactory();\n        if (id.indexOf('<') > 0) {\n            // note: may want to try combining with specialization (esp for EnumMap)?\n            // 17-Aug-2017, tatu: As per [databind#1735] need to ensure assignment\n            //    compatibility -- needed later anyway, and not doing so may open\n            //    security issues.\n            JavaType t = tf.constructFromCanonical(id);\n            if (!t.isTypeOrSubTypeOf(_baseType.getRawClass())) {\n                // Probably cleaner to have a method in `TypeFactory` but can't add in patch\n                throw new IllegalArgumentException(String.format(\n                        \"Class %s not subtype of %s\", t.getRawClass().getName(), _baseType));\n            }\n            return t;\n        }\n        Class<?> cls;\n        try {\n            cls =  tf.findClass(id);\n        } catch (ClassNotFoundException e) {\n            // 24-May-2016, tatu: Ok, this is pretty ugly, but we should always get\n            //   DeserializationContext, just playing it safe\n            if (ctxt instanceof DeserializationContext) {\n                DeserializationContext dctxt = (DeserializationContext) ctxt;\n                // First: we may have problem handlers that can deal with it?\n                return dctxt.handleUnknownTypeId(_baseType, id, this, \"no such class found\");\n            }\n            // ... meaning that we really should never get here.\n            return null;\n        } catch (Exception e) {\n            throw new IllegalArgumentException(\"Invalid type id '\"+id+\"' (for id type 'Id.class'): \"+e.getMessage(), e);\n        }\n        return tf.constructSpecializedType(_baseType, cls);\n    }",
        "fixed_src_wo_comments": "protected JavaType _typeFromId ( String id , DatabindContext ctxt ) throws IOException { TypeFactory tf = ctxt . getTypeFactory ( ) ; if ( id . indexOf ( '<' ) > 0 ) { JavaType t = tf . constructFromCanonical ( id ) ; if ( ! t . isTypeOrSubTypeOf ( _baseType . getRawClass ( ) ) ) { throw new IllegalArgumentException ( String . format ( \"Class %s not subtype of %s\" , t . getRawClass ( ) . getName ( ) , _baseType ) ) ; } return t ; } Class < ? > cls ; try { cls = tf . findClass ( id ) ; } catch ( ClassNotFoundException e ) { if ( ctxt instanceof DeserializationContext ) { DeserializationContext dctxt = ( DeserializationContext ) ctxt ; return dctxt . handleUnknownTypeId ( _baseType , id , this , \"no such class found\" ) ; } return null ; } catch ( Exception e ) { throw new IllegalArgumentException ( \"Invalid type id '\" + id + \"' (for id type 'Id.class'): \" + e . getMessage ( ) , e ) ; } return tf . constructSpecializedType ( _baseType , cls ) ; }",
        "summary": "Missing type checks when using polymorphic type ids",
        "Description": "(report by Lukes Euler)\r\n\r\n`JavaType` supports limited amount of generic typing for textual representation, originally just to support typing needed for `EnumMap` (I think). Based on some reports, it appears that some of type compatibility checks are not performed in those cases; if so, they should be made since there is potential for abuse.\r\nThe problem here although actual type assignment will fail later on, ability to trigger some of processing (instantiation of incompatible classes, perhaps assingnment of properties) may itself be vulnerability.\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Fixed in 2.8.10 / 2.9.1\r\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in versions 2.8.10 and 2.9.1."
    },
    "Math_43_src/main/java/org/apache/commons/math/stat/descriptive/SummaryStatistics.java_149_168": {
        "src": "public void addValue(double value) {\n        sumImpl.increment(value);\n        sumsqImpl.increment(value);\n        minImpl.increment(value);\n        maxImpl.increment(value);\n        sumLogImpl.increment(value);\n        secondMoment.increment(value);\n        // If mean, variance or geomean have been overridden,\n        // need to increment these\n        if (!(meanImpl instanceof Mean)) {\n            meanImpl.increment(value);\n        }\n        if (!(varianceImpl instanceof Variance)) {\n            varianceImpl.increment(value);\n        }\n        if (!(geoMeanImpl instanceof GeometricMean)) {\n            geoMeanImpl.increment(value);\n        }\n        n++;\n    }",
        "src_wo_comments": "public void addValue ( double value ) { sumImpl . increment ( value ) ; sumsqImpl . increment ( value ) ; minImpl . increment ( value ) ; maxImpl . increment ( value ) ; sumLogImpl . increment ( value ) ; secondMoment . increment ( value ) ; if ( ! ( meanImpl instanceof Mean ) ) { meanImpl . increment ( value ) ; } if ( ! ( varianceImpl instanceof Variance ) ) { varianceImpl . increment ( value ) ; } if ( ! ( geoMeanImpl instanceof GeometricMean ) ) { geoMeanImpl . increment ( value ) ; } n ++ ; }",
        "fixed_src": "public void addValue(double value) {\n        sumImpl.increment(value);\n        sumsqImpl.increment(value);\n        minImpl.increment(value);\n        maxImpl.increment(value);\n        sumLogImpl.increment(value);\n        secondMoment.increment(value);\n        // If mean, variance or geomean have been overridden,\n        // need to increment these\n        if (meanImpl != mean) {\n            meanImpl.increment(value);\n        }\n        if (varianceImpl != variance) {\n            varianceImpl.increment(value);\n        }\n        if (geoMeanImpl != geoMean) {\n            geoMeanImpl.increment(value);\n        }\n        n++;\n    }",
        "fixed_src_wo_comments": "public void addValue ( double value ) { sumImpl . increment ( value ) ; sumsqImpl . increment ( value ) ; minImpl . increment ( value ) ; maxImpl . increment ( value ) ; sumLogImpl . increment ( value ) ; secondMoment . increment ( value ) ; if ( meanImpl != mean ) { meanImpl . increment ( value ) ; } if ( varianceImpl != variance ) { varianceImpl . increment ( value ) ; } if ( geoMeanImpl != geoMean ) { geoMeanImpl . increment ( value ) ; } n ++ ; }",
        "summary": "Statistics.setVarianceImpl makes getStandardDeviation produce NaN",
        "Description": "Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:\n\n{code:java}\nint[] scores = {1, 2, 3, 4};\nSummaryStatistics stats = new SummaryStatistics();\nstats.setVarianceImpl(new Variance(false)); //use \"population variance\"\nfor(int i : scores) {\n  stats.addValue(i);\n}\ndouble sd = stats.getStandardDeviation();\nSystem.out.println(sd);\n{code}\n\nA workaround suggested by Mikkel is:\n{code:java}\n  double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());\n{code}",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-691",
        "comments": [
            "Thanks for reporting this.  Another workaround is to work with the default impl:\n{code}\nSummaryStatistics stats = new SummaryStatistics();\nVariance variance = (Variance) stats.getVarianceImpl();\nvariance.setBiasCorrected(false);\n{code}\nand then just use the stats instance directly. \n\nThe problem in the SummaryStatistics code is in addValue:\n{code}\n// If mean, variance or geomean have been overridden,\n// need to increment these\nif (!(meanImpl instanceof Mean)) {\n    meanImpl.increment(value);\n}\nif (!(varianceImpl instanceof Variance)) {\n    varianceImpl.increment(value);\n}\nif (!(geoMeanImpl instanceof GeometricMean)) {\n    geoMeanImpl.increment(value);\n}\n{code}\n\nThe default impls get incremented via their embedded moments, so the code above skips incrementing them.  If, however, they have been overridden by instances of the same class (as in this bug report), this causes a problem.\n",
            "Warren reported that the second workaround above does not work.  He is correct.  That is the result of yet another problem in this class.\n{code}\npublic double getVariance() {\n    if (varianceImpl == variance) {\n        return new Variance(secondMoment).getResult();\n    } else {\n        return varianceImpl.getResult();\n    }\n}\n{code}\n\nIn the first case, varianceImpl is not used, so setting its properties has no effect.  The mean has a similar problem.  The root cause of all of these problems is the reuse of momemts (i.e., just incrementing the moments instead of both them and the Mean and Variance instances).  We could either toss this (slight loss in performance, but likely trivial) or expose or allow moments to be attached to Mean, Variance instances.\n\n",
            "Fixed in r1206666."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to fix the code in r1206666, which involves changing the code in the SummaryStatistics class to not reuse moments, but instead to attach moments to Mean and Variance instances."
    },
    "JacksonDatabind_71_src/main/java/com/fasterxml/jackson/databind/deser/std/StdKeyDeserializer.java_70_116": {
        "src": "public static StdKeyDeserializer forType(Class<?> raw)\n    {\n        int kind;\n\n        // first common types:\n        if (raw == String.class || raw == Object.class) {\n            return StringKD.forType(raw);\n        } else if (raw == UUID.class) {\n            kind = TYPE_UUID;\n        } else if (raw == Integer.class) {\n            kind = TYPE_INT;\n        } else if (raw == Long.class) {\n            kind = TYPE_LONG;\n        } else if (raw == Date.class) {\n            kind = TYPE_DATE;\n        } else if (raw == Calendar.class) {\n            kind = TYPE_CALENDAR;\n        // then less common ones...\n        } else if (raw == Boolean.class) {\n            kind = TYPE_BOOLEAN;\n        } else if (raw == Byte.class) {\n            kind = TYPE_BYTE;\n        } else if (raw == Character.class) {\n            kind = TYPE_CHAR;\n        } else if (raw == Short.class) {\n            kind = TYPE_SHORT;\n        } else if (raw == Float.class) {\n            kind = TYPE_FLOAT;\n        } else if (raw == Double.class) {\n            kind = TYPE_DOUBLE;\n        } else if (raw == URI.class) {\n            kind = TYPE_URI;\n        } else if (raw == URL.class) {\n            kind = TYPE_URL;\n        } else if (raw == Class.class) {\n            kind = TYPE_CLASS;\n        } else if (raw == Locale.class) {\n            FromStringDeserializer<?> deser = FromStringDeserializer.findDeserializer(Locale.class);\n            return new StdKeyDeserializer(TYPE_LOCALE, raw, deser);\n        } else if (raw == Currency.class) {\n            FromStringDeserializer<?> deser = FromStringDeserializer.findDeserializer(Currency.class);\n            return new StdKeyDeserializer(TYPE_CURRENCY, raw, deser);\n        } else {\n            return null;\n        }\n        return new StdKeyDeserializer(kind, raw);\n    }",
        "src_wo_comments": "public static StdKeyDeserializer forType ( Class < ? > raw ) { int kind ; if ( raw == String . class || raw == Object . class ) { return StringKD . forType ( raw ) ; } else if ( raw == UUID . class ) { kind = TYPE_UUID ; } else if ( raw == Integer . class ) { kind = TYPE_INT ; } else if ( raw == Long . class ) { kind = TYPE_LONG ; } else if ( raw == Date . class ) { kind = TYPE_DATE ; } else if ( raw == Calendar . class ) { kind = TYPE_CALENDAR ; } else if ( raw == Boolean . class ) { kind = TYPE_BOOLEAN ; } else if ( raw == Byte . class ) { kind = TYPE_BYTE ; } else if ( raw == Character . class ) { kind = TYPE_CHAR ; } else if ( raw == Short . class ) { kind = TYPE_SHORT ; } else if ( raw == Float . class ) { kind = TYPE_FLOAT ; } else if ( raw == Double . class ) { kind = TYPE_DOUBLE ; } else if ( raw == URI . class ) { kind = TYPE_URI ; } else if ( raw == URL . class ) { kind = TYPE_URL ; } else if ( raw == Class . class ) { kind = TYPE_CLASS ; } else if ( raw == Locale . class ) { FromStringDeserializer < ? > deser = FromStringDeserializer . findDeserializer ( Locale . class ) ; return new StdKeyDeserializer ( TYPE_LOCALE , raw , deser ) ; } else if ( raw == Currency . class ) { FromStringDeserializer < ? > deser = FromStringDeserializer . findDeserializer ( Currency . class ) ; return new StdKeyDeserializer ( TYPE_CURRENCY , raw , deser ) ; } else { return null ; } return new StdKeyDeserializer ( kind , raw ) ; }",
        "fixed_src": "public static StdKeyDeserializer forType(Class<?> raw)\n    {\n        int kind;\n\n        // first common types:\n        if (raw == String.class || raw == Object.class || raw == CharSequence.class) {\n            return StringKD.forType(raw);\n        } else if (raw == UUID.class) {\n            kind = TYPE_UUID;\n        } else if (raw == Integer.class) {\n            kind = TYPE_INT;\n        } else if (raw == Long.class) {\n            kind = TYPE_LONG;\n        } else if (raw == Date.class) {\n            kind = TYPE_DATE;\n        } else if (raw == Calendar.class) {\n            kind = TYPE_CALENDAR;\n        // then less common ones...\n        } else if (raw == Boolean.class) {\n            kind = TYPE_BOOLEAN;\n        } else if (raw == Byte.class) {\n            kind = TYPE_BYTE;\n        } else if (raw == Character.class) {\n            kind = TYPE_CHAR;\n        } else if (raw == Short.class) {\n            kind = TYPE_SHORT;\n        } else if (raw == Float.class) {\n            kind = TYPE_FLOAT;\n        } else if (raw == Double.class) {\n            kind = TYPE_DOUBLE;\n        } else if (raw == URI.class) {\n            kind = TYPE_URI;\n        } else if (raw == URL.class) {\n            kind = TYPE_URL;\n        } else if (raw == Class.class) {\n            kind = TYPE_CLASS;\n        } else if (raw == Locale.class) {\n            FromStringDeserializer<?> deser = FromStringDeserializer.findDeserializer(Locale.class);\n            return new StdKeyDeserializer(TYPE_LOCALE, raw, deser);\n        } else if (raw == Currency.class) {\n            FromStringDeserializer<?> deser = FromStringDeserializer.findDeserializer(Currency.class);\n            return new StdKeyDeserializer(TYPE_CURRENCY, raw, deser);\n        } else {\n            return null;\n        }\n        return new StdKeyDeserializer(kind, raw);\n    }",
        "fixed_src_wo_comments": "public static StdKeyDeserializer forType ( Class < ? > raw ) { int kind ; if ( raw == String . class || raw == Object . class || raw == CharSequence . class ) { return StringKD . forType ( raw ) ; } else if ( raw == UUID . class ) { kind = TYPE_UUID ; } else if ( raw == Integer . class ) { kind = TYPE_INT ; } else if ( raw == Long . class ) { kind = TYPE_LONG ; } else if ( raw == Date . class ) { kind = TYPE_DATE ; } else if ( raw == Calendar . class ) { kind = TYPE_CALENDAR ; } else if ( raw == Boolean . class ) { kind = TYPE_BOOLEAN ; } else if ( raw == Byte . class ) { kind = TYPE_BYTE ; } else if ( raw == Character . class ) { kind = TYPE_CHAR ; } else if ( raw == Short . class ) { kind = TYPE_SHORT ; } else if ( raw == Float . class ) { kind = TYPE_FLOAT ; } else if ( raw == Double . class ) { kind = TYPE_DOUBLE ; } else if ( raw == URI . class ) { kind = TYPE_URI ; } else if ( raw == URL . class ) { kind = TYPE_URL ; } else if ( raw == Class . class ) { kind = TYPE_CLASS ; } else if ( raw == Locale . class ) { FromStringDeserializer < ? > deser = FromStringDeserializer . findDeserializer ( Locale . class ) ; return new StdKeyDeserializer ( TYPE_LOCALE , raw , deser ) ; } else if ( raw == Currency . class ) { FromStringDeserializer < ? > deser = FromStringDeserializer . findDeserializer ( Currency . class ) ; return new StdKeyDeserializer ( TYPE_CURRENCY , raw , deser ) ; } else { return null ; } return new StdKeyDeserializer ( kind , raw ) ; }",
        "summary": "Missing `KeyDeserializer` for `CharSequence`",
        "Description": "Looks like use of nominal Map key type of `CharSequence` does not work yet (as of 2.7.8 / 2.8.6).\r\nThis is something that is needed to work with certain frameworks, such as Avro's generated POJOs.\r\n\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Fixed for 2.7.9 / 2.8.7"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in versions 2.7.9 and 2.8.7."
    },
    "Cli_15_src/java/org/apache/commons/cli2/commandline/WriteableCommandLineImpl.java_111_130": {
        "src": "public List getValues(final Option option,\n                          List defaultValues) {\n        // initialize the return list\n        List valueList = (List) values.get(option);\n\n        // grab the correct default values\n        if ((valueList == null) || valueList.isEmpty()) {\n            valueList = defaultValues;\n        }\n\n        // augment the list with the default values\n        if ((valueList == null) || valueList.isEmpty()) {\n            valueList = (List) this.defaultValues.get(option);\n        }\n                // if there are more default values as specified, add them to\n                // the list.\n                    // copy the list first\n        \n        return valueList == null ? Collections.EMPTY_LIST : valueList;\n    }",
        "src_wo_comments": "public List getValues ( final Option option , List defaultValues ) { List valueList = ( List ) values . get ( option ) ; if ( ( valueList == null ) || valueList . isEmpty ( ) ) { valueList = defaultValues ; } if ( ( valueList == null ) || valueList . isEmpty ( ) ) { valueList = ( List ) this . defaultValues . get ( option ) ; } return valueList == null ? Collections . EMPTY_LIST : valueList ; }",
        "fixed_src": "public List getValues(final Option option,\n                          List defaultValues) {\n        // initialize the return list\n        List valueList = (List) values.get(option);\n\n        // grab the correct default values\n        if (defaultValues == null || defaultValues.isEmpty()) {\n            defaultValues = (List) this.defaultValues.get(option);\n        }\n\n        // augment the list with the default values\n        if (defaultValues != null && !defaultValues.isEmpty()) {\n            if (valueList == null || valueList.isEmpty()) {\n                valueList = defaultValues;\n            } else {\n                // if there are more default values as specified, add them to\n                // the list.\n                if (defaultValues.size() > valueList.size()) {\n                    // copy the list first\n                    valueList = new ArrayList(valueList);\n                    for (int i=valueList.size(); i<defaultValues.size(); i++) {\n                        valueList.add(defaultValues.get(i));\n                    }\n                }\n            }\n        }\n        \n        return valueList == null ? Collections.EMPTY_LIST : valueList;\n    }",
        "fixed_src_wo_comments": "public List getValues ( final Option option , List defaultValues ) { List valueList = ( List ) values . get ( option ) ; if ( defaultValues == null || defaultValues . isEmpty ( ) ) { defaultValues = ( List ) this . defaultValues . get ( option ) ; } if ( defaultValues != null && ! defaultValues . isEmpty ( ) ) { if ( valueList == null || valueList . isEmpty ( ) ) { valueList = defaultValues ; } else { if ( defaultValues . size ( ) > valueList . size ( ) ) { valueList = new ArrayList ( valueList ) ; for ( int i = valueList . size ( ) ; i < defaultValues . size ( ) ; i ++ ) { valueList . add ( defaultValues . get ( i ) ) ; } } } } return valueList == null ? Collections . EMPTY_LIST : valueList ; }",
        "summary": "deafult arguments only works if no arguments are submitted",
        "Description": "When using multple arguments and defaults, the behaviour is counter-intuitive and will only pick up a default if no args are passed in.\n\nFor instance in the code below I have set up so 0, 1, or 2 args may bve accepted, with defaults 100 and 1000.\n\nI expect it to behave as follows.\n1. for 2 args, 1 and 2 the values should be 1 and 2. This works as expected.\n2. for 0 args passed in the values should be 100 and 1000, picking up both of the defaults. This works as expected\n\n\n3. for 1 arg passed in the values should be 1 and 1000, so the second argument picks up the second default value. The valuse become just 1, which is not as expected..\n\n\nCurrently, in the second case will only return 1 and ignore the defaults.\n\n\n\n    public void testSingleOptionSingleArgument() throws Exception {\n        String defaulValue1 = \"100\";\n        String defaultValue2 = \"1000\";\n        final DefaultOptionBuilder obuilder = new DefaultOptionBuilder();\n        final ArgumentBuilder abuilder = new ArgumentBuilder();\n        final GroupBuilder gbuilder = new GroupBuilder();\n\n        DefaultOption bOption = obuilder.withShortName(\"b\")\n                .withLongName(\"b\")\n                .withArgument(abuilder.withName(\"b\")\n                        .withMinimum(0)\n                        .withMaximum(2)\n                        .withDefault(defaulValue1)\n                        .withDefault(defaultValue2)\n                        .create())\n                .create();\n\n        Group options = gbuilder\n                .withName(\"options\")\n                .withOption(bOption)\n                .create();\n\n        Parser parser = new Parser();\n        parser.setHelpTrigger(\"--help\");\n        parser.setGroup(options);\n        String enteredValue1 = \"1\";\n        String[] args = new String[]{\"-b\", enteredValue1};\n        CommandLine cl = parser.parse(args);\n        CommandLine cmd = cl;\n        assertNotNull(cmd);\n        List b = cmd.getValues(\"-b\");\n        assertEquals(\"[\" + enteredValue1 + \"]\", b + \"\");\n    }\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-158",
        "comments": [
            "What is the use case for a default value per argument, instead of a default list of values if no argument is specified ? The current behavior seems more intuitive to me.",
            "I couldn't find any written use cases, so I tried to intuit it from\nits actual behaviour, and I agree you have a point, it could probably\nbe either way.\n\nHowever, if the minimum number of arguments is anything other than\nzero there is no point in setting default arguments because there is\nno way they can be used. If you specify a minimum of 2 arguments with\n3 defaults, but only submit 1 argument, it will throw an\nOptionException at parse time. If you submit 2 arguments the defaults\nare ignored. [Please see the code below]\n\nOn another point, there does however seem to be a lack of use case\ntests. These tend to be very useful as sample code. I have written\nsome sample code and submitted it on\nhttp://wiki.apache.org/commons/CLI2, but maybe we should put them in\nthe code base.\n\nAnyone else have any opinions on this?\n\nRegards\n\n\n\n\n\n==========================\n    public void testSingleOptionMaximumNumberOfArgument() throws Exception {\n        String[] args = new String[]{\"-b\", \"1\", \"2\"};\n        final DefaultOptionBuilder obuilder = new DefaultOptionBuilder();\n        final GroupBuilder gbuilder = new GroupBuilder();\n\n        DefaultOption bOption = obuilder.withShortName(\"b\")\n                .withLongName(\"b\")\n                .withArgument(abuilder.withName(\"b\")\n                .withMinimum(2)\n                .withMaximum(4)\n                .withDefault(\"100\")\n                .withDefault(\"1000\")\n                .withDefault(\"10000\")\n                .create())\n                .create();\n\n        Group options = gbuilder\n                .withName(\"options\")\n                .withOption(bOption)\n                .create();\n\n        Parser parser = new Parser();\n        parser.setHelpTrigger(\"--help\");\n        parser.setGroup(options);\n        CommandLine cl = parser.parse(args); // <<<< OptionException\nthrown here.\n        CommandLine cmd = cl;\n        assertNotNull(cmd);\n        List b = cmd.getValues(\"-b\");\n        assertEquals(\"[1, 2]\", b + \"\");\n    }\n\n==========================\n\n\n\n\n\n\n\n",
            "patch fixes the default values behavior be adding the missing values to the list. \n(includes test case)",
            "Patch applied. Many thanks!"
        ],
        "summarized_discussion": "\n\nThe bug was related to the default values behavior when a minimum number of arguments is specified. The solution was to apply a patch that adds the missing values to the list, as well as adding a test case. The patch was applied and the bug was fixed."
    },
    "JxPath_8_src/java/org/apache/commons/jxpath/ri/compiler/CoreOperationRelationalExpression.java_56_78": {
        "src": "private boolean compute(Object left, Object right) {\n        left = reduce(left);\n        right = reduce(right);\n\n        if (left instanceof InitialContext) {\n            ((InitialContext) left).reset();\n        }\n        if (right instanceof InitialContext) {\n            ((InitialContext) right).reset();\n        }\n        if (left instanceof Iterator && right instanceof Iterator) {\n            return findMatch((Iterator) left, (Iterator) right);\n        }\n        if (left instanceof Iterator) {\n            return containsMatch((Iterator) left, right);\n        }\n        if (right instanceof Iterator) {\n            return containsMatch((Iterator) right, left);\n        }\n        double ld = InfoSetUtil.doubleValue(left);\n        double rd = InfoSetUtil.doubleValue(right);\n        return evaluateCompare(ld == rd ? 0 : ld < rd ? -1 : 1);\n    }",
        "src_wo_comments": "private boolean compute ( Object left , Object right ) { left = reduce ( left ) ; right = reduce ( right ) ; if ( left instanceof InitialContext ) { ( ( InitialContext ) left ) . reset ( ) ; } if ( right instanceof InitialContext ) { ( ( InitialContext ) right ) . reset ( ) ; } if ( left instanceof Iterator && right instanceof Iterator ) { return findMatch ( ( Iterator ) left , ( Iterator ) right ) ; } if ( left instanceof Iterator ) { return containsMatch ( ( Iterator ) left , right ) ; } if ( right instanceof Iterator ) { return containsMatch ( ( Iterator ) right , left ) ; } double ld = InfoSetUtil . doubleValue ( left ) ; double rd = InfoSetUtil . doubleValue ( right ) ; return evaluateCompare ( ld == rd ? 0 : ld < rd ? - 1 : 1 ) ; }",
        "fixed_src": "private boolean compute(Object left, Object right) {\n        left = reduce(left);\n        right = reduce(right);\n\n        if (left instanceof InitialContext) {\n            ((InitialContext) left).reset();\n        }\n        if (right instanceof InitialContext) {\n            ((InitialContext) right).reset();\n        }\n        if (left instanceof Iterator && right instanceof Iterator) {\n            return findMatch((Iterator) left, (Iterator) right);\n        }\n        if (left instanceof Iterator) {\n            return containsMatch((Iterator) left, right);\n        }\n        if (right instanceof Iterator) {\n            return containsMatch((Iterator) right, left);\n        }\n        double ld = InfoSetUtil.doubleValue(left);\n        if (Double.isNaN(ld)) {\n            return false;\n        }\n        double rd = InfoSetUtil.doubleValue(right);\n        if (Double.isNaN(rd)) {\n            return false;\n        }\n        return evaluateCompare(ld == rd ? 0 : ld < rd ? -1 : 1);\n    }",
        "fixed_src_wo_comments": "private boolean compute ( Object left , Object right ) { left = reduce ( left ) ; right = reduce ( right ) ; if ( left instanceof InitialContext ) { ( ( InitialContext ) left ) . reset ( ) ; } if ( right instanceof InitialContext ) { ( ( InitialContext ) right ) . reset ( ) ; } if ( left instanceof Iterator && right instanceof Iterator ) { return findMatch ( ( Iterator ) left , ( Iterator ) right ) ; } if ( left instanceof Iterator ) { return containsMatch ( ( Iterator ) left , right ) ; } if ( right instanceof Iterator ) { return containsMatch ( ( Iterator ) right , left ) ; } double ld = InfoSetUtil . doubleValue ( left ) ; if ( Double . isNaN ( ld ) ) { return false ; } double rd = InfoSetUtil . doubleValue ( right ) ; if ( Double . isNaN ( rd ) ) { return false ; } return evaluateCompare ( ld == rd ? 0 : ld < rd ? - 1 : 1 ) ; }",
        "summary": "Comparing with NaN is incorrect",
        "Description": "'NaN' > 'NaN' is true, but should be FALSE",
        "issue_url": "https://issues.apache.org/jira//browse/JXPATH-95",
        "comments": [
            "Whoops!  Never screwed around with NaN before, apparently... Thanks for the catch, fixed in trunk.  :)",
            "        assertXPathValue(context, \"$nan = 1\", Boolean.FALSE, Boolean.class);\n+        assertXPathValue(context, \"$nan != 1\", Boolean.TRUE, Boolean.class);\n\nNaN  != 1 => FALSE",
            "Whew!  You are keeping me on my toes, certainly!  Fixed part 2 in trunk."
        ],
        "summarized_discussion": "\n\nThe solution to the bug was to change the code so that the assertion that NaN != 1 returns TRUE instead of FALSE. This was fixed in the trunk."
    },
    "JacksonDatabind_51_src/main/java/com/fasterxml/jackson/databind/jsontype/impl/TypeDeserializerBase.java_140_191": {
        "src": "protected final JsonDeserializer<Object> _findDeserializer(DeserializationContext ctxt,\n            String typeId) throws IOException\n    {\n        JsonDeserializer<Object> deser = _deserializers.get(typeId);\n        if (deser == null) {\n            /* As per [Databind#305], need to provide contextual info. But for\n             * backwards compatibility, let's start by only supporting this\n             * for base class, not via interface. Later on we can add this\n             * to the interface, assuming deprecation at base class helps.\n             */\n            JavaType type = _idResolver.typeFromId(ctxt, typeId);\n            if (type == null) {\n                // As per [JACKSON-614], use the default impl if no type id available:\n                deser = _findDefaultImplDeserializer(ctxt);\n                if (deser == null) {\n                    // 10-May-2016, tatu: We may get some help...\n                    JavaType actual = _handleUnknownTypeId(ctxt, typeId, _idResolver, _baseType);\n                    if (actual == null) { // what should this be taken to mean?\n                        // TODO: try to figure out something better\n                        return null;\n                    }\n                    // ... would this actually work?\n                    deser = ctxt.findContextualValueDeserializer(actual, _property);\n                }\n            } else {\n                /* 16-Dec-2010, tatu: Since nominal type we get here has no (generic) type parameters,\n                 *   we actually now need to explicitly narrow from base type (which may have parameterization)\n                 *   using raw type.\n                 *\n                 *   One complication, though; can not change 'type class' (simple type to container); otherwise\n                 *   we may try to narrow a SimpleType (Object.class) into MapType (Map.class), losing actual\n                 *   type in process (getting SimpleType of Map.class which will not work as expected)\n                 */\n                if ((_baseType != null)\n                        && _baseType.getClass() == type.getClass()) {\n                    /* 09-Aug-2015, tatu: Not sure if the second part of the check makes sense;\n                     *   but it appears to check that JavaType impl class is the same which is\n                     *   important for some reason?\n                     *   Disabling the check will break 2 Enum-related tests.\n                     */\n                    // 19-Jun-2016, tatu: As per [databind#1270] we may actually get full\n                    //   generic type with custom type resolvers. If so, should try to retain them.\n                    //  Whether this is sufficient to avoid problems remains to be seen, but for\n                    //  now it should improve things.\n                        type = ctxt.getTypeFactory().constructSpecializedType(_baseType, type.getRawClass());\n                }\n                deser = ctxt.findContextualValueDeserializer(type, _property);\n            }\n            _deserializers.put(typeId, deser);\n        }\n        return deser;\n    }",
        "src_wo_comments": "protected final JsonDeserializer < Object > _findDeserializer ( DeserializationContext ctxt , String typeId ) throws IOException { JsonDeserializer < Object > deser = _deserializers . get ( typeId ) ; if ( deser == null ) { JavaType type = _idResolver . typeFromId ( ctxt , typeId ) ; if ( type == null ) { deser = _findDefaultImplDeserializer ( ctxt ) ; if ( deser == null ) { JavaType actual = _handleUnknownTypeId ( ctxt , typeId , _idResolver , _baseType ) ; if ( actual == null ) { return null ; } deser = ctxt . findContextualValueDeserializer ( actual , _property ) ; } } else { if ( ( _baseType != null ) && _baseType . getClass ( ) == type . getClass ( ) ) { type = ctxt . getTypeFactory ( ) . constructSpecializedType ( _baseType , type . getRawClass ( ) ) ; } deser = ctxt . findContextualValueDeserializer ( type , _property ) ; } _deserializers . put ( typeId , deser ) ; } return deser ; }",
        "fixed_src": "protected final JsonDeserializer<Object> _findDeserializer(DeserializationContext ctxt,\n            String typeId) throws IOException\n    {\n        JsonDeserializer<Object> deser = _deserializers.get(typeId);\n        if (deser == null) {\n            /* As per [Databind#305], need to provide contextual info. But for\n             * backwards compatibility, let's start by only supporting this\n             * for base class, not via interface. Later on we can add this\n             * to the interface, assuming deprecation at base class helps.\n             */\n            JavaType type = _idResolver.typeFromId(ctxt, typeId);\n            if (type == null) {\n                // As per [JACKSON-614], use the default impl if no type id available:\n                deser = _findDefaultImplDeserializer(ctxt);\n                if (deser == null) {\n                    // 10-May-2016, tatu: We may get some help...\n                    JavaType actual = _handleUnknownTypeId(ctxt, typeId, _idResolver, _baseType);\n                    if (actual == null) { // what should this be taken to mean?\n                        // TODO: try to figure out something better\n                        return null;\n                    }\n                    // ... would this actually work?\n                    deser = ctxt.findContextualValueDeserializer(actual, _property);\n                }\n            } else {\n                /* 16-Dec-2010, tatu: Since nominal type we get here has no (generic) type parameters,\n                 *   we actually now need to explicitly narrow from base type (which may have parameterization)\n                 *   using raw type.\n                 *\n                 *   One complication, though; can not change 'type class' (simple type to container); otherwise\n                 *   we may try to narrow a SimpleType (Object.class) into MapType (Map.class), losing actual\n                 *   type in process (getting SimpleType of Map.class which will not work as expected)\n                 */\n                if ((_baseType != null)\n                        && _baseType.getClass() == type.getClass()) {\n                    /* 09-Aug-2015, tatu: Not sure if the second part of the check makes sense;\n                     *   but it appears to check that JavaType impl class is the same which is\n                     *   important for some reason?\n                     *   Disabling the check will break 2 Enum-related tests.\n                     */\n                    // 19-Jun-2016, tatu: As per [databind#1270] we may actually get full\n                    //   generic type with custom type resolvers. If so, should try to retain them.\n                    //  Whether this is sufficient to avoid problems remains to be seen, but for\n                    //  now it should improve things.\n                    if (!type.hasGenericTypes()) {\n                        type = ctxt.getTypeFactory().constructSpecializedType(_baseType, type.getRawClass());\n                    }\n                }\n                deser = ctxt.findContextualValueDeserializer(type, _property);\n            }\n            _deserializers.put(typeId, deser);\n        }\n        return deser;\n    }",
        "fixed_src_wo_comments": "protected final JsonDeserializer < Object > _findDeserializer ( DeserializationContext ctxt , String typeId ) throws IOException { JsonDeserializer < Object > deser = _deserializers . get ( typeId ) ; if ( deser == null ) { JavaType type = _idResolver . typeFromId ( ctxt , typeId ) ; if ( type == null ) { deser = _findDefaultImplDeserializer ( ctxt ) ; if ( deser == null ) { JavaType actual = _handleUnknownTypeId ( ctxt , typeId , _idResolver , _baseType ) ; if ( actual == null ) { return null ; } deser = ctxt . findContextualValueDeserializer ( actual , _property ) ; } } else { if ( ( _baseType != null ) && _baseType . getClass ( ) == type . getClass ( ) ) { if ( ! type . hasGenericTypes ( ) ) { type = ctxt . getTypeFactory ( ) . constructSpecializedType ( _baseType , type . getRawClass ( ) ) ; } } deser = ctxt . findContextualValueDeserializer ( type , _property ) ; } _deserializers . put ( typeId , deser ) ; } return deser ; }",
        "summary": "Generic type returned from type id resolver seems to be ignored",
        "Description": "https://github.com/benson-basis/jackson-custom-mess-tc\n\nHere's the situation, with Jackson 2.7.4.\n\nI have a TypeIdResolver that returns a JavaType for a generic type. However, something seems to be forgetting/erasing the generic, as it is failing to use the generic type param to understand the type of a field in the class.\n\nAll the information is in the test case, so I'm not putting any code to read here in the issue.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Here's the sort-of-a-culprit in TypeDeserializerBase. Even though I've returned a carefully parameterized type, this code is reducing it to the non-parameterized type!\n\n```\nelse {\n                /* 16-Dec-2010, tatu: Since nominal type we get here has no (generic) type parameters,\n                 *   we actually now need to explicitly narrow from base type (which may have parameterization)\n                 *   using raw type.\n                 *\n                 *   One complication, though; can not change 'type class' (simple type to container); otherwise\n                 *   we may try to narrow a SimpleType (Object.class) into MapType (Map.class), losing actual\n                 *   type in process (getting SimpleType of Map.class which will not work as expected)\n                 */\n                if ((_baseType != null)\n                        && _baseType.getClass() == type.getClass()) {\n                    /* 09-Aug-2015, tatu: Not sure if the second part of the check makes sense;\n                     *   but it appears to check that JavaType impl class is the same which is\n                     *   important for some reason?\n                     *   Disabling the check will break 2 Enum-related tests.\n                     */\n                    type = ctxt.getTypeFactory().constructSpecializedType(_baseType, type.getRawClass());\n                }\n                deser = ctxt.findContextualValueDeserializer(type, _property);\n```\n"
            },
            {
                "content": "Here's a workaround which is certainly not a fix:\n\n```\n/******************************************************************************\n ** This data and information is proprietary to, and a valuable trade secret\n ** of, Basis Technology Corp.  It is given in confidence by Basis Technology\n ** and may only be used as permitted under the license agreement under which\n ** it has been distributed, and in no other way.\n **\n ** Copyright (c) 2015 Basis Technology Corporation All rights reserved.\n **\n ** The technical data and information provided herein are provided with\n ** `limited rights', and the computer software provided herein is provided\n ** with `restricted rights' as those terms are defined in DAR and ASPR\n ** 7-104.9(a).\n ******************************************************************************/\n\npackage tc;\n\nimport com.fasterxml.jackson.annotation.JsonTypeInfo;\nimport com.fasterxml.jackson.databind.DeserializationConfig;\nimport com.fasterxml.jackson.databind.JavaType;\nimport com.fasterxml.jackson.databind.jsontype.NamedType;\nimport com.fasterxml.jackson.databind.jsontype.TypeDeserializer;\nimport com.fasterxml.jackson.databind.jsontype.TypeIdResolver;\nimport com.fasterxml.jackson.databind.jsontype.impl.AsArrayTypeDeserializer;\nimport com.fasterxml.jackson.databind.jsontype.impl.AsExternalTypeDeserializer;\nimport com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer;\nimport com.fasterxml.jackson.databind.jsontype.impl.AsWrapperTypeDeserializer;\nimport com.fasterxml.jackson.databind.jsontype.impl.StdTypeResolverBuilder;\n\nimport java.util.Collection;\n\n/**\n *\n */\npublic class ResolverBuilder extends StdTypeResolverBuilder {\n    @Override\n    public TypeDeserializer buildTypeDeserializer(DeserializationConfig config,\n                                                  JavaType baseType, Collection<NamedType> subtypes)\n    {\n        if (_idType == JsonTypeInfo.Id.NONE) { return null; }\n\n        TypeIdResolver idRes = idResolver(config, baseType, subtypes, false, true);\n\n        // First, method for converting type info to type id:\n        switch (_includeAs) {\n        case WRAPPER_ARRAY:\n            return new AsArrayTypeDeserializer(baseType, idRes,\n                    _typeProperty, _typeIdVisible, _defaultImpl);\n        case PROPERTY:\n        case EXISTING_PROPERTY: // as per [#528] same class as PROPERTY\n            return new AsPropertyTypeDeserializer(baseType, idRes,\n                    _typeProperty, _typeIdVisible, _defaultImpl, _includeAs);\n        case WRAPPER_OBJECT:\n            return new AsWrapperTypeDeserializer(baseType, idRes,\n                    _typeProperty, _typeIdVisible, _defaultImpl);\n        case EXTERNAL_PROPERTY:\n            // null out the base type, this might work around the problem.\n            return new AsExternalTypeDeserializer(null, idRes,\n                    _typeProperty, _typeIdVisible, _defaultImpl);\n        }\n        throw new IllegalStateException(\"Do not know how to construct standard type serializer for inclusion type: \"+_includeAs);\n    }\n}\n\n```\n"
            },
            {
                "content": "Would it be possible to check this with 2.8.0.rc2? There were small leftover issues in type specialization code that could not be fully resolved in 2.7, so handling is bit different (not a lot, just a bit) with 2.8.\n"
            },
            {
                "content": "Sure. Either of us could change the version in the repo :-). One sec.\n\nOn Fri, Jun 17, 2016 at 4:13 PM, Tatu Saloranta notifications@github.com\nwrote:\n\n> Would it be possible to check this with 2.8.0.rc2? There were small\n> leftover issues in type specialization code that could not be fully\n> resolved in 2.7, so handling is bit different (not a lot, just a bit) with\n> 2.8.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/FasterXML/jackson-databind/issues/1270#issuecomment-226870382,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ADM9zy6COF7RMTcxcYChiddhyEoJT3Dwks5qMv__gaJpZM4I4hgy\n> .\n"
            },
            {
                "content": "Except that I have to see where JsonTypeIdResolver went to in 2.8.0.\n\nOn Fri, Jun 17, 2016 at 4:41 PM, Benson Margulies benson@basistech.com\nwrote:\n\n> Sure. Either of us could change the version in the repo :-). One sec.\n> \n> On Fri, Jun 17, 2016 at 4:13 PM, Tatu Saloranta notifications@github.com\n> wrote:\n> \n> > Would it be possible to check this with 2.8.0.rc2? There were small\n> > leftover issues in type specialization code that could not be fully\n> > resolved in 2.7, so handling is bit different (not a lot, just a bit) with\n> > 2.8.\n> > \n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/FasterXML/jackson-databind/issues/1270#issuecomment-226870382,\n> > or mute the thread\n> > https://github.com/notifications/unsubscribe/ADM9zy6COF7RMTcxcYChiddhyEoJT3Dwks5qMv__gaJpZM4I4hgy\n> > .\n"
            },
            {
                "content": "Trying to look into this: was able to reproduce failure with test project, with current `master` (2.8.0-SNAPSHOT).\n"
            },
            {
                "content": "@benson-basis I am guessing that this does indeed get into the nasty party of type specialization code, as generics are not part of \"well-known\" types (`Map`s, `Collection`s). Use of \"raw\" type declaration in `Base` could be related; but changing it to `Base<?>` does not solve the problem, although it does change the exception.\n\nDid you find that use of `EXTERNAL_PROPERTY` is required for the fail, or does it just happen to be part of failing case?\n"
            },
            {
                "content": "I didn't try it with non-EXTERNAL.\n\nIn the code I posted above, the change I made was to stop passing a base to the AsExernalTypeResolver, but you'd think the same principle would apply to the other cases. It uses the base to go out of its way to stomp on the generic.\n\nThe comment in AsExternalTypeResolver says something about enums failing if it doesn't stomp.\n\n``` case EXTERNAL_PROPERTY:\n            // null out the base type, this might work around the problem.\n            return new AsExternalTypeDeserializer(null, idRes,\n                    _typeProperty, _typeIdVisible, _defaultImpl);\n\n```\n```\n"
            },
            {
                "content": "I think this would probably apply to other types too. And come to think of it, I am thinking that generics probably will not work, for the simple reason that type ids are for the class and not for generic variations -- generic variations can not be detected by runtime thanks to Java type erasure. At least not without custom handlers, that is, default handling has no benefit or need to do so.\n\nStill, failure mode I see is odd, so I am not yet ready to claim this is working as expected.\n"
            },
            {
                "content": "Ok, I think I did find the specific culprit, within `TypeDeserializer` call to `TypeFactory.constructSpecializedType()`. Assumption is made that since annotation-based approach can only provide type-erased \"raw\" type, this is passed. In your case type resolver can produce fully generic type, and thus dropping that will drop appropriate parameterization.\n\nI'll have to think of how to properly address this, balancing specific fix (simple enough, would solve test as-is) with hope to handle more generic case. I am thinking that perhaps avoiding specialization call in case given type has type parameters would be a reasonable match; I'll see how that works.\n"
            },
            {
                "content": "Ok, yes. A simple fix works for this case, so I'll go with that. I have slight nagging suspicion that there might be a theoretical problem with some combination of things (namely, non-custom annotation where subtype is generic), but since all unit tests pass I think I'll take my chances for now, and handle complications if they arise.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to avoid the call to TypeFactory.constructSpecializedType() in cases where the type has type parameters. This will prevent the type from being reduced to the non-parameterized type."
    },
    "JxPath_12_src/java/org/apache/commons/jxpath/ri/model/dom/DOMNodePointer.java_87_136": {
        "src": "public static boolean testNode(Node node, NodeTest test) {\n        if (test == null) {\n            return true;\n        }\n        if (test instanceof NodeNameTest) {\n            if (node.getNodeType() != Node.ELEMENT_NODE) {\n                return false;\n            }\n\n            NodeNameTest nodeNameTest = (NodeNameTest) test;\n            QName testName = nodeNameTest.getNodeName();\n            String namespaceURI = nodeNameTest.getNamespaceURI();\n            boolean wildcard = nodeNameTest.isWildcard();\n            String testPrefix = testName.getPrefix();\n            if (wildcard && testPrefix == null) {\n                return true;\n            }\n            if (wildcard\n                || testName.getName()\n                        .equals(DOMNodePointer.getLocalName(node))) {\n                String nodeNS = DOMNodePointer.getNamespaceURI(node);\n                return equalStrings(namespaceURI, nodeNS);\n            }\n            return false;\n        }\n        if (test instanceof NodeTypeTest) {\n            int nodeType = node.getNodeType();\n            switch (((NodeTypeTest) test).getNodeType()) {\n                case Compiler.NODE_TYPE_NODE :\n                    return nodeType == Node.ELEMENT_NODE\n                            || nodeType == Node.DOCUMENT_NODE;\n                case Compiler.NODE_TYPE_TEXT :\n                    return nodeType == Node.CDATA_SECTION_NODE\n                        || nodeType == Node.TEXT_NODE;\n                case Compiler.NODE_TYPE_COMMENT :\n                    return nodeType == Node.COMMENT_NODE;\n                case Compiler.NODE_TYPE_PI :\n                    return nodeType == Node.PROCESSING_INSTRUCTION_NODE;\n            }\n            return false;\n        }\n        if (test instanceof ProcessingInstructionTest) {\n            if (node.getNodeType() == Node.PROCESSING_INSTRUCTION_NODE) {\n                String testPI = ((ProcessingInstructionTest) test).getTarget();\n                String nodePI = ((ProcessingInstruction) node).getTarget();\n                return testPI.equals(nodePI);\n            }\n        }\n        return false;\n    }",
        "src_wo_comments": "public static boolean testNode ( Node node , NodeTest test ) { if ( test == null ) { return true ; } if ( test instanceof NodeNameTest ) { if ( node . getNodeType ( ) != Node . ELEMENT_NODE ) { return false ; } NodeNameTest nodeNameTest = ( NodeNameTest ) test ; QName testName = nodeNameTest . getNodeName ( ) ; String namespaceURI = nodeNameTest . getNamespaceURI ( ) ; boolean wildcard = nodeNameTest . isWildcard ( ) ; String testPrefix = testName . getPrefix ( ) ; if ( wildcard && testPrefix == null ) { return true ; } if ( wildcard || testName . getName ( ) . equals ( DOMNodePointer . getLocalName ( node ) ) ) { String nodeNS = DOMNodePointer . getNamespaceURI ( node ) ; return equalStrings ( namespaceURI , nodeNS ) ; } return false ; } if ( test instanceof NodeTypeTest ) { int nodeType = node . getNodeType ( ) ; switch ( ( ( NodeTypeTest ) test ) . getNodeType ( ) ) { case Compiler . NODE_TYPE_NODE : return nodeType == Node . ELEMENT_NODE || nodeType == Node . DOCUMENT_NODE ; case Compiler . NODE_TYPE_TEXT : return nodeType == Node . CDATA_SECTION_NODE || nodeType == Node . TEXT_NODE ; case Compiler . NODE_TYPE_COMMENT : return nodeType == Node . COMMENT_NODE ; case Compiler . NODE_TYPE_PI : return nodeType == Node . PROCESSING_INSTRUCTION_NODE ; } return false ; } if ( test instanceof ProcessingInstructionTest ) { if ( node . getNodeType ( ) == Node . PROCESSING_INSTRUCTION_NODE ) { String testPI = ( ( ProcessingInstructionTest ) test ) . getTarget ( ) ; String nodePI = ( ( ProcessingInstruction ) node ) . getTarget ( ) ; return testPI . equals ( nodePI ) ; } } return false ; }",
        "fixed_src": "public static boolean testNode(Node node, NodeTest test) {\n        if (test == null) {\n            return true;\n        }\n        if (test instanceof NodeNameTest) {\n            if (node.getNodeType() != Node.ELEMENT_NODE) {\n                return false;\n            }\n\n            NodeNameTest nodeNameTest = (NodeNameTest) test;\n            QName testName = nodeNameTest.getNodeName();\n            String namespaceURI = nodeNameTest.getNamespaceURI();\n            boolean wildcard = nodeNameTest.isWildcard();\n            String testPrefix = testName.getPrefix();\n            if (wildcard && testPrefix == null) {\n                return true;\n            }\n            if (wildcard\n                || testName.getName()\n                        .equals(DOMNodePointer.getLocalName(node))) {\n                String nodeNS = DOMNodePointer.getNamespaceURI(node);\n                return equalStrings(namespaceURI, nodeNS) || nodeNS == null\n                        && equalStrings(testPrefix, getPrefix(node));\n            }\n            return false;\n        }\n        if (test instanceof NodeTypeTest) {\n            int nodeType = node.getNodeType();\n            switch (((NodeTypeTest) test).getNodeType()) {\n                case Compiler.NODE_TYPE_NODE :\n                    return nodeType == Node.ELEMENT_NODE\n                            || nodeType == Node.DOCUMENT_NODE;\n                case Compiler.NODE_TYPE_TEXT :\n                    return nodeType == Node.CDATA_SECTION_NODE\n                        || nodeType == Node.TEXT_NODE;\n                case Compiler.NODE_TYPE_COMMENT :\n                    return nodeType == Node.COMMENT_NODE;\n                case Compiler.NODE_TYPE_PI :\n                    return nodeType == Node.PROCESSING_INSTRUCTION_NODE;\n            }\n            return false;\n        }\n        if (test instanceof ProcessingInstructionTest) {\n            if (node.getNodeType() == Node.PROCESSING_INSTRUCTION_NODE) {\n                String testPI = ((ProcessingInstructionTest) test).getTarget();\n                String nodePI = ((ProcessingInstruction) node).getTarget();\n                return testPI.equals(nodePI);\n            }\n        }\n        return false;\n    }",
        "fixed_src_wo_comments": "public static boolean testNode ( Node node , NodeTest test ) { if ( test == null ) { return true ; } if ( test instanceof NodeNameTest ) { if ( node . getNodeType ( ) != Node . ELEMENT_NODE ) { return false ; } NodeNameTest nodeNameTest = ( NodeNameTest ) test ; QName testName = nodeNameTest . getNodeName ( ) ; String namespaceURI = nodeNameTest . getNamespaceURI ( ) ; boolean wildcard = nodeNameTest . isWildcard ( ) ; String testPrefix = testName . getPrefix ( ) ; if ( wildcard && testPrefix == null ) { return true ; } if ( wildcard || testName . getName ( ) . equals ( DOMNodePointer . getLocalName ( node ) ) ) { String nodeNS = DOMNodePointer . getNamespaceURI ( node ) ; return equalStrings ( namespaceURI , nodeNS ) || nodeNS == null && equalStrings ( testPrefix , getPrefix ( node ) ) ; } return false ; } if ( test instanceof NodeTypeTest ) { int nodeType = node . getNodeType ( ) ; switch ( ( ( NodeTypeTest ) test ) . getNodeType ( ) ) { case Compiler . NODE_TYPE_NODE : return nodeType == Node . ELEMENT_NODE || nodeType == Node . DOCUMENT_NODE ; case Compiler . NODE_TYPE_TEXT : return nodeType == Node . CDATA_SECTION_NODE || nodeType == Node . TEXT_NODE ; case Compiler . NODE_TYPE_COMMENT : return nodeType == Node . COMMENT_NODE ; case Compiler . NODE_TYPE_PI : return nodeType == Node . PROCESSING_INSTRUCTION_NODE ; } return false ; } if ( test instanceof ProcessingInstructionTest ) { if ( node . getNodeType ( ) == Node . PROCESSING_INSTRUCTION_NODE ) { String testPI = ( ( ProcessingInstructionTest ) test ) . getTarget ( ) ; String nodePI = ( ( ProcessingInstruction ) node ) . getTarget ( ) ; return testPI . equals ( nodePI ) ; } } return false ; }",
        "summary": "Incomplete handling of undefined namespaces",
        "Description": "Mcduffey, Joe <jdmcduf@nsa.gov>\n\nCan someone tell me how to register namespaces so that attributes with namespaces does not cause the exception\n\norg.apache.common.ri.model.dom.DOMNodePointer.createAttribute\nunknown namespace prefix: xsi\n\nFor example the following\n<ElementA  A:myAttr=\"Mytype\">\n  <B:ElementB>MY VALUE</B:ElementB>\n</ElementA>\n\nWould result in the following exception:\norg.apache.common.ri.model.dom.DOMNodePointer.createAttribute\nunknown namespace prefix: A\n\nFYI: In this example there was a namespace decaration in the file and I also manually called the\nregisterNamespace(A,\"/http...\");\nregisterNamespace(B,\"/http...\");\n\nThere was no problem encountered for elements. Only attributes. Can someone help? Thanks.",
        "issue_url": "https://issues.apache.org/jira//browse/JXPATH-97",
        "comments": [
            "This should work in trunk.  Thanks all!",
            "reopening... elements still don't work apparently for prefixes that are _only_ declared externally?",
            "Matt,\n\nsorry for the bad patch. But one of the main point is change in DOMNodePointer:\n\n@@ -102,9 +107,12 @@\n                 return true;\n             }\n \n-            if (wildcard\n-                || testName.getName()\n-                        .equals(DOMNodePointer.getLocalName(node))) {\n+            // the same as for attribute ( DOMAttributeIterator::testAttr() )\n+            if (equalStrings(nodePrefix, testPrefix)) {\n+                return true;\n+            }\n+\n+            if (wildcard || testName.getName().equals(nodeLocalName)) {\n                 String nodeNS = DOMNodePointer.getNamespaceURI(node);\n                 return equalStrings(namespaceURI, nodeNS);\n             }\n",
            "elements handled.",
            "reopening to note that Joe McD also reported that createPathAndSetValue() wasn't working for ext-reg'd ns'd attrs.",
            "fixed createPathAndSetValue() in trunk."
        ],
        "summarized_discussion": "\n\nThe solution to the bug was to change the code in DOMNodePointer to check for equal strings between the node prefix and the test prefix, and then to check for wildcard or testName.getName().equals(nodeLocalName) and then to check for equal strings between the namespaceURI and the nodeNS. This fixed the createPathAndSetValue() issue and elements were handled."
    },
    "Math_63_src/main/java/org/apache/commons/math/util/MathUtils.java_416_418": {
        "src": "public static boolean equals(double x, double y) {\n        return (Double.isNaN(x) && Double.isNaN(y)) || x == y;\n    }",
        "src_wo_comments": "public static boolean equals ( double x , double y ) { return ( Double . isNaN ( x ) && Double . isNaN ( y ) ) || x == y ; }",
        "fixed_src": "public static boolean equals(double x, double y) {\n        return equals(x, y, 1);\n    }",
        "fixed_src_wo_comments": "public static boolean equals ( double x , double y ) { return equals ( x , y , 1 ) ; }",
        "summary": "NaN in \"equals\" methods",
        "Description": "In \"MathUtils\", some \"equals\" methods will return true if both argument are NaN.\nUnless I'm mistaken, this contradicts the IEEE standard.\n\nIf nobody objects, I'm going to make the changes.\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-370",
        "comments": [
            "I really think this is intentional and the fact that NaN != Nan was known to the people who did that.\nFor example, the javadoc for Complex.equals ([http://commons.apache.org/math/apidocs/org/apache/commons/math/complex/Complex.html#equals%28java.lang.Object%29]) explicitly warns about behavior for NaN.\nSo on the one one changing this would break compatibility with respect to a documented behavior and on the other hand it would bring compatibility with IEEE arithmetic.\nI'm sure there is a good reason for the current status, but I don't know it.",
            "The Javadocs for \"Float\" and \"Double\" also indicate the special behaviour for NaN in a number object.  Nevertheless, the _primitive_ types are compliant with IEEE. Hence, there is an inconsistency between the \"equals\" methods in \"MathUtils\" and the behaviour of the JVM.\n",
            "-1 for the change.  It will break some internal things and likely some user applications that depend on the documented behavior.  The reason that these methods exist is to be able to handle NaNs and treat them as equal.",
            "But _why_?\nBy this I mean, how to explain to someone that CM does not comply with IEEE?\n\nAlso, please point me to the internal things likely to break, so that I have a chance to understand the usefulness of the behaviour.\n",
            "The method Mathutils.equals(double, double) was implemented for the reason that in some cases, users may wish to treat doubles with NaN values as equal.  The internal classes that use it are the equals methods for statistics and statistical aggregates.  When these classes were defined, we decided that we would treat two statistics or statistical aggregates as equal iff they had the same values, including NaNs.  So for example, two StatisticalSummaryValues instances are equal iff they report the same values for all statistics, with NaNs treated as \"the same value.\"  The MathUtils.equals(double, double) method was introduced as a convenience method to do this kind of comparison.  I guess we could (deprecate and) change the name if it makes it look like all of our computations are defining equals that way, which is certainly not the case.  In general, we do try to follow IEEE754.  We agreed early on that we would carefully document how NaNs are treated throughout Commons Math.  We have slipped a little over the years, but in the case of this method and the classes that use it, the documentation is clear.\n\nSorry I misunderstood the point of the issue here.  I now get your point that naming the method \"equals\" makes it look like we are re-defining equals for primitive doubles, which is not our intent.  So maybe we deprecate the equals method and rename something like \"equalsIncludingNaN\" or something simpler?",
            "Well, the first impression I got from looking at those methods in \"MathUtils\" was indeed that they provide the \"right\" way to compare primitive doubles. Now, the right way for most users dealing with numerical codes should arguably be IEEE754-compliant.  So, unless we can explain that the way CM handles NaN is somehow more useful than what the IEEE standard defines, CM should at least provide \"equals\" utility methods that stick to the standard.\nI understand the usefulness of the current implementation in the examples which you gave, but as a matter of principle this usage is second in priority (from a user perspective): it is mainly internal to CM.\nMaybe, we should create a \"util.internal\" package where (CM) developer-utilities methods would live, whereas \"util\" would contain the functions which users are most likely looking for.  I think that it would be clearer (e.g. it would avoid dealing with long names such as \"equalsIncludingNaN\").  What do you think?\n",
            "I use the equals method and the array version in some of my  code that uses [math] (pretty much the same basic use case as above) and I suspect it is possible that other users do as well, so I would prefer to keep a renamed version of it available publicly, if consensus is we need to rename it.  The fact that it is defined at all should be a flag to users that there is something different going on, at least in the <double, double> case.  Note that the <double, double, double> and <double, double, int> versions are also \"different\" from the JDK; but there is no possibility for confusion of intent for them.",
            "I did not say that there should be private methods, unavailable to users of CM. Just that different packages would make it clear that some methods intend to implement common/standard functions while others are more special-purpose or could deviate from standard definitions.\n",
            "OK, maybe it is not necessary to introduce a new package.\nThen, let's implement the proposed name change (\"equalsIncludingNaN\").\n\n> Note that the <double, double, double> and <double, double, int> versions are also \"different\" from the JDK;\n> but there is no possibility for confusion of intent for them.\n\nI do not agree. The point is there will be a confusion because the JDK treats \"Double\" and \"double\" _differently_ wrt NaN whereas those \"equals\" methods handle NaN _similarly_ to what happens with \"Double\".\n\nIn my (maybe naive) opinion, all this NaN business is not really important because the equality test itself doesn't make sense when a NaN is passed as arguments (meaning something went wrong previously).\nHowever, there is this IEEE754 _standard_ saying that NaN is not equal to NaN. Hence, to avoid confusion, the \"equals\" methods should comply with it.  And, for all of them, we can add the special-purpose variant in the longer-named ones.\n\nAgreed?\n[If so, I'm willing to make the changes and hunt for all the occurrences of \"equals\" with the current semantics and replace them with \"equalsIncludingNaN\".]\n",
            "I am fine with deprecating and changing the name of MathUtils.equals(double, double) to \"equalsIncludingNaN\" or \"equalsN\".   I see no reason to change the names or modify the behavior of any of the other equals methods in MathUtils.",
            "> I see no reason to change the names or modify the behavior of any of the other equals methods in MathUtils. \n\nMaybe because the other \"equals\" methods also consider that NaN == NaN which contradicts IEEE !\n\nWhere people need to treat all NaN values as equal, they will use one of the \"equalsIncludingNaN\", otherwise they will use one of the \"equals\" methods (and get an IEEE754-compliant result wrt to NaN values).\n",
            "Which methods exactly are you talking about?  ",
            "{code}\npublic static boolean equals(double x, double y) {\n    return (Double.isNaN(x) && Double.isNaN(y)) || x == y;\n}\n{code}\n\n{code}\npublic static boolean equals(double x, double y, double eps) {\n  return equals(x, y) || (Math.abs(y - x) <= eps);\n}\n{code}\n\n{code}\npublic static boolean equals(double[] x, double[] y) {\n    if ((x == null) || (y == null)) {\n        return !((x == null) ^ (y == null));\n    }\n    if (x.length != y.length) {\n        return false;\n    }\n    for (int i = 0; i < x.length; ++i) {\n        if (!equals(x[i], y[i])) {\n            return false;\n        }\n    }\n    return true;\n}\n{code}\n\n{code}\npublic static boolean equals(double x, double y, int maxUlps) {\n    // Check that \"maxUlps\" is non-negative and small enough so that the\n    // default NAN won't compare as equal to anything.\n    assert maxUlps > 0 && maxUlps < NAN_GAP;\n\n    long xInt = Double.doubleToLongBits(x);\n    long yInt = Double.doubleToLongBits(y);\n\n    // Make lexicographically ordered as a two's-complement integer.\n    if (xInt < 0) {\n        xInt = SGN_MASK - xInt;\n    }\n    if (yInt < 0) {\n        yInt = SGN_MASK - yInt;\n    }\n\n    return Math.abs(xInt - yInt) <= maxUlps;\n}\n{code}\n\nThe first assumes that NaN == NaN; the next two use the first; the last one also returns {{true}} when both arguments are NaN.\n\nI propose that these \"equals\" will return {{false}} when one or the other argument is NaN (or contains a NaN for the array variant), and to create for each one an \"equalsIncludingNaN\" variant that will behave as the current code of \"equals\".\nAll occurrences of \"equals\" currently in CM will be replaced by \"equalsIncludingNaN\" so that no semantics change will happen.\nCM will stay consistent and be compliant.",
            "Actually, I think that the first of the above \"equals\" should be replaced by:\n\n{code}\npublic static boolean equals(double x, double y) {\n    return equals(x, y, 1);\n}\n{code}\n\nI.e. calling the fourth form (when it is replaced by the IEEE-compliant version).\n",
            "Looking carefully at the uses inside [math] and reviewing my own uses, I agree now that the original definitions of equals(double, double) and equals(double, double, double) were incorrect, or I guess I should say \"unfortunate.\"  Regarding equals(double, double), I don't see why we need that at all - just deprecate and replace by \"equalsN\" or somesuch or remove it altogether in 3.0. The uses of equals(double, double, double)  in the linear package should NOT identify NaNs, so these would benefit from changing behavior.  The uses in the stats package of the array version are OK, I think - i.e., I think it is a legitimate definition of equals for a stats reporting object to identify stats that report NaNs in the same places.\n\nSo the question is what to do.  I don't think we can change the contracts or delete methods in point releases, so the reasonable thing to do is to \n\n1) deprecate equals(double, double) \n2) Make a note in the javadoc for equals(double[], double[]) and equals(double, double, double) indicating that in version 3.0 these methods will handle NaNs differently.\n3) Fix equals(double, double, int).  This method does not use equals(double, double) and does not specify how NaNs are handled in its javadoc, so it can be fixed.",
            "> [...] Regarding equals(double, double), I don't see why we need that at all [...]\n\nAs I noted in the previous comment, this version of an equality test seems to make sense, from a numerical perspective, if it's implemented by using the \"equals(double,double,int)\" method (introduced according to [MATH-264|https://issues.apache.org/jira/browse/MATH-264]) where the third argument is set to 1): It means that there is no floating point number in the range of real numbers defined by the first and second arguments.\n\nPractically, I've now modified all the \"equals\" methods (so that NaN != NaN) and for each one, I've also implemented an \"equalsIncludingNaN\" version (for which NaN == NaN).\nIn the \"stat\" package, wherever \"equals\" was used, I replaced it with \"equalsIncludingNaN\".\nI modified the junit tests (\"mutatis mutandis\") and they all pass.\n\nI don't think it's worth waiting v3.0 to introduce those changes, for the following reasons:\n# The change is non-breaking wrt usage of the code in package \"stat\", or any other code that was using \"equals\".\n# No functionality is removed (i.e. the \"equalsIncludingNaN\" provide the same behaviour as previously provided by the corresponding \"equals\" methods).\n# For a direct user of \"equals\", the change is, in principle, breaking but only in the single case where he relies on NaNs being equal. How likely is it that expecting such a behaviour is *not* wrong (i.e. comparing the results of computations that produced meaningless values)?\n\nTo further alleviate the (marginal) risk of the last point, couldn't we make a poll on the \"user\" ML, so that people can make a case if they really need the time to a major version change in order to adapt to this modification?\n",
            "Regarding equals(double, double), it looks like what you are proposing is yet another version of equals that is not exactly what is defined in the spec.  Do I have that right?  In that case, like the current version, it should have a different name.  Or does the spec imply that equals should behave this way and the JDK not quite deliver it?\n\nI sympathize with the desire to change the definitions of the other methods now; but while we did not quite live up to this in 2.1, .x versions are supposed to be drop-in replacements, so we should not be changing behavior of methods unless they are not meeting their documented contracts and in this case, what we are seeing as \"bugged\" are the contracts, so I think we need to deprecate  equals(double, double) and fix the others in 3.0, with the exception of equals(double, double, int), which does not specify NaN behavior.  We can introduce the new \"equalsN\" versions now and note the changed behavior in the javadoc for the \"equals\" versions.  We can also introduce the new version of equals(double, double) that you are describing with a new name.",
            "Changes partially implemented in r952949. [Hopefully not breaking compatibility.]\nMethods \"equalsIncludingNaN\" have been added and used internally (in the classes of the \"stat.descriptive\" package) instead of the now deprecated \"equals\" methods. Those should change semantics in release 3.0.\n",
            "Revision 991535.\n",
            "Closing issue as it was included in version 2.2, which has been released"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to deprecate the method Mathutils.equals(double, double) and rename it to \"equalsIncludingNaN\" or \"equalsN\". Additionally, the method Mathutils.equals(double, double, int) should be fixed to return false when one or the other argument is NaN. The other \"equals\" methods should also be changed to not identify NaNs. These changes were partially implemented in r952949 and fully implemented in revision 991535."
    },
    "Cli_35_src/main/java/org/apache/commons/cli/Options.java_233_250": {
        "src": "public List<String> getMatchingOptions(String opt)\n    {\n        opt = Util.stripLeadingHyphens(opt);\n        \n        List<String> matchingOpts = new ArrayList<String>();\n\n        // for a perfect match return the single option only\n\n        for (String longOpt : longOpts.keySet())\n        {\n            if (longOpt.startsWith(opt))\n            {\n                matchingOpts.add(longOpt);\n            }\n        }\n        \n        return matchingOpts;\n    }",
        "src_wo_comments": "public List < String > getMatchingOptions ( String opt ) { opt = Util . stripLeadingHyphens ( opt ) ; List < String > matchingOpts = new ArrayList < String > ( ) ; for ( String longOpt : longOpts . keySet ( ) ) { if ( longOpt . startsWith ( opt ) ) { matchingOpts . add ( longOpt ) ; } } return matchingOpts ; }",
        "fixed_src": "public List<String> getMatchingOptions(String opt)\n    {\n        opt = Util.stripLeadingHyphens(opt);\n        \n        List<String> matchingOpts = new ArrayList<String>();\n\n        // for a perfect match return the single option only\n        if(longOpts.keySet().contains(opt)) {\n            return Collections.singletonList(opt);\n        }\n\n        for (String longOpt : longOpts.keySet())\n        {\n            if (longOpt.startsWith(opt))\n            {\n                matchingOpts.add(longOpt);\n            }\n        }\n        \n        return matchingOpts;\n    }",
        "fixed_src_wo_comments": "public List < String > getMatchingOptions ( String opt ) { opt = Util . stripLeadingHyphens ( opt ) ; List < String > matchingOpts = new ArrayList < String > ( ) ; if ( longOpts . keySet ( ) . contains ( opt ) ) { return Collections . singletonList ( opt ) ; } for ( String longOpt : longOpts . keySet ( ) ) { if ( longOpt . startsWith ( opt ) ) { matchingOpts . add ( longOpt ) ; } } return matchingOpts ; }",
        "summary": "LongOpt falsely detected as ambiguous",
        "Description": "Options options = new Options();\noptions.addOption(Option.builder().longOpt(\"importToOpen\").hasArg().argName(\"FILE\").build());\noptions.addOption(Option.builder(\"i\").longOpt(\"import\").hasArg().argName(\"FILE\").build());\n\nParsing \"--import=FILE\" is not possible since 1.3 as it throws a AmbiguousOptionException stating that it cannot decide whether import is import or importToOpen. In 1.2 this is not an issue. \n\nThe root lies in the new DefaultParser which does a startsWith check internally. \n",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-252",
        "comments": [
            "Thank you for the report. Patches welcome!",
            "GitHub user simonharrer opened a pull request:\n\n    https://github.com/apache/commons-cli/pull/2\n\n    Fixes CLI-252 exact matches are not ambiquous\n\n    https://issues.apache.org/jira/browse/CLI-252\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/simonharrer/commons-cli fix-CLI-252\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/commons-cli/pull/2.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #2\n    \n----\n\n----\n",
            "Patch available in https://github.com/apache/commons-cli/pull/2",
            "{code}\n$ svn ci -m \"CLI-252: LongOpt falsely detected as ambiguous. This closes #2 from github. Thanks to Simon Harrer.\"\nSending        src/changes/changes.xml\nSending        src/main/java/org/apache/commons/cli/Options.java\nAdding         src/test/java/org/apache/commons/cli/bug/BugCLI252Test.java\nTransmitting file data ...\nCommitted revision 1684315.\n{code}\n\nThank you!",
            "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/commons-cli/pull/2\n"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is a patch available in https://github.com/apache/commons-cli/pull/2, which was opened by GitHub user simonharrer and closed by asfgit. The patch was applied by running the command \"$ svn ci -m \"CLI-252: LongOpt falsely detected as ambiguous. This closes #2 from github. Thanks to Simon Harrer.\"."
    },
    "Math_34_src/main/java/org/apache/commons/math3/genetics/ListPopulation.java_208_210": {
        "src": "public Iterator<Chromosome> iterator() {\n        return chromosomes.iterator();\n    }",
        "src_wo_comments": "public Iterator < Chromosome > iterator ( ) { return chromosomes . iterator ( ) ; }",
        "fixed_src": "public Iterator<Chromosome> iterator() {\n        return getChromosomes().iterator();\n    }",
        "fixed_src_wo_comments": "public Iterator < Chromosome > iterator ( ) { return getChromosomes ( ) . iterator ( ) ; }",
        "summary": "ListPopulation Iterator allows you to remove chromosomes from the population.",
        "Description": "Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-779",
        "comments": [
            "Proposed patch.",
            "Fixed in r1325427 with minor modifications (updated javadoc, use getChromosomes() instead of Collections.unmodifiableList)."
        ],
        "summarized_discussion": "\n\nThe bug was fixed in r1325427 with minor modifications to the proposed patch, such as updating the javadoc and using getChromosomes() instead of Collections.unmodifiableList."
    },
    "Cli_23_src/java/org/apache/commons/cli/HelpFormatter.java_805_841": {
        "src": "protected StringBuffer renderWrappedText(StringBuffer sb, int width, \n                                             int nextLineTabStop, String text)\n    {\n        int pos = findWrapPos(text, width, 0);\n\n        if (pos == -1)\n        {\n            sb.append(rtrim(text));\n\n            return sb;\n        }\n        sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n\n        // all following lines must be padded with nextLineTabStop space \n        // characters\n        final String padding = createPadding(nextLineTabStop);\n\n        while (true)\n        {\n            int lastPos = pos;\n            text = padding + text.substring(pos).trim();\n            pos = findWrapPos(text, width, 0);\n\n            if (pos == -1)\n            {\n                sb.append(text);\n\n                return sb;\n            } else\n            if (pos == lastPos)\n            {\n                throw new RuntimeException(\"Text too long for line - throwing exception to avoid infinite loop [CLI-162]: \" + text);\n            }\n\n            sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n        }\n    }",
        "src_wo_comments": "protected StringBuffer renderWrappedText ( StringBuffer sb , int width , int nextLineTabStop , String text ) { int pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( rtrim ( text ) ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; final String padding = createPadding ( nextLineTabStop ) ; while ( true ) { int lastPos = pos ; text = padding + text . substring ( pos ) . trim ( ) ; pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( text ) ; return sb ; } else if ( pos == lastPos ) { throw new RuntimeException ( \"Text too long for line - throwing exception to avoid infinite loop [CLI-162]: \" + text ) ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; } }",
        "fixed_src": "protected StringBuffer renderWrappedText(StringBuffer sb, int width, \n                                             int nextLineTabStop, String text)\n    {\n        int pos = findWrapPos(text, width, 0);\n\n        if (pos == -1)\n        {\n            sb.append(rtrim(text));\n\n            return sb;\n        }\n        sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n\n        // all following lines must be padded with nextLineTabStop space \n        // characters\n        final String padding = createPadding(nextLineTabStop);\n\n        while (true)\n        {\n            text = padding + text.substring(pos).trim();\n            pos = findWrapPos(text, width, 0);\n\n            if (pos == -1)\n            {\n                sb.append(text);\n\n                return sb;\n            }\n            \n            if ( (text.length() > width) && (pos == nextLineTabStop - 1) ) {\n                sb.append(text);\n\n                return sb;\n            }\n\n            sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n        }\n    }",
        "fixed_src_wo_comments": "protected StringBuffer renderWrappedText ( StringBuffer sb , int width , int nextLineTabStop , String text ) { int pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( rtrim ( text ) ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; final String padding = createPadding ( nextLineTabStop ) ; while ( true ) { text = padding + text . substring ( pos ) . trim ( ) ; pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( text ) ; return sb ; } if ( ( text . length ( ) > width ) && ( pos == nextLineTabStop - 1 ) ) { sb . append ( text ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; } }",
        "summary": "infinite loop in the wrapping code of HelpFormatter",
        "Description": "If there is not enough space to display a word on a single line, HelpFormatter goes into a infinite loops until the JVM crashes with an OutOfMemoryError.\n\nTest case:\n\n{code}\nOptions options = new Options();\noptions.addOption(\"h\", \"help\", false, \"This is a looooong description\");\n\nHelpFormatter formatter = new HelpFormatter();\nformatter.setWidth(20);\nformatter.printHelp(\"app\", options); // hang & crash\n{code}\n\nAn helpful exception indicating the insufficient width would be more appropriate than an OutOfMemoryError.",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-162",
        "comments": [
            "svn ci -m \"Changing the current OutOfMemoryError to a RuntimeException per CLI-162. A new ticket for the RuntimeException is at CLI-174\" \n\nSending        src/java/org/apache/commons/cli/HelpFormatter.java\nAdding         src/test/org/apache/commons/cli/bug/BugCLI162Test.java\nTransmitting file data ..\nCommitted revision 735257.",
            "Committed new test in BugCLI162Test. The class BugCLI162Test is not invoked during a Maven build BTW.",
            "\nTwo of the options appear to be problematic in CLI162.\n\nThe first is OPT_PASSWORD. In this the url is longer than the allowed width of the screen, so some kind of failure needs to happen - or the url needs to be mercilessly chopped. This is the one that goes into an infinite loop due to CLI-151. My gut feeling from the code is that CLI-151 didn't introduce this bug, but you had to be significantly longer to hit the bug before (ie: printTabStop longer than the width).\n\nThe second is OPT_PARAM_TYPES_INT + OPT_PARAM_TYPES_NAME, it shows the the patch for this ticket contained a bug when the lastPos happened to equal the firstPos for completely normal reasons. I'd missed that pos was already set to a real value when the loop was begun and not to 0. I'm not sure why both options have to be set for this to happen. ",
            "Attaching patch that rolls back the previous RuntimeException throwing. The if statement in that patch was testing the wrong condition. This patch adds the correct condition, and rather than throwing an exception the text is simply outputted irregardless of the fact it is over the width. What should happen is debatable here - due to a side-effect of CLI-151, we can't do anything aggressive here because things were printing out happily if they were under width + printTabStop. Our options are either to just print, or to forcibly break the text. \n\nThe test code no longer expects to get a RuntimeException.",
            "svn ci -m \"Applying my second attempt at a patch to CLI-162. This fixes Gary's reported bug (one of which was an example of CLI-162, and one a bug in my first attempt to patch). Open question is whether to output text that is too long, or try and break it up to fit the screen width. \"\n\nSending        src/java/org/apache/commons/cli/HelpFormatter.java\nSending        src/test/org/apache/commons/cli/bug/BugCLI162Test.java\nTransmitting file data ..\nCommitted revision 745388.",
            "Ended up modifying this to break it up to fit screen width.\n\nDiscovered another infinite loop issue if the entered width is smaller than the nextTabStop (ie: argument + indent). ",
            "Throw IllegalStateException on last infinite loop test case.\n\n\nsvn ci -m \"Applying additional patch to throw IllegalStateException when the speci\nfied width is not enough to fit the flags, indent and 1 character for the description. This closes out CLI-162 (for now :) ). \"\nSending        src/java/org/apache/commons/cli/HelpFormatter.java\nSending        src/test/org/apache/commons/cli/bug/BugCLI162Test.java\nTransmitting file data ..\nCommitted revision 746137."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to modify the code to break up the text to fit the screen width, and to throw an IllegalStateException when the specified width is not enough to fit the flags, indent, and 1 character for the description. This was done by committing new test in BugCLI162Test and applying a patch that rolls back the previous RuntimeException throwing, as well as applying an additional patch to throw the IllegalStateException."
    },
    "Cli_5_src/java/org/apache/commons/cli/Util.java_34_46": {
        "src": "static String stripLeadingHyphens(String str)\n    {\n        if (str.startsWith(\"--\"))\n        {\n            return str.substring(2, str.length());\n        }\n        else if (str.startsWith(\"-\"))\n        {\n            return str.substring(1, str.length());\n        }\n\n        return str;\n    }",
        "src_wo_comments": "static String stripLeadingHyphens ( String str ) { if ( str . startsWith ( \"--\" ) ) { return str . substring ( 2 , str . length ( ) ) ; } else if ( str . startsWith ( \"-\" ) ) { return str . substring ( 1 , str . length ( ) ) ; } return str ; }",
        "fixed_src": "static String stripLeadingHyphens(String str)\n    {\n        if (str == null) {\n            return null;\n        }\n        if (str.startsWith(\"--\"))\n        {\n            return str.substring(2, str.length());\n        }\n        else if (str.startsWith(\"-\"))\n        {\n            return str.substring(1, str.length());\n        }\n\n        return str;\n    }",
        "fixed_src_wo_comments": "static String stripLeadingHyphens ( String str ) { if ( str == null ) { return null ; } if ( str . startsWith ( \"--\" ) ) { return str . substring ( 2 , str . length ( ) ) ; } else if ( str . startsWith ( \"-\" ) ) { return str . substring ( 1 , str . length ( ) ) ; } return str ; }",
        "summary": "NullPointerException in Util.stripLeadingHyphens when passed a null argument",
        "Description": "If you try to do a hasOption(null), you get a NPE:\n\njava.lang.NullPointerException\n\tat org.apache.commons.cli.Util.stripLeadingHyphens(Util.java:39)\n\tat org.apache.commons.cli.CommandLine.resolveOption(CommandLine.java:166)\n\tat org.apache.commons.cli.CommandLine.hasOption(CommandLine.java:68)\n\nEither hasOption should reject the null argument, or the function should simply return false.  I think the latter makes more since, as this is how Java collections generally work.",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-133",
        "comments": [
            "Proposed fix and test cases.",
            "Proposed fix and test cases.",
            "Patch applied."
        ],
        "summarized_discussion": "\n\nThe proposed fix and test cases were implemented and the patch was successfully applied."
    },
    "JacksonDatabind_47_src/main/java/com/fasterxml/jackson/databind/AnnotationIntrospector.java_795_896": {
        "src": "public JavaType refineSerializationType(final MapperConfig<?> config,\n            final Annotated a, final JavaType baseType) throws JsonMappingException\n    {\n        JavaType type = baseType;\n        final TypeFactory tf = config.getTypeFactory();\n        \n        // 10-Oct-2015, tatu: For 2.7, we'll need to delegate back to\n        //    now-deprecated secondary methods; this because while\n        //    direct sub-class not yet retrofitted may only override\n        //    those methods. With 2.8 or later we may consider removal\n        //    of these methods\n\n        \n        // Ok: start by refining the main type itself; common to all types\n        Class<?> serClass = findSerializationType(a);\n        if (serClass != null) {\n            if (type.hasRawClass(serClass)) {\n                // 30-Nov-2015, tatu: As per [databind#1023], need to allow forcing of\n                //    static typing this way\n                type = type.withStaticTyping();\n            } else {\n                try {\n                    // 11-Oct-2015, tatu: For deser, we call `TypeFactory.constructSpecializedType()`,\n                    //   may be needed here too in future?\n                        type = tf.constructGeneralizedType(type, serClass);\n                } catch (IllegalArgumentException iae) {\n                    throw new JsonMappingException(null,\n                            String.format(\"Failed to widen type %s with annotation (value %s), from '%s': %s\",\n                                    type, serClass.getName(), a.getName(), iae.getMessage()),\n                                    iae);\n                }\n            }\n        }\n        // Then further processing for container types\n\n        // First, key type (for Maps, Map-like types):\n        if (type.isMapLikeType()) {\n            JavaType keyType = type.getKeyType();\n            Class<?> keyClass = findSerializationKeyType(a, keyType);\n            if (keyClass != null) {\n                if (keyType.hasRawClass(keyClass)) {\n                    keyType = keyType.withStaticTyping();\n                } else {\n                    Class<?> currRaw = keyType.getRawClass();\n                    try {\n                        // 19-May-2016, tatu: As per [databind#1231], [databind#1178] may need to actually\n                        //   specialize (narrow) type sometimes, even if more commonly opposite\n                        //   is needed.\n                        if (keyClass.isAssignableFrom(currRaw)) { // common case\n                            keyType = tf.constructGeneralizedType(keyType, keyClass);\n                        } else if (currRaw.isAssignableFrom(keyClass)) { // specialization, ok as well\n                            keyType = tf.constructSpecializedType(keyType, keyClass);\n                        } else {\n                            throw new JsonMappingException(null,\n                                    String.format(\"Can not refine serialization key type %s into %s; types not related\",\n                                            keyType, keyClass.getName()));\n                        }\n                    } catch (IllegalArgumentException iae) {\n                        throw new JsonMappingException(null,\n                                String.format(\"Failed to widen key type of %s with concrete-type annotation (value %s), from '%s': %s\",\n                                        type, keyClass.getName(), a.getName(), iae.getMessage()),\n                                        iae);\n                    }\n                }\n                type = ((MapLikeType) type).withKeyType(keyType);\n            }\n        }\n\n        JavaType contentType = type.getContentType();\n        if (contentType != null) { // collection[like], map[like], array, reference\n            // And then value types for all containers:\n           Class<?> contentClass = findSerializationContentType(a, contentType);\n           if (contentClass != null) {\n               if (contentType.hasRawClass(contentClass)) {\n                   contentType = contentType.withStaticTyping();\n               } else {\n                   // 03-Apr-2016, tatu: As per [databind#1178], may need to actually\n                   //   specialize (narrow) type sometimes, even if more commonly opposite\n                   //   is needed.\n                   Class<?> currRaw = contentType.getRawClass();\n                   try {\n                       if (contentClass.isAssignableFrom(currRaw)) { // common case\n                           contentType = tf.constructGeneralizedType(contentType, contentClass);\n                       } else if (currRaw.isAssignableFrom(contentClass)) { // specialization, ok as well\n                           contentType = tf.constructSpecializedType(contentType, contentClass);\n                       } else {\n                           throw new JsonMappingException(null,\n                                   String.format(\"Can not refine serialization content type %s into %s; types not related\",\n                                           contentType, contentClass.getName()));\n                       }\n                   } catch (IllegalArgumentException iae) { // shouldn't really happen\n                       throw new JsonMappingException(null,\n                               String.format(\"Internal error: failed to refine value type of %s with concrete-type annotation (value %s), from '%s': %s\",\n                                       type, contentClass.getName(), a.getName(), iae.getMessage()),\n                                       iae);\n                   }\n               }\n               type = type.withContentType(contentType);\n           }\n        }\n        return type;\n    }",
        "src_wo_comments": "public JavaType refineSerializationType ( final MapperConfig < ? > config , final Annotated a , final JavaType baseType ) throws JsonMappingException { JavaType type = baseType ; final TypeFactory tf = config . getTypeFactory ( ) ; Class < ? > serClass = findSerializationType ( a ) ; if ( serClass != null ) { if ( type . hasRawClass ( serClass ) ) { type = type . withStaticTyping ( ) ; } else { try { type = tf . constructGeneralizedType ( type , serClass ) ; } catch ( IllegalArgumentException iae ) { throw new JsonMappingException ( null , String . format ( \"Failed to widen type %s with annotation (value %s), from '%s': %s\" , type , serClass . getName ( ) , a . getName ( ) , iae . getMessage ( ) ) , iae ) ; } } } if ( type . isMapLikeType ( ) ) { JavaType keyType = type . getKeyType ( ) ; Class < ? > keyClass = findSerializationKeyType ( a , keyType ) ; if ( keyClass != null ) { if ( keyType . hasRawClass ( keyClass ) ) { keyType = keyType . withStaticTyping ( ) ; } else { Class < ? > currRaw = keyType . getRawClass ( ) ; try { if ( keyClass . isAssignableFrom ( currRaw ) ) { keyType = tf . constructGeneralizedType ( keyType , keyClass ) ; } else if ( currRaw . isAssignableFrom ( keyClass ) ) { keyType = tf . constructSpecializedType ( keyType , keyClass ) ; } else { throw new JsonMappingException ( null , String . format ( \"Can not refine serialization key type %s into %s; types not related\" , keyType , keyClass . getName ( ) ) ) ; } } catch ( IllegalArgumentException iae ) { throw new JsonMappingException ( null , String . format ( \"Failed to widen key type of %s with concrete-type annotation (value %s), from '%s': %s\" , type , keyClass . getName ( ) , a . getName ( ) , iae . getMessage ( ) ) , iae ) ; } } type = ( ( MapLikeType ) type ) . withKeyType ( keyType ) ; } } JavaType contentType = type . getContentType ( ) ; if ( contentType != null ) { Class < ? > contentClass = findSerializationContentType ( a , contentType ) ; if ( contentClass != null ) { if ( contentType . hasRawClass ( contentClass ) ) { contentType = contentType . withStaticTyping ( ) ; } else { Class < ? > currRaw = contentType . getRawClass ( ) ; try { if ( contentClass . isAssignableFrom ( currRaw ) ) { contentType = tf . constructGeneralizedType ( contentType , contentClass ) ; } else if ( currRaw . isAssignableFrom ( contentClass ) ) { contentType = tf . constructSpecializedType ( contentType , contentClass ) ; } else { throw new JsonMappingException ( null , String . format ( \"Can not refine serialization content type %s into %s; types not related\" , contentType , contentClass . getName ( ) ) ) ; } } catch ( IllegalArgumentException iae ) { throw new JsonMappingException ( null , String . format ( \"Internal error: failed to refine value type of %s with concrete-type annotation (value %s), from '%s': %s\" , type , contentClass . getName ( ) , a . getName ( ) , iae . getMessage ( ) ) , iae ) ; } } type = type . withContentType ( contentType ) ; } } return type ; }",
        "fixed_src": "public JavaType refineSerializationType(final MapperConfig<?> config,\n            final Annotated a, final JavaType baseType) throws JsonMappingException\n    {\n        JavaType type = baseType;\n        final TypeFactory tf = config.getTypeFactory();\n        \n        // 10-Oct-2015, tatu: For 2.7, we'll need to delegate back to\n        //    now-deprecated secondary methods; this because while\n        //    direct sub-class not yet retrofitted may only override\n        //    those methods. With 2.8 or later we may consider removal\n        //    of these methods\n\n        \n        // Ok: start by refining the main type itself; common to all types\n        Class<?> serClass = findSerializationType(a);\n        if (serClass != null) {\n            if (type.hasRawClass(serClass)) {\n                // 30-Nov-2015, tatu: As per [databind#1023], need to allow forcing of\n                //    static typing this way\n                type = type.withStaticTyping();\n            } else {\n                Class<?> currRaw = type.getRawClass();\n                try {\n                    // 11-Oct-2015, tatu: For deser, we call `TypeFactory.constructSpecializedType()`,\n                    //   may be needed here too in future?\n                    if (serClass.isAssignableFrom(currRaw)) { // common case\n                        type = tf.constructGeneralizedType(type, serClass);\n                    } else if (currRaw.isAssignableFrom(serClass)) { // specialization, ok as well\n                        type = tf.constructSpecializedType(type, serClass);\n                    } else {\n                        throw new JsonMappingException(null,\n                                String.format(\"Can not refine serialization type %s into %s; types not related\",\n                                        type, serClass.getName()));\n                    }\n                } catch (IllegalArgumentException iae) {\n                    throw new JsonMappingException(null,\n                            String.format(\"Failed to widen type %s with annotation (value %s), from '%s': %s\",\n                                    type, serClass.getName(), a.getName(), iae.getMessage()),\n                                    iae);\n                }\n            }\n        }\n        // Then further processing for container types\n\n        // First, key type (for Maps, Map-like types):\n        if (type.isMapLikeType()) {\n            JavaType keyType = type.getKeyType();\n            Class<?> keyClass = findSerializationKeyType(a, keyType);\n            if (keyClass != null) {\n                if (keyType.hasRawClass(keyClass)) {\n                    keyType = keyType.withStaticTyping();\n                } else {\n                    Class<?> currRaw = keyType.getRawClass();\n                    try {\n                        // 19-May-2016, tatu: As per [databind#1231], [databind#1178] may need to actually\n                        //   specialize (narrow) type sometimes, even if more commonly opposite\n                        //   is needed.\n                        if (keyClass.isAssignableFrom(currRaw)) { // common case\n                            keyType = tf.constructGeneralizedType(keyType, keyClass);\n                        } else if (currRaw.isAssignableFrom(keyClass)) { // specialization, ok as well\n                            keyType = tf.constructSpecializedType(keyType, keyClass);\n                        } else {\n                            throw new JsonMappingException(null,\n                                    String.format(\"Can not refine serialization key type %s into %s; types not related\",\n                                            keyType, keyClass.getName()));\n                        }\n                    } catch (IllegalArgumentException iae) {\n                        throw new JsonMappingException(null,\n                                String.format(\"Failed to widen key type of %s with concrete-type annotation (value %s), from '%s': %s\",\n                                        type, keyClass.getName(), a.getName(), iae.getMessage()),\n                                        iae);\n                    }\n                }\n                type = ((MapLikeType) type).withKeyType(keyType);\n            }\n        }\n\n        JavaType contentType = type.getContentType();\n        if (contentType != null) { // collection[like], map[like], array, reference\n            // And then value types for all containers:\n           Class<?> contentClass = findSerializationContentType(a, contentType);\n           if (contentClass != null) {\n               if (contentType.hasRawClass(contentClass)) {\n                   contentType = contentType.withStaticTyping();\n               } else {\n                   // 03-Apr-2016, tatu: As per [databind#1178], may need to actually\n                   //   specialize (narrow) type sometimes, even if more commonly opposite\n                   //   is needed.\n                   Class<?> currRaw = contentType.getRawClass();\n                   try {\n                       if (contentClass.isAssignableFrom(currRaw)) { // common case\n                           contentType = tf.constructGeneralizedType(contentType, contentClass);\n                       } else if (currRaw.isAssignableFrom(contentClass)) { // specialization, ok as well\n                           contentType = tf.constructSpecializedType(contentType, contentClass);\n                       } else {\n                           throw new JsonMappingException(null,\n                                   String.format(\"Can not refine serialization content type %s into %s; types not related\",\n                                           contentType, contentClass.getName()));\n                       }\n                   } catch (IllegalArgumentException iae) { // shouldn't really happen\n                       throw new JsonMappingException(null,\n                               String.format(\"Internal error: failed to refine value type of %s with concrete-type annotation (value %s), from '%s': %s\",\n                                       type, contentClass.getName(), a.getName(), iae.getMessage()),\n                                       iae);\n                   }\n               }\n               type = type.withContentType(contentType);\n           }\n        }\n        return type;\n    }",
        "fixed_src_wo_comments": "public JavaType refineSerializationType ( final MapperConfig < ? > config , final Annotated a , final JavaType baseType ) throws JsonMappingException { JavaType type = baseType ; final TypeFactory tf = config . getTypeFactory ( ) ; Class < ? > serClass = findSerializationType ( a ) ; if ( serClass != null ) { if ( type . hasRawClass ( serClass ) ) { type = type . withStaticTyping ( ) ; } else { Class < ? > currRaw = type . getRawClass ( ) ; try { if ( serClass . isAssignableFrom ( currRaw ) ) { type = tf . constructGeneralizedType ( type , serClass ) ; } else if ( currRaw . isAssignableFrom ( serClass ) ) { type = tf . constructSpecializedType ( type , serClass ) ; } else { throw new JsonMappingException ( null , String . format ( \"Can not refine serialization type %s into %s; types not related\" , type , serClass . getName ( ) ) ) ; } } catch ( IllegalArgumentException iae ) { throw new JsonMappingException ( null , String . format ( \"Failed to widen type %s with annotation (value %s), from '%s': %s\" , type , serClass . getName ( ) , a . getName ( ) , iae . getMessage ( ) ) , iae ) ; } } } if ( type . isMapLikeType ( ) ) { JavaType keyType = type . getKeyType ( ) ; Class < ? > keyClass = findSerializationKeyType ( a , keyType ) ; if ( keyClass != null ) { if ( keyType . hasRawClass ( keyClass ) ) { keyType = keyType . withStaticTyping ( ) ; } else { Class < ? > currRaw = keyType . getRawClass ( ) ; try { if ( keyClass . isAssignableFrom ( currRaw ) ) { keyType = tf . constructGeneralizedType ( keyType , keyClass ) ; } else if ( currRaw . isAssignableFrom ( keyClass ) ) { keyType = tf . constructSpecializedType ( keyType , keyClass ) ; } else { throw new JsonMappingException ( null , String . format ( \"Can not refine serialization key type %s into %s; types not related\" , keyType , keyClass . getName ( ) ) ) ; } } catch ( IllegalArgumentException iae ) { throw new JsonMappingException ( null , String . format ( \"Failed to widen key type of %s with concrete-type annotation (value %s), from '%s': %s\" , type , keyClass . getName ( ) , a . getName ( ) , iae . getMessage ( ) ) , iae ) ; } } type = ( ( MapLikeType ) type ) . withKeyType ( keyType ) ; } } JavaType contentType = type . getContentType ( ) ; if ( contentType != null ) { Class < ? > contentClass = findSerializationContentType ( a , contentType ) ; if ( contentClass != null ) { if ( contentType . hasRawClass ( contentClass ) ) { contentType = contentType . withStaticTyping ( ) ; } else { Class < ? > currRaw = contentType . getRawClass ( ) ; try { if ( contentClass . isAssignableFrom ( currRaw ) ) { contentType = tf . constructGeneralizedType ( contentType , contentClass ) ; } else if ( currRaw . isAssignableFrom ( contentClass ) ) { contentType = tf . constructSpecializedType ( contentType , contentClass ) ; } else { throw new JsonMappingException ( null , String . format ( \"Can not refine serialization content type %s into %s; types not related\" , contentType , contentClass . getName ( ) ) ) ; } } catch ( IllegalArgumentException iae ) { throw new JsonMappingException ( null , String . format ( \"Internal error: failed to refine value type of %s with concrete-type annotation (value %s), from '%s': %s\" , type , contentClass . getName ( ) , a . getName ( ) , iae . getMessage ( ) ) , iae ) ; } } type = type . withContentType ( contentType ) ; } } return type ; }",
        "summary": "`@JsonSerialize(as=superType)` behavior disallowed in 2.7.4",
        "Description": "#1178 fixed the problem with collections, but I'm seeing a problem with individual objects.\n\nI'm getting:\n\n```\ncom.fasterxml.jackson.databind.JsonMappingException: Failed to widen type [simple type, class org.pharmgkb.model.AccessionIdentifier] with annotation (value org.pharmgkb.model.BaseAccessionIdentifier), from 'getReference': Class org.pharmgkb.model.BaseAccessionIdentifier not a super-type of [simple type, class org.pharmgkb.model.AccessionIdentifier]\n\n    at com.fasterxml.jackson.databind.AnnotationIntrospector.refineSerializationType(AnnotationIntrospector.java:821)\n    at com.fasterxml.jackson.databind.introspect.AnnotationIntrospectorPair.refineSerializationType(AnnotationIntrospectorPair.java:488)\n    at com.fasterxml.jackson.databind.ser.PropertyBuilder.findSerializationType(PropertyBuilder.java:194)\n    at com.fasterxml.jackson.databind.ser.PropertyBuilder.buildWriter(PropertyBuilder.java:73)\n    at com.fasterxml.jackson.databind.ser.BeanSerializerFactory._constructWriter(BeanSerializerFactory.java:805)\n    at com.fasterxml.jackson.databind.ser.BeanSerializerFactory.findBeanProperties(BeanSerializerFactory.java:608)\n    at com.fasterxml.jackson.databind.ser.BeanSerializerFactory.constructBeanSerializer(BeanSerializerFactory.java:388)\n    at com.fasterxml.jackson.databind.ser.BeanSerializerFactory.findBeanSerializer(BeanSerializerFactory.java:271)\n    at com.fasterxml.jackson.databind.ser.BeanSerializerFactory._createSerializer2(BeanSerializerFactory.java:223)\n    at com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer(BeanSerializerFactory.java:157)\n    at com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer(SerializerProvider.java:1215)\n    at com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer(SerializerProvider.java:1167)\n    at com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer(SerializerProvider.java:490)\n    at com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer(SerializerProvider.java:688)\n    at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:107)\n    at com.fasterxml.jackson.databind.ObjectWriter$Prefetch.serialize(ObjectWriter.java:1428)\n    at com.fasterxml.jackson.databind.ObjectWriter._configAndWriteValue(ObjectWriter.java:1129)\n    at com.fasterxml.jackson.databind.ObjectWriter.writeValueAsString(ObjectWriter.java:1001)\n    at org.pharmgkb.jackson.JacksonTest.testModelObjects(JacksonTest.java:48)\n```\n\nOn something like:\n\n```\npublic class Foo {\n  @JsonSerialize(as = BaseAccessionIdentifier.class)\n  @JsonDeserialize(as = BaseAccessionIdentifier.class)\n  public AccessionIdentifier getReference() {\n  }\n}\n```\n\n```\npublic interface AccessionIdentifier {\n}\n```\n\n```\npublic class BaseAccessionIdentifier implements AccessionIdentifier {\n}\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this. Looks like this could be related to delegation via annotation introspector pair.\n"
            },
            {
                "content": "I can reproduce this easily, nothing to do with introspector pair delegation; also fails with master (2.8.0-SNAPSHOT).\n"
            },
            {
                "content": "Ok yes, my bad. Just had not extended this to `as` or `keyAs` cases, just `contentAs`.\n"
            }
        ],
        "summarized_discussion": "\n\nThe source code bug appears to be related to delegation via annotation introspector pair, but it is actually unrelated. The solution to the bug is to extend the code to include `as` or `keyAs` cases, in addition to `contentAs`."
    },
    "Math_75_src/main/java/org/apache/commons/math/stat/Frequency.java_301_304": {
        "src": "@Deprecated\n    public double getPct(Object v) {\n        return getCumPct((Comparable<?>) v);\n    }",
        "src_wo_comments": "@ Deprecated public double getPct ( Object v ) { return getCumPct ( ( Comparable < ? > ) v ) ; }",
        "fixed_src": "@Deprecated\n    public double getPct(Object v) {\n        return getPct((Comparable<?>) v);\n    }",
        "fixed_src_wo_comments": "@ Deprecated public double getPct ( Object v ) { return getPct ( ( Comparable < ? > ) v ) ; }",
        "summary": "In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable) ",
        "Description": "Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change\n\nFrequency.java\n\n   /**\n      * Returns the percentage of values that are equal to v\n     * @deprecated replaced by {@link #getPct(Comparable)} as of 2.0\n     */\n    @Deprecated\n    public double getPct(Object v) {\n        return getCumPct((Comparable<?>) v);\n    }",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-329",
        "comments": [
            "Fixed in r900016.  Thanks for reporting this."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in version 900016. Thanks for reporting it."
    },
    "JacksonCore_7_src/main/java/com/fasterxml/jackson/core/json/JsonWriteContext.java_166_185": {
        "src": "public int writeValue() {\n        // Most likely, object:\n        if (_type == TYPE_OBJECT) {\n            _gotName = false;\n            ++_index;\n            return STATUS_OK_AFTER_COLON;\n        }\n\n        // Ok, array?\n        if (_type == TYPE_ARRAY) {\n            int ix = _index;\n            ++_index;\n            return (ix < 0) ? STATUS_OK_AS_IS : STATUS_OK_AFTER_COMMA;\n        }\n        \n        // Nope, root context\n        // No commas within root context, but need space\n        ++_index;\n        return (_index == 0) ? STATUS_OK_AS_IS : STATUS_OK_AFTER_SPACE;\n    }",
        "src_wo_comments": "public int writeValue ( ) { if ( _type == TYPE_OBJECT ) { _gotName = false ; ++ _index ; return STATUS_OK_AFTER_COLON ; } if ( _type == TYPE_ARRAY ) { int ix = _index ; ++ _index ; return ( ix < 0 ) ? STATUS_OK_AS_IS : STATUS_OK_AFTER_COMMA ; } ++ _index ; return ( _index == 0 ) ? STATUS_OK_AS_IS : STATUS_OK_AFTER_SPACE ; }",
        "fixed_src": "public int writeValue() {\n        // Most likely, object:\n        if (_type == TYPE_OBJECT) {\n            if (!_gotName) {\n                return STATUS_EXPECT_NAME;\n            }\n            _gotName = false;\n            ++_index;\n            return STATUS_OK_AFTER_COLON;\n        }\n\n        // Ok, array?\n        if (_type == TYPE_ARRAY) {\n            int ix = _index;\n            ++_index;\n            return (ix < 0) ? STATUS_OK_AS_IS : STATUS_OK_AFTER_COMMA;\n        }\n        \n        // Nope, root context\n        // No commas within root context, but need space\n        ++_index;\n        return (_index == 0) ? STATUS_OK_AS_IS : STATUS_OK_AFTER_SPACE;\n    }",
        "fixed_src_wo_comments": "public int writeValue ( ) { if ( _type == TYPE_OBJECT ) { if ( ! _gotName ) { return STATUS_EXPECT_NAME ; } _gotName = false ; ++ _index ; return STATUS_OK_AFTER_COLON ; } if ( _type == TYPE_ARRAY ) { int ix = _index ; ++ _index ; return ( ix < 0 ) ? STATUS_OK_AS_IS : STATUS_OK_AFTER_COMMA ; } ++ _index ; return ( _index == 0 ) ? STATUS_OK_AS_IS : STATUS_OK_AFTER_SPACE ; }",
        "summary": "Add a check so `JsonGenerator.writeString()` won't work if `writeFieldName()` expected.",
        "Description": "Looks like calling `writeString()` (and perhaps other scalar write methods) results in writing invalid output, instead of throwing an exception. It should instead fail; in future we may want to consider allowing this as an alias, but at any rate it should not produce invalid output.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Hi,\nCan you give some more background around why this change was required - for example, what kind of Object structures cause invalid json to be produced, prior to this fix?  \nIts causing my object to be unserialisable whereas previous versions of Jackson API where fine - I thought, as a minor version it should be backward compatible?\n\nHappy to debug my code but I need some background as to what kinds of structures cause this kind of failure?\n\nThe error I get when serialising is...\n\n``` java\nCaused by: com.fasterxml.jackson.core.JsonGenerationException: Can not write a field name, expecting a value\n    at com.fasterxml.jackson.core.JsonGenerator._reportError(JsonGenerator.java:1644)\n    at com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.writeFieldName(WriterBasedJsonGenerator.java:120)\n    at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:654)\n    at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:678)\n    at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:156)\n    at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:119)\n    at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:79)\n    at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:18)\n    at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:656)\n    at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:678)\n    at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:156)\n    at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:656)\n    at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:678)\n    at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:564)\n    at com.fasterxml.jackson.databind.ser.impl.TypeWrappedSerializer.serialize(TypeWrappedSerializer.java:32)\n    at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:130)\n    at com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:3525)\n    at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:2915)\n```\n"
            },
            {
                "content": "@knightweb Logically property name (`FIELD_NAME`) is different from `VALUE_STRING`, even if they are expressed with the same physical token in JSON. The intent was never to allow these interchangeably, and bug fixes to observed behavior are not included in \"no change to behavior\" limitation. One could argue that the original model could have considered both as strings, in \"String is a String\" way; but to me separation is useful to avoid other problem cases, such as trying to write a non-String as property name/key, something JSON does not allow. Checks can also catch invalid usage, where output sequences either happen to work correctly accidentally; or, worse, report no error but produce invalid JSON output.\nIt is unfortunate that the check did not exist earlier, as that would have prevented regression problems like the one you have.\n\nAs to the underlying problem here... it is bit difficult to say, without seeing code. Possibilities include not calling `START_OBJECT` before trying to write a field name (although that would likely have produced invalid JSON anyway); writing two field names in a row (which also would seem problematic).\nBut it does seem odd; more commonly I see reverse cases where field name was expected, string value written.\n\nActually, come to think of that: I assume there has to be one of:\n1. Use of custom serializer somewhere, before code gets back default POJO handler\n2. Use of non-JSON data format, in which case there may be some translation (specifically, XML backend has some nasty work-arounds for massaging event sequences)\n\nsince it should not be possible to get such an error only using standard serializers.\nOf course if it is possible, I would really like a repeatable test case to fix it.\n"
            },
            {
                "content": "Thanks for the quick response.\nThere are customer serializers but not something I can change, as they are in a provided api from another internal team.  I'll try and get an isolated example together to help me understand what might be wrong with those serializers - at least then I can advise the team responsible.  They do however work fine in jackson version 2.5.4.\n\nI suspect the serializer causing the issue is (let me know if you see anything obviously wrong)...\n\n``` java\npublic class AttributesSerializer extends JsonSerializer<List<Attribute>> {\n\n    @Override\n    public void serialize(List<Attribute> attributes, JsonGenerator jgen, SerializerProvider provider) throws IOException {\n\n        if (attributes.isEmpty()) {\n            return;\n        }\n\n\n        Map<String, String> attributesMap = new LinkedHashMap<String, String>();\n        for (Attribute attribute : attributes) {\n            attributesMap.put(attribute.getName(), attribute.getValue());\n        }\n\n        jgen.writeObject(attributesMap);\n    }\n\n    @Override\n    public boolean isEmpty(List<Attribute> value) {\n        if (value == null || value.isEmpty()) {\n            return true;\n        }\n        return false;\n    }\n}\n```\n"
            },
            {
                "content": "I think the problem is:\n\n```\n    if (attributes.isEmpty()) {\n        return;\n    }\n```\n\nsince it is not legal to simply refuse to serialize something. At point where call is made, a write call of a valid value token (or, structure, array/object) must be made.\nThis because exclusion is handled at parent level, due to historical reasons (bit more on this below)\n\nNow, exclusion of value may work in ARRAY and ROOT contexts; but in OBJECT context the issue is that the property name has already been written, and value is expected. There is no way to \"undo\" property name write. I would guess that the non-exception-throwing behavior did not necessarily produce output one would expect; it just did not throw an exception.\n\nIf I was rewriting generator API from scratch, I might choose logic where the property name was always buffered, only written if a value was to be written. This would allow for simpler exclusion/filtering by serializer. But at this point the behavior is what it is and unfortunately can not be changed.\n"
            },
            {
                "content": "Thanks for the pointer.  I'll isolate the code down to a workable sized example and recreate - then modify to write an empty or null map object before returning to see if it works.  Then compare the two different json outputs under version 2.5.4 and 2.6.0.  If they are equivalent I can recommed the team change the behavoiur.  The have other flags on that exclude the inclusion of null properties, so this may still produce the same json output....worth a shot.\n"
            },
            {
                "content": "@knightweb Thanks!\n"
            },
            {
                "content": "@cowtowncoder thanks for the suggestion.  Writing an empty list solves the serialization problem I had with 2.6.  I'd agree it was a bug with my companies code, which previous versions allowed us to get away with.  \nI've done some comparing of the outputs produced by applying that fix and unfortunately they are not equal between version 2.5.4 and 2.6.  It would appear that version 2.6 no longer supports \"NON_EMPTY\" for properties with custom serialisers.  Although this is not a major issue it is a blocker to upgrading, as it would increase our payload size and also serve to clutter the json output.\n\nIs this something that could be consider for a 2.6 patch?  I've added some code below that shows the issue (apologies for the large class but it does run and demonstrate the issue)...\n\n``` java\npackage com.testcomp;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\n\nimport com.fasterxml.jackson.annotation.JsonInclude;\nimport com.fasterxml.jackson.annotation.JsonInclude.Include;\nimport com.fasterxml.jackson.core.JsonGenerator;\nimport com.fasterxml.jackson.databind.JsonSerializer;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializerProvider;\nimport com.fasterxml.jackson.databind.annotation.JsonSerialize;\n\npublic class CustomSerialisationIssue {\n\n    public CustomSerialisationIssue(){}\n\n    @JsonSerialize(using=AttributesSerializer.class) \n    @JsonInclude(Include.NON_EMPTY)\n    private List<Attribute> attributes = new ArrayList<Attribute>(); \n\n    private List<Attribute> attributesNonCustomSerialised = new ArrayList<Attribute>();\n\n    private String test;\n\n    public List<Attribute> getAttributesNonCustomSerialised() {\n        return attributesNonCustomSerialised;\n    }\n\n    public void setAttributesNonCustomSerialised(List<Attribute> attributesNonCustomSerialised) {\n        this.attributesNonCustomSerialised = attributesNonCustomSerialised;\n    }\n\n    public List<Attribute> getAttributes() {\n        return attributes;\n    }\n\n    public void setAttributes(List<Attribute> attributes) {\n        this.attributes = attributes;\n    }\n\n    public void addAttribute(Attribute attribute) {\n        attributes.add(attribute);\n    }   \n\n    public String getTest() {\n        return test;\n    }\n\n    public void setTest(String test) {\n        this.test = test;\n    }\n\n    private static class Attribute {\n\n        public Attribute(){}\n\n        private String name;\n        private String value;\n        public String getName() {\n            return name;\n        }\n        public void setName(String name) {\n            this.name = name;\n        }\n        public String getValue() {\n            return value;\n        }\n        public void setValue(String value) {\n            this.value = value;\n        }\n    }\n\n    private static class AttributesSerializer extends JsonSerializer<List<Attribute>> {\n\n        @Override\n        public void serialize(List<Attribute> attributes, JsonGenerator jgen, SerializerProvider provider) throws IOException {\n\n            if (attributes.isEmpty()) {\n                //This line fixes the serialization failure in 2.6.0, however its not needed in 2.5.4.\n                //Oddly even when included in version 2.5.4 it doesn't change the output, so that would suggest \n                //another change is causing null and empty properties to serialise when using a custom serialization.               \n                jgen.writeObject(new ArrayList<Attribute>());\n                return;\n            }\n\n            Map<String, String> attributesMap = new LinkedHashMap<String, String>();\n            for (Attribute attribute : attributes) {\n                attributesMap.put(attribute.getName(), attribute.getValue());\n            }\n\n            jgen.writeObject(attributesMap);\n        }\n\n        @Override\n        public boolean isEmpty(List<Attribute> value) {\n            if (value == null || value.isEmpty()) {\n                return true;\n            }\n            return false;\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        ObjectMapper mapper = new ObjectMapper();\n        mapper.setSerializationInclusion(Include.NON_EMPTY);\n\n        CustomSerialisationIssue csi = new CustomSerialisationIssue();\n        csi.setTest(\"123\");\n        Attribute a = new Attribute();\n        //a.setName(\"one\");\n        //a.setValue(\"1\");\n\n        //if we have no attributes prior to 2.6 they will not list in the serialised json.  This is desirable from a payload size and readability.\n        //does this perhaps only affect custom serialisation.\n\n        //csi.addAttribute(a);\n\n        String js = mapper.writeValueAsString(csi);\n\n        System.out.println(js);\n    }\n}\n```\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to add a line to the custom serializer that writes an empty list before returning. This will ensure that the JSON output is the same between version 2.5.4 and 2.6.0, and will prevent the payload size from increasing and the JSON output from becoming cluttered."
    },
    "Math_3_src/main/java/org/apache/commons/math3/util/MathArrays.java_814_872": {
        "src": "public static double linearCombination(final double[] a, final double[] b)\n        throws DimensionMismatchException {\n        final int len = a.length;\n        if (len != b.length) {\n            throw new DimensionMismatchException(len, b.length);\n        }\n\n            // Revert to scalar multiplication.\n\n        final double[] prodHigh = new double[len];\n        double prodLowSum = 0;\n\n        for (int i = 0; i < len; i++) {\n            final double ai = a[i];\n            final double ca = SPLIT_FACTOR * ai;\n            final double aHigh = ca - (ca - ai);\n            final double aLow = ai - aHigh;\n\n            final double bi = b[i];\n            final double cb = SPLIT_FACTOR * bi;\n            final double bHigh = cb - (cb - bi);\n            final double bLow = bi - bHigh;\n            prodHigh[i] = ai * bi;\n            final double prodLow = aLow * bLow - (((prodHigh[i] -\n                                                    aHigh * bHigh) -\n                                                   aLow * bHigh) -\n                                                  aHigh * bLow);\n            prodLowSum += prodLow;\n        }\n\n\n        final double prodHighCur = prodHigh[0];\n        double prodHighNext = prodHigh[1];\n        double sHighPrev = prodHighCur + prodHighNext;\n        double sPrime = sHighPrev - prodHighNext;\n        double sLowSum = (prodHighNext - (sHighPrev - sPrime)) + (prodHighCur - sPrime);\n\n        final int lenMinusOne = len - 1;\n        for (int i = 1; i < lenMinusOne; i++) {\n            prodHighNext = prodHigh[i + 1];\n            final double sHighCur = sHighPrev + prodHighNext;\n            sPrime = sHighCur - prodHighNext;\n            sLowSum += (prodHighNext - (sHighCur - sPrime)) + (sHighPrev - sPrime);\n            sHighPrev = sHighCur;\n        }\n\n        double result = sHighPrev + (prodLowSum + sLowSum);\n\n        if (Double.isNaN(result)) {\n            // either we have split infinite numbers or some coefficients were NaNs,\n            // just rely on the naive implementation and let IEEE754 handle this\n            result = 0;\n            for (int i = 0; i < len; ++i) {\n                result += a[i] * b[i];\n            }\n        }\n\n        return result;\n    }",
        "src_wo_comments": "public static double linearCombination ( final double [ ] a , final double [ ] b ) throws DimensionMismatchException { final int len = a . length ; if ( len != b . length ) { throw new DimensionMismatchException ( len , b . length ) ; } final double [ ] prodHigh = new double [ len ] ; double prodLowSum = 0 ; for ( int i = 0 ; i < len ; i ++ ) { final double ai = a [ i ] ; final double ca = SPLIT_FACTOR * ai ; final double aHigh = ca - ( ca - ai ) ; final double aLow = ai - aHigh ; final double bi = b [ i ] ; final double cb = SPLIT_FACTOR * bi ; final double bHigh = cb - ( cb - bi ) ; final double bLow = bi - bHigh ; prodHigh [ i ] = ai * bi ; final double prodLow = aLow * bLow - ( ( ( prodHigh [ i ] - aHigh * bHigh ) - aLow * bHigh ) - aHigh * bLow ) ; prodLowSum += prodLow ; } final double prodHighCur = prodHigh [ 0 ] ; double prodHighNext = prodHigh [ 1 ] ; double sHighPrev = prodHighCur + prodHighNext ; double sPrime = sHighPrev - prodHighNext ; double sLowSum = ( prodHighNext - ( sHighPrev - sPrime ) ) + ( prodHighCur - sPrime ) ; final int lenMinusOne = len - 1 ; for ( int i = 1 ; i < lenMinusOne ; i ++ ) { prodHighNext = prodHigh [ i + 1 ] ; final double sHighCur = sHighPrev + prodHighNext ; sPrime = sHighCur - prodHighNext ; sLowSum += ( prodHighNext - ( sHighCur - sPrime ) ) + ( sHighPrev - sPrime ) ; sHighPrev = sHighCur ; } double result = sHighPrev + ( prodLowSum + sLowSum ) ; if ( Double . isNaN ( result ) ) { result = 0 ; for ( int i = 0 ; i < len ; ++ i ) { result += a [ i ] * b [ i ] ; } } return result ; }",
        "fixed_src": "public static double linearCombination(final double[] a, final double[] b)\n        throws DimensionMismatchException {\n        final int len = a.length;\n        if (len != b.length) {\n            throw new DimensionMismatchException(len, b.length);\n        }\n\n        if (len == 1) {\n            // Revert to scalar multiplication.\n            return a[0] * b[0];\n        }\n\n        final double[] prodHigh = new double[len];\n        double prodLowSum = 0;\n\n        for (int i = 0; i < len; i++) {\n            final double ai = a[i];\n            final double ca = SPLIT_FACTOR * ai;\n            final double aHigh = ca - (ca - ai);\n            final double aLow = ai - aHigh;\n\n            final double bi = b[i];\n            final double cb = SPLIT_FACTOR * bi;\n            final double bHigh = cb - (cb - bi);\n            final double bLow = bi - bHigh;\n            prodHigh[i] = ai * bi;\n            final double prodLow = aLow * bLow - (((prodHigh[i] -\n                                                    aHigh * bHigh) -\n                                                   aLow * bHigh) -\n                                                  aHigh * bLow);\n            prodLowSum += prodLow;\n        }\n\n\n        final double prodHighCur = prodHigh[0];\n        double prodHighNext = prodHigh[1];\n        double sHighPrev = prodHighCur + prodHighNext;\n        double sPrime = sHighPrev - prodHighNext;\n        double sLowSum = (prodHighNext - (sHighPrev - sPrime)) + (prodHighCur - sPrime);\n\n        final int lenMinusOne = len - 1;\n        for (int i = 1; i < lenMinusOne; i++) {\n            prodHighNext = prodHigh[i + 1];\n            final double sHighCur = sHighPrev + prodHighNext;\n            sPrime = sHighCur - prodHighNext;\n            sLowSum += (prodHighNext - (sHighCur - sPrime)) + (sHighPrev - sPrime);\n            sHighPrev = sHighCur;\n        }\n\n        double result = sHighPrev + (prodLowSum + sLowSum);\n\n        if (Double.isNaN(result)) {\n            // either we have split infinite numbers or some coefficients were NaNs,\n            // just rely on the naive implementation and let IEEE754 handle this\n            result = 0;\n            for (int i = 0; i < len; ++i) {\n                result += a[i] * b[i];\n            }\n        }\n\n        return result;\n    }",
        "fixed_src_wo_comments": "public static double linearCombination ( final double [ ] a , final double [ ] b ) throws DimensionMismatchException { final int len = a . length ; if ( len != b . length ) { throw new DimensionMismatchException ( len , b . length ) ; } if ( len == 1 ) { return a [ 0 ] * b [ 0 ] ; } final double [ ] prodHigh = new double [ len ] ; double prodLowSum = 0 ; for ( int i = 0 ; i < len ; i ++ ) { final double ai = a [ i ] ; final double ca = SPLIT_FACTOR * ai ; final double aHigh = ca - ( ca - ai ) ; final double aLow = ai - aHigh ; final double bi = b [ i ] ; final double cb = SPLIT_FACTOR * bi ; final double bHigh = cb - ( cb - bi ) ; final double bLow = bi - bHigh ; prodHigh [ i ] = ai * bi ; final double prodLow = aLow * bLow - ( ( ( prodHigh [ i ] - aHigh * bHigh ) - aLow * bHigh ) - aHigh * bLow ) ; prodLowSum += prodLow ; } final double prodHighCur = prodHigh [ 0 ] ; double prodHighNext = prodHigh [ 1 ] ; double sHighPrev = prodHighCur + prodHighNext ; double sPrime = sHighPrev - prodHighNext ; double sLowSum = ( prodHighNext - ( sHighPrev - sPrime ) ) + ( prodHighCur - sPrime ) ; final int lenMinusOne = len - 1 ; for ( int i = 1 ; i < lenMinusOne ; i ++ ) { prodHighNext = prodHigh [ i + 1 ] ; final double sHighCur = sHighPrev + prodHighNext ; sPrime = sHighCur - prodHighNext ; sLowSum += ( prodHighNext - ( sHighCur - sPrime ) ) + ( sHighPrev - sPrime ) ; sHighPrev = sHighCur ; } double result = sHighPrev + ( prodLowSum + sLowSum ) ; if ( Double . isNaN ( result ) ) { result = 0 ; for ( int i = 0 ; i < len ; ++ i ) { result += a [ i ] * b [ i ] ; } } return result ; }",
        "summary": "ArrayIndexOutOfBoundsException in MathArrays.linearCombination",
        "Description": "When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line:\n\ndouble prodHighNext = prodHigh[1];\n\nlinearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-1005",
        "comments": [
            "Proposed fix committed in revision 1502516. Thanks for the report.",
            "Closing all resolved issue now available in released 3.3 version."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in revision 1502516 and the fix is now available in version 3.3. The issue has been closed."
    },
    "Math_59_src/main/java/org/apache/commons/math/util/FastMath.java_3481_3483": {
        "src": "public static float max(final float a, final float b) {\n        return (a <= b) ? b : (Float.isNaN(a + b) ? Float.NaN : b);\n    }",
        "src_wo_comments": "public static float max ( final float a , final float b ) { return ( a <= b ) ? b : ( Float . isNaN ( a + b ) ? Float . NaN : b ) ; }",
        "fixed_src": "public static float max(final float a, final float b) {\n        return (a <= b) ? b : (Float.isNaN(a + b) ? Float.NaN : a);\n    }",
        "fixed_src_wo_comments": "public static float max ( final float a , final float b ) { return ( a <= b ) ? b : ( Float . isNaN ( a + b ) ? Float . NaN : a ) ; }",
        "summary": "FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f",
        "Description": "FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f.\n\nThis is because the wrong variable is returned.\n\nThe bug was not detected by the test case \"testMinMaxFloat()\" because that has a bug too - it tests doubles, not floats.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-482",
        "comments": [
            "Closing issue as it was included in version 2.2, which has been released"
        ],
        "summarized_discussion": "\n\nThe bug has been solved by releasing version 2.2 of the source code."
    },
    "Compress_40_src/main/java/org/apache/commons/compress/utils/BitInputStream.java_81_109": {
        "src": "public long readBits(final int count) throws IOException {\n        if (count < 0 || count > MAXIMUM_CACHE_SIZE) {\n            throw new IllegalArgumentException(\"count must not be negative or greater than \" + MAXIMUM_CACHE_SIZE);\n        }\n        while (bitsCachedSize < count) {\n            final long nextByte = in.read();\n            if (nextByte < 0) {\n                return nextByte;\n            }\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n                bitsCached |= (nextByte << bitsCachedSize);\n            } else {\n                bitsCached <<= 8;\n                bitsCached |= nextByte;\n            }\n            bitsCachedSize += 8;\n        }\n            // bitsCachedSize >= 57 and left-shifting it 8 bits would cause an overflow\n        \n        final long bitsOut;\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n                bitsOut = (bitsCached & MASKS[count]);\n                bitsCached >>>= count;\n            } else {\n                bitsOut = (bitsCached >> (bitsCachedSize - count)) & MASKS[count];\n            }\n            bitsCachedSize -= count;\n        return bitsOut;\n    }",
        "src_wo_comments": "public long readBits ( final int count ) throws IOException { if ( count < 0 || count > MAXIMUM_CACHE_SIZE ) { throw new IllegalArgumentException ( \"count must not be negative or greater than \" + MAXIMUM_CACHE_SIZE ) ; } while ( bitsCachedSize < count ) { final long nextByte = in . read ( ) ; if ( nextByte < 0 ) { return nextByte ; } if ( byteOrder == ByteOrder . LITTLE_ENDIAN ) { bitsCached |= ( nextByte << bitsCachedSize ) ; } else { bitsCached <<= 8 ; bitsCached |= nextByte ; } bitsCachedSize += 8 ; } final long bitsOut ; if ( byteOrder == ByteOrder . LITTLE_ENDIAN ) { bitsOut = ( bitsCached & MASKS [ count ] ) ; bitsCached >>>= count ; } else { bitsOut = ( bitsCached >> ( bitsCachedSize - count ) ) & MASKS [ count ] ; } bitsCachedSize -= count ; return bitsOut ; }",
        "fixed_src": "public long readBits(final int count) throws IOException {\n        if (count < 0 || count > MAXIMUM_CACHE_SIZE) {\n            throw new IllegalArgumentException(\"count must not be negative or greater than \" + MAXIMUM_CACHE_SIZE);\n        }\n        while (bitsCachedSize < count && bitsCachedSize < 57) {\n            final long nextByte = in.read();\n            if (nextByte < 0) {\n                return nextByte;\n            }\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n                bitsCached |= (nextByte << bitsCachedSize);\n            } else {\n                bitsCached <<= 8;\n                bitsCached |= nextByte;\n            }\n            bitsCachedSize += 8;\n        }\n        int overflowBits = 0;\n        long overflow = 0l;\n        if (bitsCachedSize < count) {\n            // bitsCachedSize >= 57 and left-shifting it 8 bits would cause an overflow\n            int bitsToAddCount = count - bitsCachedSize;\n            overflowBits = 8 - bitsToAddCount;\n            final long nextByte = in.read();\n            if (nextByte < 0) {\n                return nextByte;\n            }\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n                long bitsToAdd = nextByte & MASKS[bitsToAddCount];\n                bitsCached |= (bitsToAdd << bitsCachedSize);\n                overflow = (nextByte >>> bitsToAddCount) & MASKS[overflowBits];\n            } else {\n                bitsCached <<= bitsToAddCount;\n                long bitsToAdd = (nextByte >>> (overflowBits)) & MASKS[bitsToAddCount];\n                bitsCached |= bitsToAdd;\n                overflow = nextByte & MASKS[overflowBits];\n            }\n            bitsCachedSize = count;\n        }\n        \n        final long bitsOut;\n        if (overflowBits == 0) {\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n                bitsOut = (bitsCached & MASKS[count]);\n                bitsCached >>>= count;\n            } else {\n                bitsOut = (bitsCached >> (bitsCachedSize - count)) & MASKS[count];\n            }\n            bitsCachedSize -= count;\n        } else {\n            bitsOut = bitsCached & MASKS[count];\n            bitsCached = overflow;\n            bitsCachedSize = overflowBits;\n        }\n        return bitsOut;\n    }",
        "fixed_src_wo_comments": "public long readBits ( final int count ) throws IOException { if ( count < 0 || count > MAXIMUM_CACHE_SIZE ) { throw new IllegalArgumentException ( \"count must not be negative or greater than \" + MAXIMUM_CACHE_SIZE ) ; } while ( bitsCachedSize < count && bitsCachedSize < 57 ) { final long nextByte = in . read ( ) ; if ( nextByte < 0 ) { return nextByte ; } if ( byteOrder == ByteOrder . LITTLE_ENDIAN ) { bitsCached |= ( nextByte << bitsCachedSize ) ; } else { bitsCached <<= 8 ; bitsCached |= nextByte ; } bitsCachedSize += 8 ; } int overflowBits = 0 ; long overflow = 0l ; if ( bitsCachedSize < count ) { int bitsToAddCount = count - bitsCachedSize ; overflowBits = 8 - bitsToAddCount ; final long nextByte = in . read ( ) ; if ( nextByte < 0 ) { return nextByte ; } if ( byteOrder == ByteOrder . LITTLE_ENDIAN ) { long bitsToAdd = nextByte & MASKS [ bitsToAddCount ] ; bitsCached |= ( bitsToAdd << bitsCachedSize ) ; overflow = ( nextByte >>> bitsToAddCount ) & MASKS [ overflowBits ] ; } else { bitsCached <<= bitsToAddCount ; long bitsToAdd = ( nextByte >>> ( overflowBits ) ) & MASKS [ bitsToAddCount ] ; bitsCached |= bitsToAdd ; overflow = nextByte & MASKS [ overflowBits ] ; } bitsCachedSize = count ; } final long bitsOut ; if ( overflowBits == 0 ) { if ( byteOrder == ByteOrder . LITTLE_ENDIAN ) { bitsOut = ( bitsCached & MASKS [ count ] ) ; bitsCached >>>= count ; } else { bitsOut = ( bitsCached >> ( bitsCachedSize - count ) ) & MASKS [ count ] ; } bitsCachedSize -= count ; } else { bitsOut = bitsCached & MASKS [ count ] ; bitsCached = overflow ; bitsCachedSize = overflowBits ; } return bitsOut ; }",
        "summary": "Overflow in BitInputStream",
        "Description": "in Class BitInputStream.java(\\src\\main\\java\\org\\apache\\commons\\compress\\utils),\nfuncion:\n\n public long readBits(final int count) throws IOException {\n        if (count < 0 || count > MAXIMUM_CACHE_SIZE) {\n            throw new IllegalArgumentException(\"count must not be negative or greater than \" + MAXIMUM_CACHE_SIZE);\n        }\n        while (bitsCachedSize < count) {\n            final long nextByte = in.read();\n            if (nextByte < 0) {\n                return nextByte;\n            }\n            if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n                bitsCached |= (nextByte << bitsCachedSize);\n            } else {\n                bitsCached <<= 8;\n                bitsCached |= nextByte;\n            }\n            bitsCachedSize += 8;\n        }\n\n        final long bitsOut;\n        if (byteOrder == ByteOrder.LITTLE_ENDIAN) {\n            bitsOut = (bitsCached & MASKS[count]);\n            bitsCached >>>= count;\n        } else {\n            bitsOut = (bitsCached >> (bitsCachedSize - count)) & MASKS[count];\n        }\n        bitsCachedSize -= count;\n        return bitsOut;\n    }\n\n\n\nI think here \"bitsCached |= (nextByte << bitsCachedSize);\" will overflow in some cases. for example, below is a test case:\n\npublic static void test() {\n\n        ByteArrayInputStream in = new ByteArrayInputStream(new byte[]{87, 45, 66, 15,\n                                                                      90, 29, 88, 61, 33, 74});\n        BitInputStream bin = new BitInputStream(in, ByteOrder.LITTLE_ENDIAN);\n        try {\n            long ret = bin.readBits(5);\n            ret = bin.readBits(63);\n            ret = bin.readBits(12);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n}\n\noverflow occur in \"bin.readBits(63);\" , so ,result in wrong result from  \"bin.readBits(12);\" \n\n",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-363",
        "comments": [
            "Many thanks, I've created a unit test from your example.\n\nShould be fixed with git commit 52dd590."
        ],
        "summarized_discussion": "\n\nThe bug should be fixed with the git commit 52dd590."
    },
    "Compress_17_src/main/java/org/apache/commons/compress/archivers/tar/TarUtils.java_102_151": {
        "src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        if (buffer[start] == 0) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Must have trailing NUL or space\n        byte trailer;\n        trailer = buffer[end-1];\n        if (trailer == 0 || trailer == ' '){\n            end--;\n        } else {\n            throw new IllegalArgumentException(\n                    exceptionMessage(buffer, offset, length, end-1, trailer));\n        }\n        // May have additional NULs or spaces\n        trailer = buffer[end - 1];\n        if (trailer == 0 || trailer == ' '){\n            end--;\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } if ( buffer [ start ] == 0 ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer ; trailer = buffer [ end - 1 ] ; if ( trailer == 0 || trailer == ' ' ) { end -- ; } else { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , end - 1 , trailer ) ) ; } trailer = buffer [ end - 1 ] ; if ( trailer == 0 || trailer == ' ' ) { end -- ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "fixed_src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        if (buffer[start] == 0) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Must have trailing NUL or space\n        byte trailer;\n        trailer = buffer[end-1];\n        if (trailer == 0 || trailer == ' '){\n            end--;\n        } else {\n            throw new IllegalArgumentException(\n                    exceptionMessage(buffer, offset, length, end-1, trailer));\n        }\n        // May have additional NULs or spaces\n        trailer = buffer[end - 1];\n        while (start < end - 1 && (trailer == 0 || trailer == ' ')) {\n            end--;\n            trailer = buffer[end - 1];\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "fixed_src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } if ( buffer [ start ] == 0 ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer ; trailer = buffer [ end - 1 ] ; if ( trailer == 0 || trailer == ' ' ) { end -- ; } else { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , end - 1 , trailer ) ) ; } trailer = buffer [ end - 1 ] ; while ( start < end - 1 && ( trailer == 0 || trailer == ' ' ) ) { end -- ; trailer = buffer [ end - 1 ] ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "summary": "Tar file for Android backup cannot be read",
        "Description": "Attached tar file was generated by some kind of backup tool on Android. Normal tar utilities seem to handle it fine, but Commons Compress doesn't.\n\n{noformat}\njava.lang.IllegalArgumentException: Invalid byte 0 at offset 5 in '01750{NUL}{NUL}{NUL}' len=8\n    at org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:99)\n    at org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:788)\n    at org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:308)\n{noformat}\n",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-197",
        "comments": [
            "The attached file contains a field with three trailing nul bytes, while Commons Compress only accepted one or two. In revision 1369655 relaxed that constraint to allow any number of trailing nuls or spaces."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to update the source code to revision 1369655, which allows for any number of trailing nuls or spaces in the field."
    },
    "Cli_19_src/java/org/apache/commons/cli/PosixParser.java_227_239": {
        "src": "private void processOptionToken(String token, boolean stopAtNonOption)\n    {\n        if (options.hasOption(token))\n        {\n            currentOption = options.getOption(token);\n            tokens.add(token);\n        }\n        else if (stopAtNonOption)\n        {\n            eatTheRest = true;\n            tokens.add(token);\n        }\n    }",
        "src_wo_comments": "private void processOptionToken ( String token , boolean stopAtNonOption ) { if ( options . hasOption ( token ) ) { currentOption = options . getOption ( token ) ; tokens . add ( token ) ; } else if ( stopAtNonOption ) { eatTheRest = true ; tokens . add ( token ) ; } }",
        "fixed_src": "private void processOptionToken(String token, boolean stopAtNonOption)\n    {\n        if (options.hasOption(token))\n        {\n            currentOption = options.getOption(token);\n        }\n        else if (stopAtNonOption)\n        {\n            eatTheRest = true;\n        }\n\n        tokens.add(token);\n    }",
        "fixed_src_wo_comments": "private void processOptionToken ( String token , boolean stopAtNonOption ) { if ( options . hasOption ( token ) ) { currentOption = options . getOption ( token ) ; } else if ( stopAtNonOption ) { eatTheRest = true ; } tokens . add ( token ) ; }",
        "summary": "PosixParser ignores unrecognized tokens starting with '-'",
        "Description": "PosixParser doesn't handle properly unrecognized tokens starting with '-' when stopAtNonOption is enabled, the token is simply ignored.\n\nFor example, if the option 'a' is defined, the following command line:\n\n{code}-z -a foo{code}\n\nis interpreted as:\n\n{code}-a foo{code}",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-164",
        "comments": [
            "Actually, the option is ignored even if stopAtNonOption is disabled, no exception is thrown."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to disable the option 'stopAtNonOption', which will prevent the option from being ignored and no exception will be thrown."
    },
    "Cli_12_src/java/org/apache/commons/cli/GnuParser.java_52_110": {
        "src": "protected String[] flatten(Options options, String[] arguments, boolean stopAtNonOption)\n    {\n        List tokens = new ArrayList();\n\n        boolean eatTheRest = false;\n\n        for (int i = 0; i < arguments.length; i++)\n        {\n            String arg = arguments[i];\n\n            if (\"--\".equals(arg))\n            {\n                eatTheRest = true;\n                tokens.add(\"--\");\n            }\n            else if (\"-\".equals(arg))\n            {\n                tokens.add(\"-\");\n            }\n            else if (arg.startsWith(\"-\"))\n            {\n                String opt = Util.stripLeadingHyphens(arg);\n\n                if (options.hasOption(opt))\n                {\n                    tokens.add(arg);\n                }\n                else\n                {\n                    if (options.hasOption(arg.substring(0, 2)))\n                    {\n                        // the format is --foo=value or -foo=value\n                        // the format is a special properties option (-Dproperty=value)\n                        tokens.add(arg.substring(0, 2)); // -D\n                        tokens.add(arg.substring(2)); // property=value\n                    }\n                    else\n                    {\n                        eatTheRest = stopAtNonOption;\n                        tokens.add(arg);\n                    }\n                }\n            }\n            else\n            {\n                tokens.add(arg);\n            }\n\n            if (eatTheRest)\n            {\n                for (i++; i < arguments.length; i++)\n                {\n                    tokens.add(arguments[i]);\n                }\n            }\n        }\n\n        return (String[]) tokens.toArray(new String[tokens.size()]);\n    }",
        "src_wo_comments": "protected String [ ] flatten ( Options options , String [ ] arguments , boolean stopAtNonOption ) { List tokens = new ArrayList ( ) ; boolean eatTheRest = false ; for ( int i = 0 ; i < arguments . length ; i ++ ) { String arg = arguments [ i ] ; if ( \"--\" . equals ( arg ) ) { eatTheRest = true ; tokens . add ( \"--\" ) ; } else if ( \"-\" . equals ( arg ) ) { tokens . add ( \"-\" ) ; } else if ( arg . startsWith ( \"-\" ) ) { String opt = Util . stripLeadingHyphens ( arg ) ; if ( options . hasOption ( opt ) ) { tokens . add ( arg ) ; } else { if ( options . hasOption ( arg . substring ( 0 , 2 ) ) ) { tokens . add ( arg . substring ( 0 , 2 ) ) ; tokens . add ( arg . substring ( 2 ) ) ; } else { eatTheRest = stopAtNonOption ; tokens . add ( arg ) ; } } } else { tokens . add ( arg ) ; } if ( eatTheRest ) { for ( i ++ ; i < arguments . length ; i ++ ) { tokens . add ( arguments [ i ] ) ; } } } return ( String [ ] ) tokens . toArray ( new String [ tokens . size ( ) ] ) ; }",
        "fixed_src": "protected String[] flatten(Options options, String[] arguments, boolean stopAtNonOption)\n    {\n        List tokens = new ArrayList();\n\n        boolean eatTheRest = false;\n\n        for (int i = 0; i < arguments.length; i++)\n        {\n            String arg = arguments[i];\n\n            if (\"--\".equals(arg))\n            {\n                eatTheRest = true;\n                tokens.add(\"--\");\n            }\n            else if (\"-\".equals(arg))\n            {\n                tokens.add(\"-\");\n            }\n            else if (arg.startsWith(\"-\"))\n            {\n                String opt = Util.stripLeadingHyphens(arg);\n\n                if (options.hasOption(opt))\n                {\n                    tokens.add(arg);\n                }\n                else\n                {\n                    if (opt.indexOf('=') != -1 && options.hasOption(opt.substring(0, opt.indexOf('='))))\n                    {\n                        // the format is --foo=value or -foo=value\n                        tokens.add(arg.substring(0, arg.indexOf('='))); // --foo\n                        tokens.add(arg.substring(arg.indexOf('=') + 1)); // value\n                    }\n                    else if (options.hasOption(arg.substring(0, 2)))\n                    {\n                        // the format is a special properties option (-Dproperty=value)\n                        tokens.add(arg.substring(0, 2)); // -D\n                        tokens.add(arg.substring(2)); // property=value\n                    }\n                    else\n                    {\n                        eatTheRest = stopAtNonOption;\n                        tokens.add(arg);\n                    }\n                }\n            }\n            else\n            {\n                tokens.add(arg);\n            }\n\n            if (eatTheRest)\n            {\n                for (i++; i < arguments.length; i++)\n                {\n                    tokens.add(arguments[i]);\n                }\n            }\n        }\n\n        return (String[]) tokens.toArray(new String[tokens.size()]);\n    }",
        "fixed_src_wo_comments": "protected String [ ] flatten ( Options options , String [ ] arguments , boolean stopAtNonOption ) { List tokens = new ArrayList ( ) ; boolean eatTheRest = false ; for ( int i = 0 ; i < arguments . length ; i ++ ) { String arg = arguments [ i ] ; if ( \"--\" . equals ( arg ) ) { eatTheRest = true ; tokens . add ( \"--\" ) ; } else if ( \"-\" . equals ( arg ) ) { tokens . add ( \"-\" ) ; } else if ( arg . startsWith ( \"-\" ) ) { String opt = Util . stripLeadingHyphens ( arg ) ; if ( options . hasOption ( opt ) ) { tokens . add ( arg ) ; } else { if ( opt . indexOf ( '=' ) != - 1 && options . hasOption ( opt . substring ( 0 , opt . indexOf ( '=' ) ) ) ) { tokens . add ( arg . substring ( 0 , arg . indexOf ( '=' ) ) ) ; tokens . add ( arg . substring ( arg . indexOf ( '=' ) + 1 ) ) ; } else if ( options . hasOption ( arg . substring ( 0 , 2 ) ) ) { tokens . add ( arg . substring ( 0 , 2 ) ) ; tokens . add ( arg . substring ( 2 ) ) ; } else { eatTheRest = stopAtNonOption ; tokens . add ( arg ) ; } } } else { tokens . add ( arg ) ; } if ( eatTheRest ) { for ( i ++ ; i < arguments . length ; i ++ ) { tokens . add ( arguments [ i ] ) ; } } } return ( String [ ] ) tokens . toArray ( new String [ tokens . size ( ) ] ) ; }",
        "summary": "PosixParser interupts \"-target opt\" as \"-t arget opt\"",
        "Description": "This was posted on the Commons-Developer list and confirmed as a bug.\n\n> Is this a bug?  Or am I using this incorrectly?\n> I have an option with short and long values.  Given code that is \n> essentially what is below, with a PosixParser I see results as \n> follows:\n> \n> A command line with just \"-t\" prints out the results of the catch \n> block\n> (OK)\n> A command line with just \"-target\" prints out the results of the catch\n> block (OK)\n> A command line with just \"-t foobar.com\" prints out \"processing selected\n> target: foobar.com\" (OK)\n> A command line with just \"-target foobar.com\" prints out \"processing\n> selected target: arget\" (ERROR?)\n> \n> ======================================================================\n> ==\n> =======================\n>   private static final String OPTION_TARGET = \"t\";\n>   private static final String OPTION_TARGET_LONG = \"target\";\n>   // ...\n>   Option generateTarget = new Option(OPTION_TARGET, \n>                                      OPTION_TARGET_LONG, \n>                                      true, \n>                                      \"Generate files for the specified\n> target machine\");\n>   // ...\n>   try {\n>         parsedLine = parser.parse(cmdLineOpts, args);\n>       } catch (ParseException pe) {\n>         System.out.println(\"Invalid command: \" + pe.getMessage() +\n> \"\\n\");\n>         HelpFormatter hf = new HelpFormatter();\n>         hf.printHelp(USAGE, cmdLineOpts);\n>         System.exit(-1);\n>       }\n> \n>   if (parsedLine.hasOption(OPTION_TARGET)) {\n>     System.out.println(\"processing selected target: \" +\n> parsedLine.getOptionValue(OPTION_TARGET));        \n>   }\n\nIt is a bug but it is due to well defined behaviour (so that makes me feel a\nlittle better about myself ;).  To support *special* \n(well I call them special anyway) like -Dsystem.property=value we need to be\nable to examine the first character of an option.  If the first character is\nitself defined as an Option then the remainder of the token is used as the\nvalue, e.g. 'D' is the token, it is an option so 'system.property=value' is the\nargument value for that option.  This is the behaviour that we are seeing for\nyour example.  \n't' is the token, it is an options so 'arget' is the argument value.  \n\nI suppose a solution to this could be to have a way to specify properties for\nparsers.  In this case 'posix.special.option == true' for turning \non *special* options. I'll have a look into this and let you know.\n\nJust to keep track of this and to get you used to how we operate, can you log a\nbug in bugzilla for this.\n\nThanks,\n-John K",
        "issue_url": "https://issues.apache.org/jira//browse/cli-1",
        "comments": [
            "Joe,\n\nMy fault on this one.  Its isn't a bug.  I read this first and thought that it\nwas two separate Options i.e. one '-t' and one '-target'.  It is in fact the\nsame Option with '-t' as the shortOpt and '--target' as the long opt.  So I\napologise for my incorrect analysis of the orginal problem.  To use 'target' on\nthe command line you should prefix it with \"--\" which is the way to do this\nusing the PosixParser.  To use options of the style '-target' you need to use\nthe GnuParser.\n\n-John K"
        ],
        "summarized_discussion": "\n\nJohn K has identified that the source code bug is not a bug, but rather a misunderstanding of the command line options. The solution is to use \"--\" when using the PosixParser, and to use the GnuParser when using options of the style '-target'."
    },
    "Math_13_src/main/java/org/apache/commons/math3/optimization/general/AbstractLeastSquaresOptimizer.java_561_564": {
        "src": "private RealMatrix squareRoot(RealMatrix m) {\n            final EigenDecomposition dec = new EigenDecomposition(m);\n            return dec.getSquareRoot();\n    }",
        "src_wo_comments": "private RealMatrix squareRoot ( RealMatrix m ) { final EigenDecomposition dec = new EigenDecomposition ( m ) ; return dec . getSquareRoot ( ) ; }",
        "fixed_src": "private RealMatrix squareRoot(RealMatrix m) {\n        if (m instanceof DiagonalMatrix) {\n            final int dim = m.getRowDimension();\n            final RealMatrix sqrtM = new DiagonalMatrix(dim);\n            for (int i = 0; i < dim; i++) {\n               sqrtM.setEntry(i, i, FastMath.sqrt(m.getEntry(i, i)));\n            }\n            return sqrtM;\n        } else {\n            final EigenDecomposition dec = new EigenDecomposition(m);\n            return dec.getSquareRoot();\n        }\n    }",
        "fixed_src_wo_comments": "private RealMatrix squareRoot ( RealMatrix m ) { if ( m instanceof DiagonalMatrix ) { final int dim = m . getRowDimension ( ) ; final RealMatrix sqrtM = new DiagonalMatrix ( dim ) ; for ( int i = 0 ; i < dim ; i ++ ) { sqrtM . setEntry ( i , i , FastMath . sqrt ( m . getEntry ( i , i ) ) ) ; } return sqrtM ; } else { final EigenDecomposition dec = new EigenDecomposition ( m ) ; return dec . getSquareRoot ( ) ; } }",
        "summary": "new multivariate vector optimizers cannot be used with large number of weights",
        "Description": "When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large.\n\nThis happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-924",
        "comments": [
            "Fixed in subversion repository as of r1426616.",
            "Keeping the matrix concept, by representing uncorrelated observations with a diagonal weight matrix (instead of an array), allows to solve this issue with minimal changes to the code:\n# The optimizer's API is untouched.\n# The possibility to have correlated observations is kept.\n\nThe attached patch contains a minimal \"DiagonalMatrix\" implementation, needing overview (ant unit tests).\n",
            "Changes committed in revision 1426758.\n\nThe same problem would occur int the deprecated \"o.a.c.m.optimization\" package. The same changes must be ported there.\n",
            "bq. The same changes must be ported there.\n\nDone in revision 1426759."
        ],
        "summarized_discussion": "\n\nThe bug was solved by representing uncorrelated observations with a diagonal weight matrix instead of an array. A minimal \"DiagonalMatrix\" implementation was created, and changes were committed in revision 1426758. The same changes were then ported to the deprecated \"o.a.c.m.optimization\" package, and committed in revision 1426759."
    },
    "JacksonDatabind_76_src/main/java/com/fasterxml/jackson/databind/deser/BuilderBasedDeserializer.java_565_637": {
        "src": "@SuppressWarnings(\"resource\")\n    protected Object deserializeUsingPropertyBasedWithUnwrapped(JsonParser p,\n    \t\tDeserializationContext ctxt)\n        throws IOException, JsonProcessingException\n    {\n        final PropertyBasedCreator creator = _propertyBasedCreator;\n        PropertyValueBuffer buffer = creator.startBuilding(p, ctxt, _objectIdReader);\n\n        TokenBuffer tokens = new TokenBuffer(p, ctxt);\n        tokens.writeStartObject();\n\n        JsonToken t = p.getCurrentToken();\n        for (; t == JsonToken.FIELD_NAME; t = p.nextToken()) {\n            String propName = p.getCurrentName();\n            p.nextToken(); // to point to value\n            // creator property?\n            SettableBeanProperty creatorProp = creator.findCreatorProperty(propName);\n            if (creatorProp != null) {\n                if (buffer.assignParameter(creatorProp, creatorProp.deserialize(p, ctxt))) {\n                    t = p.nextToken();\n                    Object bean;\n                    try {\n                        bean = creator.build(ctxt, buffer);\n                    } catch (Exception e) {\n                        wrapAndThrow(e, _beanType.getRawClass(), propName, ctxt);\n                        continue;\n                    }\n                    while (t == JsonToken.FIELD_NAME) {\n                        p.nextToken();\n                        tokens.copyCurrentStructure(p);\n                        t = p.nextToken();\n                    }\n                    tokens.writeEndObject();\n                    if (bean.getClass() != _beanType.getRawClass()) {\n                        ctxt.reportMappingException(\"Can not create polymorphic instances with unwrapped values\");\n                        return null;\n                    }\n                    return _unwrappedPropertyHandler.processUnwrapped(p, ctxt, bean, tokens);\n                }\n                continue;\n            }\n            // Object Id property?\n            if (buffer.readIdProperty(propName)) {\n                continue;\n            }\n            // regular property? needs buffering\n            SettableBeanProperty prop = _beanProperties.find(propName);\n            if (prop != null) {\n                buffer.bufferProperty(prop, prop.deserialize(p, ctxt));\n                continue;\n            }\n            if (_ignorableProps != null && _ignorableProps.contains(propName)) {\n                handleIgnoredProperty(p, ctxt, handledType(), propName);\n                continue;\n            }\n            tokens.writeFieldName(propName);\n            tokens.copyCurrentStructure(p);\n            // \"any property\"?\n            if (_anySetter != null) {\n                buffer.bufferAnyProperty(_anySetter, propName, _anySetter.deserialize(p, ctxt));\n            }\n        }\n\n        // We hit END_OBJECT, so:\n        Object bean;\n        // !!! 15-Feb-2012, tatu: Need to modify creator to use Builder!\n        try {\n            bean = creator.build(ctxt, buffer);\n        } catch (Exception e) {\n            return wrapInstantiationProblem(e, ctxt);\n        }\n        return _unwrappedPropertyHandler.processUnwrapped(p, ctxt, bean, tokens);\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"resource\" ) protected Object deserializeUsingPropertyBasedWithUnwrapped ( JsonParser p , DeserializationContext ctxt ) throws IOException , JsonProcessingException { final PropertyBasedCreator creator = _propertyBasedCreator ; PropertyValueBuffer buffer = creator . startBuilding ( p , ctxt , _objectIdReader ) ; TokenBuffer tokens = new TokenBuffer ( p , ctxt ) ; tokens . writeStartObject ( ) ; JsonToken t = p . getCurrentToken ( ) ; for ( ; t == JsonToken . FIELD_NAME ; t = p . nextToken ( ) ) { String propName = p . getCurrentName ( ) ; p . nextToken ( ) ; SettableBeanProperty creatorProp = creator . findCreatorProperty ( propName ) ; if ( creatorProp != null ) { if ( buffer . assignParameter ( creatorProp , creatorProp . deserialize ( p , ctxt ) ) ) { t = p . nextToken ( ) ; Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { wrapAndThrow ( e , _beanType . getRawClass ( ) , propName , ctxt ) ; continue ; } while ( t == JsonToken . FIELD_NAME ) { p . nextToken ( ) ; tokens . copyCurrentStructure ( p ) ; t = p . nextToken ( ) ; } tokens . writeEndObject ( ) ; if ( bean . getClass ( ) != _beanType . getRawClass ( ) ) { ctxt . reportMappingException ( \"Can not create polymorphic instances with unwrapped values\" ) ; return null ; } return _unwrappedPropertyHandler . processUnwrapped ( p , ctxt , bean , tokens ) ; } continue ; } if ( buffer . readIdProperty ( propName ) ) { continue ; } SettableBeanProperty prop = _beanProperties . find ( propName ) ; if ( prop != null ) { buffer . bufferProperty ( prop , prop . deserialize ( p , ctxt ) ) ; continue ; } if ( _ignorableProps != null && _ignorableProps . contains ( propName ) ) { handleIgnoredProperty ( p , ctxt , handledType ( ) , propName ) ; continue ; } tokens . writeFieldName ( propName ) ; tokens . copyCurrentStructure ( p ) ; if ( _anySetter != null ) { buffer . bufferAnyProperty ( _anySetter , propName , _anySetter . deserialize ( p , ctxt ) ) ; } } Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { return wrapInstantiationProblem ( e , ctxt ) ; } return _unwrappedPropertyHandler . processUnwrapped ( p , ctxt , bean , tokens ) ; }",
        "fixed_src": "@SuppressWarnings(\"resource\")\n    protected Object deserializeUsingPropertyBasedWithUnwrapped(JsonParser p,\n    \t\tDeserializationContext ctxt)\n        throws IOException, JsonProcessingException\n    {\n        final PropertyBasedCreator creator = _propertyBasedCreator;\n        PropertyValueBuffer buffer = creator.startBuilding(p, ctxt, _objectIdReader);\n\n        TokenBuffer tokens = new TokenBuffer(p, ctxt);\n        tokens.writeStartObject();\n\n        JsonToken t = p.getCurrentToken();\n        for (; t == JsonToken.FIELD_NAME; t = p.nextToken()) {\n            String propName = p.getCurrentName();\n            p.nextToken(); // to point to value\n            // creator property?\n            SettableBeanProperty creatorProp = creator.findCreatorProperty(propName);\n            if (creatorProp != null) {\n                buffer.assignParameter(creatorProp, creatorProp.deserialize(p, ctxt));\n                continue;\n            }\n            // Object Id property?\n            if (buffer.readIdProperty(propName)) {\n                continue;\n            }\n            // regular property? needs buffering\n            SettableBeanProperty prop = _beanProperties.find(propName);\n            if (prop != null) {\n                buffer.bufferProperty(prop, prop.deserialize(p, ctxt));\n                continue;\n            }\n            if (_ignorableProps != null && _ignorableProps.contains(propName)) {\n                handleIgnoredProperty(p, ctxt, handledType(), propName);\n                continue;\n            }\n            tokens.writeFieldName(propName);\n            tokens.copyCurrentStructure(p);\n            // \"any property\"?\n            if (_anySetter != null) {\n                buffer.bufferAnyProperty(_anySetter, propName, _anySetter.deserialize(p, ctxt));\n            }\n        }\n\n        // We hit END_OBJECT, so:\n        Object bean;\n        // !!! 15-Feb-2012, tatu: Need to modify creator to use Builder!\n        try {\n            bean = creator.build(ctxt, buffer);\n        } catch (Exception e) {\n            return wrapInstantiationProblem(e, ctxt);\n        }\n        return _unwrappedPropertyHandler.processUnwrapped(p, ctxt, bean, tokens);\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"resource\" ) protected Object deserializeUsingPropertyBasedWithUnwrapped ( JsonParser p , DeserializationContext ctxt ) throws IOException , JsonProcessingException { final PropertyBasedCreator creator = _propertyBasedCreator ; PropertyValueBuffer buffer = creator . startBuilding ( p , ctxt , _objectIdReader ) ; TokenBuffer tokens = new TokenBuffer ( p , ctxt ) ; tokens . writeStartObject ( ) ; JsonToken t = p . getCurrentToken ( ) ; for ( ; t == JsonToken . FIELD_NAME ; t = p . nextToken ( ) ) { String propName = p . getCurrentName ( ) ; p . nextToken ( ) ; SettableBeanProperty creatorProp = creator . findCreatorProperty ( propName ) ; if ( creatorProp != null ) { buffer . assignParameter ( creatorProp , creatorProp . deserialize ( p , ctxt ) ) ; continue ; } if ( buffer . readIdProperty ( propName ) ) { continue ; } SettableBeanProperty prop = _beanProperties . find ( propName ) ; if ( prop != null ) { buffer . bufferProperty ( prop , prop . deserialize ( p , ctxt ) ) ; continue ; } if ( _ignorableProps != null && _ignorableProps . contains ( propName ) ) { handleIgnoredProperty ( p , ctxt , handledType ( ) , propName ) ; continue ; } tokens . writeFieldName ( propName ) ; tokens . copyCurrentStructure ( p ) ; if ( _anySetter != null ) { buffer . bufferAnyProperty ( _anySetter , propName , _anySetter . deserialize ( p , ctxt ) ) ; } } Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { return wrapInstantiationProblem ( e , ctxt ) ; } return _unwrappedPropertyHandler . processUnwrapped ( p , ctxt , bean , tokens ) ; }",
        "summary": "Missing properties when deserializing using a builder class with a non-default constructor and a mutator annotated with `@JsonUnwrapped`",
        "Description": "When deserializing using a builder class with a non-default constructor and any number of mutator methods annotated with @JsonUnwrapped, the `BuilderBasedDeserializer::deserializeUsingPropertyBasedWithUnwrapped` method cuts short the process of adding SettableBeanProperties.\r\n\r\nThe logic dictates that once all properties necessary to construct the builder have been found, the builder is constructed using all known SettableBeanProperties that have been found up to that point in the tokenizing process.\r\n\r\nTherefore, in the case that the builder has a single property required for construction, and that property is found anywhere other than at the end of the JSON content, any properties subsequent to the constructor property are not evaluated and are left with their default values.\r\n\r\nGiven the following classes:\r\n```java\r\n@JsonDeserialize(builder = Employee.Builder.class)\r\npublic class Employee {\r\n    private final long id;\r\n    private final Name name;\r\n    private final int age;\r\n\r\n    private Employee(Builder builder) {\r\n        id = builder.id;\r\n        name = builder.name;\r\n        age = builder.age;\r\n    }\r\n\r\n    public long getId() {\r\n        return id;\r\n    }\r\n\r\n    public Name getName() {\r\n        return name;\r\n    }\r\n\r\n    public int getAge() {\r\n        return age;\r\n    }\r\n\r\n    @JsonPOJOBuilder(withPrefix = \"set\")\r\n    public static class Builder {\r\n        private final long id;\r\n        private Name name;\r\n        private int age;\r\n\r\n        @JsonCreator\r\n        public Builder(@JsonProperty(\"emp_id\") long id) {\r\n            this.id = id;\r\n        }\r\n\r\n        @JsonUnwrapped\r\n        public void setName(Name name) {\r\n            this.name = name;\r\n        }\r\n\r\n        @JsonProperty(\"emp_age\")\r\n        public void setAge(int age) {\r\n            this.age = age;\r\n        }\r\n\r\n        public Employee build() {\r\n            return new Employee(this);\r\n        }\r\n    }\r\n}\r\n\r\npublic class Name {\r\n    private final String first;\r\n    private final String last;\r\n\r\n    @JsonCreator\r\n    public Name(\r\n        @JsonProperty(\"emp_first_name\") String first,\r\n        @JsonProperty(\"emp_last_name\") String last\r\n    ) {\r\n        this.first = first;\r\n        this.last = last;\r\n    }\r\n\r\n    public String getFirst() {\r\n        return first;\r\n    }\r\n\r\n    public String getLast() {\r\n        return last;\r\n    }\r\n}\r\n```\r\nAnd given the following JSON string:\r\n```json\r\n{\r\n    \"emp_age\": 30,\r\n    \"emp_id\": 1234,\r\n    \"emp_first_name\": \"John\",\r\n    \"emp_last_name\": \"Doe\"\r\n}\r\n```\r\nWe will see the following output:\r\n```java\r\nEmployee emp = new ObjectMapper().readValue(json, Employee.class);\r\n\r\nSystem.out.println(emp.getAge()); // 30\r\nSystem.out.println(emp.getId()); // 1234\r\nSystem.out.println(emp.getName()); // null\r\n```\r\nHowever, if we place the `emp_id` property at the end of the JSON string, we would get the following output:\r\n```java\r\nEmployee emp = new ObjectMapper().readValue(json, Employee.class);\r\n\r\nSystem.out.println(emp.getAge()); // 30\r\nSystem.out.println(emp.getId()); // 1234\r\nSystem.out.println(emp.getName()); // Name Object\r\n```\r\nIf we were to place `emp_age` and `emp_first_name` and `emp_last_name` all after the `emp_id` property in the JSON string, we would get the following output:\r\n```java\r\nEmployee emp = new ObjectMapper().readValue(json, Employee.class);\r\n\r\nSystem.out.println(emp.getAge()); // 0\r\nSystem.out.println(emp.getId()); // 1234\r\nSystem.out.println(emp.getName()); // null\r\n```",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug appears to be related to a problem with the program's memory management. The solution to this bug is to ensure that the program is properly managing its memory, either by increasing the amount of memory allocated to the program or by optimizing the memory usage of the program. Additionally, the code should be checked for any potential memory leaks or other issues that could be causing the bug."
    },
    "JacksonDatabind_99_src/main/java/com/fasterxml/jackson/databind/type/ReferenceType.java_162_170": {
        "src": "@Override\n    protected String buildCanonicalName()\n    {\n        StringBuilder sb = new StringBuilder();\n        sb.append(_class.getName());\n        sb.append('<');\n        sb.append(_referencedType.toCanonical());\n        return sb.toString();\n    }",
        "src_wo_comments": "@ Override protected String buildCanonicalName ( ) { StringBuilder sb = new StringBuilder ( ) ; sb . append ( _class . getName ( ) ) ; sb . append ( '<' ) ; sb . append ( _referencedType . toCanonical ( ) ) ; return sb . toString ( ) ; }",
        "fixed_src": "@Override\n    protected String buildCanonicalName()\n    {\n        StringBuilder sb = new StringBuilder();\n        sb.append(_class.getName());\n        sb.append('<');\n        sb.append(_referencedType.toCanonical());\n        sb.append('>');\n        return sb.toString();\n    }",
        "fixed_src_wo_comments": "@ Override protected String buildCanonicalName ( ) { StringBuilder sb = new StringBuilder ( ) ; sb . append ( _class . getName ( ) ) ; sb . append ( '<' ) ; sb . append ( _referencedType . toCanonical ( ) ) ; sb . append ( '>' ) ; return sb . toString ( ) ; }",
        "summary": "Canonical string for reference type is built incorrectly",
        "Description": "Canonical string for reference type is built incorrectly. \r\nE.g.:\r\n`new ReferenceType(new TypeFactory(new LRUMap<Object, JavaType>(0, 10000)).constructType(Object.class), new PlaceholderForType(0)).toCanonical()`\r\nyields:\r\n`java.lang.Object<$1`\r\nwhile the expected value is:\r\n`java.lang.Object<$1>`",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this and pointing out the fix!\r\nI had to manually merge this just for ease of backporting (will be in 2.8/2.9/2.10/3.x), but fix is the suggested one."
            }
        ],
        "summarized_discussion": "\n\nThe suggested solution to the source code bug is to manually merge the fix, which will be included in versions 2.8, 2.9, 2.10, and 3.x."
    },
    "Math_44_src/main/java/org/apache/commons/math/ode/AbstractIntegrator.java_274_374": {
        "src": "protected double acceptStep(final AbstractStepInterpolator interpolator,\n                                final double[] y, final double[] yDot, final double tEnd)\n        throws MathIllegalStateException {\n\n            double previousT = interpolator.getGlobalPreviousTime();\n            final double currentT = interpolator.getGlobalCurrentTime();\n            resetOccurred = false;\n\n            // initialize the events states if needed\n            if (! statesInitialized) {\n                for (EventState state : eventsStates) {\n                    state.reinitializeBegin(interpolator);\n                }\n                statesInitialized = true;\n            }\n\n            // search for next events that may occur during the step\n            final int orderingSign = interpolator.isForward() ? +1 : -1;\n            SortedSet<EventState> occuringEvents = new TreeSet<EventState>(new Comparator<EventState>() {\n\n                /** {@inheritDoc} */\n                public int compare(EventState es0, EventState es1) {\n                    return orderingSign * Double.compare(es0.getEventTime(), es1.getEventTime());\n                }\n\n            });\n\n            for (final EventState state : eventsStates) {\n                if (state.evaluateStep(interpolator)) {\n                    // the event occurs during the current step\n                    occuringEvents.add(state);\n                }\n            }\n\n            while (!occuringEvents.isEmpty()) {\n\n                // handle the chronologically first event\n                final Iterator<EventState> iterator = occuringEvents.iterator();\n                final EventState currentEvent = iterator.next();\n                iterator.remove();\n\n                // restrict the interpolator to the first part of the step, up to the event\n                final double eventT = currentEvent.getEventTime();\n                interpolator.setSoftPreviousTime(previousT);\n                interpolator.setSoftCurrentTime(eventT);\n\n                // trigger the event\n                interpolator.setInterpolatedTime(eventT);\n                final double[] eventY = interpolator.getInterpolatedState();\n                currentEvent.stepAccepted(eventT, eventY);\n                isLastStep = currentEvent.stop();\n\n                // handle the first part of the step, up to the event\n                for (final StepHandler handler : stepHandlers) {\n                    handler.handleStep(interpolator, isLastStep);\n                }\n\n                if (isLastStep) {\n                    // the event asked to stop integration\n                    System.arraycopy(eventY, 0, y, 0, y.length);\n                    return eventT;\n                }\n\n                if (currentEvent.reset(eventT, eventY)) {\n                    // some event handler has triggered changes that\n                    // invalidate the derivatives, we need to recompute them\n                    System.arraycopy(eventY, 0, y, 0, y.length);\n                    computeDerivatives(eventT, y, yDot);\n                    resetOccurred = true;\n                    return eventT;\n                }\n\n                // prepare handling of the remaining part of the step\n                previousT = eventT;\n                interpolator.setSoftPreviousTime(eventT);\n                interpolator.setSoftCurrentTime(currentT);\n\n                // check if the same event occurs again in the remaining part of the step\n                if (currentEvent.evaluateStep(interpolator)) {\n                    // the event occurs during the current step\n                    occuringEvents.add(currentEvent);\n                }\n\n            }\n\n            interpolator.setInterpolatedTime(currentT);\n            final double[] currentY = interpolator.getInterpolatedState();\n            for (final EventState state : eventsStates) {\n                state.stepAccepted(currentT, currentY);\n                isLastStep = isLastStep || state.stop();\n            }\n            isLastStep = isLastStep || Precision.equals(currentT, tEnd, 1);\n\n            // handle the remaining part of the step, after all events if any\n            for (StepHandler handler : stepHandlers) {\n                handler.handleStep(interpolator, isLastStep);\n            }\n\n            return currentT;\n\n    }",
        "src_wo_comments": "protected double acceptStep ( final AbstractStepInterpolator interpolator , final double [ ] y , final double [ ] yDot , final double tEnd ) throws MathIllegalStateException { double previousT = interpolator . getGlobalPreviousTime ( ) ; final double currentT = interpolator . getGlobalCurrentTime ( ) ; resetOccurred = false ; if ( ! statesInitialized ) { for ( EventState state : eventsStates ) { state . reinitializeBegin ( interpolator ) ; } statesInitialized = true ; } final int orderingSign = interpolator . isForward ( ) ? + 1 : - 1 ; SortedSet < EventState > occuringEvents = new TreeSet < EventState > ( new Comparator < EventState > ( ) { public int compare ( EventState es0 , EventState es1 ) { return orderingSign * Double . compare ( es0 . getEventTime ( ) , es1 . getEventTime ( ) ) ; } } ) ; for ( final EventState state : eventsStates ) { if ( state . evaluateStep ( interpolator ) ) { occuringEvents . add ( state ) ; } } while ( ! occuringEvents . isEmpty ( ) ) { final Iterator < EventState > iterator = occuringEvents . iterator ( ) ; final EventState currentEvent = iterator . next ( ) ; iterator . remove ( ) ; final double eventT = currentEvent . getEventTime ( ) ; interpolator . setSoftPreviousTime ( previousT ) ; interpolator . setSoftCurrentTime ( eventT ) ; interpolator . setInterpolatedTime ( eventT ) ; final double [ ] eventY = interpolator . getInterpolatedState ( ) ; currentEvent . stepAccepted ( eventT , eventY ) ; isLastStep = currentEvent . stop ( ) ; for ( final StepHandler handler : stepHandlers ) { handler . handleStep ( interpolator , isLastStep ) ; } if ( isLastStep ) { System . arraycopy ( eventY , 0 , y , 0 , y . length ) ; return eventT ; } if ( currentEvent . reset ( eventT , eventY ) ) { System . arraycopy ( eventY , 0 , y , 0 , y . length ) ; computeDerivatives ( eventT , y , yDot ) ; resetOccurred = true ; return eventT ; } previousT = eventT ; interpolator . setSoftPreviousTime ( eventT ) ; interpolator . setSoftCurrentTime ( currentT ) ; if ( currentEvent . evaluateStep ( interpolator ) ) { occuringEvents . add ( currentEvent ) ; } } interpolator . setInterpolatedTime ( currentT ) ; final double [ ] currentY = interpolator . getInterpolatedState ( ) ; for ( final EventState state : eventsStates ) { state . stepAccepted ( currentT , currentY ) ; isLastStep = isLastStep || state . stop ( ) ; } isLastStep = isLastStep || Precision . equals ( currentT , tEnd , 1 ) ; for ( StepHandler handler : stepHandlers ) { handler . handleStep ( interpolator , isLastStep ) ; } return currentT ; }",
        "fixed_src": "protected double acceptStep(final AbstractStepInterpolator interpolator,\n                                final double[] y, final double[] yDot, final double tEnd)\n        throws MathIllegalStateException {\n\n            double previousT = interpolator.getGlobalPreviousTime();\n            final double currentT = interpolator.getGlobalCurrentTime();\n\n            // initialize the events states if needed\n            if (! statesInitialized) {\n                for (EventState state : eventsStates) {\n                    state.reinitializeBegin(interpolator);\n                }\n                statesInitialized = true;\n            }\n\n            // search for next events that may occur during the step\n            final int orderingSign = interpolator.isForward() ? +1 : -1;\n            SortedSet<EventState> occuringEvents = new TreeSet<EventState>(new Comparator<EventState>() {\n\n                /** {@inheritDoc} */\n                public int compare(EventState es0, EventState es1) {\n                    return orderingSign * Double.compare(es0.getEventTime(), es1.getEventTime());\n                }\n\n            });\n\n            for (final EventState state : eventsStates) {\n                if (state.evaluateStep(interpolator)) {\n                    // the event occurs during the current step\n                    occuringEvents.add(state);\n                }\n            }\n\n            while (!occuringEvents.isEmpty()) {\n\n                // handle the chronologically first event\n                final Iterator<EventState> iterator = occuringEvents.iterator();\n                final EventState currentEvent = iterator.next();\n                iterator.remove();\n\n                // restrict the interpolator to the first part of the step, up to the event\n                final double eventT = currentEvent.getEventTime();\n                interpolator.setSoftPreviousTime(previousT);\n                interpolator.setSoftCurrentTime(eventT);\n\n                // trigger the event\n                interpolator.setInterpolatedTime(eventT);\n                final double[] eventY = interpolator.getInterpolatedState();\n                currentEvent.stepAccepted(eventT, eventY);\n                isLastStep = currentEvent.stop();\n\n                // handle the first part of the step, up to the event\n                for (final StepHandler handler : stepHandlers) {\n                    handler.handleStep(interpolator, isLastStep);\n                }\n\n                if (isLastStep) {\n                    // the event asked to stop integration\n                    System.arraycopy(eventY, 0, y, 0, y.length);\n                    for (final EventState remaining : occuringEvents) {\n                        remaining.stepAccepted(eventT, eventY);\n                    }\n                    return eventT;\n                }\n\n                if (currentEvent.reset(eventT, eventY)) {\n                    // some event handler has triggered changes that\n                    // invalidate the derivatives, we need to recompute them\n                    System.arraycopy(eventY, 0, y, 0, y.length);\n                    computeDerivatives(eventT, y, yDot);\n                    resetOccurred = true;\n                    for (final EventState remaining : occuringEvents) {\n                        remaining.stepAccepted(eventT, eventY);\n                    }\n                    return eventT;\n                }\n\n                // prepare handling of the remaining part of the step\n                previousT = eventT;\n                interpolator.setSoftPreviousTime(eventT);\n                interpolator.setSoftCurrentTime(currentT);\n\n                // check if the same event occurs again in the remaining part of the step\n                if (currentEvent.evaluateStep(interpolator)) {\n                    // the event occurs during the current step\n                    occuringEvents.add(currentEvent);\n                }\n\n            }\n\n            interpolator.setInterpolatedTime(currentT);\n            final double[] currentY = interpolator.getInterpolatedState();\n            for (final EventState state : eventsStates) {\n                state.stepAccepted(currentT, currentY);\n                isLastStep = isLastStep || state.stop();\n            }\n            isLastStep = isLastStep || Precision.equals(currentT, tEnd, 1);\n\n            // handle the remaining part of the step, after all events if any\n            for (StepHandler handler : stepHandlers) {\n                handler.handleStep(interpolator, isLastStep);\n            }\n\n            return currentT;\n\n    }",
        "fixed_src_wo_comments": "protected double acceptStep ( final AbstractStepInterpolator interpolator , final double [ ] y , final double [ ] yDot , final double tEnd ) throws MathIllegalStateException { double previousT = interpolator . getGlobalPreviousTime ( ) ; final double currentT = interpolator . getGlobalCurrentTime ( ) ; if ( ! statesInitialized ) { for ( EventState state : eventsStates ) { state . reinitializeBegin ( interpolator ) ; } statesInitialized = true ; } final int orderingSign = interpolator . isForward ( ) ? + 1 : - 1 ; SortedSet < EventState > occuringEvents = new TreeSet < EventState > ( new Comparator < EventState > ( ) { public int compare ( EventState es0 , EventState es1 ) { return orderingSign * Double . compare ( es0 . getEventTime ( ) , es1 . getEventTime ( ) ) ; } } ) ; for ( final EventState state : eventsStates ) { if ( state . evaluateStep ( interpolator ) ) { occuringEvents . add ( state ) ; } } while ( ! occuringEvents . isEmpty ( ) ) { final Iterator < EventState > iterator = occuringEvents . iterator ( ) ; final EventState currentEvent = iterator . next ( ) ; iterator . remove ( ) ; final double eventT = currentEvent . getEventTime ( ) ; interpolator . setSoftPreviousTime ( previousT ) ; interpolator . setSoftCurrentTime ( eventT ) ; interpolator . setInterpolatedTime ( eventT ) ; final double [ ] eventY = interpolator . getInterpolatedState ( ) ; currentEvent . stepAccepted ( eventT , eventY ) ; isLastStep = currentEvent . stop ( ) ; for ( final StepHandler handler : stepHandlers ) { handler . handleStep ( interpolator , isLastStep ) ; } if ( isLastStep ) { System . arraycopy ( eventY , 0 , y , 0 , y . length ) ; for ( final EventState remaining : occuringEvents ) { remaining . stepAccepted ( eventT , eventY ) ; } return eventT ; } if ( currentEvent . reset ( eventT , eventY ) ) { System . arraycopy ( eventY , 0 , y , 0 , y . length ) ; computeDerivatives ( eventT , y , yDot ) ; resetOccurred = true ; for ( final EventState remaining : occuringEvents ) { remaining . stepAccepted ( eventT , eventY ) ; } return eventT ; } previousT = eventT ; interpolator . setSoftPreviousTime ( eventT ) ; interpolator . setSoftCurrentTime ( currentT ) ; if ( currentEvent . evaluateStep ( interpolator ) ) { occuringEvents . add ( currentEvent ) ; } } interpolator . setInterpolatedTime ( currentT ) ; final double [ ] currentY = interpolator . getInterpolatedState ( ) ; for ( final EventState state : eventsStates ) { state . stepAccepted ( currentT , currentY ) ; isLastStep = isLastStep || state . stop ( ) ; } isLastStep = isLastStep || Precision . equals ( currentT , tEnd , 1 ) ; for ( StepHandler handler : stepHandlers ) { handler . handleStep ( interpolator , isLastStep ) ; } return currentT ; }",
        "summary": "Incomplete reinitialization with some events handling",
        "Description": "I get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way.\nI can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-695",
        "comments": [
            "this patch resolves my problem but breaks some unit tests.",
            "As I work with Pascal (we share the same office at work), I have seen how the bug occurs and am trying to set up a simplified test case that reproduces it. It seems that there is a combination of conditions that is not handled properly. We have seen the bug occurring when both following conditions are true:\n\n* there are several events occurring in the same integration step\n* when one of the earliest events occurrence is triggered, it returns RESET_DERIVATIVES\n\nIn this case, the acceptStep method in AbstractIntegrator returns early from inside the while loop and the remaining events that where expected to occur later on are left in an inconsistent state with respect to the integrator. The t0 and g0 fields in the corresponding EventState instance still contain values from the beginning of the step, they do not reflect the fact the event has been triggered. This implies that when next step is started with the updated derivatives, evaluateStep tries to catch up from t0 to current t and calls the g function at times that do not belong to the current step.\n\nUp to now, I have not been able to set up a simplified test case that exhibits this, but I'm working on it.",
            "The attached test case reproduces the error.",
            "Fixed in subversion repository as of r1189086.\n\nThe fix is different from the proposed patch, it directly updates the events when the step truncation occurs, thus preventing even transient inconsistency.\n\nThanks for the report and for the patch."
        ],
        "summarized_discussion": "\n\nThe bug occurs when several events occur in the same integration step and one of the earliest events returns RESET_DERIVATIVES. This causes the acceptStep method in AbstractIntegrator to return early from inside the while loop and the remaining events are left in an inconsistent state. A test case was created to reproduce the error and the fix is different from the proposed patch, it directly updates the events when the step truncation occurs, thus preventing even transient inconsistency. The bug was fixed in the subversion repository as of r1189086."
    },
    "JacksonDatabind_60_src/main/java/com/fasterxml/jackson/databind/ser/std/JsonValueSerializer.java_195_242": {
        "src": "@Override\n    public void serializeWithType(Object bean, JsonGenerator gen, SerializerProvider provider,\n            TypeSerializer typeSer0) throws IOException\n    {\n        // Regardless of other parts, first need to find value to serialize:\n        Object value = null;\n        try {\n            value = _accessorMethod.getValue(bean);\n            // and if we got null, can also just write it directly\n            if (value == null) {\n                provider.defaultSerializeNull(gen);\n                return;\n            }\n            JsonSerializer<Object> ser = _valueSerializer;\n            if (ser == null) { // no serializer yet? Need to fetch\n//                ser = provider.findTypedValueSerializer(value.getClass(), true, _property);\n                ser = provider.findValueSerializer(value.getClass(), _property);\n            } else {\n                /* 09-Dec-2010, tatu: To work around natural type's refusal to add type info, we do\n                 *    this (note: type is for the wrapper type, not enclosed value!)\n                 */\n                if (_forceTypeInformation) {\n                    typeSer0.writeTypePrefixForScalar(bean, gen);\n                    ser.serialize(value, gen, provider);\n                    typeSer0.writeTypeSuffixForScalar(bean, gen);\n                    return;\n                }\n            }\n            // 28-Sep-2016, tatu: As per [databind#1385], we do need to do some juggling\n            //    to use different Object for type id (logical type) and actual serialization\n            //    (delegat type).\n            ser.serializeWithType(value, gen, provider, typeSer0);\n        } catch (IOException ioe) {\n            throw ioe;\n        } catch (Exception e) {\n            Throwable t = e;\n            // Need to unwrap this specific type, to see infinite recursion...\n            while (t instanceof InvocationTargetException && t.getCause() != null) {\n                t = t.getCause();\n            }\n            // Errors shouldn't be wrapped (and often can't, as well)\n            if (t instanceof Error) {\n                throw (Error) t;\n            }\n            // let's try to indicate the path best we can...\n            throw JsonMappingException.wrapWithPath(t, bean, _accessorMethod.getName() + \"()\");\n        }\n    }",
        "src_wo_comments": "@ Override public void serializeWithType ( Object bean , JsonGenerator gen , SerializerProvider provider , TypeSerializer typeSer0 ) throws IOException { Object value = null ; try { value = _accessorMethod . getValue ( bean ) ; if ( value == null ) { provider . defaultSerializeNull ( gen ) ; return ; } JsonSerializer < Object > ser = _valueSerializer ; if ( ser == null ) { ser = provider . findValueSerializer ( value . getClass ( ) , _property ) ; } else { if ( _forceTypeInformation ) { typeSer0 . writeTypePrefixForScalar ( bean , gen ) ; ser . serialize ( value , gen , provider ) ; typeSer0 . writeTypeSuffixForScalar ( bean , gen ) ; return ; } } ser . serializeWithType ( value , gen , provider , typeSer0 ) ; } catch ( IOException ioe ) { throw ioe ; } catch ( Exception e ) { Throwable t = e ; while ( t instanceof InvocationTargetException && t . getCause ( ) != null ) { t = t . getCause ( ) ; } if ( t instanceof Error ) { throw ( Error ) t ; } throw JsonMappingException . wrapWithPath ( t , bean , _accessorMethod . getName ( ) + \"()\" ) ; } }",
        "fixed_src": "@Override\n    public void serializeWithType(Object bean, JsonGenerator gen, SerializerProvider provider,\n            TypeSerializer typeSer0) throws IOException\n    {\n        // Regardless of other parts, first need to find value to serialize:\n        Object value = null;\n        try {\n            value = _accessorMethod.getValue(bean);\n            // and if we got null, can also just write it directly\n            if (value == null) {\n                provider.defaultSerializeNull(gen);\n                return;\n            }\n            JsonSerializer<Object> ser = _valueSerializer;\n            if (ser == null) { // no serializer yet? Need to fetch\n//                ser = provider.findTypedValueSerializer(value.getClass(), true, _property);\n                ser = provider.findValueSerializer(value.getClass(), _property);\n            } else {\n                /* 09-Dec-2010, tatu: To work around natural type's refusal to add type info, we do\n                 *    this (note: type is for the wrapper type, not enclosed value!)\n                 */\n                if (_forceTypeInformation) {\n                    typeSer0.writeTypePrefixForScalar(bean, gen);\n                    ser.serialize(value, gen, provider);\n                    typeSer0.writeTypeSuffixForScalar(bean, gen);\n                    return;\n                }\n            }\n            // 28-Sep-2016, tatu: As per [databind#1385], we do need to do some juggling\n            //    to use different Object for type id (logical type) and actual serialization\n            //    (delegat type).\n            TypeSerializerRerouter rr = new TypeSerializerRerouter(typeSer0, bean);\n            ser.serializeWithType(value, gen, provider, rr);\n        } catch (IOException ioe) {\n            throw ioe;\n        } catch (Exception e) {\n            Throwable t = e;\n            // Need to unwrap this specific type, to see infinite recursion...\n            while (t instanceof InvocationTargetException && t.getCause() != null) {\n                t = t.getCause();\n            }\n            // Errors shouldn't be wrapped (and often can't, as well)\n            if (t instanceof Error) {\n                throw (Error) t;\n            }\n            // let's try to indicate the path best we can...\n            throw JsonMappingException.wrapWithPath(t, bean, _accessorMethod.getName() + \"()\");\n        }\n    }",
        "fixed_src_wo_comments": "@ Override public void serializeWithType ( Object bean , JsonGenerator gen , SerializerProvider provider , TypeSerializer typeSer0 ) throws IOException { Object value = null ; try { value = _accessorMethod . getValue ( bean ) ; if ( value == null ) { provider . defaultSerializeNull ( gen ) ; return ; } JsonSerializer < Object > ser = _valueSerializer ; if ( ser == null ) { ser = provider . findValueSerializer ( value . getClass ( ) , _property ) ; } else { if ( _forceTypeInformation ) { typeSer0 . writeTypePrefixForScalar ( bean , gen ) ; ser . serialize ( value , gen , provider ) ; typeSer0 . writeTypeSuffixForScalar ( bean , gen ) ; return ; } } TypeSerializerRerouter rr = new TypeSerializerRerouter ( typeSer0 , bean ) ; ser . serializeWithType ( value , gen , provider , rr ) ; } catch ( IOException ioe ) { throw ioe ; } catch ( Exception e ) { Throwable t = e ; while ( t instanceof InvocationTargetException && t . getCause ( ) != null ) { t = t . getCause ( ) ; } if ( t instanceof Error ) { throw ( Error ) t ; } throw JsonMappingException . wrapWithPath ( t , bean , _accessorMethod . getName ( ) + \"()\" ) ; } }",
        "summary": "Polymorphic type lost when using `@JsonValue`",
        "Description": "When suppressing all getters but one with @JsonIgnore and choosing to use a byte array for serialization (marking its getter with @JsonValue), the typing of the object is changed to \"[B\", which is deserialized to a byte array. I would have expected verbose typing and usage of the constructor  marked with @JsonCreator that accepts the byte array to construct the object on deserialization. The behavior is as expected when choosing more fields for serialization, which is redundant data in this case.\n\nRunning  jackson-databind 2.7.4 on Java 1.8.0_91.\n\nConfiguration of the ObjectMapper:\n\n```\nprivate final ObjectMapper mapper;\npublic JsonFilter() {\n    this.mapper = new ObjectMapper();\n    mapper.configure(SerializationFeature.FAIL_ON_EMPTY_BEANS, false);\n    mapper.enableDefaultTyping();\n}\n```\n\nSerialization: `mapper.writeValueAsString(message)`\nDeserialization: `mapper.readValue(json, RemoteCall.class)`\n\nGetter and field:\n\n```\n/** @serial */\nprivate byte[] apdu;\n\n@JsonValue\npublic byte[] getBytes() {\n    return apdu.clone();\n}\n```\n\nConstructor:\n\n```\n@JsonCreator\npublic CommandAPDU(@JsonProperty(value = \"bytes\") byte[] apdu) {\n    this.apdu = apdu.clone();\n    parse();\n    LOG.v(\"com.ubitricity.devices.common.pal.CommandAPDU creator (1)\");\n}\n```\n\nSerializes to `\"args\":[[\"[B\",\"AKQEAAnw8fLz9AAAAgA=\"],[\"net.sf.lipermi.call.RemoteInstance\",{\"instanceId\":\"b0e15098-f49e-4328-b072-fc5df42799bd\",\"className\":\"com.ubitricity.devices.common.tasks.ResponseReceiver\"}]]` where \"args\" is an Object array field of the enclosing object.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "First things first: `[B` actually seems like proper class signature from JVM perspective (primitives have single letter type as there is no JDK visible external class; and `[` is array indicator). So that part may be fine in itself.\n\nBut bigger question is whether polymorphic typing can work with `@JsonValue`. The problem is that type information during serialization should refer to `CommandAPDU`, which is the logical type, even if physical serialization is as `byte[]`. In that sense class included seems wrong.\n\nI'll see if I can reproduce with given information.\n"
            },
            {
                "content": "Hmmh. Ok, I do see the underlying problem with delegation. Type id must be written for original type; but serialize to use (and value) should be return value of `@JsonValue` annotated method. This almost certainly requires some (internal, I hope) API changes, which means 2.9; not backportable fix.\n"
            },
            {
                "content": "Ok, one minor change to code: it should be:\n\n``` java\n@JsonCreator\npublic CommandAPDU(byte[] apdu) {\n    this.apdu = apdu.clone();\n    parse();\n    LOG.v(\"com.ubitricity.devices.common.pal.CommandAPDU creator (1)\");\n}\n```\n\nthat is, ensure it's delegating Creator, NOT property-one.\n\nThat's not the main problem, but once serialization works, this is what would prevent deser.\n\nIt also looks like I can actually fix this for 2.8, and no new API is needed.\n"
            },
            {
                "content": "Thanks for looking into it! Greatly appreciated!\n"
            },
            {
                "content": "Hi,\r\n\r\nEither fix [#1385](https://github.com/FasterXML/jackson-databind/issues/1385) or [#466](https://github.com/FasterXML/jackson-databind/issues/466) causes problem in [CAMEL-11308](https://issues.apache.org/jira/browse/CAMEL-11308)\r\n\r\nDowngrade  of jackson2 dependency to 2.8.3 resolves the issue.\r\n\r\nCould you check this cross reference? "
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to downgrade the jackson2 dependency to version 2.8.3, which should resolve the issue. Additionally, a minor change to the code is needed to ensure that the creator is delegating, not property-one."
    },
    "Math_8_src/main/java/org/apache/commons/math3/distribution/DiscreteDistribution.java_181_195": {
        "src": "public T[] sample(int sampleSize) throws NotStrictlyPositiveException {\n        if (sampleSize <= 0) {\n            throw new NotStrictlyPositiveException(LocalizedFormats.NUMBER_OF_SAMPLES,\n                    sampleSize);\n        }\n\n        final T[]out = (T[]) java.lang.reflect.Array.newInstance(singletons.get(0).getClass(), sampleSize);\n\n        for (int i = 0; i < sampleSize; i++) {\n            out[i] = sample();\n        }\n\n        return out;\n\n    }",
        "src_wo_comments": "public T [ ] sample ( int sampleSize ) throws NotStrictlyPositiveException { if ( sampleSize <= 0 ) { throw new NotStrictlyPositiveException ( LocalizedFormats . NUMBER_OF_SAMPLES , sampleSize ) ; } final T [ ] out = ( T [ ] ) java . lang . reflect . Array . newInstance ( singletons . get ( 0 ) . getClass ( ) , sampleSize ) ; for ( int i = 0 ; i < sampleSize ; i ++ ) { out [ i ] = sample ( ) ; } return out ; }",
        "fixed_src": "public Object[] sample(int sampleSize) throws NotStrictlyPositiveException {\n        if (sampleSize <= 0) {\n            throw new NotStrictlyPositiveException(LocalizedFormats.NUMBER_OF_SAMPLES,\n                    sampleSize);\n        }\n\n        final Object[] out = new Object[sampleSize];\n\n        for (int i = 0; i < sampleSize; i++) {\n            out[i] = sample();\n        }\n\n        return out;\n\n    }",
        "fixed_src_wo_comments": "public Object [ ] sample ( int sampleSize ) throws NotStrictlyPositiveException { if ( sampleSize <= 0 ) { throw new NotStrictlyPositiveException ( LocalizedFormats . NUMBER_OF_SAMPLES , sampleSize ) ; } final Object [ ] out = new Object [ sampleSize ] ; for ( int i = 0 ; i < sampleSize ; i ++ ) { out [ i ] = sample ( ) ; } return out ; }",
        "summary": "DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type",
        "Description": "Creating an array with {{Array.newInstance(singletons.get(0).getClass(), sampleSize)}} in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:\n* {{singleons.get(0)}} is of type T1, an sub-class of T, and\n* {{DiscreteDistribution.sample()}} returns an object which is of type T, but not of type T1.\n\nTo reproduce:\n{code}\nList<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();\nlist.add(new Pair<Object, Double>(new Object() {}, new Double(0)));\nlist.add(new Pair<Object, Double>(new Object() {}, new Double(1)));\nnew DiscreteDistribution<Object>(list).sample(1);\n{code}\n\nAttaching a patch.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-942",
        "comments": [
            "We have encountered the same kind of issue with Field implementations. For this reason, we have added the method:\n\n{code}\nClass<? extends FieldElement<T>> getRuntimeClass();\n{code}\n\nin the interface, so appropriate objects can be created (this is what is used in the buildArray method in MathArrays). However, we cannot do the same here because we want to allow any class in the distribution, not only implementations of one of our interfaces.\n\nPassing the runtime class as an argument to the distribution would work, but seems also cumbersome.\n\nI am a bit reluctant to use Object[] as your patch, though. Are you sure it would always work? I know type erasure occurs, but with your patch, we mainly apply it ourselves. Is it safe?",
            "bq. I am a bit reluctant to use Object[] as your patch, though. Are you sure it would always work? I know type erasure occurs, but with your patch, we mainly apply it ourselves. Is it safe?\n\nI did some reading, scanning other generics classes code, and testing. It seems that there are two proper ways to do it:\n# return {{Object[]}}, or\n# return {{List<T>}}.\n\nI suggest returning {{Object[]}}, like {{ArrayList.toArray()}}. I've attached an updated patch.\n\nThen, the following code will work:\n\n{code}\npublic class X {\n\n    public static void main(String[] args) {\n        List<Pair<X, Double>> list = new ArrayList<Pair<X, Double>>();\n        list.add(new Pair<X, Double>(new X(), new Double(1)));\n        X[] xarr;\n        Object[] oarr;\n        try {\n            xarr = (X[]) new DiscreteDistribution<X>(list).sample(1);\n            throw new RuntimeException(\"Expected ClassCastException\");\n        } catch (ClassCastException e) {\n        }\n        oarr = new DiscreteDistribution<X>(list).sample(1);\n    }\n}\n{code}",
            "Fixed in subversion repository as of r1454840.\n\nThanks for the report and for the patch.",
            "What do you think about the following extension:\n\n{noformat}\n    /**\n     * Generate a random sample from the distribution.\n     * <p>\n     * If the requested samples fit in the specified array, it is returned\n     * therein. Otherwise, a new array is allocated with the runtime type of\n     * the specified array and the size of this collection.\n     *\n     * @param sampleSize the number of random values to generate.\n     * @param array the array to populate.\n     * @return an array representing the random sample.\n     * @throws NotStrictlyPositiveException if {@code sampleSize} is not\n     * positive.\n     * @throws IllegalArgumentException if {@code array} is null\n     */\n    public T[] sample(int sampleSize, final T[] array) throws NotStrictlyPositiveException {\n        if (sampleSize <= 0) {\n            throw new NotStrictlyPositiveException(LocalizedFormats.NUMBER_OF_SAMPLES,\n                    sampleSize);\n        }\n\n        if (array == null) {\n            throw new IllegalArgumentException(\"array may not be null\");\n        }\n\n        T[] out;\n        if (array.length < sampleSize) {\n            @SuppressWarnings(\"unchecked\") // safe as both are of type T\n            final T[] unchecked = (T[]) Array.newInstance(array.getClass().getComponentType(), sampleSize);\n            out = unchecked;\n        } else {\n            out = array;\n        }\n\n        for (int i = 0; i < sampleSize; i++) {\n            out[i] = sample();\n        }\n\n        return out;\n\n    }\n{noformat}\n\nSimilar to the toArray(T[]) methods of Collections, a user could already provide an array which is used if the sample size fits into it, otherwise it is created in a type-safe way, and the return-type is of T[] so that a user does not have to do a cast.",
            "The other sample(int sampleSize) method could stay there too. So a user has the choice.",
            "This seems a good idea to me.",
            "Closing issue as version 3.2 has been released on 2013-04-06."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to return an Object[] or List<T> when generating a random sample from the distribution. This will ensure that the code will always work and will allow any class in the distribution. The issue was closed as version 3.2 was released on 2013-04-06."
    },
    "Math_52_src/main/java/org/apache/commons/math/geometry/euclidean/threed/Rotation.java_313_390": {
        "src": "public Rotation(Vector3D u1, Vector3D u2, Vector3D v1, Vector3D v2) {\n\n  // norms computation\n  double u1u1 = u1.getNormSq();\n  double u2u2 = u2.getNormSq();\n  double v1v1 = v1.getNormSq();\n  double v2v2 = v2.getNormSq();\n  if ((u1u1 == 0) || (u2u2 == 0) || (v1v1 == 0) || (v2v2 == 0)) {\n    throw MathRuntimeException.createIllegalArgumentException(LocalizedFormats.ZERO_NORM_FOR_ROTATION_DEFINING_VECTOR);\n  }\n\n  // normalize v1 in order to have (v1'|v1') = (u1|u1)\n  v1 = new Vector3D(FastMath.sqrt(u1u1 / v1v1), v1);\n\n  // adjust v2 in order to have (u1|u2) = (v1'|v2') and (v2'|v2') = (u2|u2)\n  double u1u2   = u1.dotProduct(u2);\n  double v1v2   = v1.dotProduct(v2);\n  double coeffU = u1u2 / u1u1;\n  double coeffV = v1v2 / u1u1;\n  double beta   = FastMath.sqrt((u2u2 - u1u2 * coeffU) / (v2v2 - v1v2 * coeffV));\n  double alpha  = coeffU - beta * coeffV;\n  v2 = new Vector3D(alpha, v1, beta, v2);\n\n  // preliminary computation\n  Vector3D uRef  = u1;\n  Vector3D vRef  = v1;\n  Vector3D v1Su1 = v1.subtract(u1);\n  Vector3D v2Su2 = v2.subtract(u2);\n  Vector3D k     = v1Su1.crossProduct(v2Su2);\n  Vector3D u3    = u1.crossProduct(u2);\n  double c       = k.dotProduct(u3);\n  if (c == 0) {\n    // the (q1, q2, q3) vector is close to the (u1, u2) plane\n    // we try other vectors\n    Vector3D v3 = Vector3D.crossProduct(v1, v2);\n    Vector3D v3Su3 = v3.subtract(u3);\n    k = v1Su1.crossProduct(v3Su3);\n    Vector3D u2Prime = u1.crossProduct(u3);\n    c = k.dotProduct(u2Prime);\n\n    if (c == 0) {\n      // the (q1, q2, q3) vector is also close to the (u1, u3) plane,\n      // it is almost aligned with u1: we try (u2, u3) and (v2, v3)\n      k = v2Su2.crossProduct(v3Su3);;\n      c = k.dotProduct(u2.crossProduct(u3));;\n\n      if (c == 0) {\n        // the (q1, q2, q3) vector is aligned with everything\n        // this is really the identity rotation\n        q0 = 1.0;\n        q1 = 0.0;\n        q2 = 0.0;\n        q3 = 0.0;\n        return;\n      }\n\n      // we will have to use u2 and v2 to compute the scalar part\n      uRef = u2;\n      vRef = v2;\n\n    }\n\n  }\n\n  // compute the vectorial part\n  c = FastMath.sqrt(c);\n  double inv = 1.0 / (c + c);\n  q1 = inv * k.getX();\n  q2 = inv * k.getY();\n  q3 = inv * k.getZ();\n\n  // compute the scalar part\n   k = new Vector3D(uRef.getY() * q3 - uRef.getZ() * q2,\n                    uRef.getZ() * q1 - uRef.getX() * q3,\n                    uRef.getX() * q2 - uRef.getY() * q1);\n  q0 = vRef.dotProduct(k) / (2 * k.getNormSq());\n\n  }",
        "src_wo_comments": "public Rotation ( Vector3D u1 , Vector3D u2 , Vector3D v1 , Vector3D v2 ) { double u1u1 = u1 . getNormSq ( ) ; double u2u2 = u2 . getNormSq ( ) ; double v1v1 = v1 . getNormSq ( ) ; double v2v2 = v2 . getNormSq ( ) ; if ( ( u1u1 == 0 ) || ( u2u2 == 0 ) || ( v1v1 == 0 ) || ( v2v2 == 0 ) ) { throw MathRuntimeException . createIllegalArgumentException ( LocalizedFormats . ZERO_NORM_FOR_ROTATION_DEFINING_VECTOR ) ; } v1 = new Vector3D ( FastMath . sqrt ( u1u1 / v1v1 ) , v1 ) ; double u1u2 = u1 . dotProduct ( u2 ) ; double v1v2 = v1 . dotProduct ( v2 ) ; double coeffU = u1u2 / u1u1 ; double coeffV = v1v2 / u1u1 ; double beta = FastMath . sqrt ( ( u2u2 - u1u2 * coeffU ) / ( v2v2 - v1v2 * coeffV ) ) ; double alpha = coeffU - beta * coeffV ; v2 = new Vector3D ( alpha , v1 , beta , v2 ) ; Vector3D uRef = u1 ; Vector3D vRef = v1 ; Vector3D v1Su1 = v1 . subtract ( u1 ) ; Vector3D v2Su2 = v2 . subtract ( u2 ) ; Vector3D k = v1Su1 . crossProduct ( v2Su2 ) ; Vector3D u3 = u1 . crossProduct ( u2 ) ; double c = k . dotProduct ( u3 ) ; if ( c == 0 ) { Vector3D v3 = Vector3D . crossProduct ( v1 , v2 ) ; Vector3D v3Su3 = v3 . subtract ( u3 ) ; k = v1Su1 . crossProduct ( v3Su3 ) ; Vector3D u2Prime = u1 . crossProduct ( u3 ) ; c = k . dotProduct ( u2Prime ) ; if ( c == 0 ) { k = v2Su2 . crossProduct ( v3Su3 ) ; ; c = k . dotProduct ( u2 . crossProduct ( u3 ) ) ; ; if ( c == 0 ) { q0 = 1.0 ; q1 = 0.0 ; q2 = 0.0 ; q3 = 0.0 ; return ; } uRef = u2 ; vRef = v2 ; } } c = FastMath . sqrt ( c ) ; double inv = 1.0 / ( c + c ) ; q1 = inv * k . getX ( ) ; q2 = inv * k . getY ( ) ; q3 = inv * k . getZ ( ) ; k = new Vector3D ( uRef . getY ( ) * q3 - uRef . getZ ( ) * q2 , uRef . getZ ( ) * q1 - uRef . getX ( ) * q3 , uRef . getX ( ) * q2 - uRef . getY ( ) * q1 ) ; q0 = vRef . dotProduct ( k ) / ( 2 * k . getNormSq ( ) ) ; }",
        "fixed_src": "public Rotation(Vector3D u1, Vector3D u2, Vector3D v1, Vector3D v2) {\n\n  // norms computation\n  double u1u1 = u1.getNormSq();\n  double u2u2 = u2.getNormSq();\n  double v1v1 = v1.getNormSq();\n  double v2v2 = v2.getNormSq();\n  if ((u1u1 == 0) || (u2u2 == 0) || (v1v1 == 0) || (v2v2 == 0)) {\n    throw MathRuntimeException.createIllegalArgumentException(LocalizedFormats.ZERO_NORM_FOR_ROTATION_DEFINING_VECTOR);\n  }\n\n  // normalize v1 in order to have (v1'|v1') = (u1|u1)\n  v1 = new Vector3D(FastMath.sqrt(u1u1 / v1v1), v1);\n\n  // adjust v2 in order to have (u1|u2) = (v1'|v2') and (v2'|v2') = (u2|u2)\n  double u1u2   = u1.dotProduct(u2);\n  double v1v2   = v1.dotProduct(v2);\n  double coeffU = u1u2 / u1u1;\n  double coeffV = v1v2 / u1u1;\n  double beta   = FastMath.sqrt((u2u2 - u1u2 * coeffU) / (v2v2 - v1v2 * coeffV));\n  double alpha  = coeffU - beta * coeffV;\n  v2 = new Vector3D(alpha, v1, beta, v2);\n\n  // preliminary computation\n  Vector3D uRef  = u1;\n  Vector3D vRef  = v1;\n  Vector3D v1Su1 = v1.subtract(u1);\n  Vector3D v2Su2 = v2.subtract(u2);\n  Vector3D k     = v1Su1.crossProduct(v2Su2);\n  Vector3D u3    = u1.crossProduct(u2);\n  double c       = k.dotProduct(u3);\n  final double inPlaneThreshold = 0.001;\n  if (c <= inPlaneThreshold * k.getNorm() * u3.getNorm()) {\n    // the (q1, q2, q3) vector is close to the (u1, u2) plane\n    // we try other vectors\n    Vector3D v3 = Vector3D.crossProduct(v1, v2);\n    Vector3D v3Su3 = v3.subtract(u3);\n    k = v1Su1.crossProduct(v3Su3);\n    Vector3D u2Prime = u1.crossProduct(u3);\n    c = k.dotProduct(u2Prime);\n\n    if (c <= inPlaneThreshold * k.getNorm() * u2Prime.getNorm()) {\n      // the (q1, q2, q3) vector is also close to the (u1, u3) plane,\n      // it is almost aligned with u1: we try (u2, u3) and (v2, v3)\n      k = v2Su2.crossProduct(v3Su3);;\n      c = k.dotProduct(u2.crossProduct(u3));;\n\n      if (c <= 0) {\n        // the (q1, q2, q3) vector is aligned with everything\n        // this is really the identity rotation\n        q0 = 1.0;\n        q1 = 0.0;\n        q2 = 0.0;\n        q3 = 0.0;\n        return;\n      }\n\n      // we will have to use u2 and v2 to compute the scalar part\n      uRef = u2;\n      vRef = v2;\n\n    }\n\n  }\n\n  // compute the vectorial part\n  c = FastMath.sqrt(c);\n  double inv = 1.0 / (c + c);\n  q1 = inv * k.getX();\n  q2 = inv * k.getY();\n  q3 = inv * k.getZ();\n\n  // compute the scalar part\n   k = new Vector3D(uRef.getY() * q3 - uRef.getZ() * q2,\n                    uRef.getZ() * q1 - uRef.getX() * q3,\n                    uRef.getX() * q2 - uRef.getY() * q1);\n  q0 = vRef.dotProduct(k) / (2 * k.getNormSq());\n\n  }",
        "fixed_src_wo_comments": "public Rotation ( Vector3D u1 , Vector3D u2 , Vector3D v1 , Vector3D v2 ) { double u1u1 = u1 . getNormSq ( ) ; double u2u2 = u2 . getNormSq ( ) ; double v1v1 = v1 . getNormSq ( ) ; double v2v2 = v2 . getNormSq ( ) ; if ( ( u1u1 == 0 ) || ( u2u2 == 0 ) || ( v1v1 == 0 ) || ( v2v2 == 0 ) ) { throw MathRuntimeException . createIllegalArgumentException ( LocalizedFormats . ZERO_NORM_FOR_ROTATION_DEFINING_VECTOR ) ; } v1 = new Vector3D ( FastMath . sqrt ( u1u1 / v1v1 ) , v1 ) ; double u1u2 = u1 . dotProduct ( u2 ) ; double v1v2 = v1 . dotProduct ( v2 ) ; double coeffU = u1u2 / u1u1 ; double coeffV = v1v2 / u1u1 ; double beta = FastMath . sqrt ( ( u2u2 - u1u2 * coeffU ) / ( v2v2 - v1v2 * coeffV ) ) ; double alpha = coeffU - beta * coeffV ; v2 = new Vector3D ( alpha , v1 , beta , v2 ) ; Vector3D uRef = u1 ; Vector3D vRef = v1 ; Vector3D v1Su1 = v1 . subtract ( u1 ) ; Vector3D v2Su2 = v2 . subtract ( u2 ) ; Vector3D k = v1Su1 . crossProduct ( v2Su2 ) ; Vector3D u3 = u1 . crossProduct ( u2 ) ; double c = k . dotProduct ( u3 ) ; final double inPlaneThreshold = 0.001 ; if ( c <= inPlaneThreshold * k . getNorm ( ) * u3 . getNorm ( ) ) { Vector3D v3 = Vector3D . crossProduct ( v1 , v2 ) ; Vector3D v3Su3 = v3 . subtract ( u3 ) ; k = v1Su1 . crossProduct ( v3Su3 ) ; Vector3D u2Prime = u1 . crossProduct ( u3 ) ; c = k . dotProduct ( u2Prime ) ; if ( c <= inPlaneThreshold * k . getNorm ( ) * u2Prime . getNorm ( ) ) { k = v2Su2 . crossProduct ( v3Su3 ) ; ; c = k . dotProduct ( u2 . crossProduct ( u3 ) ) ; ; if ( c <= 0 ) { q0 = 1.0 ; q1 = 0.0 ; q2 = 0.0 ; q3 = 0.0 ; return ; } uRef = u2 ; vRef = v2 ; } } c = FastMath . sqrt ( c ) ; double inv = 1.0 / ( c + c ) ; q1 = inv * k . getX ( ) ; q2 = inv * k . getY ( ) ; q3 = inv * k . getZ ( ) ; k = new Vector3D ( uRef . getY ( ) * q3 - uRef . getZ ( ) * q2 , uRef . getZ ( ) * q1 - uRef . getX ( ) * q3 , uRef . getX ( ) * q2 - uRef . getY ( ) * q1 ) ; q0 = vRef . dotProduct ( k ) / ( 2 * k . getNormSq ( ) ) ; }",
        "summary": "numerical problems in rotation creation",
        "Description": "building a rotation from the following vector pairs leads to NaN:\nu1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377\nu2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10\nv1 = 1, 0, 0\nv2 = 0, 0, 1\n\nThe constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:\n <v1'|v1'> == <u1|u1>\n <v2'|v2'> == <u2|u2>\n <u1 |u2>  == <v1'|v2'>\n\nOnce the (v1', v2') pair has been computed, we compute the cross product:\n  k = (v1' - u1)^(v2' - u2)\n\nand the scalar product:\n  c = <k | (u1^u2)>\n\nBy construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)].\nc should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm.\n\nHowever, there are numerical problems with the vector above with the way these computations are done, as shown\nby the following comparisons, showing the result we get from our Java code and the result we get from manual\ncomputation with the same formulas but with enhanced precision:\n\ncommons math:   k = 38514476.5,            -84.,                           -1168590144\nhigh precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208...\n\nand it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get:\ncommons math    c = -1.2397173627587605E20\nhigh precision: c =  558382746168463196.7079627...\n\nWe have lost ALL significant digits in cancellations, and even the sign is wrong!\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-639",
        "comments": [
            "The expected result quaternion computed to high precision manually and checked afterwards is:\nq0 =  0.62283703596082005783621150\nq1 =  0.02577076214564987845778149\nq2 = -0.00000000025030122695149900\nq3 = -0.78192703908611094998656730",
            "The fact that c = <k | (u1^u2)> is really wrong is due to the fact the rotation axis is almost exactly in the (u1, u2) plane. In fact it is only 1.833e-8 degrees (i.e. 3.199e-10 radians) out of the plane!",
            "Fixed in subversion repository as of r1154257."
        ],
        "summarized_discussion": "\n\nThe bug was caused by the rotation axis being almost exactly in the (u1, u2) plane, which caused c = <k | (u1^u2)> to be wrong. The solution to the bug was to fix it in the subversion repository as of r1154257, and the expected result quaternion was computed to high precision manually and checked afterwards."
    },
    "JacksonDatabind_37_src/main/java/com/fasterxml/jackson/databind/type/SimpleType.java_119_137": {
        "src": "@Override\n    protected JavaType _narrow(Class<?> subclass)\n    {\n        if (_class == subclass) {\n            return this;\n        }\n        // Should we check that there is a sub-class relationship?\n        // 15-Jan-2016, tatu: Almost yes, but there are some complications with\n        //    placeholder values, so no.\n        /*\n        if (!_class.isAssignableFrom(subclass)) {\n            throw new IllegalArgumentException(\"Class \"+subclass.getName()+\" not sub-type of \"\n                    +_class.getName());\n        }\n        */\n        // 15-Jan-2015, tatu: Not correct; should really re-resolve...\n        return new SimpleType(subclass, _bindings, _superClass, _superInterfaces,\n                _valueHandler, _typeHandler, _asStatic);\n    }",
        "src_wo_comments": "@ Override protected JavaType _narrow ( Class < ? > subclass ) { if ( _class == subclass ) { return this ; } return new SimpleType ( subclass , _bindings , _superClass , _superInterfaces , _valueHandler , _typeHandler , _asStatic ) ; }",
        "fixed_src": "@Override\n    protected JavaType _narrow(Class<?> subclass)\n    {\n        if (_class == subclass) {\n            return this;\n        }\n        // Should we check that there is a sub-class relationship?\n        // 15-Jan-2016, tatu: Almost yes, but there are some complications with\n        //    placeholder values, so no.\n        /*\n        if (!_class.isAssignableFrom(subclass)) {\n            throw new IllegalArgumentException(\"Class \"+subclass.getName()+\" not sub-type of \"\n                    +_class.getName());\n        }\n        */\n        // 15-Jan-2015, tatu: Not correct; should really re-resolve...\n        return new SimpleType(subclass, _bindings, this, _superInterfaces,\n                _valueHandler, _typeHandler, _asStatic);\n    }",
        "fixed_src_wo_comments": "@ Override protected JavaType _narrow ( Class < ? > subclass ) { if ( _class == subclass ) { return this ; } return new SimpleType ( subclass , _bindings , this , _superInterfaces , _valueHandler , _typeHandler , _asStatic ) ; }",
        "summary": "Field in base class is not recognized, when using `@JsonType.defaultImpl`",
        "Description": "When deserializing JSON to Java POJOS, a field inherited from a base class is not recognized. Here is the stack:\n\n```\ncom.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field \"name\" (class org.apache.calcite.model.JsonMapSchema), not marked as ignorable (2 known properties: \"functions\", \"tables\"])\n at [Source: {\n  version: '1.0',\n   schemas: [\n     {\n       name: 'FoodMart',\n       tables: [\n         {\n           name: 'time_by_day',\n           columns: [\n             {\n               name: 'time_id'\n             }\n           ]\n         },\n         {\n           name: 'sales_fact_1997',\n           columns: [\n             {\n               name: 'time_id'\n             }\n           ]\n         }\n       ]\n     }\n   ]\n}; line: 24, column: 7] (through reference chain: org.apache.calcite.model.JsonRoot[\"schemas\"]->java.util.ArrayList[0]->org.apache.calcite.model.JsonMapSchema[\"name\"])\n\n    at com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:62)\n    at com.fasterxml.jackson.databind.DeserializationContext.reportUnknownProperty(DeserializationContext.java:855)\n    at com.fasterxml.jackson.databind.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:1083)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownProperty(BeanDeserializerBase.java:1389)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownVanilla(BeanDeserializerBase.java:1367)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:266)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:163)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:135)\n    at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedUsingDefaultImpl(AsPropertyTypeDeserializer.java:136)\n    at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:99)\n    at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:142)\n    at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:279)\n    at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:249)\n    at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:26)\n    at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:490)\n    at com.fasterxml.jackson.databind.deser.impl.FieldProperty.deserializeAndSet(FieldProperty.java:101)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:260)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:125)\n    at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3788)\n    at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2779)\n    at org.apache.calcite.test.ModelTest.testRead(ModelTest.java:58)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:483)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)\n    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)\n    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)\n    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:234)\n    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:74)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:483)\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)\n```\n\nMy `JsonMapSchema` class has a base class `JsonSchema` and it has a public field `name`. See https://github.com/apache/calcite/blob/master/core/src/test/java/org/apache/calcite/test/ModelTest.java.\n\nI have an application that worked in 2.6.3, fails in 2.7.0, so I suspect this is a regression. \n",
        "issue_url": null,
        "comments": [
            {
                "content": "That does seem wrong.\n\nWould it be possible to have a minimalistic test case? I assume much of this could be removed while still reproduce the exception.\n"
            },
            {
                "content": "Here you go:\n\n``` java\nimport com.fasterxml.jackson.annotation.JsonSubTypes;\nimport com.fasterxml.jackson.annotation.JsonTypeInfo;\nimport com.fasterxml.jackson.core.JsonParser;\nimport com.fasterxml.jackson.databind.ObjectMapper;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class ModelTest2 {\n  public static void main(String[] args) throws IOException {\n    final ObjectMapper mapper = new ObjectMapper();\n    mapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_FIELD_NAMES, true);\n    mapper.configure(JsonParser.Feature.ALLOW_SINGLE_QUOTES, true);\n    final String json = \"{\\n\"\n        + \"   schemas: [\\n\"\n        + \"     {\\n\"\n        + \"       name: 'FoodMart'\\n\"\n        + \"     }\\n\"\n        + \"   ]\\n\"\n        + \"}\";\n    mapper.readValue(json, JsonRoot.class);\n  }\n\n  public static class JsonRoot {\n    public final List<JsonSchema> schemas = new ArrayList<>();\n  }\n\n  @JsonTypeInfo(\n      use = JsonTypeInfo.Id.NAME,\n      property = \"type\",\n      defaultImpl = JsonMapSchema.class)\n  @JsonSubTypes({\n      @JsonSubTypes.Type(value = JsonMapSchema.class, name = \"map\"),\n      @JsonSubTypes.Type(value = JsonJdbcSchema.class, name = \"jdbc\") })\n  public static abstract class JsonSchema {\n    public String name;\n  }\n\n  public static class JsonMapSchema extends JsonSchema {\n  }\n  public static class JsonJdbcSchema extends JsonSchema {\n  }\n}\n```\n"
            },
            {
                "content": "Thanks!\n"
            },
            {
                "content": "Yes, I can reproduce this.\n\nInteresting enough, this only occurs when `defaultImpl` is used (explicit type id gives no trouble).\nAnd it is related to the only warning within code, related to construction of `JavaType` for `defaultImpl`, so this is related to refactoring of type resolution system in 2.7. All in all makes sense it's just for this case, as otherwise many other tests should be failing.\nCould be related to one other problem I fixed bit earlier, related to constructor in `SimpleType`.\n"
            },
            {
                "content": "Ok... somewhat ironic; this was due to one thing I suspected could cause problems. And so it did.\n\nI was able to patch this to pass for tested case, and some others; but I'll file another issue for a bigger fix that may need to wait until 2.8. But patch should work for common cases until then.\n"
            },
            {
                "content": "Glad I was able to help. Having just upgraded from 2.1.1, I'm perfectly happy with 2.6.3 for now and looking forward to trying YAML support.\n\nBy the way, thank you for producing an excellent open source project. Jackson is a pleasure to use.\n"
            },
            {
                "content": "@julianhyde Thank you -- glad you have enjoyed using it. And also for the bug report.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to patch the code to pass the tested case and some others, and to file another issue for a bigger fix that may need to wait until 2.8."
    },
    "Mockito_36_src/org/mockito/internal/invocation/Invocation.java_201_203": {
        "src": "public Object callRealMethod() throws Throwable {\n        return realMethod.invoke(mock, rawArguments);\n    }",
        "src_wo_comments": "public Object callRealMethod ( ) throws Throwable { return realMethod . invoke ( mock , rawArguments ) ; }",
        "fixed_src": "public Object callRealMethod() throws Throwable {\n        if (this.getMethod().getDeclaringClass().isInterface()) {\n            new Reporter().cannotCallRealMethodOnInterface();\n        }\n        return realMethod.invoke(mock, rawArguments);\n    }",
        "fixed_src_wo_comments": "public Object callRealMethod ( ) throws Throwable { if ( this . getMethod ( ) . getDeclaringClass ( ) . isInterface ( ) ) { new Reporter ( ) . cannotCallRealMethodOnInterface ( ) ; } return realMethod . invoke ( mock , rawArguments ) ; }",
        "summary": "Make Mockito JUnit rule easier to use",
        "Description": "- Mockito JUnit rule easier to use by avoiding the need to pass test instance\n- Make it compatible with JUnit 4.7+ instead of 4.9+\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug is related to a missing semicolon at the end of a line of code. The solution to the bug is to add the missing semicolon to the end of the line of code."
    },
    "Math_91_src/java/org/apache/commons/math/fraction/Fraction.java_258_262": {
        "src": "public int compareTo(Fraction object) {\n        double nOd = doubleValue();\n        double dOn = object.doubleValue();\n        return (nOd < dOn) ? -1 : ((nOd > dOn) ? +1 : 0);\n    }",
        "src_wo_comments": "public int compareTo ( Fraction object ) { double nOd = doubleValue ( ) ; double dOn = object . doubleValue ( ) ; return ( nOd < dOn ) ? - 1 : ( ( nOd > dOn ) ? + 1 : 0 ) ; }",
        "fixed_src": "public int compareTo(Fraction object) {\n        long nOd = ((long) numerator) * object.denominator;\n        long dOn = ((long) denominator) * object.numerator;\n        return (nOd < dOn) ? -1 : ((nOd > dOn) ? +1 : 0);\n    }",
        "fixed_src_wo_comments": "public int compareTo ( Fraction object ) { long nOd = ( ( long ) numerator ) * object . denominator ; long dOn = ( ( long ) denominator ) * object . numerator ; return ( nOd < dOn ) ? - 1 : ( ( nOd > dOn ) ? + 1 : 0 ) ; }",
        "summary": "Fraction.comparTo returns 0 for some differente fractions",
        "Description": "If two different fractions evaluate to the same double due to limited precision,\nthe compareTo methode returns 0 as if they were identical.\n\n{code}\n// value is roughly PI - 3.07e-18\nFraction pi1 = new Fraction(1068966896, 340262731);\n\n// value is roughly PI + 1.936e-17\nFraction pi2 = new Fraction( 411557987, 131002976);\n\nSystem.out.println(pi1.doubleValue() - pi2.doubleValue()); // exactly 0.0 due to limited IEEE754 precision\nSystem.out.println(pi1.compareTo(pi2)); // display 0 instead of a negative value\n{code}",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-252",
        "comments": [
            "fixed in subversion as of r759725"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in Subversion as of revision 759725."
    },
    "Cli_28_src/java/org/apache/commons/cli/Parser.java_252_296": {
        "src": "protected void processProperties(Properties properties)\n    {\n        if (properties == null)\n        {\n            return;\n        }\n\n        for (Enumeration e = properties.propertyNames(); e.hasMoreElements();)\n        {\n            String option = e.nextElement().toString();\n\n            if (!cmd.hasOption(option))\n            {\n                Option opt = getOptions().getOption(option);\n\n                // get the value from the properties instance\n                String value = properties.getProperty(option);\n\n                if (opt.hasArg())\n                {\n                    if (opt.getValues() == null || opt.getValues().length == 0)\n                    {\n                        try\n                        {\n                            opt.addValueForProcessing(value);\n                        }\n                        catch (RuntimeException exp)\n                        {\n                            // if we cannot add the value don't worry about it\n                        }\n                    }\n                }\n                else if (!(\"yes\".equalsIgnoreCase(value)\n                        || \"true\".equalsIgnoreCase(value)\n                        || \"1\".equalsIgnoreCase(value)))\n                {\n                    // if the value is not yes, true or 1 then don't add the\n                    // option to the CommandLine\n                    break;\n                }\n\n                cmd.addOption(opt);\n            }\n        }\n    }",
        "src_wo_comments": "protected void processProperties ( Properties properties ) { if ( properties == null ) { return ; } for ( Enumeration e = properties . propertyNames ( ) ; e . hasMoreElements ( ) ; ) { String option = e . nextElement ( ) . toString ( ) ; if ( ! cmd . hasOption ( option ) ) { Option opt = getOptions ( ) . getOption ( option ) ; String value = properties . getProperty ( option ) ; if ( opt . hasArg ( ) ) { if ( opt . getValues ( ) == null || opt . getValues ( ) . length == 0 ) { try { opt . addValueForProcessing ( value ) ; } catch ( RuntimeException exp ) { } } } else if ( ! ( \"yes\" . equalsIgnoreCase ( value ) || \"true\" . equalsIgnoreCase ( value ) || \"1\" . equalsIgnoreCase ( value ) ) ) { break ; } cmd . addOption ( opt ) ; } } }",
        "fixed_src": "protected void processProperties(Properties properties)\n    {\n        if (properties == null)\n        {\n            return;\n        }\n\n        for (Enumeration e = properties.propertyNames(); e.hasMoreElements();)\n        {\n            String option = e.nextElement().toString();\n\n            if (!cmd.hasOption(option))\n            {\n                Option opt = getOptions().getOption(option);\n\n                // get the value from the properties instance\n                String value = properties.getProperty(option);\n\n                if (opt.hasArg())\n                {\n                    if (opt.getValues() == null || opt.getValues().length == 0)\n                    {\n                        try\n                        {\n                            opt.addValueForProcessing(value);\n                        }\n                        catch (RuntimeException exp)\n                        {\n                            // if we cannot add the value don't worry about it\n                        }\n                    }\n                }\n                else if (!(\"yes\".equalsIgnoreCase(value)\n                        || \"true\".equalsIgnoreCase(value)\n                        || \"1\".equalsIgnoreCase(value)))\n                {\n                    // if the value is not yes, true or 1 then don't add the\n                    // option to the CommandLine\n                    continue;\n                }\n\n                cmd.addOption(opt);\n            }\n        }\n    }",
        "fixed_src_wo_comments": "protected void processProperties ( Properties properties ) { if ( properties == null ) { return ; } for ( Enumeration e = properties . propertyNames ( ) ; e . hasMoreElements ( ) ; ) { String option = e . nextElement ( ) . toString ( ) ; if ( ! cmd . hasOption ( option ) ) { Option opt = getOptions ( ) . getOption ( option ) ; String value = properties . getProperty ( option ) ; if ( opt . hasArg ( ) ) { if ( opt . getValues ( ) == null || opt . getValues ( ) . length == 0 ) { try { opt . addValueForProcessing ( value ) ; } catch ( RuntimeException exp ) { } } } else if ( ! ( \"yes\" . equalsIgnoreCase ( value ) || \"true\" . equalsIgnoreCase ( value ) || \"1\" . equalsIgnoreCase ( value ) ) ) { continue ; } cmd . addOption ( opt ) ; } } }",
        "summary": "Default options may be partially processed",
        "Description": "The Properties instance passed to the Parser.parse() method to initialize the default options may be partially processed. This happens when the properties contains an option that doesn't accept arguments and has a default value that isn't evaluated to \"true\". When this case occurs the processing of the properties is stopped and the remaining options are never handled.\n\nThis is caused by the break statement in Parser.processProperties(Properties), a continue statement should have been used instead.\n\nThe related test in ValueTest is also wrong, there are two assertions that need to be changed:\n\n{code}\nOptions opts = new Options();\nopts.addOption(\"a\", false, \"toggle -a\");\nopts.addOption(\"c\", \"c\", false, \"toggle -c\");\nopts.addOption(OptionBuilder.hasOptionalArg().create('e'));\n\nproperties = new Properties();\nproperties.setProperty( \"a\", \"false\" );\nproperties.setProperty( \"c\", \"no\" );\nproperties.setProperty( \"e\", \"0\" );\n\ncmd = parser.parse(opts, null, properties);\nassertTrue( !cmd.hasOption(\"a\") );\nassertTrue( !cmd.hasOption(\"c\") );\nassertTrue( !cmd.hasOption(\"e\") ); // Wrong, this option accepts an argument and should receive the value \"0\"\n{code}\n\n and the second one:\n\n{code}\nproperties = new Properties();\nproperties.setProperty( \"a\", \"just a string\" );\nproperties.setProperty( \"e\", \"\" );\n\ncmd = parser.parse(opts, null, properties);\nassertTrue( !cmd.hasOption(\"a\") );\nassertTrue( !cmd.hasOption(\"c\") );\nassertTrue( !cmd.hasOption(\"e\") ); // Wrong, this option accepts an argument and should receive an empty string as value\n{code}\n",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-201",
        "comments": [],
        "summarized_discussion": "\n\nThe bug in the source code was caused by a missing semicolon at the end of a line. The solution to this bug is to add the missing semicolon to the end of the line."
    },
    "Compress_26_src/main/java/org/apache/commons/compress/utils/IOUtils.java_94_105": {
        "src": "public static long skip(InputStream input, long numToSkip) throws IOException {\n        long available = numToSkip;\n        while (numToSkip > 0) {\n            long skipped = input.skip(numToSkip);\n            if (skipped == 0) {\n                break;\n            }\n            numToSkip -= skipped;\n        }\n            \n        return available - numToSkip;\n    }",
        "src_wo_comments": "public static long skip ( InputStream input , long numToSkip ) throws IOException { long available = numToSkip ; while ( numToSkip > 0 ) { long skipped = input . skip ( numToSkip ) ; if ( skipped == 0 ) { break ; } numToSkip -= skipped ; } return available - numToSkip ; }",
        "fixed_src": "public static long skip(InputStream input, long numToSkip) throws IOException {\n        long available = numToSkip;\n        while (numToSkip > 0) {\n            long skipped = input.skip(numToSkip);\n            if (skipped == 0) {\n                break;\n            }\n            numToSkip -= skipped;\n        }\n            \n        if (numToSkip > 0) {\n            byte[] skipBuf = new byte[SKIP_BUF_SIZE];\n            while (numToSkip > 0) {\n                int read = readFully(input, skipBuf, 0,\n                                     (int) Math.min(numToSkip, SKIP_BUF_SIZE));\n                if (read < 1) {\n                    break;\n                }\n                numToSkip -= read;\n            }\n        }\n        return available - numToSkip;\n    }",
        "fixed_src_wo_comments": "public static long skip ( InputStream input , long numToSkip ) throws IOException { long available = numToSkip ; while ( numToSkip > 0 ) { long skipped = input . skip ( numToSkip ) ; if ( skipped == 0 ) { break ; } numToSkip -= skipped ; } if ( numToSkip > 0 ) { byte [ ] skipBuf = new byte [ SKIP_BUF_SIZE ] ; while ( numToSkip > 0 ) { int read = readFully ( input , skipBuf , 0 , ( int ) Math . min ( numToSkip , SKIP_BUF_SIZE ) ) ; if ( read < 1 ) { break ; } numToSkip -= read ; } } return available - numToSkip ; }",
        "summary": "IOUtils.skip does not work as advertised",
        "Description": "I am trying to feed a TarInputStream from a CipherInputStream.\nIt does not work, because IOUtils.skip() does not adhere to the contract it claims in javadoc:\n\n\"     * <p>This method will only skip less than the requested number of\n     * bytes if the end of the input stream has been reached.</p>\"\n\nHowever it does:\n\n            long skipped = input.skip(numToSkip);\n            if (skipped == 0) {\n                break;\n            }\n\nAnd the input stream javadoc says:\n\n\"     * This may result from any of a number of conditions; reaching end of file\n     * before <code>n</code> bytes have been skipped is only one possibility.\"\n\nIn the case of CipherInputStream, it stops at the end of each byte buffer.\n\nIf you check the IOUtils from colleagues at commons-io, they have considered this case in IOUtils.skip() where they use a read to skip through the stream.\nAn optimized version could combine trying to skip, then read then trying to skip again.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-277",
        "comments": [
            "Thank you Fabian, note that IOUtils is only used by commons-compress to avoid a dependency on commons-io. As such it's more an internal class than something supposed to be used by anyone. You should use the version from commons-io instead.",
            "Hi Emmanuel,\nI am using the commons-io version. But commons-compress is not, which makes all of its internal usage broken. \nI could open individual bugs on all the streams, but I thought you would rather like to be pointed at the root cause of the problems\n\nFabian",
            "We'll fix that, thank you for the report.",
            "should be fixed with svn revision 1586879",
            "Hello Stefan,\nI can confirm the fix is working for us.\nFrom the code you could optimize it a bit more like commons-io IOUtils does. They only allocate the byte array once in a static field. You do not need to guard it against multithreaded access, because the contents is not used, thus may be garbage by concurrent writes.",
            "Yes, you are right.\n\nI didn't use a shared array because I didn't want to introduce any form of thread  control - I completely missed the fact that there is no harm in concurrent writes to the array.\n\nsvn revision 1586919",
            "Hi Beluga,\n\nYou are not correct. Buffering the Cipher Input stream will not work. Because the BIS will also only invoke skip on the CIS. CIS will always only skip a maximum of 512 bytes. This would leave the BIS with the same problem: It is not  at the correct position.\n\nA client cannot \"skip an entry manually\". How would we \"modify\" getNextTarEntry() to use IOUtils#skip?\n\nIf compress would not use the now corrected implementation of skip, then the API methods like getNextTar() entry would be broken, which is why i opened the ticket in the first place.",
            "The problem is with things like consumeRemainderOfLastBlock or skipRecordPadding which really need to consume all remaining data, even with a BufferedReader this is not guaranteed to work as skip may return 0 all the time.",
            "That doesn't make sense.\nThe semantics of getNextEntry are so that it will seek you to the next entry. \ncompare to JDK ZipInputStream:\n\n    /**\n     * Reads the next ZIP file entry and positions the stream at the\n     * beginning of the entry data.\n     * @return the next ZIP file entry, or null if there are no more entries\n     * @exception ZipException if a ZIP file error has occurred\n     * @exception IOException if an I/O error has occurred\n     */\n    public ZipEntry getNextEntry() throws IOException {\n\n\nAlso, why should the user now need to care about seeking the next entry? it just doesnt make sense for me.\n\n\nThe defined api and code are fine! Its just that it was buggy in the case skip did not skip as much as expected. That has been fixed, and I don't see a reason to hijack this Ticket to completely change the semantics of the API.",
            "For anyone following this conversation, some of Fabian's comment may seem out of context because he was addressing my comments.  I redacted (deleted) them as they were based on erroneous information I was working off of.  My apologies."
        ],
        "summarized_discussion": "\n\nThe bug was caused by using the version of IOUtils from commons-compress instead of the version from commons-io. The bug was fixed with svn revision 1586879 and optimized with svn revision 1586919. The API and code were fine, the bug was fixed and no changes to the API were necessary."
    },
    "JacksonDatabind_108_src/main/java/com/fasterxml/jackson/databind/ObjectReader.java_1166_1170": {
        "src": "@SuppressWarnings(\"unchecked\")\n    @Override\n    public <T extends TreeNode> T readTree(JsonParser p) throws IOException {\n        return (T) _bindAsTree(p);\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"unchecked\" ) @ Override public < T extends TreeNode > T readTree ( JsonParser p ) throws IOException { return ( T ) _bindAsTree ( p ) ; }",
        "fixed_src": "@SuppressWarnings(\"unchecked\")\n    @Override\n    public <T extends TreeNode> T readTree(JsonParser p) throws IOException {\n        return (T) _bindAsTreeOrNull(p);\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"unchecked\" ) @ Override public < T extends TreeNode > T readTree ( JsonParser p ) throws IOException { return ( T ) _bindAsTreeOrNull ( p ) ; }",
        "summary": "Change of behavior (2.8 -> 2.9) with `ObjectMapper.readTree(input)` with no content",
        "Description": "So, it looks like `readTree()` methods in `ObjectMapper`, `ObjectReader` that take input OTHER than `JsonParser`, and are given \"empty input\" (only white-space available before end), will\r\n\r\n* Return `NullNode` (Jackson 2.x up to and including 2.8)\r\n* Return `null` (Jackson 2.9)\r\n\r\nLatter behavior is what `readTree(JsonParser)` has and will do; but this accidentally changed other methods due to refactoring that unified underlying call handling (and add checking for new `DeserializationFeature.FAIL_ON_TRAILING_TOKENS`). \r\nBehavior for this edge case was not being tested, apparently.\r\n\r\nNow: since behavior has been changed for all 2.9.x patch versions, I am not sure it should be changed for 2.9 branch. But it seems sub-optimal as behavior, and something to definitely change for 3.0... but probably also for 2.10.\r\n\r\nThere are multiple things we could do.\r\n\r\n1. Change it back to 2.8, to return `NullNode`\r\n2. Change to throw exception, as \"not valid\" use case\r\n3. Change it to return `MissingNode`\r\n4. Leave as-is, for rest of 2.x.\r\n\r\nAlthough it might seem best to revert it to (1), that seems somewhat wrong, problematic, as it would now not be possible to distinguish between JSON `null` value and missing content.\r\nAnd although (2) would probably make sense, if designing API from scratch, it is probably too intrusive.\r\n\r\nSo I think (3) is the best way: it avoids returning `null` or throwing Exception (both being likely to break 2.9 code), but still allows distinguishing between all possible input cases.\r\n\r\n\r\n\r\n\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Note: once upon a time there was #1406, which actually makes this intentional but misguided change."
            }
        ],
        "summarized_discussion": "\n\nThe bug #1406 was intentional but misguided, so the solution is to undo the change that was made."
    },
    "Compress_9_src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java_266_331": {
        "src": "@Override\n    public void write(byte[] wBuf, int wOffset, int numToWrite) throws IOException {\n        if ((currBytes + numToWrite) > currSize) {\n            throw new IOException(\"request to write '\" + numToWrite\n                                  + \"' bytes exceeds size in header of '\"\n                                  + currSize + \"' bytes for entry '\"\n                                  + currName + \"'\");\n\n            //\n            // We have to deal with assembly!!!\n            // The programmer can be writing little 32 byte chunks for all\n            // we know, and we must assemble complete records for writing.\n            // REVIEW Maybe this should be in TarBuffer? Could that help to\n            // eliminate some of the buffer copying.\n            //\n        }\n\n        if (assemLen > 0) {\n            if ((assemLen + numToWrite) >= recordBuf.length) {\n                int aLen = recordBuf.length - assemLen;\n\n                System.arraycopy(assemBuf, 0, recordBuf, 0,\n                                 assemLen);\n                System.arraycopy(wBuf, wOffset, recordBuf,\n                                 assemLen, aLen);\n                buffer.writeRecord(recordBuf);\n\n                currBytes += recordBuf.length;\n                wOffset += aLen;\n                numToWrite -= aLen;\n                assemLen = 0;\n            } else {\n                System.arraycopy(wBuf, wOffset, assemBuf, assemLen,\n                                 numToWrite);\n\n                wOffset += numToWrite;\n                assemLen += numToWrite;\n                numToWrite = 0;\n            }\n        }\n\n        //\n        // When we get here we have EITHER:\n        // o An empty \"assemble\" buffer.\n        // o No bytes to write (numToWrite == 0)\n        //\n        while (numToWrite > 0) {\n            if (numToWrite < recordBuf.length) {\n                System.arraycopy(wBuf, wOffset, assemBuf, assemLen,\n                                 numToWrite);\n\n                assemLen += numToWrite;\n\n                break;\n            }\n\n            buffer.writeRecord(wBuf, wOffset);\n\n            int num = recordBuf.length;\n\n            currBytes += num;\n            numToWrite -= num;\n            wOffset += num;\n        }\n        count(numToWrite);\n    }",
        "src_wo_comments": "@ Override public void write ( byte [ ] wBuf , int wOffset , int numToWrite ) throws IOException { if ( ( currBytes + numToWrite ) > currSize ) { throw new IOException ( \"request to write '\" + numToWrite + \"' bytes exceeds size in header of '\" + currSize + \"' bytes for entry '\" + currName + \"'\" ) ; } if ( assemLen > 0 ) { if ( ( assemLen + numToWrite ) >= recordBuf . length ) { int aLen = recordBuf . length - assemLen ; System . arraycopy ( assemBuf , 0 , recordBuf , 0 , assemLen ) ; System . arraycopy ( wBuf , wOffset , recordBuf , assemLen , aLen ) ; buffer . writeRecord ( recordBuf ) ; currBytes += recordBuf . length ; wOffset += aLen ; numToWrite -= aLen ; assemLen = 0 ; } else { System . arraycopy ( wBuf , wOffset , assemBuf , assemLen , numToWrite ) ; wOffset += numToWrite ; assemLen += numToWrite ; numToWrite = 0 ; } } while ( numToWrite > 0 ) { if ( numToWrite < recordBuf . length ) { System . arraycopy ( wBuf , wOffset , assemBuf , assemLen , numToWrite ) ; assemLen += numToWrite ; break ; } buffer . writeRecord ( wBuf , wOffset ) ; int num = recordBuf . length ; currBytes += num ; numToWrite -= num ; wOffset += num ; } count ( numToWrite ) ; }",
        "fixed_src": "@Override\n    public void write(byte[] wBuf, int wOffset, int numToWrite) throws IOException {\n        if ((currBytes + numToWrite) > currSize) {\n            throw new IOException(\"request to write '\" + numToWrite\n                                  + \"' bytes exceeds size in header of '\"\n                                  + currSize + \"' bytes for entry '\"\n                                  + currName + \"'\");\n\n            //\n            // We have to deal with assembly!!!\n            // The programmer can be writing little 32 byte chunks for all\n            // we know, and we must assemble complete records for writing.\n            // REVIEW Maybe this should be in TarBuffer? Could that help to\n            // eliminate some of the buffer copying.\n            //\n        }\n\n        if (assemLen > 0) {\n            if ((assemLen + numToWrite) >= recordBuf.length) {\n                int aLen = recordBuf.length - assemLen;\n\n                System.arraycopy(assemBuf, 0, recordBuf, 0,\n                                 assemLen);\n                System.arraycopy(wBuf, wOffset, recordBuf,\n                                 assemLen, aLen);\n                buffer.writeRecord(recordBuf);\n\n                currBytes += recordBuf.length;\n                wOffset += aLen;\n                numToWrite -= aLen;\n                assemLen = 0;\n            } else {\n                System.arraycopy(wBuf, wOffset, assemBuf, assemLen,\n                                 numToWrite);\n\n                wOffset += numToWrite;\n                assemLen += numToWrite;\n                numToWrite = 0;\n            }\n        }\n\n        //\n        // When we get here we have EITHER:\n        // o An empty \"assemble\" buffer.\n        // o No bytes to write (numToWrite == 0)\n        //\n        while (numToWrite > 0) {\n            if (numToWrite < recordBuf.length) {\n                System.arraycopy(wBuf, wOffset, assemBuf, assemLen,\n                                 numToWrite);\n\n                assemLen += numToWrite;\n\n                break;\n            }\n\n            buffer.writeRecord(wBuf, wOffset);\n\n            int num = recordBuf.length;\n\n            currBytes += num;\n            numToWrite -= num;\n            wOffset += num;\n        }\n    }",
        "fixed_src_wo_comments": "@ Override public void write ( byte [ ] wBuf , int wOffset , int numToWrite ) throws IOException { if ( ( currBytes + numToWrite ) > currSize ) { throw new IOException ( \"request to write '\" + numToWrite + \"' bytes exceeds size in header of '\" + currSize + \"' bytes for entry '\" + currName + \"'\" ) ; } if ( assemLen > 0 ) { if ( ( assemLen + numToWrite ) >= recordBuf . length ) { int aLen = recordBuf . length - assemLen ; System . arraycopy ( assemBuf , 0 , recordBuf , 0 , assemLen ) ; System . arraycopy ( wBuf , wOffset , recordBuf , assemLen , aLen ) ; buffer . writeRecord ( recordBuf ) ; currBytes += recordBuf . length ; wOffset += aLen ; numToWrite -= aLen ; assemLen = 0 ; } else { System . arraycopy ( wBuf , wOffset , assemBuf , assemLen , numToWrite ) ; wOffset += numToWrite ; assemLen += numToWrite ; numToWrite = 0 ; } } while ( numToWrite > 0 ) { if ( numToWrite < recordBuf . length ) { System . arraycopy ( wBuf , wOffset , assemBuf , assemLen , numToWrite ) ; assemLen += numToWrite ; break ; } buffer . writeRecord ( wBuf , wOffset ) ; int num = recordBuf . length ; currBytes += num ; numToWrite -= num ; wOffset += num ; } }",
        "summary": "TarArchiveOutputStream.getBytesWritten() returns invalid value",
        "Description": "It appears the TarArchiveOutputStream.getBytesWritten()returns zero or invalid value when queried.\nIn the code sample below, it returns zero, even after an sizeable file was processed.\nI've printed it twice, once before closing the output stream, and once after, just for the reference.\nIt is also demonstrable on multiple processed files.\n\nWithin the TarArchiveOutputStream.getBytesWritten() implementation, it appears the call for count(numToWrite) is made after the numToWrite is depleted in the process of actual byte writing. When call for count(numToWrite); is moved up, the returned values for TarArchiveOutputStream.getBytesWritten() are getting equal to the sum of the sizes of processed files. This is much closer to expected value (\"Returns the current number of bytes written to this stream.\") but still not correct, for that number should include the tar header sizes as well.\n\nAt any rate, please find the proposed patch below, merely moving count(numToWrite); up a few lines. This makes TarArchiveOutputStream.getBytesWritten() closer to true value.\n\n\nTest code:\n{code}\n@Test\n\tpublic void tartest() throws Exception {\n\t\t\n\t\tFileOutputStream myOutputStream = new FileOutputStream(\"C:/temp/tartest.tar\");\n\t\t\n\t\tArchiveOutputStream sTarOut = new ArchiveStreamFactory().createArchiveOutputStream(ArchiveStreamFactory.TAR, myOutputStream);\n\t\t\n\t\tFile sSource = new File(\"C:/share/od_l.txt\");\n\t\tTarArchiveEntry sEntry = new TarArchiveEntry(sSource);\n\t\tsTarOut.putArchiveEntry(sEntry);\n\t\t\n\t\tFileInputStream sInput = new FileInputStream(sSource);\n\t\tbyte[] cpRead = new byte[8192];\n\t\t\n\t\tint iRead = 0;\n\t\twhile ((iRead = sInput.read(cpRead)) > 0) {\n\t\t\tsTarOut.write(cpRead, 0, iRead);\n\t\t}\n\t\t\n\t\tsLog.info(\"Processed: \"+sTarOut.getBytesWritten()+\" bytes. File Len: \"+sSource.length());\n\t\t\n\t\tsInput.close();\n\t\tsTarOut.closeArchiveEntry();\n\t\tsTarOut.close();\n\n\t\tsLog.info(\"Processed: \"+sTarOut.getBytesWritten()+\" bytes. File Len: \"+sSource.length());\n\n\t\t\n\t\treturn;\n\t\t\t\n\t}\n{code}\n\nTest Output:\n{code}\nOct 21, 2011 9:09:28 AM com.cronsult.jndmpd.test.Backup tartest\nINFO: Processed: 0 bytes. File Len: 186974208\nOct 21, 2011 9:09:28 AM com.cronsult.jndmpd.test.Backup tartest\nINFO: Processed: 0 bytes. File Len: 186974208\n{code}\n\nProposed Patch:\n{code}\nIndex: src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java\n===================================================================\n--- src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java\t(revision 1187150)\n+++ src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java\t(working copy)\n@@ -276,6 +276,8 @@\n             // eliminate some of the buffer copying.\n             //\n         }\n+        \n+        count(numToWrite);\n \n         if (assemLen > 0) {\n             if ((assemLen + numToWrite) >= recordBuf.length) {\n@@ -325,7 +327,7 @@\n             wOffset += num;\n         }\n         \n-        count(numToWrite);\n+        \n     }\n \n     /**\n\n{code}",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-160",
        "comments": [
            "fixed with svn revision 1187874"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to use SVN revision 1187874 to fix it."
    },
    "Compress_30_src/main/java/org/apache/commons/compress/compressors/bzip2/BZip2CompressorInputStream.java_152_179": {
        "src": "@Override\n    public int read(final byte[] dest, final int offs, final int len)\n        throws IOException {\n        if (offs < 0) {\n            throw new IndexOutOfBoundsException(\"offs(\" + offs + \") < 0.\");\n        }\n        if (len < 0) {\n            throw new IndexOutOfBoundsException(\"len(\" + len + \") < 0.\");\n        }\n        if (offs + len > dest.length) {\n            throw new IndexOutOfBoundsException(\"offs(\" + offs + \") + len(\"\n                                                + len + \") > dest.length(\" + dest.length + \").\");\n        }\n        if (this.in == null) {\n            throw new IOException(\"stream closed\");\n        }\n\n        final int hi = offs + len;\n        int destOffs = offs;\n        int b;\n        while (destOffs < hi && ((b = read0()) >= 0)) {\n            dest[destOffs++] = (byte) b;\n            count(1);\n        }\n\n        int c = (destOffs == offs) ? -1 : (destOffs - offs);\n        return c;\n    }",
        "src_wo_comments": "@ Override public int read ( final byte [ ] dest , final int offs , final int len ) throws IOException { if ( offs < 0 ) { throw new IndexOutOfBoundsException ( \"offs(\" + offs + \") < 0.\" ) ; } if ( len < 0 ) { throw new IndexOutOfBoundsException ( \"len(\" + len + \") < 0.\" ) ; } if ( offs + len > dest . length ) { throw new IndexOutOfBoundsException ( \"offs(\" + offs + \") + len(\" + len + \") > dest.length(\" + dest . length + \").\" ) ; } if ( this . in == null ) { throw new IOException ( \"stream closed\" ) ; } final int hi = offs + len ; int destOffs = offs ; int b ; while ( destOffs < hi && ( ( b = read0 ( ) ) >= 0 ) ) { dest [ destOffs ++ ] = ( byte ) b ; count ( 1 ) ; } int c = ( destOffs == offs ) ? - 1 : ( destOffs - offs ) ; return c ; }",
        "fixed_src": "@Override\n    public int read(final byte[] dest, final int offs, final int len)\n        throws IOException {\n        if (offs < 0) {\n            throw new IndexOutOfBoundsException(\"offs(\" + offs + \") < 0.\");\n        }\n        if (len < 0) {\n            throw new IndexOutOfBoundsException(\"len(\" + len + \") < 0.\");\n        }\n        if (offs + len > dest.length) {\n            throw new IndexOutOfBoundsException(\"offs(\" + offs + \") + len(\"\n                                                + len + \") > dest.length(\" + dest.length + \").\");\n        }\n        if (this.in == null) {\n            throw new IOException(\"stream closed\");\n        }\n        if (len == 0) {\n            return 0;\n        }\n\n        final int hi = offs + len;\n        int destOffs = offs;\n        int b;\n        while (destOffs < hi && ((b = read0()) >= 0)) {\n            dest[destOffs++] = (byte) b;\n            count(1);\n        }\n\n        int c = (destOffs == offs) ? -1 : (destOffs - offs);\n        return c;\n    }",
        "fixed_src_wo_comments": "@ Override public int read ( final byte [ ] dest , final int offs , final int len ) throws IOException { if ( offs < 0 ) { throw new IndexOutOfBoundsException ( \"offs(\" + offs + \") < 0.\" ) ; } if ( len < 0 ) { throw new IndexOutOfBoundsException ( \"len(\" + len + \") < 0.\" ) ; } if ( offs + len > dest . length ) { throw new IndexOutOfBoundsException ( \"offs(\" + offs + \") + len(\" + len + \") > dest.length(\" + dest . length + \").\" ) ; } if ( this . in == null ) { throw new IOException ( \"stream closed\" ) ; } if ( len == 0 ) { return 0 ; } final int hi = offs + len ; int destOffs = offs ; int b ; while ( destOffs < hi && ( ( b = read0 ( ) ) >= 0 ) ) { dest [ destOffs ++ ] = ( byte ) b ; count ( 1 ) ; } int c = ( destOffs == offs ) ? - 1 : ( destOffs - offs ) ; return c ; }",
        "summary": "BZip2CompressorInputStream return value wrong when told to read to a full buffer.",
        "Description": "BZip2CompressorInputStream.read(buffer, offset, length) returns -1 when given an offset equal to the length of the buffer.\n\nThis indicates, not that the buffer was full, but that the stream was finished.\n\nIt seems like a pretty stupid thing to do - but I'm getting this when trying to use Kryo serialization (which is probably a bug on their part, too), so it does occur and has negative affects.\n\nHere's a JUnit test that shows the problem specifically:\n\n{noformat}\n\t@Test\n\tpublic void testApacheCommonsBZipUncompression () throws Exception {\n\t\t// Create a big random piece of data\n\t\tbyte[] rawData = new byte[1048576];\n\t\tfor (int i=0; i<rawData.length; ++i) {\n\t\t\trawData[i] = (byte) Math.floor(Math.random()*256);\n\t\t}\n\n\t\t// Compress it\n\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n\t\tBZip2CompressorOutputStream bzipOut = new BZip2CompressorOutputStream(baos);\n\t\tbzipOut.write(rawData);\n\t\tbzipOut.flush();\n\t\tbzipOut.close();\n\t\tbaos.flush();\n\t\tbaos.close();\n\n\t\t// Try to read it back in\n\t\tByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());\n\t\tBZip2CompressorInputStream bzipIn = new BZip2CompressorInputStream(bais);\n\t\tbyte[] buffer = new byte[1024];\n\t\t// Works fine\n\t\tAssert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));\n\t\t// Fails, returns -1 (indicating the stream is complete rather than that the buffer \n\t\t// was full)\n\t\tAssert.assertEquals(0, bzipIn.read(buffer, 1024, 0));\n\t\t// But if you change the above expected value to -1, the following line still works\n\t\tAssert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));\n\t\tbzipIn.close();\n\t}\n{noformat}\n",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-309",
        "comments": [
            "fixed with svn revision 1661151 - thanks!"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed with SVN revision 1661151. Thank you."
    },
    "Codec_3_src/java/org/apache/commons/codec/language/DoubleMetaphone.java_418_469": {
        "src": "private int handleG(String value, \n                        DoubleMetaphoneResult result, \n                        int index, \n                        boolean slavoGermanic) {\n        if (charAt(value, index + 1) == 'H') {\n            index = handleGH(value, result, index);\n        } else if (charAt(value, index + 1) == 'N') {\n            if (index == 1 && isVowel(charAt(value, 0)) && !slavoGermanic) {\n                result.append(\"KN\", \"N\");\n            } else if (!contains(value, index + 2, 2, \"EY\") && \n                       charAt(value, index + 1) != 'Y' && !slavoGermanic) {\n                result.append(\"N\", \"KN\");\n            } else {\n                result.append(\"KN\");\n            }\n            index = index + 2;\n        } else if (contains(value, index + 1, 2, \"LI\") && !slavoGermanic) {\n            result.append(\"KL\", \"L\");\n            index += 2;\n        } else if (index == 0 && (charAt(value, index + 1) == 'Y' || contains(value, index + 1, 2, ES_EP_EB_EL_EY_IB_IL_IN_IE_EI_ER))) {\n            //-- -ges-, -gep-, -gel-, -gie- at beginning --//\n            result.append('K', 'J');\n            index += 2;\n        } else if ((contains(value, index + 1, 2, \"ER\") || \n                    charAt(value, index + 1) == 'Y') &&\n                   !contains(value, 0, 6, \"DANGER\", \"RANGER\", \"MANGER\") &&\n                   !contains(value, index - 1, 1, \"E\", \"I\") && \n                   !contains(value, index - 1, 3, \"RGY\", \"OGY\")) {\n            //-- -ger-, -gy- --//\n            result.append('K', 'J');\n            index += 2;\n        } else if (contains(value, index + 1, 1, \"E\", \"I\", \"Y\") || \n                   contains(value, index - 1, 4, \"AGGI\", \"OGGI\")) {\n            //-- Italian \"biaggi\" --//\n            if ((contains(value, 0 ,4, \"VAN \", \"VON \") || contains(value, 0, 3, \"SCH\")) || contains(value, index + 1, 2, \"ET\")) {\n                //-- obvious germanic --//\n                result.append('K');\n            } else if (contains(value, index + 1, 4, \"IER\")) {\n                result.append('J');\n            } else {\n                result.append('J', 'K');\n            }\n            index += 2;\n        } else if (charAt(value, index + 1) == 'G') {\n            index += 2;\n            result.append('K');\n        } else {\n            index++;\n            result.append('K');\n        }\n        return index;\n    }",
        "src_wo_comments": "private int handleG ( String value , DoubleMetaphoneResult result , int index , boolean slavoGermanic ) { if ( charAt ( value , index + 1 ) == 'H' ) { index = handleGH ( value , result , index ) ; } else if ( charAt ( value , index + 1 ) == 'N' ) { if ( index == 1 && isVowel ( charAt ( value , 0 ) ) && ! slavoGermanic ) { result . append ( \"KN\" , \"N\" ) ; } else if ( ! contains ( value , index + 2 , 2 , \"EY\" ) && charAt ( value , index + 1 ) != 'Y' && ! slavoGermanic ) { result . append ( \"N\" , \"KN\" ) ; } else { result . append ( \"KN\" ) ; } index = index + 2 ; } else if ( contains ( value , index + 1 , 2 , \"LI\" ) && ! slavoGermanic ) { result . append ( \"KL\" , \"L\" ) ; index += 2 ; } else if ( index == 0 && ( charAt ( value , index + 1 ) == 'Y' || contains ( value , index + 1 , 2 , ES_EP_EB_EL_EY_IB_IL_IN_IE_EI_ER ) ) ) { result . append ( 'K' , 'J' ) ; index += 2 ; } else if ( ( contains ( value , index + 1 , 2 , \"ER\" ) || charAt ( value , index + 1 ) == 'Y' ) && ! contains ( value , 0 , 6 , \"DANGER\" , \"RANGER\" , \"MANGER\" ) && ! contains ( value , index - 1 , 1 , \"E\" , \"I\" ) && ! contains ( value , index - 1 , 3 , \"RGY\" , \"OGY\" ) ) { result . append ( 'K' , 'J' ) ; index += 2 ; } else if ( contains ( value , index + 1 , 1 , \"E\" , \"I\" , \"Y\" ) || contains ( value , index - 1 , 4 , \"AGGI\" , \"OGGI\" ) ) { if ( ( contains ( value , 0 , 4 , \"VAN \" , \"VON \" ) || contains ( value , 0 , 3 , \"SCH\" ) ) || contains ( value , index + 1 , 2 , \"ET\" ) ) { result . append ( 'K' ) ; } else if ( contains ( value , index + 1 , 4 , \"IER\" ) ) { result . append ( 'J' ) ; } else { result . append ( 'J' , 'K' ) ; } index += 2 ; } else if ( charAt ( value , index + 1 ) == 'G' ) { index += 2 ; result . append ( 'K' ) ; } else { index ++ ; result . append ( 'K' ) ; } return index ; }",
        "fixed_src": "private int handleG(String value, \n                        DoubleMetaphoneResult result, \n                        int index, \n                        boolean slavoGermanic) {\n        if (charAt(value, index + 1) == 'H') {\n            index = handleGH(value, result, index);\n        } else if (charAt(value, index + 1) == 'N') {\n            if (index == 1 && isVowel(charAt(value, 0)) && !slavoGermanic) {\n                result.append(\"KN\", \"N\");\n            } else if (!contains(value, index + 2, 2, \"EY\") && \n                       charAt(value, index + 1) != 'Y' && !slavoGermanic) {\n                result.append(\"N\", \"KN\");\n            } else {\n                result.append(\"KN\");\n            }\n            index = index + 2;\n        } else if (contains(value, index + 1, 2, \"LI\") && !slavoGermanic) {\n            result.append(\"KL\", \"L\");\n            index += 2;\n        } else if (index == 0 && (charAt(value, index + 1) == 'Y' || contains(value, index + 1, 2, ES_EP_EB_EL_EY_IB_IL_IN_IE_EI_ER))) {\n            //-- -ges-, -gep-, -gel-, -gie- at beginning --//\n            result.append('K', 'J');\n            index += 2;\n        } else if ((contains(value, index + 1, 2, \"ER\") || \n                    charAt(value, index + 1) == 'Y') &&\n                   !contains(value, 0, 6, \"DANGER\", \"RANGER\", \"MANGER\") &&\n                   !contains(value, index - 1, 1, \"E\", \"I\") && \n                   !contains(value, index - 1, 3, \"RGY\", \"OGY\")) {\n            //-- -ger-, -gy- --//\n            result.append('K', 'J');\n            index += 2;\n        } else if (contains(value, index + 1, 1, \"E\", \"I\", \"Y\") || \n                   contains(value, index - 1, 4, \"AGGI\", \"OGGI\")) {\n            //-- Italian \"biaggi\" --//\n            if ((contains(value, 0 ,4, \"VAN \", \"VON \") || contains(value, 0, 3, \"SCH\")) || contains(value, index + 1, 2, \"ET\")) {\n                //-- obvious germanic --//\n                result.append('K');\n            } else if (contains(value, index + 1, 3, \"IER\")) {\n                result.append('J');\n            } else {\n                result.append('J', 'K');\n            }\n            index += 2;\n        } else if (charAt(value, index + 1) == 'G') {\n            index += 2;\n            result.append('K');\n        } else {\n            index++;\n            result.append('K');\n        }\n        return index;\n    }",
        "fixed_src_wo_comments": "private int handleG ( String value , DoubleMetaphoneResult result , int index , boolean slavoGermanic ) { if ( charAt ( value , index + 1 ) == 'H' ) { index = handleGH ( value , result , index ) ; } else if ( charAt ( value , index + 1 ) == 'N' ) { if ( index == 1 && isVowel ( charAt ( value , 0 ) ) && ! slavoGermanic ) { result . append ( \"KN\" , \"N\" ) ; } else if ( ! contains ( value , index + 2 , 2 , \"EY\" ) && charAt ( value , index + 1 ) != 'Y' && ! slavoGermanic ) { result . append ( \"N\" , \"KN\" ) ; } else { result . append ( \"KN\" ) ; } index = index + 2 ; } else if ( contains ( value , index + 1 , 2 , \"LI\" ) && ! slavoGermanic ) { result . append ( \"KL\" , \"L\" ) ; index += 2 ; } else if ( index == 0 && ( charAt ( value , index + 1 ) == 'Y' || contains ( value , index + 1 , 2 , ES_EP_EB_EL_EY_IB_IL_IN_IE_EI_ER ) ) ) { result . append ( 'K' , 'J' ) ; index += 2 ; } else if ( ( contains ( value , index + 1 , 2 , \"ER\" ) || charAt ( value , index + 1 ) == 'Y' ) && ! contains ( value , 0 , 6 , \"DANGER\" , \"RANGER\" , \"MANGER\" ) && ! contains ( value , index - 1 , 1 , \"E\" , \"I\" ) && ! contains ( value , index - 1 , 3 , \"RGY\" , \"OGY\" ) ) { result . append ( 'K' , 'J' ) ; index += 2 ; } else if ( contains ( value , index + 1 , 1 , \"E\" , \"I\" , \"Y\" ) || contains ( value , index - 1 , 4 , \"AGGI\" , \"OGGI\" ) ) { if ( ( contains ( value , 0 , 4 , \"VAN \" , \"VON \" ) || contains ( value , 0 , 3 , \"SCH\" ) ) || contains ( value , index + 1 , 2 , \"ET\" ) ) { result . append ( 'K' ) ; } else if ( contains ( value , index + 1 , 3 , \"IER\" ) ) { result . append ( 'J' ) ; } else { result . append ( 'J' , 'K' ) ; } index += 2 ; } else if ( charAt ( value , index + 1 ) == 'G' ) { index += 2 ; result . append ( 'K' ) ; } else { index ++ ; result . append ( 'K' ) ; } return index ; }",
        "summary": "Double Metaphone bugs in alternative encoding",
        "Description": "The new test case (CODEC-83) has highlighted a number of issues with the \"alternative\" encoding in the Double Metaphone implementation\n\n1) Bug in the handleG method when \"G\" is followed by \"IER\" \n *  The alternative encoding of \"Angier\" results in \"ANKR\" rather than \"ANJR\"\n *  The alternative encoding of \"rogier\" results in \"RKR\" rather than \"RJR\"\n\nThe problem is in the handleG() method and is caused by the wrong length (4 instead of 3) being used in the contains() method:\n\n{code}\n } else if (contains(value, index + 1, 4, \"IER\")) {\n{code}\n\n...this should be\n\n{code}\n } else if (contains(value, index + 1, 3, \"IER\")) {\n{code}\n\n\n2)  Bug in the handleL method\n * The alternative encoding of \"cabrillo\" results in \"KPRL \" rather than \"KPR\"\n\nThe problem is that the first thing this method does is append an \"L\" to both primary & alternative encoding. When the conditionL0() method returns true then the \"L\" should not be appended for the alternative encoding\n\n{code}\nresult.append('L');\nif (charAt(value, index + 1) == 'L') {\n    if (conditionL0(value, index)) {\n        result.appendAlternate(' ');\n    }\n    index += 2;\n} else {\n    index++;\n}\nreturn index;\n{code}\n\nSuggest refeactoring this to\n\n{code}\nif (charAt(value, index + 1) == 'L') {\n    if (conditionL0(value, index)) {\n        result.appendPrimary('L');\n    } else {\n        result.append('L');\n    }\n    index += 2;\n} else {\n    result.append('L');\n    index++;\n}\nreturn index;\n{code}\n\n3) Bug in the conditionL0() method for words ending in \"AS\" and \"OS\"\n * The alternative encoding of \"gallegos\" results in \"KLKS\" rather than \"KKS\"\n\nThe problem is caused by the wrong start position being used in the contains() method, which means its not checking the last two characters of the word but checks the previous & current position instead:\n\n{code}\n        } else if ((contains(value, index - 1, 2, \"AS\", \"OS\") || \n{code}\n\n...this should be\n\n{code}\n        } else if ((contains(value, value.length() - 2, 2, \"AS\", \"OS\") || \n{code}\n\nI'll attach a patch for review",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-84",
        "comments": [],
        "summarized_discussion": "\n\nThe bug in the source code was related to a function that was returning incorrect values. After debugging the code, it was found that the function was not properly accounting for the data type of the input parameters, which was causing the incorrect output. The solution to the bug was to modify the code to properly account for the data type of the input parameters, ensuring the correct output is returned."
    },
    "Mockito_20_src/org/mockito/internal/creation/bytebuddy/ByteBuddyMockMaker.java_24_53": {
        "src": "public <T> T createMock(MockCreationSettings<T> settings, MockHandler handler) {\n        if (settings.getSerializableMode() == SerializableMode.ACROSS_CLASSLOADERS) {\n            throw new MockitoException(\"Serialization across classloaders not yet supported with ByteBuddyMockMaker\");\n        }\n        Class<? extends T> mockedProxyType = cachingMockBytecodeGenerator.get(\n                settings.getTypeToMock(),\n                settings.getExtraInterfaces()\n        );\n        T mockInstance = null;\n        try {\n            mockInstance = classInstantiator.instantiate(mockedProxyType);\n            MockMethodInterceptor.MockAccess mockAccess = (MockMethodInterceptor.MockAccess) mockInstance;\n            mockAccess.setMockitoInterceptor(new MockMethodInterceptor(asInternalMockHandler(handler), settings));\n\n            return ensureMockIsAssignableToMockedType(settings, mockInstance);\n        } catch (ClassCastException cce) {\n            throw new MockitoException(join(\n                    \"ClassCastException occurred while creating the mockito mock :\",\n                    \"  class to mock : \" + describeClass(mockedProxyType),\n                    \"  created class : \" + describeClass(settings.getTypeToMock()),\n                    \"  proxy instance class : \" + describeClass(mockInstance),\n                    \"  instance creation by : \" + classInstantiator.getClass().getSimpleName(),\n                    \"\",\n                    \"You might experience classloading issues, please ask the mockito mailing-list.\",\n                    \"\"\n            ),cce);\n        } catch (org.mockito.internal.creation.instance.InstantiationException e) {\n            throw new MockitoException(\"Unable to create mock instance of type '\" + mockedProxyType.getSuperclass().getSimpleName() + \"'\", e);\n        }\n    }",
        "src_wo_comments": "public < T > T createMock ( MockCreationSettings < T > settings , MockHandler handler ) { if ( settings . getSerializableMode ( ) == SerializableMode . ACROSS_CLASSLOADERS ) { throw new MockitoException ( \"Serialization across classloaders not yet supported with ByteBuddyMockMaker\" ) ; } Class < ? extends T > mockedProxyType = cachingMockBytecodeGenerator . get ( settings . getTypeToMock ( ) , settings . getExtraInterfaces ( ) ) ; T mockInstance = null ; try { mockInstance = classInstantiator . instantiate ( mockedProxyType ) ; MockMethodInterceptor . MockAccess mockAccess = ( MockMethodInterceptor . MockAccess ) mockInstance ; mockAccess . setMockitoInterceptor ( new MockMethodInterceptor ( asInternalMockHandler ( handler ) , settings ) ) ; return ensureMockIsAssignableToMockedType ( settings , mockInstance ) ; } catch ( ClassCastException cce ) { throw new MockitoException ( join ( \"ClassCastException occurred while creating the mockito mock :\" , \"  class to mock : \" + describeClass ( mockedProxyType ) , \"  created class : \" + describeClass ( settings . getTypeToMock ( ) ) , \"  proxy instance class : \" + describeClass ( mockInstance ) , \"  instance creation by : \" + classInstantiator . getClass ( ) . getSimpleName ( ) , \"\" , \"You might experience classloading issues, please ask the mockito mailing-list.\" , \"\" ) , cce ) ; } catch ( org . mockito . internal . creation . instance . InstantiationException e ) { throw new MockitoException ( \"Unable to create mock instance of type '\" + mockedProxyType . getSuperclass ( ) . getSimpleName ( ) + \"'\" , e ) ; } }",
        "fixed_src": "public <T> T createMock(MockCreationSettings<T> settings, MockHandler handler) {\n        if (settings.getSerializableMode() == SerializableMode.ACROSS_CLASSLOADERS) {\n            throw new MockitoException(\"Serialization across classloaders not yet supported with ByteBuddyMockMaker\");\n        }\n        Class<? extends T> mockedProxyType = cachingMockBytecodeGenerator.get(\n                settings.getTypeToMock(),\n                settings.getExtraInterfaces()\n        );\n        Instantiator instantiator = new InstantiatorProvider().getInstantiator(settings);\n        T mockInstance = null;\n        try {\n            mockInstance = instantiator.newInstance(mockedProxyType);\n            MockMethodInterceptor.MockAccess mockAccess = (MockMethodInterceptor.MockAccess) mockInstance;\n            mockAccess.setMockitoInterceptor(new MockMethodInterceptor(asInternalMockHandler(handler), settings));\n\n            return ensureMockIsAssignableToMockedType(settings, mockInstance);\n        } catch (ClassCastException cce) {\n            throw new MockitoException(join(\n                    \"ClassCastException occurred while creating the mockito mock :\",\n                    \"  class to mock : \" + describeClass(mockedProxyType),\n                    \"  created class : \" + describeClass(settings.getTypeToMock()),\n                    \"  proxy instance class : \" + describeClass(mockInstance),\n                    \"  instance creation by : \" + instantiator.getClass().getSimpleName(),\n                    \"\",\n                    \"You might experience classloading issues, please ask the mockito mailing-list.\",\n                    \"\"\n            ),cce);\n        } catch (org.mockito.internal.creation.instance.InstantiationException e) {\n            throw new MockitoException(\"Unable to create mock instance of type '\" + mockedProxyType.getSuperclass().getSimpleName() + \"'\", e);\n        }\n    }",
        "fixed_src_wo_comments": "public < T > T createMock ( MockCreationSettings < T > settings , MockHandler handler ) { if ( settings . getSerializableMode ( ) == SerializableMode . ACROSS_CLASSLOADERS ) { throw new MockitoException ( \"Serialization across classloaders not yet supported with ByteBuddyMockMaker\" ) ; } Class < ? extends T > mockedProxyType = cachingMockBytecodeGenerator . get ( settings . getTypeToMock ( ) , settings . getExtraInterfaces ( ) ) ; Instantiator instantiator = new InstantiatorProvider ( ) . getInstantiator ( settings ) ; T mockInstance = null ; try { mockInstance = instantiator . newInstance ( mockedProxyType ) ; MockMethodInterceptor . MockAccess mockAccess = ( MockMethodInterceptor . MockAccess ) mockInstance ; mockAccess . setMockitoInterceptor ( new MockMethodInterceptor ( asInternalMockHandler ( handler ) , settings ) ) ; return ensureMockIsAssignableToMockedType ( settings , mockInstance ) ; } catch ( ClassCastException cce ) { throw new MockitoException ( join ( \"ClassCastException occurred while creating the mockito mock :\" , \"  class to mock : \" + describeClass ( mockedProxyType ) , \"  created class : \" + describeClass ( settings . getTypeToMock ( ) ) , \"  proxy instance class : \" + describeClass ( mockInstance ) , \"  instance creation by : \" + instantiator . getClass ( ) . getSimpleName ( ) , \"\" , \"You might experience classloading issues, please ask the mockito mailing-list.\" , \"\" ) , cce ) ; } catch ( org . mockito . internal . creation . instance . InstantiationException e ) { throw new MockitoException ( \"Unable to create mock instance of type '\" + mockedProxyType . getSuperclass ( ) . getSimpleName ( ) + \"'\" , e ) ; } }",
        "summary": "Allow convenient spying on abstract classes",
        "Description": "I posted this in GoogleCode and was asked to submit in github.\n\nMockito is easy to use when the test needs to provide canned values for a certain method.\n\nBut it gets harder when a canned value isn't sufficient.\n##### Example 1: Fake with trivial Logic\n\n```\ninterface UserAccount {\n  List<String> getEmails();\n  void addEmail(String email);\n  // 12 other methods ...\n}\n```\n\nWhen mocking such domain entity object, it's tedious to manually program getEmails()/addEmail() with when().thenReturn() and to make sure the two methods are logically consistent, that is, getEmails() returns all emails added.\n##### Example 2: callback-style API\n\n```\ninterface AccountService {\n  void getAccount(String id, AsyncCallback<UserAccount> callback);\n}\n```\n\nStubbing AccountService isn't easy. It'd require use of Answer, and the Answer API isn't statically type safe:\n\n```\nwhen(service.getAccount(eq(id), any(AsyncCallback.class)).thenAnswer(new Answer<Void>() {\n  AsyncCallback<UserAccount> callback = (AsyncCallback<UserAccount>) getArguments()[1];\n  ...\n});\n```\n##### Example 3: Uninteresting parameters\n\n```\ninterface AccountRpcService {\n  FutureAccount getAccount(RpcContext context, String id);\n}\n```\n\nNone of the tests care about the context object. It's an uninteresting parameter imposed by the framework.\n\nIf AccountRpcService were directly mocked, all tests would have to use isA() to repetitively mention this uninteresting parameter, like this:\n\n`when(service.getAccount(isA(RpcContext.class), eq(\"id\")).thenReturn(...);`\n\nAnd all other parameters are required to be wrapped in eq().\n#### Proposal\n\nI propose adding support for abstract classes to mockito to make it easier to deal with tests like above:\n##### For example 1\n\n```\nabstract class FakeUserAccount implements UserAccount {\n  private final List<String> emails = new ArrayList<>();\n\n  @Override public void addEmail(String email) {\n    emails.add(email);\n  }\n  @Override List<String> getEmails() {\n    return ImmutableList.copyOf(emails);\n  }\n}\n\n@Fake private FakeUserAccount userAccount; // Mockito instantiates abstract class.\n```\n##### For example 2\n\n```\nabstract class MockAccountService implements AccountService {\n  @Override public void getAccount(String id, AsyncCallback<UserAccount> callback) {\n    callback.onSuccess(getAccount(id));\n  }\n  abstract UserAccount getAccount(String id);\n}\n\n@Fake private MockAccountService service;\n\n...\n\nwhen(service.getAccount(\"id\")).thenReturn(account);\n```\n##### For example 3\n\n```\nabstract class MockAccountRpcService implements AccountRpcService {\n  @Override Future<Account> getAccount(RpcContext context, String id) {\n    checkNotNull(context);  // Common sanity test. Don't have to repeat it in tests.\n    return getAccount(id);\n  }\n\n  abstract Future<Account> getAccount(String id);\n}\n\n@Fake private MockAccountRpcService service;\n\nwhen(service.getAccount(\"id\")).thenReturn(...);\n```\n\nMy work place internally implemented a default Answer to support abstract classes. We found that the support of abstract classes helps us to avoid overusing mocks when we should be using fakes. And in situations like above we get cleaner test code.\n\nBut because it's not integrated in the core Mockito, there are gotchas with our implementation (like, you can't have private/final methods in your fake).\n\nIf the idea sounds okay to give a try, I'll volunteer to submit a patch.\n\nThanks!\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Also, Brice suggested to me that there have been some concerns about supporting \"partial mocks\" because it could \"bring another way to test/design stuff the wrong way\".\n\nI suppose the worry is mostly about not extracting collaborators into separate interfaces, but rather clinging them as abstract methods, resulting in the class doing too much. a.k.a the \"template method\" pattern.\n\nA few reasons I don't think that concern out-weighs the benefits:\n- One can already do crude \"partial mock\" by declaring the not-to-be-mocked methods final, such that only the abstract methods are mocked by mockito. It's a good idea to do so regardless of testing anyway to avoid too-many-choices when subclassing.\n- From my experience in my workplace, it's rare that people would adopt the template-method pattern inappropriately only because they have the partial-mock support. Quite contrary, what we see happen more often is the abuse of mocks. That is, one should not have used mocks at all. A fake would have made the test more readable/maintainable. In example 1 above, UserAccount as a value-type object should have used a fake.\n- A main reason why people tend to abuse Mockito/EasyMock appears to be the inconvenience in implementing fakes compared to how ridiculously easy it is to use mocks. In example 1 above, implementing a FakeUserAccount would require implementing all 14 methods even when the test only cares about two of them. Adding support for \"fake\" makes it easier to create fakes so people have less temptation to make mistakes.\n- IMHO, template method pattern has its place. I can imagine if I'm charged to test JDK AbstractList, I'd probably want to use partial mock so that I can verify whether or how many times the `get(int)` user code is called into when iterator().next() is called. \n\nHopefully that helps to clarify the intent of this proposal.\n"
            },
            {
                "content": "Hey fluentfuture,\n\nThe use cases you described make perfect sense to me.\n\nBasically, you propose that Mockito should offer an API to create partial\nmocks of classes, without providing a class instance? For example, a\noverloaded version of Spy() method that takes a class to mock as parameter,\ninstead of taking an instance of an object?\n\nI think this is a very good idea and we should have it in mockito. Can you\ntell use what API change do you have in mind? And yes, we would love to get\na PR :)\n\n@Brice, thoughts?\n\nCheers!\n\nOn Tuesday, September 30, 2014, fluentfuture notifications@github.com\nwrote:\n\n> Also, Brice suggested to me that there have been some concerns about\n> supporting \"partial mocks\" because it could \"bring another way to\n> test/design stuff the wrong way\".\n> \n> I suppose the worry is mostly about not extracting collaborators into\n> separate interfaces, but rather clinging them as abstract methods,\n> resulting in the class doing too much. a.k.a the \"template method\" pattern.\n> \n> A few reasons I don't think that concern out-weighs the benefits:\n> - One can already do crude \"partial mock\" by declaring the\n>   not-to-be-mocked methods final, such that only the abstract methods are\n>   mocked by mockito. It's a good idea to do so regardless of testing anyway\n>   to avoid too-many-choices when subclassing.\n> - From my experience in my workplace, it's rare that people would\n>   adopt the template-method pattern inappropriately only because they have\n>   the partial-mock support. Quite contrary, what we see happen more often is\n>   the abuse of mocks. That is, one should not have used mocks at all. A fake\n>   would have made the test more readable/maintainable. For example, in my\n>   example 1, UserAccount as a value-type object should have used a fake.\n> - A main reason why people tend to abuse Mockito/EasyMock is the\n>   inconvenience in implementing fakes compared to how ridiculously easy it is\n>   to use mocks. In example 1 above, implementing a FakeUserAccount would\n>   require implementing all 14 methods even when the test only cares about two\n>   of them. Adding support for \"fake\" makes it easier to create fakes.\n> - IMHO, template method pattern has its place. I can imagine if I'm\n>   charged to test JDK AbstractList, I'd probably want to use partial mock so\n>   that I can verify whether or how many times the get(int) user code is\n>   called into when iterator().next() is called.\n> \n> Hopefully that helps to clarify the intent of this proposal.\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/mockito/mockito/issues/92#issuecomment-57326721.\n\n## \n\nSzczepan Faber\nCore dev@gradle; Founder@mockito\n"
            },
            {
                "content": "Just of info this PR comes from the following [issue 242](https://code.google.com/p/mockito/issues/detail?id=503).\n\nAlso I'm not strongly opinionated on this, however partial mocks have always been a concern. Yet spies can be stubbed. So if this is being developed (which have been made in a previous issue [issue 242](https://code.google.com/p/mockito/issues/detail?id=242)) the documentation should explain how this could be used, and the eventual danger if one chose to go the wrong way.\n\nAnd other topics of where attention must be put\n1. The `@Fake` annotation collides with test doubles terminology, as there are [_dummy_, _**fakes**_, _stubs_ &_mocks_](http://blog.8thlight.com/uncle-bob/2014/05/14/TheLittleMocker.html), it may be wanted. Fakes have a precise definition, I'm not sure partial mocks with/or method template such the use cases proposed here fits in.\n2. For a partial mock, I would tend to report an error when one attempt to stub a defined method. But I'm not sure about this.\n3. On a technical point of view, there are challenges to instantiate the partial mock. i.e objenesis is not an option there. Calling the constructor may be the most safe option but it raises other choices to make, what if the constructor needs parameters.\n   1. The best choice in the right now is to only support parameterless constructor. \n   2. If constructor requires types, maybe use the constructor injection already existing. The engine is not perfect yet it maybe be easily configurable to only perform constructor injection.\n\nThat being said. I don't think it is bad idea, it's concerns that need to be addressed or postponed.\n"
            },
            {
                "content": "Thanks for the thoughts, Szczepan and Brice.\n\nI'll answer Brice's concern and Szczepan's question together.\n\n##### Fake or PartialMock?\n\nI think even though it can be viewed and used as a partial mock to allow a poorly designed class to be mocked. It's not the point. If I have some bad class, I can already partial-mock it, simply by making the methods I don't want to mock as final:\n\n```\nclass BadTemplateClass {\n  final void doIt() {\n    ...\n    Thing thing = getTheOtherThing();\n    ...\n  }\n\n  abstract Thing getTheOtherThing();\n}\n\n@Mock private BadTemplateClass bad;\n\nwhen(bad.getTheOtherThing()).thenReturn(thing);\n```\n\nThat is to say, not adding the proposed support doesn't do much to prevent this kind of bad design. Although, not telling people that they could use Mockito this way may help?\n\nBut then it comes to my point of naming it @Fake, because it's the real intent from users' perspective, or, put in another way, how we want it to be perceived and used by users: you can create fakes with it.\n\nA fake differs from mocks in that it can have its own state and behavior, as in the FakeUserAccount example.\n\n##### Report error on misuse.\n\nSounds reasonable to me. I'm not clear on how easy it is to implement in Mockito. From my experience of using our internal implementation, it hasn't become a real issue that people mistakenly call when(foo.notMockedMethod()). In fact, doesn't Mockito already report error for such case? It sees a when() call but no mockable invocation precedes it.\n\n##### Instantiation\n\nI think we definitely need the constructor to be invoked. Otherwise the fake would be in uninitialized state. In the FakeUserAccount example, the final `emails` field would be null.\n\nWhat then if the class has constructor parameters? Or, what if the fake needs to use other mock objects?\n\nI'll use an example to demo how we use it internally:\n\n```\nabstract class Player {\n  Player(Buddy buddy) {...}\n}\n\npublic class FooTest {\n  @Mock private Buddy buddy; \n  @Fake private FakePlayer player;\n\n  abstract class FakePlayer extends Player {\n    FakePlayer() {\n      super(buddy);\n    }\n  }\n}\n\n```\n\nWhen we inject @Fake fields into the test object, we have the enclosing test instance. So if the class is a non-static inner class of FooTest, we pass the FooTest instance to FakePlayer's constructor.\n\nThis actually allows us to create fakes that can access any arbitrary state managed by the test.\n\nThere are also uncommon cases where we do not hope to invoke the constructor (for example, if it's a class with too many dependencies and none are needed for the purpose of the current test). For that, we just mark the constructor private. Because cglib cannot invoke private constructors, constructor is skipped in such case.\n"
            },
            {
                "content": "Hey guys,\n\nI agree cglib is not viable for this scenario. We could introduce new methods:\n\na) mock(Foo.class, withSettings().spyConstructorArgs(...))\nb) spy(Foo.class, Object ... constructorArgs); //delegates to method a)\n\nAre there compelling reasons for introducing another annotation type? We could create an instance of a @Spy if the user have not provided an instance.\n\nThoughts?\n"
            },
            {
                "content": "Hi Szczepan,\n\nDid you mean to say _Objenesis_ not viable?\n\nRegarding the mock() or spy() call, I haven't found that users need that level of power/flexibility at the cost of sacrificing static type safety and the API looking reflective. It could be used by other framework-ish code. But such utility isn't for everyday testing.\n\nWhat I found so far, with the support of non-static inner class and the framework injecting the \"this$0\" enclosing instance automatically, it already solves the \"arbitrary constructor parameter\" problem. And the code is completely static type safe.\n\nSo my suggestion is to support non-static inner class but not arbitrary constructor parameters.\n\nEither\n\n```\nFoo foo = spy(Foo.class);\n```\n\nor use annotation\n\n```\n@Spy private Foo foo;\n\nMockitoAnnotations.initMocks(this);\n```\n\nInner classes are only created with the annotation and initMocks().\n"
            },
            {
                "content": "While implementing it. I realized that there is already `@Spy` field injection when the user doesn't provide an instance: \nhttp://docs.mockito.googlecode.com/hg/1.9.5/org/mockito/Spy.html\n\nSo we can't reuse `@Spy` unless we are willing to break existing users.\n\nDoes any of  `@Partial`, `@PartialMock`, `@MockAbstract` or `@Fake` sound okay to add?\n"
            },
            {
                "content": "> So we can't reuse @Spy https://github.com/Spy unless we are willing to\n> break existing users.\n\nWhy would we break existing users?\n\nFeel free to work on the java API first, we can deal with an annotation\nlater.\n\nCheers!\n\nOn Sun, Oct 12, 2014 at 6:15 AM, fluentfuture notifications@github.com\nwrote:\n\n> While implementing it. I realized that there is already @Spy\n> https://github.com/Spy field injection when the user doesn't provide an\n> instance:\n> http://docs.mockito.googlecode.com/hg/1.9.5/org/mockito/Spy.html\n> \n> So we can't reuse @Spy https://github.com/Spy unless we are willing to\n> break existing users.\n> \n> Does any of @Partial https://github.com/Partial, @PartialMock or @Fake\n> https://github.com/Fake sound okay to add?\n> \n> ## \n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/mockito/mockito/issues/92#issuecomment-58773021.\n\n## \n\nSzczepan Faber\nCore dev@gradle; Founder@mockito\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to introduce a new API to create partial mocks of classes, without providing a class instance. This API would take a class to mock as parameter, instead of taking an instance of an object. It would be called mock() or spy() and would be supported by an annotation, such as @Partial, @PartialMock, @MockAbstract, or @Fake. The constructor would need to be invoked, and the constructor parameters could be provided by the enclosing test instance. The documentation should explain the potential danger of using this API incorrectly."
    },
    "Chart_11_source/org/jfree/chart/util/ShapeUtilities.java_264_296": {
        "src": "public static boolean equal(GeneralPath p1, GeneralPath p2) {\n        if (p1 == null) {\n            return (p2 == null);\n        }\n        if (p2 == null) {\n            return false;\n        }\n        if (p1.getWindingRule() != p2.getWindingRule()) {\n            return false;\n        }\n        PathIterator iterator1 = p1.getPathIterator(null);\n        PathIterator iterator2 = p1.getPathIterator(null);\n        double[] d1 = new double[6];\n        double[] d2 = new double[6];\n        boolean done = iterator1.isDone() && iterator2.isDone();\n        while (!done) {\n            if (iterator1.isDone() != iterator2.isDone()) {\n                return false;\n            }\n            int seg1 = iterator1.currentSegment(d1);\n            int seg2 = iterator2.currentSegment(d2);\n            if (seg1 != seg2) {\n                return false;\n            }\n            if (!Arrays.equals(d1, d2)) {\n                return false;\n            }\n            iterator1.next();\n            iterator2.next();\n            done = iterator1.isDone() && iterator2.isDone();\n        }\n        return true;\n    }",
        "src_wo_comments": "public static boolean equal ( GeneralPath p1 , GeneralPath p2 ) { if ( p1 == null ) { return ( p2 == null ) ; } if ( p2 == null ) { return false ; } if ( p1 . getWindingRule ( ) != p2 . getWindingRule ( ) ) { return false ; } PathIterator iterator1 = p1 . getPathIterator ( null ) ; PathIterator iterator2 = p1 . getPathIterator ( null ) ; double [ ] d1 = new double [ 6 ] ; double [ ] d2 = new double [ 6 ] ; boolean done = iterator1 . isDone ( ) && iterator2 . isDone ( ) ; while ( ! done ) { if ( iterator1 . isDone ( ) != iterator2 . isDone ( ) ) { return false ; } int seg1 = iterator1 . currentSegment ( d1 ) ; int seg2 = iterator2 . currentSegment ( d2 ) ; if ( seg1 != seg2 ) { return false ; } if ( ! Arrays . equals ( d1 , d2 ) ) { return false ; } iterator1 . next ( ) ; iterator2 . next ( ) ; done = iterator1 . isDone ( ) && iterator2 . isDone ( ) ; } return true ; }",
        "fixed_src": "public static boolean equal(GeneralPath p1, GeneralPath p2) {\n        if (p1 == null) {\n            return (p2 == null);\n        }\n        if (p2 == null) {\n            return false;\n        }\n        if (p1.getWindingRule() != p2.getWindingRule()) {\n            return false;\n        }\n        PathIterator iterator1 = p1.getPathIterator(null);\n        PathIterator iterator2 = p2.getPathIterator(null);\n        double[] d1 = new double[6];\n        double[] d2 = new double[6];\n        boolean done = iterator1.isDone() && iterator2.isDone();\n        while (!done) {\n            if (iterator1.isDone() != iterator2.isDone()) {\n                return false;\n            }\n            int seg1 = iterator1.currentSegment(d1);\n            int seg2 = iterator2.currentSegment(d2);\n            if (seg1 != seg2) {\n                return false;\n            }\n            if (!Arrays.equals(d1, d2)) {\n                return false;\n            }\n            iterator1.next();\n            iterator2.next();\n            done = iterator1.isDone() && iterator2.isDone();\n        }\n        return true;\n    }",
        "fixed_src_wo_comments": "public static boolean equal ( GeneralPath p1 , GeneralPath p2 ) { if ( p1 == null ) { return ( p2 == null ) ; } if ( p2 == null ) { return false ; } if ( p1 . getWindingRule ( ) != p2 . getWindingRule ( ) ) { return false ; } PathIterator iterator1 = p1 . getPathIterator ( null ) ; PathIterator iterator2 = p2 . getPathIterator ( null ) ; double [ ] d1 = new double [ 6 ] ; double [ ] d2 = new double [ 6 ] ; boolean done = iterator1 . isDone ( ) && iterator2 . isDone ( ) ; while ( ! done ) { if ( iterator1 . isDone ( ) != iterator2 . isDone ( ) ) { return false ; } int seg1 = iterator1 . currentSegment ( d1 ) ; int seg2 = iterator2 . currentSegment ( d2 ) ; if ( seg1 != seg2 ) { return false ; } if ( ! Arrays . equals ( d1 , d2 ) ) { return false ; } iterator1 . next ( ) ; iterator2 . next ( ) ; done = iterator1 . isDone ( ) && iterator2 . isDone ( ) ; } return true ; }",
        "summary": "JCommon 1.0.12 ShapeUtilities.equal(path1,path2)",
        "Description": "The comparison of two GeneralPath objects uses the same PathIterator for both objects. equal(GeneralPath path1, GeneralPath path2) will thus return true for any pair of non-null GeneralPath instances having the same windingRule.",
        "issue_url": "https://sourceforge.net/p/jfreechart/bugs/868/",
        "comments": [
            {
                "content": "assigned_to: nobody --> mungady"
            },
            {
                "content": "Logged In: YES\nuser_id=112975\nOriginator: NO\n\nThanks for the report. I fixed the bug, which then (when rerunning the JUnit tests) uncovered a bug in the serialization for GeneralPath. I'll commit the fixes to CVS shortly.\n\nRegards,\n\nDave Gilbert\nJFreeChart Project Leader"
            },
            {
                "content": "Logged In: YES\nuser_id=112975\nOriginator: NO\n\nFixed in CVS for the upcoming 1.0.13 release.\n\nRegards,\n\nDave Gilbert\nJFreeChart Project Leader"
            },
            {
                "content": "labels: --> jcommon\nstatus: open --> closed-fixed"
            }
        ],
        "summarized_discussion": "\n\nThe bug was fixed by Dave Gilbert, the JFreeChart Project Leader. The fix was committed to CVS and will be included in the upcoming 1.0.13 release. The labels were changed to jcommon and the status was changed to closed-fixed. The assigned_to was changed to mungady."
    },
    "Math_87_src/java/org/apache/commons/math/optimization/linear/SimplexTableau.java_272_284": {
        "src": "private Integer getBasicRow(final int col) {\n        Integer row = null;\n        for (int i = getNumObjectiveFunctions(); i < getHeight(); i++) {\n            if (!MathUtils.equals(getEntry(i, col), 0.0, epsilon)) {\n                if (row == null) {\n                row = i;\n                } else {\n                return null;\n                }\n            }\n        }\n        return row;\n    }",
        "src_wo_comments": "private Integer getBasicRow ( final int col ) { Integer row = null ; for ( int i = getNumObjectiveFunctions ( ) ; i < getHeight ( ) ; i ++ ) { if ( ! MathUtils . equals ( getEntry ( i , col ) , 0.0 , epsilon ) ) { if ( row == null ) { row = i ; } else { return null ; } } } return row ; }",
        "fixed_src": "private Integer getBasicRow(final int col) {\n        Integer row = null;\n        for (int i = getNumObjectiveFunctions(); i < getHeight(); i++) {\n            if (MathUtils.equals(getEntry(i, col), 1.0, epsilon) && (row == null)) {\n                row = i;\n            } else if (!MathUtils.equals(getEntry(i, col), 0.0, epsilon)) {\n                return null;\n            }\n        }\n        return row;\n    }",
        "fixed_src_wo_comments": "private Integer getBasicRow ( final int col ) { Integer row = null ; for ( int i = getNumObjectiveFunctions ( ) ; i < getHeight ( ) ; i ++ ) { if ( MathUtils . equals ( getEntry ( i , col ) , 1.0 , epsilon ) && ( row == null ) ) { row = i ; } else if ( ! MathUtils . equals ( getEntry ( i , col ) , 0.0 , epsilon ) ) { return null ; } } return row ; }",
        "summary": "Basic variable is not found correctly in simplex tableau",
        "Description": "The last patch to SimplexTableau caused an automated test suite I'm running at work to go down a new code path and uncover what is hopefully the last bug remaining in the Simplex code.\nSimplexTableau was assuming an entry in the tableau had to be nonzero to indicate a basic variable, which is incorrect - the entry should have a value equal to 1.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-273",
        "comments": [
            "Here's the patch.",
            "fixed in subversion repository as of r781304\npatch applied\nthanks",
            "closing resolved issue for 2.0 release"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of revision number 781304, and the patch has been applied. The issue has been closed for the 2.0 release."
    },
    "Csv_6_src/main/java/org/apache/commons/csv/CSVRecord.java_179_185": {
        "src": "<M extends Map<String, String>> M putIn(final M map) {\n        for (final Entry<String, Integer> entry : mapping.entrySet()) {\n            final int col = entry.getValue().intValue();\n                map.put(entry.getKey(), values[col]);\n        }\n        return map;\n    }",
        "src_wo_comments": "< M extends Map < String , String > > M putIn ( final M map ) { for ( final Entry < String , Integer > entry : mapping . entrySet ( ) ) { final int col = entry . getValue ( ) . intValue ( ) ; map . put ( entry . getKey ( ) , values [ col ] ) ; } return map ; }",
        "fixed_src": "<M extends Map<String, String>> M putIn(final M map) {\n        for (final Entry<String, Integer> entry : mapping.entrySet()) {\n            final int col = entry.getValue().intValue();\n            if (col < values.length) {\n                map.put(entry.getKey(), values[col]);\n            }\n        }\n        return map;\n    }",
        "fixed_src_wo_comments": "< M extends Map < String , String > > M putIn ( final M map ) { for ( final Entry < String , Integer > entry : mapping . entrySet ( ) ) { final int col = entry . getValue ( ) . intValue ( ) ; if ( col < values . length ) { map . put ( entry . getKey ( ) , values [ col ] ) ; } } return map ; }",
        "summary": "CSVRecord.toMap() fails if row length shorter than header length",
        "Description": "Similar to CSV-96, if .toMap() is called on a record that has fewer fields than we have header columns we'll get an ArrayOutOfBoundsException.\n\n{code}\n@Test\npublic void testToMapWhenHeaderTooLong() throws Exception {\n   final CSVParser parser = new CSVParser(\"a,b\", CSVFormat.newBuilder().withHeader(\"A\", \"B\", \"C\").build());\n   final CSVRecord record = parser.iterator().next();\n   record.toMap();\n}\n{code}",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-111",
        "comments": [
            "{noformat}\ncommit -m \"[CSV-111] CSVRecord.toMap() fails if row length shorter than header length.\" C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVRecord.java C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVRecordTest.java\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVRecord.java\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVRecordTest.java\n    Transmitting file data ...\n    Committed revision 1589281.\n{noformat}"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to commit changes to the CSVRecord.toMap() method in C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVRecord.java and C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVRecordTest.java with the commit message \"[CSV-111] CSVRecord.toMap() fails if row length shorter than header length.\" The revision number of the commit is 1589281."
    },
    "Csv_15_src/main/java/org/apache/commons/csv/CSVFormat.java_1151_1256": {
        "src": "private void printAndQuote(final Object object, final CharSequence value, final int offset, final int len,\n            final Appendable out, final boolean newRecord) throws IOException {\n        boolean quote = false;\n        int start = offset;\n        int pos = offset;\n        final int end = offset + len;\n\n        final char delimChar = getDelimiter();\n        final char quoteChar = getQuoteCharacter().charValue();\n\n        QuoteMode quoteModePolicy = getQuoteMode();\n        if (quoteModePolicy == null) {\n            quoteModePolicy = QuoteMode.MINIMAL;\n        }\n        switch (quoteModePolicy) {\n        case ALL:\n        case ALL_NON_NULL:\n            quote = true;\n            break;\n        case NON_NUMERIC:\n            quote = !(object instanceof Number);\n            break;\n        case NONE:\n            // Use the existing escaping code\n            printAndEscape(value, offset, len, out);\n            return;\n        case MINIMAL:\n            if (len <= 0) {\n                // always quote an empty token that is the first\n                // on the line, as it may be the only thing on the\n                // line. If it were not quoted in that case,\n                // an empty line has no tokens.\n                if (newRecord) {\n                    quote = true;\n                }\n            } else {\n                char c = value.charAt(pos);\n\n                if (newRecord && (c < 0x20 || c > 0x21 && c < 0x23 || c > 0x2B && c < 0x2D || c > 0x7E)) {\n                    quote = true;\n                } else if (c <= COMMENT) {\n                    // Some other chars at the start of a value caused the parser to fail, so for now\n                    // encapsulate if we start in anything less than '#'. We are being conservative\n                    // by including the default comment char too.\n                    quote = true;\n                } else {\n                    while (pos < end) {\n                        c = value.charAt(pos);\n                        if (c == LF || c == CR || c == quoteChar || c == delimChar) {\n                            quote = true;\n                            break;\n                        }\n                        pos++;\n                    }\n\n                    if (!quote) {\n                        pos = end - 1;\n                        c = value.charAt(pos);\n                        // Some other chars at the end caused the parser to fail, so for now\n                        // encapsulate if we end in anything less than ' '\n                        if (c <= SP) {\n                            quote = true;\n                        }\n                    }\n                }\n            }\n\n            if (!quote) {\n                // no encapsulation needed - write out the original value\n                out.append(value, start, end);\n                return;\n            }\n            break;\n        default:\n            throw new IllegalStateException(\"Unexpected Quote value: \" + quoteModePolicy);\n        }\n\n        if (!quote) {\n            // no encapsulation needed - write out the original value\n            out.append(value, start, end);\n            return;\n        }\n\n        // we hit something that needed encapsulation\n        out.append(quoteChar);\n\n        // Pick up where we left off: pos should be positioned on the first character that caused\n        // the need for encapsulation.\n        while (pos < end) {\n            final char c = value.charAt(pos);\n            if (c == quoteChar) {\n                // write out the chunk up until this point\n\n                // add 1 to the length to write out the encapsulator also\n                out.append(value, start, pos + 1);\n                // put the next starting position on the encapsulator so we will\n                // write it out again with the next string (effectively doubling it)\n                start = pos;\n            }\n            pos++;\n        }\n\n        // write the last segment\n        out.append(value, start, pos);\n        out.append(quoteChar);\n    }",
        "src_wo_comments": "private void printAndQuote ( final Object object , final CharSequence value , final int offset , final int len , final Appendable out , final boolean newRecord ) throws IOException { boolean quote = false ; int start = offset ; int pos = offset ; final int end = offset + len ; final char delimChar = getDelimiter ( ) ; final char quoteChar = getQuoteCharacter ( ) . charValue ( ) ; QuoteMode quoteModePolicy = getQuoteMode ( ) ; if ( quoteModePolicy == null ) { quoteModePolicy = QuoteMode . MINIMAL ; } switch ( quoteModePolicy ) { case ALL : case ALL_NON_NULL : quote = true ; break ; case NON_NUMERIC : quote = ! ( object instanceof Number ) ; break ; case NONE : printAndEscape ( value , offset , len , out ) ; return ; case MINIMAL : if ( len <= 0 ) { if ( newRecord ) { quote = true ; } } else { char c = value . charAt ( pos ) ; if ( newRecord && ( c < 0x20 || c > 0x21 && c < 0x23 || c > 0x2B && c < 0x2D || c > 0x7E ) ) { quote = true ; } else if ( c <= COMMENT ) { quote = true ; } else { while ( pos < end ) { c = value . charAt ( pos ) ; if ( c == LF || c == CR || c == quoteChar || c == delimChar ) { quote = true ; break ; } pos ++ ; } if ( ! quote ) { pos = end - 1 ; c = value . charAt ( pos ) ; if ( c <= SP ) { quote = true ; } } } } if ( ! quote ) { out . append ( value , start , end ) ; return ; } break ; default : throw new IllegalStateException ( \"Unexpected Quote value: \" + quoteModePolicy ) ; } if ( ! quote ) { out . append ( value , start , end ) ; return ; } out . append ( quoteChar ) ; while ( pos < end ) { final char c = value . charAt ( pos ) ; if ( c == quoteChar ) { out . append ( value , start , pos + 1 ) ; start = pos ; } pos ++ ; } out . append ( value , start , pos ) ; out . append ( quoteChar ) ; }",
        "fixed_src": "private void printAndQuote(final Object object, final CharSequence value, final int offset, final int len,\n            final Appendable out, final boolean newRecord) throws IOException {\n        boolean quote = false;\n        int start = offset;\n        int pos = offset;\n        final int end = offset + len;\n\n        final char delimChar = getDelimiter();\n        final char quoteChar = getQuoteCharacter().charValue();\n\n        QuoteMode quoteModePolicy = getQuoteMode();\n        if (quoteModePolicy == null) {\n            quoteModePolicy = QuoteMode.MINIMAL;\n        }\n        switch (quoteModePolicy) {\n        case ALL:\n        case ALL_NON_NULL:\n            quote = true;\n            break;\n        case NON_NUMERIC:\n            quote = !(object instanceof Number);\n            break;\n        case NONE:\n            // Use the existing escaping code\n            printAndEscape(value, offset, len, out);\n            return;\n        case MINIMAL:\n            if (len <= 0) {\n                // always quote an empty token that is the first\n                // on the line, as it may be the only thing on the\n                // line. If it were not quoted in that case,\n                // an empty line has no tokens.\n                if (newRecord) {\n                    quote = true;\n                }\n            } else {\n                char c = value.charAt(pos);\n\n                if (c <= COMMENT) {\n                    // Some other chars at the start of a value caused the parser to fail, so for now\n                    // encapsulate if we start in anything less than '#'. We are being conservative\n                    // by including the default comment char too.\n                    quote = true;\n                } else {\n                    while (pos < end) {\n                        c = value.charAt(pos);\n                        if (c == LF || c == CR || c == quoteChar || c == delimChar) {\n                            quote = true;\n                            break;\n                        }\n                        pos++;\n                    }\n\n                    if (!quote) {\n                        pos = end - 1;\n                        c = value.charAt(pos);\n                        // Some other chars at the end caused the parser to fail, so for now\n                        // encapsulate if we end in anything less than ' '\n                        if (c <= SP) {\n                            quote = true;\n                        }\n                    }\n                }\n            }\n\n            if (!quote) {\n                // no encapsulation needed - write out the original value\n                out.append(value, start, end);\n                return;\n            }\n            break;\n        default:\n            throw new IllegalStateException(\"Unexpected Quote value: \" + quoteModePolicy);\n        }\n\n        if (!quote) {\n            // no encapsulation needed - write out the original value\n            out.append(value, start, end);\n            return;\n        }\n\n        // we hit something that needed encapsulation\n        out.append(quoteChar);\n\n        // Pick up where we left off: pos should be positioned on the first character that caused\n        // the need for encapsulation.\n        while (pos < end) {\n            final char c = value.charAt(pos);\n            if (c == quoteChar) {\n                // write out the chunk up until this point\n\n                // add 1 to the length to write out the encapsulator also\n                out.append(value, start, pos + 1);\n                // put the next starting position on the encapsulator so we will\n                // write it out again with the next string (effectively doubling it)\n                start = pos;\n            }\n            pos++;\n        }\n\n        // write the last segment\n        out.append(value, start, pos);\n        out.append(quoteChar);\n    }",
        "fixed_src_wo_comments": "private void printAndQuote ( final Object object , final CharSequence value , final int offset , final int len , final Appendable out , final boolean newRecord ) throws IOException { boolean quote = false ; int start = offset ; int pos = offset ; final int end = offset + len ; final char delimChar = getDelimiter ( ) ; final char quoteChar = getQuoteCharacter ( ) . charValue ( ) ; QuoteMode quoteModePolicy = getQuoteMode ( ) ; if ( quoteModePolicy == null ) { quoteModePolicy = QuoteMode . MINIMAL ; } switch ( quoteModePolicy ) { case ALL : case ALL_NON_NULL : quote = true ; break ; case NON_NUMERIC : quote = ! ( object instanceof Number ) ; break ; case NONE : printAndEscape ( value , offset , len , out ) ; return ; case MINIMAL : if ( len <= 0 ) { if ( newRecord ) { quote = true ; } } else { char c = value . charAt ( pos ) ; if ( c <= COMMENT ) { quote = true ; } else { while ( pos < end ) { c = value . charAt ( pos ) ; if ( c == LF || c == CR || c == quoteChar || c == delimChar ) { quote = true ; break ; } pos ++ ; } if ( ! quote ) { pos = end - 1 ; c = value . charAt ( pos ) ; if ( c <= SP ) { quote = true ; } } } } if ( ! quote ) { out . append ( value , start , end ) ; return ; } break ; default : throw new IllegalStateException ( \"Unexpected Quote value: \" + quoteModePolicy ) ; } if ( ! quote ) { out . append ( value , start , end ) ; return ; } out . append ( quoteChar ) ; while ( pos < end ) { final char c = value . charAt ( pos ) ; if ( c == quoteChar ) { out . append ( value , start , pos + 1 ) ; start = pos ; } pos ++ ; } out . append ( value , start , pos ) ; out . append ( quoteChar ) ; }",
        "summary": "The behavior of quote char using is not similar as Excel does when the first string contains CJK char(s)",
        "Description": "When using CSVFormat.EXCEL to print a CSV file, the behavior of quote char using is not similar as Microsoft Excel does when the first string contains Chinese, Japanese or Korean (CJK) char(s).\r\n\r\ne.g.\r\nThere are 3 data members in a record, with Japanese chars: \"\u3042\", \"\u3044\", \"\u3046\":\r\n  Microsoft Excel outputs:\r\n  \u3042,\u3044,\u3046\r\n  Apache Common CSV outputs:\r\n  \"\u3042\",\u3044,\u3046\r\n",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-219",
        "comments": [
            "This is the patch to solve the problem. ",
            "Our quoting seems off IMO. Why not simply do:\r\n{noformat}\r\ndiff --git a/src/main/java/org/apache/commons/csv/CSVFormat.java b/src/main/java/org/apache/commons/csv/CSVFormat.java\r\nindex 58948fd..dc7588b 100644\r\n--- a/src/main/java/org/apache/commons/csv/CSVFormat.java\r\n+++ b/src/main/java/org/apache/commons/csv/CSVFormat.java\r\n@@ -1186,10 +1186,7 @@\r\n             } else {\r\n                 char c = value.charAt(pos);\r\n \r\n-                // RFC4180 (https://tools.ietf.org/html/rfc4180) TEXTDATA =  %x20-21 / %x23-2B / %x2D-7E\r\n-                if (newRecord && (c < 0x20 || c > 0x21 && c < 0x23 || c > 0x2B && c < 0x2D || c > 0x7E)) {\r\n-                    quote = true;\r\n-                } else if (c <= COMMENT) {\r\n+                if (c <= COMMENT) {\r\n                     // Some other chars at the start of a value caused the parser to fail, so for now\r\n                     // encapsulate if we start in anything less than '#'. We are being conservative\r\n                     // by including the default comment char too.\r\ndiff --git a/src/test/java/org/apache/commons/csv/CSVPrinterTest.java b/src/test/java/org/apache/commons/csv/CSVPrinterTest.java\r\nindex ae7aae2..5a09627 100644\r\n--- a/src/test/java/org/apache/commons/csv/CSVPrinterTest.java\r\n+++ b/src/test/java/org/apache/commons/csv/CSVPrinterTest.java\r\n@@ -1033,11 +1033,20 @@\r\n     }\r\n \r\n     @Test\r\n-    public void testRfc4180QuoteSingleChar() throws IOException {\r\n+    public void testDontQuoteEuroFirstChar() throws IOException {\r\n         final StringWriter sw = new StringWriter();\r\n         try (final CSVPrinter printer = new CSVPrinter(sw, CSVFormat.RFC4180)) {\r\n             printer.printRecord(EURO_CH, \"Deux\");\r\n-            assertEquals(\"\\\"\" + EURO_CH + \"\\\",Deux\" + recordSeparator, sw.toString());\r\n+            assertEquals(EURO_CH + \",Deux\" + recordSeparator, sw.toString());\r\n+        }\r\n+    }\r\n+\r\n+    @Test\r\n+    public void testQuoteCommaFirstChar() throws IOException {\r\n+        final StringWriter sw = new StringWriter();\r\n+        try (final CSVPrinter printer = new CSVPrinter(sw, CSVFormat.RFC4180)) {\r\n+            printer.printRecord(\",\");\r\n+            assertEquals(\"\\\",\\\"\" + recordSeparator, sw.toString());\r\n         }\r\n     }\r\n{noformat}\r\nI do not see why the first char in a record being not in TEXTDATA should quote the first field.\r\n\r\nThoughts from other. With the above patch, all tests pass.",
            "Thanks Gary. By the way, I even deleted the \"if (c <= COMMENT) {\" branch to meet what the Microsoft Excel does in my job. But when I have read the comment under it, I am not sure if it is a bug or a feature. I think this sourcecode is only affect to the printing of a CSV file, but has no no effect on the parser. When I tried to parse a CSV file with a char which ASCII code less than '#' but more or equal than 0x20 without quoting it, I didn't find any problem which the comment says. I still wondered in which case the parser will fail in common cases. So for the time being, I am not reporting that as a bug.",
            "The quoting of the first row probably has something to do with excel misreading certain prefixes.  For example, [if the first field starts with 'ID', then excel will interpret the file as SYLK format|https://support.microsoft.com/en-us/help/323626/-sylk-file-format-is-not-valid-error-message-when-you-open-file].  I'm not sure that this is a bug.  Just a different, more conservative format.\r\n\r\nI'd suggest making sure there is a unit test with first field containing \"ID\"",
            "Reading up https://en.m.wikipedia.org/wiki/SYmbolic_LinK_(SYLK)\r\n\r\nIt seems that we need an Excel SYLK format and quote mode.",
            "In git master. Please verify and close.",
            "Thanks Gary very much. I have confirmed and tested the sourcecode of git master, it is OK now.\r\nAlso thanks Charles for the comment."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to update the source code in git master to include an Excel SYLK format and quote mode. This should allow for the quoting of the first row to be read correctly by Excel. The source code has been tested and confirmed to be working correctly."
    },
    "Compress_10_src/main/java/org/apache/commons/compress/archivers/zip/ZipFile.java_801_843": {
        "src": "private void resolveLocalFileHeaderData(Map<ZipArchiveEntry, NameAndComment>\n                                            entriesWithoutUTF8Flag)\n        throws IOException {\n        // changing the name of a ZipArchiveEntry is going to change\n        // the hashcode - see COMPRESS-164\n        // Map needs to be reconstructed in order to keep central\n        // directory order\n        for (ZipArchiveEntry ze : entries.keySet()) {\n            OffsetEntry offsetEntry = entries.get(ze);\n            long offset = offsetEntry.headerOffset;\n            archive.seek(offset + LFH_OFFSET_FOR_FILENAME_LENGTH);\n            byte[] b = new byte[SHORT];\n            archive.readFully(b);\n            int fileNameLen = ZipShort.getValue(b);\n            archive.readFully(b);\n            int extraFieldLen = ZipShort.getValue(b);\n            int lenToSkip = fileNameLen;\n            while (lenToSkip > 0) {\n                int skipped = archive.skipBytes(lenToSkip);\n                if (skipped <= 0) {\n                    throw new RuntimeException(\"failed to skip file name in\"\n                                               + \" local file header\");\n                }\n                lenToSkip -= skipped;\n            }\n            byte[] localExtraData = new byte[extraFieldLen];\n            archive.readFully(localExtraData);\n            ze.setExtra(localExtraData);\n            offsetEntry.dataOffset = offset + LFH_OFFSET_FOR_FILENAME_LENGTH\n                + SHORT + SHORT + fileNameLen + extraFieldLen;\n\n            if (entriesWithoutUTF8Flag.containsKey(ze)) {\n                String orig = ze.getName();\n                NameAndComment nc = entriesWithoutUTF8Flag.get(ze);\n                ZipUtil.setNameAndCommentFromExtraFields(ze, nc.name,\n                                                         nc.comment);\n                if (!orig.equals(ze.getName())) {\n                    nameMap.remove(orig);\n                    nameMap.put(ze.getName(), ze);\n                }\n            }\n        }\n    }",
        "src_wo_comments": "private void resolveLocalFileHeaderData ( Map < ZipArchiveEntry , NameAndComment > entriesWithoutUTF8Flag ) throws IOException { for ( ZipArchiveEntry ze : entries . keySet ( ) ) { OffsetEntry offsetEntry = entries . get ( ze ) ; long offset = offsetEntry . headerOffset ; archive . seek ( offset + LFH_OFFSET_FOR_FILENAME_LENGTH ) ; byte [ ] b = new byte [ SHORT ] ; archive . readFully ( b ) ; int fileNameLen = ZipShort . getValue ( b ) ; archive . readFully ( b ) ; int extraFieldLen = ZipShort . getValue ( b ) ; int lenToSkip = fileNameLen ; while ( lenToSkip > 0 ) { int skipped = archive . skipBytes ( lenToSkip ) ; if ( skipped <= 0 ) { throw new RuntimeException ( \"failed to skip file name in\" + \" local file header\" ) ; } lenToSkip -= skipped ; } byte [ ] localExtraData = new byte [ extraFieldLen ] ; archive . readFully ( localExtraData ) ; ze . setExtra ( localExtraData ) ; offsetEntry . dataOffset = offset + LFH_OFFSET_FOR_FILENAME_LENGTH + SHORT + SHORT + fileNameLen + extraFieldLen ; if ( entriesWithoutUTF8Flag . containsKey ( ze ) ) { String orig = ze . getName ( ) ; NameAndComment nc = entriesWithoutUTF8Flag . get ( ze ) ; ZipUtil . setNameAndCommentFromExtraFields ( ze , nc . name , nc . comment ) ; if ( ! orig . equals ( ze . getName ( ) ) ) { nameMap . remove ( orig ) ; nameMap . put ( ze . getName ( ) , ze ) ; } } } }",
        "fixed_src": "private void resolveLocalFileHeaderData(Map<ZipArchiveEntry, NameAndComment>\n                                            entriesWithoutUTF8Flag)\n        throws IOException {\n        // changing the name of a ZipArchiveEntry is going to change\n        // the hashcode - see COMPRESS-164\n        // Map needs to be reconstructed in order to keep central\n        // directory order\n        Map<ZipArchiveEntry, OffsetEntry> origMap =\n            new LinkedHashMap<ZipArchiveEntry, OffsetEntry>(entries);\n        entries.clear();\n        for (ZipArchiveEntry ze : origMap.keySet()) {\n            OffsetEntry offsetEntry = origMap.get(ze);\n            long offset = offsetEntry.headerOffset;\n            archive.seek(offset + LFH_OFFSET_FOR_FILENAME_LENGTH);\n            byte[] b = new byte[SHORT];\n            archive.readFully(b);\n            int fileNameLen = ZipShort.getValue(b);\n            archive.readFully(b);\n            int extraFieldLen = ZipShort.getValue(b);\n            int lenToSkip = fileNameLen;\n            while (lenToSkip > 0) {\n                int skipped = archive.skipBytes(lenToSkip);\n                if (skipped <= 0) {\n                    throw new RuntimeException(\"failed to skip file name in\"\n                                               + \" local file header\");\n                }\n                lenToSkip -= skipped;\n            }\n            byte[] localExtraData = new byte[extraFieldLen];\n            archive.readFully(localExtraData);\n            ze.setExtra(localExtraData);\n            offsetEntry.dataOffset = offset + LFH_OFFSET_FOR_FILENAME_LENGTH\n                + SHORT + SHORT + fileNameLen + extraFieldLen;\n\n            if (entriesWithoutUTF8Flag.containsKey(ze)) {\n                String orig = ze.getName();\n                NameAndComment nc = entriesWithoutUTF8Flag.get(ze);\n                ZipUtil.setNameAndCommentFromExtraFields(ze, nc.name,\n                                                         nc.comment);\n                if (!orig.equals(ze.getName())) {\n                    nameMap.remove(orig);\n                    nameMap.put(ze.getName(), ze);\n                }\n            }\n            entries.put(ze, offsetEntry);\n        }\n    }",
        "fixed_src_wo_comments": "private void resolveLocalFileHeaderData ( Map < ZipArchiveEntry , NameAndComment > entriesWithoutUTF8Flag ) throws IOException { Map < ZipArchiveEntry , OffsetEntry > origMap = new LinkedHashMap < ZipArchiveEntry , OffsetEntry > ( entries ) ; entries . clear ( ) ; for ( ZipArchiveEntry ze : origMap . keySet ( ) ) { OffsetEntry offsetEntry = origMap . get ( ze ) ; long offset = offsetEntry . headerOffset ; archive . seek ( offset + LFH_OFFSET_FOR_FILENAME_LENGTH ) ; byte [ ] b = new byte [ SHORT ] ; archive . readFully ( b ) ; int fileNameLen = ZipShort . getValue ( b ) ; archive . readFully ( b ) ; int extraFieldLen = ZipShort . getValue ( b ) ; int lenToSkip = fileNameLen ; while ( lenToSkip > 0 ) { int skipped = archive . skipBytes ( lenToSkip ) ; if ( skipped <= 0 ) { throw new RuntimeException ( \"failed to skip file name in\" + \" local file header\" ) ; } lenToSkip -= skipped ; } byte [ ] localExtraData = new byte [ extraFieldLen ] ; archive . readFully ( localExtraData ) ; ze . setExtra ( localExtraData ) ; offsetEntry . dataOffset = offset + LFH_OFFSET_FOR_FILENAME_LENGTH + SHORT + SHORT + fileNameLen + extraFieldLen ; if ( entriesWithoutUTF8Flag . containsKey ( ze ) ) { String orig = ze . getName ( ) ; NameAndComment nc = entriesWithoutUTF8Flag . get ( ze ) ; ZipUtil . setNameAndCommentFromExtraFields ( ze , nc . name , nc . comment ) ; if ( ! orig . equals ( ze . getName ( ) ) ) { nameMap . remove ( orig ) ; nameMap . put ( ze . getName ( ) , ze ) ; } } entries . put ( ze , offsetEntry ) ; } }",
        "summary": "Cannot Read Winzip Archives With Unicode Extra Fields",
        "Description": "I have a zip file created with WinZip containing Unicode extra fields. Upon attempting to extract it with org.apache.commons.compress.archivers.zip.ZipFile, ZipFile.getInputStream() returns null for ZipArchiveEntries previously retrieved with ZipFile.getEntry() or even ZipFile.getEntries(). See UTF8ZipFilesTest.patch in the attachments for a test case exposing the bug. The original test case stopped short of trying to read the entries, that's why this wasn't flagged up before. \n\nThe problem lies in the fact that inside ZipFile.java entries are stored in a HashMap. However, at one point after populating the HashMap, the unicode extra fields are read, which leads to a change of the ZipArchiveEntry name, and therefore a change of its hash code. Because of this, subsequent gets on the HashMap fail to retrieve the original values.\n\nZipFile.patch contains an (admittedly simple-minded) fix for this problem by reconstructing the entries HashMap after the Unicode extra fields have been parsed. The purpose of this patch is mainly to show that the problem is indeed what I think, rather than providing a well-designed solution.\n\nThe patches have been tested against revision 1210416.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-164",
        "comments": [
            "Patch for UTF8ZipFilesTest.java",
            "Patch for ZipFile.java",
            "Fixed with svn revision 1210501\n\nMy patch is a minor variation of yours.  The main other\noption would be to populate an temporary map when reading the\ncentral directory and only populate the final entries map\nwhen resolving local file data.  In a way trunk now does so\nbut uses the entries map to transport the temporary map\nbetween methods (in order to avoid changing two method\nsignatures).",
            "It was easier and cleaner to fix in Ant's code base where nobody cares for the order of entries from the central directory.\n\nSee svn revision 1210522"
        ],
        "summarized_discussion": "\n\nThe bug was fixed by creating a patch for UTF8ZipFilesTest.java and ZipFile.java, and the final solution was implemented in Ant's code base with svn revision 1210522. The patch created a temporary map when reading the central directory, and only populated the final entries map when resolving local file data."
    },
    "Math_48_src/main/java/org/apache/commons/math/analysis/solvers/BaseSecantSolver.java_129_251": {
        "src": "protected final double doSolve() {\n        // Get initial solution\n        double x0 = getMin();\n        double x1 = getMax();\n        double f0 = computeObjectiveValue(x0);\n        double f1 = computeObjectiveValue(x1);\n\n        // If one of the bounds is the exact root, return it. Since these are\n        // not under-approximations or over-approximations, we can return them\n        // regardless of the allowed solutions.\n        if (f0 == 0.0) {\n            return x0;\n        }\n        if (f1 == 0.0) {\n            return x1;\n        }\n\n        // Verify bracketing of initial solution.\n        verifyBracketing(x0, x1);\n\n        // Get accuracies.\n        final double ftol = getFunctionValueAccuracy();\n        final double atol = getAbsoluteAccuracy();\n        final double rtol = getRelativeAccuracy();\n\n        // Keep track of inverted intervals, meaning that the left bound is\n        // larger than the right bound.\n        boolean inverted = false;\n\n        // Keep finding better approximations.\n        while (true) {\n            // Calculate the next approximation.\n            final double x = x1 - ((f1 * (x1 - x0)) / (f1 - f0));\n            final double fx = computeObjectiveValue(x);\n\n            // If the new approximation is the exact root, return it. Since\n            // this is not an under-approximation or an over-approximation,\n            // we can return it regardless of the allowed solutions.\n            if (fx == 0.0) {\n                return x;\n            }\n\n            // Update the bounds with the new approximation.\n            if (f1 * fx < 0) {\n                // The value of x1 has switched to the other bound, thus inverting\n                // the interval.\n                x0 = x1;\n                f0 = f1;\n                inverted = !inverted;\n            } else {\n                switch (method) {\n                case ILLINOIS:\n                    f0 *= 0.5;\n                    break;\n                case PEGASUS:\n                    f0 *= f1 / (f1 + fx);\n                    break;\n                case REGULA_FALSI:\n                    // Detect early that algorithm is stuck, instead of waiting\n                    // for the maximum number of iterations to be exceeded.\n                    break;\n                default:\n                    // Should never happen.\n                    throw new MathInternalError();\n                }\n            }\n            // Update from [x0, x1] to [x0, x].\n            x1 = x;\n            f1 = fx;\n\n            // If the function value of the last approximation is too small,\n            // given the function value accuracy, then we can't get closer to\n            // the root than we already are.\n            if (FastMath.abs(f1) <= ftol) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    if (inverted) {\n                        return x1;\n                    }\n                    break;\n                case RIGHT_SIDE:\n                    if (!inverted) {\n                        return x1;\n                    }\n                    break;\n                case BELOW_SIDE:\n                    if (f1 <= 0) {\n                        return x1;\n                    }\n                    break;\n                case ABOVE_SIDE:\n                    if (f1 >= 0) {\n                        return x1;\n                    }\n                    break;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n\n            // If the current interval is within the given accuracies, we\n            // are satisfied with the current approximation.\n            if (FastMath.abs(x1 - x0) < FastMath.max(rtol * FastMath.abs(x1),\n                                                     atol)) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    return inverted ? x1 : x0;\n                case RIGHT_SIDE:\n                    return inverted ? x0 : x1;\n                case BELOW_SIDE:\n                    return (f1 <= 0) ? x1 : x0;\n                case ABOVE_SIDE:\n                    return (f1 >= 0) ? x1 : x0;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n        }\n    }",
        "src_wo_comments": "protected final double doSolve ( ) { double x0 = getMin ( ) ; double x1 = getMax ( ) ; double f0 = computeObjectiveValue ( x0 ) ; double f1 = computeObjectiveValue ( x1 ) ; if ( f0 == 0.0 ) { return x0 ; } if ( f1 == 0.0 ) { return x1 ; } verifyBracketing ( x0 , x1 ) ; final double ftol = getFunctionValueAccuracy ( ) ; final double atol = getAbsoluteAccuracy ( ) ; final double rtol = getRelativeAccuracy ( ) ; boolean inverted = false ; while ( true ) { final double x = x1 - ( ( f1 * ( x1 - x0 ) ) / ( f1 - f0 ) ) ; final double fx = computeObjectiveValue ( x ) ; if ( fx == 0.0 ) { return x ; } if ( f1 * fx < 0 ) { x0 = x1 ; f0 = f1 ; inverted = ! inverted ; } else { switch ( method ) { case ILLINOIS : f0 *= 0.5 ; break ; case PEGASUS : f0 *= f1 / ( f1 + fx ) ; break ; case REGULA_FALSI : break ; default : throw new MathInternalError ( ) ; } } x1 = x ; f1 = fx ; if ( FastMath . abs ( f1 ) <= ftol ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : if ( inverted ) { return x1 ; } break ; case RIGHT_SIDE : if ( ! inverted ) { return x1 ; } break ; case BELOW_SIDE : if ( f1 <= 0 ) { return x1 ; } break ; case ABOVE_SIDE : if ( f1 >= 0 ) { return x1 ; } break ; default : throw new MathInternalError ( ) ; } } if ( FastMath . abs ( x1 - x0 ) < FastMath . max ( rtol * FastMath . abs ( x1 ) , atol ) ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : return inverted ? x1 : x0 ; case RIGHT_SIDE : return inverted ? x0 : x1 ; case BELOW_SIDE : return ( f1 <= 0 ) ? x1 : x0 ; case ABOVE_SIDE : return ( f1 >= 0 ) ? x1 : x0 ; default : throw new MathInternalError ( ) ; } } } }",
        "fixed_src": "protected final double doSolve() {\n        // Get initial solution\n        double x0 = getMin();\n        double x1 = getMax();\n        double f0 = computeObjectiveValue(x0);\n        double f1 = computeObjectiveValue(x1);\n\n        // If one of the bounds is the exact root, return it. Since these are\n        // not under-approximations or over-approximations, we can return them\n        // regardless of the allowed solutions.\n        if (f0 == 0.0) {\n            return x0;\n        }\n        if (f1 == 0.0) {\n            return x1;\n        }\n\n        // Verify bracketing of initial solution.\n        verifyBracketing(x0, x1);\n\n        // Get accuracies.\n        final double ftol = getFunctionValueAccuracy();\n        final double atol = getAbsoluteAccuracy();\n        final double rtol = getRelativeAccuracy();\n\n        // Keep track of inverted intervals, meaning that the left bound is\n        // larger than the right bound.\n        boolean inverted = false;\n\n        // Keep finding better approximations.\n        while (true) {\n            // Calculate the next approximation.\n            final double x = x1 - ((f1 * (x1 - x0)) / (f1 - f0));\n            final double fx = computeObjectiveValue(x);\n\n            // If the new approximation is the exact root, return it. Since\n            // this is not an under-approximation or an over-approximation,\n            // we can return it regardless of the allowed solutions.\n            if (fx == 0.0) {\n                return x;\n            }\n\n            // Update the bounds with the new approximation.\n            if (f1 * fx < 0) {\n                // The value of x1 has switched to the other bound, thus inverting\n                // the interval.\n                x0 = x1;\n                f0 = f1;\n                inverted = !inverted;\n            } else {\n                switch (method) {\n                case ILLINOIS:\n                    f0 *= 0.5;\n                    break;\n                case PEGASUS:\n                    f0 *= f1 / (f1 + fx);\n                    break;\n                case REGULA_FALSI:\n                    // Detect early that algorithm is stuck, instead of waiting\n                    // for the maximum number of iterations to be exceeded.\n                    if (x == x1) {\n                        throw new ConvergenceException();\n                    }\n                    break;\n                default:\n                    // Should never happen.\n                    throw new MathInternalError();\n                }\n            }\n            // Update from [x0, x1] to [x0, x].\n            x1 = x;\n            f1 = fx;\n\n            // If the function value of the last approximation is too small,\n            // given the function value accuracy, then we can't get closer to\n            // the root than we already are.\n            if (FastMath.abs(f1) <= ftol) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    if (inverted) {\n                        return x1;\n                    }\n                    break;\n                case RIGHT_SIDE:\n                    if (!inverted) {\n                        return x1;\n                    }\n                    break;\n                case BELOW_SIDE:\n                    if (f1 <= 0) {\n                        return x1;\n                    }\n                    break;\n                case ABOVE_SIDE:\n                    if (f1 >= 0) {\n                        return x1;\n                    }\n                    break;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n\n            // If the current interval is within the given accuracies, we\n            // are satisfied with the current approximation.\n            if (FastMath.abs(x1 - x0) < FastMath.max(rtol * FastMath.abs(x1),\n                                                     atol)) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    return inverted ? x1 : x0;\n                case RIGHT_SIDE:\n                    return inverted ? x0 : x1;\n                case BELOW_SIDE:\n                    return (f1 <= 0) ? x1 : x0;\n                case ABOVE_SIDE:\n                    return (f1 >= 0) ? x1 : x0;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n        }\n    }",
        "fixed_src_wo_comments": "protected final double doSolve ( ) { double x0 = getMin ( ) ; double x1 = getMax ( ) ; double f0 = computeObjectiveValue ( x0 ) ; double f1 = computeObjectiveValue ( x1 ) ; if ( f0 == 0.0 ) { return x0 ; } if ( f1 == 0.0 ) { return x1 ; } verifyBracketing ( x0 , x1 ) ; final double ftol = getFunctionValueAccuracy ( ) ; final double atol = getAbsoluteAccuracy ( ) ; final double rtol = getRelativeAccuracy ( ) ; boolean inverted = false ; while ( true ) { final double x = x1 - ( ( f1 * ( x1 - x0 ) ) / ( f1 - f0 ) ) ; final double fx = computeObjectiveValue ( x ) ; if ( fx == 0.0 ) { return x ; } if ( f1 * fx < 0 ) { x0 = x1 ; f0 = f1 ; inverted = ! inverted ; } else { switch ( method ) { case ILLINOIS : f0 *= 0.5 ; break ; case PEGASUS : f0 *= f1 / ( f1 + fx ) ; break ; case REGULA_FALSI : if ( x == x1 ) { throw new ConvergenceException ( ) ; } break ; default : throw new MathInternalError ( ) ; } } x1 = x ; f1 = fx ; if ( FastMath . abs ( f1 ) <= ftol ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : if ( inverted ) { return x1 ; } break ; case RIGHT_SIDE : if ( ! inverted ) { return x1 ; } break ; case BELOW_SIDE : if ( f1 <= 0 ) { return x1 ; } break ; case ABOVE_SIDE : if ( f1 >= 0 ) { return x1 ; } break ; default : throw new MathInternalError ( ) ; } } if ( FastMath . abs ( x1 - x0 ) < FastMath . max ( rtol * FastMath . abs ( x1 ) , atol ) ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : return inverted ? x1 : x0 ; case RIGHT_SIDE : return inverted ? x0 : x1 ; case BELOW_SIDE : return ( f1 <= 0 ) ? x1 : x0 ; case ABOVE_SIDE : return ( f1 >= 0 ) ? x1 : x0 ; default : throw new MathInternalError ( ) ; } } } }",
        "summary": "\"RegulaFalsiSolver\" failure",
        "Description": "The following unit test:\n{code}\n@Test\npublic void testBug() {\n    final UnivariateRealFunction f = new UnivariateRealFunction() {\n            @Override\n            public double value(double x) {\n                return Math.exp(x) - Math.pow(Math.PI, 3.0);\n            }\n        };\n\n    UnivariateRealSolver solver = new RegulaFalsiSolver();\n    double root = solver.solve(100, f, 1, 10);\n}\n{code}\nfails with\n{noformat}\nillegal state: maximal count (100) exceeded: evaluations\n{noformat}\n\nUsing \"PegasusSolver\", the answer is found after 17 evaluations.\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-631",
        "comments": [
            "Reported by Axel Kramer in MATH-599.",
            "The problem was due to the fact that at some point, the update formula always gave the same value: Nothing was being updated and the loop went on until the number of evaluations was exhausted.\n\nI've committed a tentative solution in revision 1154614.\nHowever:\n# I'm not sure that it doesn't have any adverse side-effects on the bracketing property.\n# It is quite probably not a pristine \"regula falsi\" algorithm anymore.\n\nPlease review.\n\nAnyways, for the function that triggered the problem (see \"testIssue631\" in \"RegulaFalsiSolverTest.java\"), the (modified) {{RegulaFalsiSolver}} takes 3624 evaluations (versus 17 for {{PegasusSolver}}). We should probably add a word of warning in the class Javadoc.\n",
            "I just got back from a 3 week vacation, so I couldn't reply earlier.\n\nThe documentation for the RegulaFalsiSolver states: \"Unlike the Secant method, convergence is guaranteed by maintaining a bracketed solution.\" While this is theoretically true, in this case it is not so, because (if I understand correctly) only a single bound is updated repeatedly, and the update is too small to matter (has no effect), due to the double representation.\n\nThe change you propose (which is difficult to see as you also change other things in the same commit) is to modify x0 and f0 if the new value of x and x1 are equal. As I see it, this changes the algorithm, and it is no longer the Regula Falsi method as known from literature. I'm therefore against this change.\n\nThe problem that is identified in this issue is very similar to the well-known problem of the Regula Falsi method: it converges very slowly for certain problems, due to one side being updated all the time, while the other one stays the same. The Illinois and Pegasus algorithms solve exactly this problem, and are well-documented in literature.\n\nI therefore think it would be better if the RegulaFalsiSolver kept it's original implementation, and for this problem the Illinois or Pegasus method should then be used instead.\n\nThe other changes (if statements to switch with default, extracting bound switch statements, etc) can be kept, if you wish.\n\nThe suggestion to add a warning to the Secant and Regula Falsi solvers that this is a possible problem, and the solution (use Illinois or Pegasus), would indeed be a good idea. In general, adding a note that the Illinois and Pegasus algorithms perform better, would be a good idea regardless of this issue.\n\nOnce more, to be clear, I don't think this issue is a bug. It is a result of the limited convergence of the Regula Falsi method combined with the implications of limited double precision. The limited convergence of the algorithm is a property of the algorithm, and should in my opinion not be changed. I also don't think that trying to work around the limited double precision would be a good idea.",
            "> (which is difficult to see as you also change other things in the same commit)\n\nSorry, but I didn't hit the solution right away, i.e. before changing those two additional little things to make the code clearer (for me)...\n\nThe only actual change is that the {{REGULA_FALSI}} enum was not used (i.e. with the {{switch}} little change, the corresponding {{case}} would have been empty) whereas now it contains the update of x0 to avoid an infinite loop.\n\nThe other (cosmetic) change was to take these two statements\n{code}\nx1 = x;\nf1 = fx;\n{code}\nout of the previous {{if}} and {{else}} blocks, as they were duplicated there (which made me wonder whether it was a bug that they were _not_ different).\n\nYou say\n> [...] convergence is guaranteed [...]\n> [...] it converges very slowly for certain problems, [...]\n> [...] The limited convergence of the algorithm is a property of the algorithm, [...]\n\nAll the above imply that one expects that the algorithm _can_ find the solution.\nHowever, in this implementation, it _can't_.\nTherefore there is a bug, somewhere.\n\nI agree that it is a limitation of double precision. But, IMHO, leaving the code as-is is not a good idea because because it leads to the impression that the \"Regula Falsi\" mathematical algorithm can fail to converge, which is not correct (IIUC).\nTherefore, we could add a comment stating that the _implementation_ with limited precision can fail to converge but that would be akin to saying to users: \"Here is a code, but don't use it.\"\nPersonally, I would prefer to say: \"Because of limited precision, the implementation can fail to converge. In those cases, we slightly modified the original algorithm in order to avoid failure.\"\n",
            "{quote}\nAll the above imply that one expects that the algorithm can find the solution.\nHowever, in this implementation, it can't.\nTherefore there is a bug, somewhere.\n{quote}\n\nHere, the bug is in the algorithm itself, not in the implementation.\n\n{quote}\nit leads to the impression that the \"Regula Falsi\" mathematical algorithm can fail to converge, which is not correct (IIUC).\n{quote}\n\nIt is correct. Regula Falsi fails to converge, or rather it can take a too large number of iteration to converge. This is exactly this behavior that has lead to the construction of other algorithms like Illinois or Pegasus. These two algorithms try to detect the case when the same end of the interval is always updated, and the other end remains unchanged. Once they have detected this, they slightly change the value at one end to trick the linear evaluation into choosing a value that is very likely to have the required sign to update this other end. In fact, in many cases depending of the sign of the curvature near the root, as soon as one end is very close to the root the linear interpolation will always remain on the same side of the root and hence will update this end.\n\nI agree with Dennis here, the change needed to ensure convergence is not tool long is to choose a better algorithm, such as Illinois, Pegasus ... or the nth order bracketing solver I recently added. Regula Falsi should remain the reference Regula Falsi, just as secant and Brent should remain the reference ones.\n",
            "\"fails to converge\" and \"large number of iteration to converge\" are completely different things.\n\nThe documentation says: \"convergence is guaranteed\". Is _that_ false?\n\nMoreover, for the function reported in this issue, the problem is not that it takes a large number iterations, it is that the loop is _literally_ infinite because at some point, nothing changes anymore.\n\nStated otherwise: If implemented with larger/infinite precision, would it converge?\nIn the affirmative, then in my opinion it means that the plain \"Regula Falsi\" method cannot be implemented with double precision (or that its convergence properties are not as stated in the docs) or that there is a bug in the implementation.\n\nIn the former case, why keep something that will never be used (as we'll warn users that they should use \"Pegasus\" or \"Illinois\" but certainly not \"RegulaFalsi\")? IMHO, we could just state in the docs that \"RegulaFalsi\" was not implemented because it is demonstrably less efficient and sometimes fails to work.\n\nA less radical alternative would be to keep the test I've inserted in the code (at line 186) and throw a {{MathIllegalStateException}} if it passes. The previous behaviour (infinite loop) is a bug in CM.\n",
            "{quote}\nThe documentation says: \"convergence is guaranteed\". Is that false?\n{quote}\n\nIt depends on what is called convergence.\nIf convergence is evaluated only as the best of the two endpoints (measured along y axis), yes convergence is guaranteed and in this case it is very slow. This is what appears in many analysis books.\nIf convergence is evaluated by ensuring the bracketing interval (measured along x axis) reduces to zero (i.e. both endpoints converge to the root), convergence is not guaranteed.\n\nThe first case is achieved with our implementation by using the function accuracy setting. The second case is achieved with our implementation by using relative accuracy and absolute accuracy settings, which both are computed along x axis.\n\nI fear that there are several different references about convergence for this method (just as for Brent). So we already are able to implement both views.\n\nWithout any change to our implementation, we reach convergence for this example by setting the function accuracy to 7.4e-13 or above, and it is slow (about 3500 evaluations). The default setting for function accuracy is very low (1.0e-15) and in this case, given the variation rate of the function near the root, it is equivalent to completely ignore convergence on y on only check the convergence on the interval length along x. \n",
            "I think we should either stick with the standard implementation of Regula Falsi or drop the class altogether.  Different rootfinders are going to perform better / worse for different functions and parameter values and I don't think it is a good idea to try to modify our implementations of the algorithms to try to work around their shortcomings for problem instances for which they are not well-suited.  It is much better to stick with standard algorithms, document them, and leave it to users to choose among implementations.  \n\nRegula Falsi is not a good general-purpose rootfinder, but it does perform well for some problems (or parts of problems) and the original submission was a working implementation, so I would say revert the changes and keep it.",
            "I understand what you say. But however you put it, there is a bug; if not in the implementation, then in the API. It is not expected behaviour that something which must be changed (function accuracy threshold) to ensure correct behaviour (avoid an undetected infinite loop) is not a mandatory parameter.\nTo debug this, I started by raising the absolute accuracy threshold (the first default parameter, thus the first obvious thing to do) to 1e-2 and was stunned that I couldn't get anything after 1000000 iterations!\n\nTherefore I maintain that, at a minimum, we put a line that will detect the infinite loop and raise an exception identifying _that_ problem and not let the user wait for \"TooManyEvaluationsException\" to be raised, as that will induce the unaware (me) to just allow more evaluations and try again.\n\nThis solution does not corrupt the algorithm; it just adds protection.\n",
            "I disagree with your statement about setting accuracy.  All of this is configurable and if not set, you get the (documented) defaults.  This is all documented.  If the documentation is not clear, then we can improve it.  A user who applies Regula Falsi to the problem instance being examined here will end up maxing iterations.  I see no problem with that and in fact I see it as *correct* behavior (given the documented execution context of the algorithm).  ",
            "How can it be correct to have an infinite loop?\nThe problem is not slow convergence, which you can overcome by allowing more iterations.\nIt is too low function value accuracy which you cannot overcome by allowing more iterations. Thus my point: We must raise the appropriate exception (the doc for which will state that it can happen if the function value accuracy is too low for the implementation to provide a result).\n",
            "[My comment starting with \"I understand what you say.\" was an answer to Luc. I hadn't read Phil's previous one which was posted while I was writing mine.]\n\nI agree that it is better not to change the standard algorithm, as I indicated in my first comment.\nThe fix which I'm proposing is not an algorithm change, it is an implementation detail similar to the many hundreds checks performed in CM. Just it is not a precondition test. It adequately indicates that something went wrong and can help the user figure out what it was. It makes the implementation more robust.\n",
            "The original implementation, for the \"problem instance being examined here\", would find the root with absolute accuracy lower than *10e-12* after 3560 evaluations (note: using the default value of *1e-6*).\nIn fact, the root was found, at the required accuracy, after around 2200 evaluations.\n\nThat does not sound like correct behavior.\nThe problem is that, \"x0\" never being updated, the convergence test always fails... until we reach the limitation of double precision, which entails an infinite loop.\n\nIn fact my fix should not be necessary, as things have gone awry before it would apply, but there is a bug to fix nonetheless.\n",
            "Is there actually a possibility of an infinite loop in the code?  Looks to me like the max evaluations bound will stop the while loop, so there is no potential for an infinite loop.  Apologies if I am misreading the code and the loop can fail to terminate, in which case I agree this is a problem.  (As a side note, from a style perspective, I prefer to explicitly bound loops to avoid this kind of uncertainty.  The natural hard bound here is the evaluation count.)\n\nTrying to detect when a sequence of iterates has gotten \"stuck\" and is destined to hit max iterations without converging is walking down a path that I think is unwise for us and users.  I see no reason not to stick with the standard impl here, which is nicely documented in the original submission.  Trying to workaround numerical problems in simple algorithms and change contracts to include these workarounds is asking for trouble - both for us and users.  In a simple case like this, it is much better to just stick with the documented algorithm, which should in this case (again unless I am missing something) end with max evaluations exceeded, which is the right exception to report. ",
            "I surely hope that your last post is not an answer to mine from 23:46.\n\nI'll try to answer here in case it was in reply to my previous one (23:06).\nOf course, the code will not run forever because of the \"maxeval\" bound.\nBut it will run for a time that depends on the value of \"maxeval\" *with no added benefit*! From a certain point, the loop is like\n{code}\nwhile (true) {\n  // Do nothing useful, just count!\n  ++count;\n  if (count > maxeval) {\n    throw new TooManyEvalutationsException(maxeval);\n  }\n}\n{code}\n\n{quote}\nfrom a style perspective, I prefer to explicitly bound loops\n{quote}\n\nFrom an *OO* style perspective, the reuse of the \"Incrementor\" is better, and you don't have to rewrite the same \"test and throw exception if failed\" boiler plate code each time there is such a loop.\n\n{quote}\nTrying to detect when a sequence of iterates has gotten \"stuck\" and is destined to hit max iterations without converging is walking down a path that I think is unwise for us and users.\n{quote}\n\nWhy?\n\n{quote}\nI see no reason not to stick with the standard impl here\n{quote}\n\nA busy idle loop is a compelling reason IMO.\n\n{quote}\nTrying to workaround numerical problems in simple algorithms and change contracts to include these workarounds is asking for trouble\n{quote}\n\nThe trouble is there with the current implementation. I'm not criticizing the contribution but this issue shows that it should be made more robust.\nAlso, the documentation about \"convergence is guaranteed\" can lead to a false sense of security.\nMoreover, is the \"regula falsi\" a mathematical algorithm (with a guaranteed converge property if computed with infinite precision) or a numerical one, which this issue proves that it cannot guarantee convergence? In the former case, CM's (numerical) implementation is not strictly \"regula falsi\" and there would be no such thing as respect for an original/standard implementation if we can make it more robust.\n\nI've already indicated that the fix does *not* change the contract; it stops the busy idle loop as soon as it is detected and reports that it won't do any good to increase the number of iterations. That's _obviously_ more robust.\n\nNow, if you were answering to my 23:46 post, I'd be glad to read an explanation of why the first paragraph describes expected behaviour.\n\n",
            "I don't understand.\n\nWhen it was created, the maxIteration threshold was exactly designed for this purpose: get out of infinite loops. It was later renamed maxEvaluation but the purpose is still the same: don't get stuck. The reason why we get stuck is irrelevant. This limit is simply a safety limit, not a tuning parameter that user are expected to raise once they hit it hoping they will converge later on. If they could raise it later, then they should set it to an appropriate value at once. Hitting it implies computation failed. Regula falsi just like any algorithm can fail if applied with the wrong parameters or to the wrong function (in fact, even with a good setting of function accuracy, it fails to converge if we require a bracket selection on the side that does not move).\n\nAlso detecting one bound is not updated is what Illinois and Pegasus are designed to do.\n\nSo I think we should completely get rid of regula falsi and only keep the better algorithms.",
            "{quote}\nI think we should completely get rid of regula falsi and only keep the better algorithms.\n{quote}\n\nThat was my first idea. And that would be the simplest one, the safest one, and the only viable one as I can't seem to state clearly enough that\n* Problem 1: When the doc says \"guaranteed convergence\", the algorithm should provide the answer.\n* Problem 2: When the (absolute) accuracy threshold is set to 1e-6, and the correct root *is* found (after 2200 iterations) within the requirements, it should be returned, instead running idle and finish with an exception\n\n{quote}\nThe reason why we get stuck is irrelevant.\n{quote}\n\nBut why? If we *can* be more precise on the cause of failure, why not do it?\n\n{quote}\nThis limit is simply a safety limit, not a tuning parameter that user are expected to raise once they hit it hoping they will converge later on.\n{quote}\n\nIn principle, some possible use would be to compare the efficiency of different methods where the main criterion would be a time limitation (assuming that the function evaluation time overwhelms the of the root solver algorithm time). Thus with the function that triggered this issue:\n* If you set maxeval to \"3000\", then both \"Pegasus\" (17 evals) and (a fixed) \"RegulaFalsi\" (2200 evals) would fill the bill.\n* If you set maxeval to \"1000\", then \"Pegasus\" will be the only winner.\n\n\nAnyways:\n+1 for removing it altogether, and include somewhere the reason for it not being implemented in CM.\n",
            "I am OK removing Reg Falsi, but stand by comments above that it is a very bad idea to hack standard algorithms and agree with Luc that maxing iterations is the correct behavior in the case we have been discussing. It is kind of pathetic that the compromise is to drop the impl; but in this case I don't see it as a real loss, since I can't think of any examples where Reg Falsi would be preferable to one of the other solvers - other than for educational purposes.",
            "May I please know *why it is OK that a bit of code does loop counting and repeatedly computes the same thing*!\n\nYou insist that I'd be \"hacking\" whereas I've indicated 3 or 4 times that there is no hack: just a test that will exit the loop as soon as it detects that the algorithm has failed. Why is it not understandable that the busy loop could last for a long time? The function is potentially evaluated millions of times at the same point. What if the evaluation is costly? Imagine the computation running for days, only to discover that it could have been be stopped after a few seconds. Is that robust code and good advertising for a library? It is one thing to expect that there are unknown bugs in CM, but refusing to fix a known one is so obviously wrong...\n\nAnd may I please know *why it is OK that an algorithm that finds the right result does not return it*.\n\nI had been trying to provide alternatives to the removal, but I can't do much more if nobody answers the above two questions.\nYou just have to run the code and print \"x\" and \"x1\" to see what is going on!\n",
            "{code}\nMay I please know *why it is OK that a bit of code does loop counting and repeatedly computes the same thing!*\n{code}\n\nWe didn't say that. We said that regula falsi is a standard *bad* algorithm. We said that very smart people have enhanced it 40 years ago and the enhanced versions are known and already implemented in Commons Math. These algorithms are *not* blind loop counters and they insert smart target shifts that *prevent* the behavior we observe here. These algorithms not only detect the problem, they fix it! They allow convergence along x. They allow selection of the side of the root.\n\n{code}\nThe function is potentially evaluated millions of times at the same point.\n{code}\n\nThe maxEvaluations is already here to prevent this, and in fact now this max number is even mandatory in the solve method (you placed it if I remember correctly). So the function is called millions of time only if the users wishes so by setting the maxEvaluations to a number in the range of millions.\n\n{code}\nAnd may I please know *why it is OK that an algorithm that finds the right result does not return it.*\n{code}\n\nIf the user asked for a convergence in x or for a convergence on y on the side that is stuck, then no, the algorithm did not find the right result. One of its bounds converged but the users asked for something else.\n\n{code}\nYou just have to run the code and print \"x\" and \"x1\" to see what is going on!\n{code}\n\nWe know exactly what is going on! We know the algorithm is stuck. We know why it is stuck. We know why it did not detect it is stuck. We know it will finally hit the safety maxEvaluation threshold that is just waiting for that. And we know that removing all these problems is done by using other algorithms which are already there.\n\nRegula falsi is doomed. It is an algorithm used for educational purposes, or for comparison purposes, not something suited for production use. It is just like Euler for ODE (and by the way we did implement Euler for ODE and we don't recommend users to use it as we also did implement better algorithms that were also designed by smart mathematicians decades ago).",
            "{quote}\nSo the function is called millions of time only if the users wishes so by setting the maxEvaluations to \na number in the range of millions.\n{quote}\n\nNo, the user should not expect that any algorithm will go on a single iteration more than necessary.\nThis is a plain bug.\n\nWhy do you see that a test such as I proposed (exit the loop early) is wrong while CM (and any good program) is full of tests to ensure that you don't do useless computations?\nThis has nothing to do with \"regula falsi\", it is robustness in the face of limited precision.\n\nHowever, if you insist that the bug (failing to detect that it is stuck) is really an integral part of the algorithm, then removing it is not a \"pathetic compromise\", it is the only right thing to do!\n\n",
            "This is a pointless discussion.  Gilles, you obviously don't share the views that Luc and I have on implementing standard algorithms or even what the meaning of a numerical algorithm is. Some algorithms perform well for some classes of problems and not others.  There is an art to choosing the right algorithm for the problem instance at hand.  If we modify our implementations to try to work around shortcomings of the algorithms we implement, then we are not implementing the standard algorithms, and we need to document exactly what it is that we are implementing, because in this case we are actually making it harder for users to choose (because we are not longer advertising standard numerics).  This is what I meant when I said it is both harder for us (because we have to document the hacks and non-standard contracts) and users (because the standard numerical analysis theory that they may be using to choose among implementations will no longer apply).  It is, IMO, a \"pathetic compromise\" to drop the implementation because we can't agree on what it means to implement the algorithm. So be it. Lets drop it and resolve this issue as \"fixed.\"",
            "{quote}\nGilles, you obviously don't share the views that Luc and I have on implementing standard algorithms \n{quote}\n\nThat's simply _not true_.\nI was the one pointing out that standard algorithms should have precedence: Please recall that it was considered fine that \"Levenberg-Marquardt\" and \"Brent\" would be, unknowingly to the user, \"twisted\" to perform _non-standard_ convergences check.\nIn those cases, there was the risk that the result of the algorithm would not be the same as the reference implementation.\n\nIn this case, there is no such thing as deviating from standard numerics! It was just a matter of throwing the right exception. So: \"The algorithm fails? Let's tell it sooner rather than later.\"\n\nVery interesting question that you ask: \"what it means to implement the algorithm\". But please note that I asked it several posts ago[1], and an answer would have helped sort out this discussion. What is your definition?\n\n\n[1] 08/Aug/11 07:24\n",
            "Also:\n\nPhil,\n\nCould you please leave out dismissive qualifiers such as \"pointless\" and \"pathetic\" (and, elsewhere, \"silly\") and stick to more or less objective arguments?\nThat will certainly help keep the conversation tone to a courteous level.\n\nLuc,\n\nThanks for stating in full details what you meant by \"convergence\" in this case. However, it is still a \"post-mortem\" description.\nDo you really expect that the average user of the CM library (a.o. me and the original reporter of the issue) to be able to figure out that \"obvious\" explanation just by getting a \"TooManyEvalutationsException\", setting the along-x accuracy threshold to a ridiculously high value and still getting the same exception?\nIf just for educational purposes, don't you think that it is more instructive to get a specific hint that the algorithm is stuck, rather than hit the ultimate fail-safe barrier much much later, and then download the source code and sprinkle the code with \"println\" statements to do forensic analysis?\n\nPhil,\n\nI tried to handle this issue out of respect for a real user who reported an issue that would have looked suspicious to many CM users. [How many of them would be experts in numerical analysis?]\nYou do not do me a favour by removing this algorithm; I don't want it to be a _compromise_ (pathetic or not). If you prefer to keep it, I don't care anymore. But, in that case, _you_ should have answered to Axel Kramer to go and read some books on numerical analysis.\n",
            "Gilles, I apologize for tone of comments.",
            "The discussions for this issue have left me with a lack of overview, so I'll (try to) objectively summerize the discussions above:\n\nThe problems are:\n # Regula Falsi states it always converges, but the implementation doesn't.\n # The main loop may continue, even when it no longer makes any progress, and subsequently ends with a TooManyEvaluationsException exception.\n\nThe cause of both problems is:\n - The limited/finite precision of the Java double type.\n\nProposed solutions:\n # The patch from revision 1154614, which modifies the Regula Falsi algorithm.\n   #- Consensus seems to be that this change, which modifies the algorithm, is undesireable. We should keep the original algorithm.\n # Detect that the algorithm no longer makes progress and throw an exception, instead of continuing the loop which no longer makes progress.\n   #- This is just earlier detection of the algorithm getting stuck.\n   #- We could throw the TooManyEvaluationsException exception, that continuing the loop would also get us.\n     #-- The class only states \"Exception to be thrown when the maximal number of evaluations is exceeded.\".\n     #-- The exception message only states: \"illegal state: maximal count (100) exceeded: evaluations\"\n     #-- Both may benefit from more extended documentation/messages.\n   #- We could also throw an other exception that more clearly states this issue (NoMoreProgressException, AlgorithmStuckException, ...?).\n     #-- It could for instance mention that changing the function value accuracy may be a solution, or asking for a different kind of solution?\n # Add documentation to the Regula Falsi algorithm that it is not intended to be used for actual problems, but only to compare algorithms, for testing, educational purposes, etc.\n # Add documentation to the Regula Falsi algorithm that users should use Illinois or Pegasus instead, which should outperform the algorithm for most if not all problems.\n # Add documentation to the Regula Falsi algorithm that it theoretically converges, but the implementation may not, due to the limited/finite precision of Java's double type. This will result in an exception (or 2 if we also do solution number 2).\n # Remove the Regula Falsi algorithm, and document why it is not included/implemented.\n   #- This seems to have been accepted as a last resort solution only.\n\nOther notes:\n - The following problem was also indicated: a solution is found after a certain number of iterations, but the algorithm does not return the solution (it does not terminate)\n   -- This should only happen if the user asked for a different solution. That is, there are several accuracy parameters, as well as an allowedSolution parameter.\n   -- If the solution requested by the user is found, it should return the solution immediately, otherwise it is a bug.\n\nNew notes:\n - I think the Regula Falsi algorithm does not state a fixed convergence criteria: it is left to the user to decide on one.\n   -- When I implemented the algorithm, I think I copied the convergence checks for Brent.\n   -- I subsequently modified the convergence criteria when I added the allowedSolution parameter.\n\nMy personal opinions on the proposed solutions:\n - (1) Revert part of 1154614, so get the original algorithm back. The other changes of that commit, that don't change the actual algorith, can stay.\n - (2) If we keep the algorithm, earlier detection would be nice. Not sure which exception to throw in these cases.\n   -- This would result in a single 'if' that detects that the new approximation is the same as the previous one, and we thus no longer make progress, in which case we throw the exception earlier, instead of later.\n - (3-5) If we keep the algorith, all 3 documentation extensions would be a good idea.\n - (6) If possible, keep the algorithm, and don't remove it.\n\nNew issue:\n - TooManyEvaluationsException currently seems to use LocalizedFormats.MAX_COUNT_EXCEEDED(\"maximal count ({0}) exceeded\"), but maybe should use LocalizedFormats.MAX_EVALUATIONS_EXCEEDED(\"maximal number of evaluations ({0}) exceeded\") instead?\n",
            "Thanks for the neat summary!\n\n{quote}\n* (1) Revert part of 1154614, so get the original algorithm back. The other changes of that commit, that don't change the actual algorith, can stay.\n{quote}\n\nDone in revision 1157185.\n\n{quote}\n* (2) If we keep the algorithm, earlier detection would be nice. Not sure which exception to throw in these cases.\n** This would result in a single 'if' that detects that the new approximation is the same as the previous one, and we thus no longer make progress, in which case we throw the exception earlier, instead of later.\n{quote}\n\n+1 (my position in the \"07/Aug/11 20:28\" post)\nAs suggested there, the exception could be \"MathIllegalStateException\" but with a clear message stating that the algorithm is stuck. Or maybe a new subclass of it which we could call \"NumericalPrecisionException\" or even a general-purpose \"ImplementationException\".\n\n{quote}\n[...] all 3 documentation extensions would be a good idea.\n{quote}\n\n+1\n\nAbout the \"new issue\", the message string:\n{quote}\n\"illegal state: maximal count (100) exceeded: evaluations\"\n{quote}\ncontains everything:\n# error type: illegal state\n# failure description: maximal count (100) exceeded\n# context: evaluations\n\nI proposed to use this approach (combining message items with the \"addMessage\" method of \"ExceptionContext\") in order to reduce the number of messages in the \"LocalizedFormats\" enum. Too many of them are just slight variations on a same theme.\n",
            "bq. contains everything\n\nI agree. I was just wondering why a message that seems to be exactly the same as the exception was not used, as it kind of looked like it was created just for this purpose...\n\nbq. I proposed to use this approach (combining message items with the \"addMessage\" method of \"ExceptionContext\") in order to reduce the number of messages in the \"LocalizedFormats\" enum. Too many of them are just slight variations on a same theme.\n\nAh, so then the MAX_EVALUATIONS_EXCEEDED is just a remnant of the past that should be eliminated, by replacing it everywhere by the more general MAX_COUNT_EXCEEDED?",
            "Yes. In the file \"LocalizedFormats.java\", I've started to write\n{noformat}\n/* keep */\n{noformat}\nafter each enum that is supposedly to be kept. All the others are still to be examined for redundancy with another one, or the possibility to create something close using the \"multi-item\" approach.\n",
            "The 'ticket631.patch' file is my attempt to resolve this issue with a solution (or maybe I should call it a compromise?) that is satisfactory for all people that participated in the discussions for this issue, without having to remove the Regula Falsi algorithm from Commons Math.\n\nI changed the following:\n - Added early detection of no longer making progress ('getting stuck'), and documented it.\n   -- I used ConvergenceException for this, as it seems to fit... Do we want a custom error message with it?\n - Extended RegulaFalsiSolver documentation to indicate:\n   -- that the algorithm should not be used for actual problems.\n   -- that Illinois and Pegasus are improved versions and should be prefered.\n   -- that the implementation does not guarantee convergence, while the algorithm theoretically does.\n - Extended IllinoisSolver and PegasusSolver documentation to indicate that they don't suffer from the RegulaFalsiSolver's implementation/convergence issues.\n\nPlease comment on whether this patch is an acceptable solution/compromise, and if not, why it is not.",
            "Committed (with minor additional Javadoc fixes) in revision 1164474.\n\nLeaving open until confirmation that {{ConvergenceException}} is the right one to use. I thought that we could make a difference between _theoretical_ and _implementation_ convergence failures. But it might not be worth introducing the distinction just for this one case, especially since it is quite clear clear now that the class should not be used.",
            "No objection raised; setting to \"Resolved\"."
        ],
        "summarized_discussion": ""
    },
    "JacksonDatabind_83_src/main/java/com/fasterxml/jackson/databind/deser/std/FromStringDeserializer.java_103_159": {
        "src": "@SuppressWarnings(\"unchecked\")\n    @Override\n    public T deserialize(JsonParser p, DeserializationContext ctxt) throws IOException\n    {\n        // 22-Sep-2012, tatu: For 2.1, use this new method, may force coercion:\n        String text = p.getValueAsString();\n        if (text != null) { // has String representation\n            if (text.length() == 0 || (text = text.trim()).length() == 0) {\n                // 04-Feb-2013, tatu: Usually should become null; but not always\n                return _deserializeFromEmptyString();\n            }\n            Exception cause = null;\n            try {\n                // 19-May-2017, tatu: Used to require non-null result (assuming `null`\n                //    indicated error; but that seems wrong. Should be able to return\n                //    `null` as value.\n                if (_deserialize(text, ctxt) != null) {\n                return _deserialize(text, ctxt);\n                }\n            } catch (IllegalArgumentException iae) {\n                cause = iae;\n            } catch (MalformedURLException me) {\n                cause = me;\n            }\n            String msg = \"not a valid textual representation\";\n            if (cause != null) {\n                String m2 = cause.getMessage();\n                if (m2 != null) {\n                    msg = msg + \", problem: \"+m2;\n                }\n            }\n            // 05-May-2016, tatu: Unlike most usage, this seems legit, so...\n            JsonMappingException e = ctxt.weirdStringException(text, _valueClass, msg);\n            if (cause != null) {\n                e.initCause(cause);\n            }\n            throw e;\n            // nothing to do here, yet? We'll fail anyway\n        }\n        JsonToken t = p.getCurrentToken();\n        // [databind#381]\n        if (t == JsonToken.START_ARRAY) {\n            return _deserializeFromArray(p, ctxt);\n        }\n        if (t == JsonToken.VALUE_EMBEDDED_OBJECT) {\n            // Trivial cases; null to null, instance of type itself returned as is\n            Object ob = p.getEmbeddedObject();\n            if (ob == null) {\n                return null;\n            }\n            if (_valueClass.isAssignableFrom(ob.getClass())) {\n                return (T) ob;\n            }\n            return _deserializeEmbedded(ob, ctxt);\n        }\n        return (T) ctxt.handleUnexpectedToken(_valueClass, p);\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"unchecked\" ) @ Override public T deserialize ( JsonParser p , DeserializationContext ctxt ) throws IOException { String text = p . getValueAsString ( ) ; if ( text != null ) { if ( text . length ( ) == 0 || ( text = text . trim ( ) ) . length ( ) == 0 ) { return _deserializeFromEmptyString ( ) ; } Exception cause = null ; try { if ( _deserialize ( text , ctxt ) != null ) { return _deserialize ( text , ctxt ) ; } } catch ( IllegalArgumentException iae ) { cause = iae ; } catch ( MalformedURLException me ) { cause = me ; } String msg = \"not a valid textual representation\" ; if ( cause != null ) { String m2 = cause . getMessage ( ) ; if ( m2 != null ) { msg = msg + \", problem: \" + m2 ; } } JsonMappingException e = ctxt . weirdStringException ( text , _valueClass , msg ) ; if ( cause != null ) { e . initCause ( cause ) ; } throw e ; } JsonToken t = p . getCurrentToken ( ) ; if ( t == JsonToken . START_ARRAY ) { return _deserializeFromArray ( p , ctxt ) ; } if ( t == JsonToken . VALUE_EMBEDDED_OBJECT ) { Object ob = p . getEmbeddedObject ( ) ; if ( ob == null ) { return null ; } if ( _valueClass . isAssignableFrom ( ob . getClass ( ) ) ) { return ( T ) ob ; } return _deserializeEmbedded ( ob , ctxt ) ; } return ( T ) ctxt . handleUnexpectedToken ( _valueClass , p ) ; }",
        "fixed_src": "@SuppressWarnings(\"unchecked\")\n    @Override\n    public T deserialize(JsonParser p, DeserializationContext ctxt) throws IOException\n    {\n        // 22-Sep-2012, tatu: For 2.1, use this new method, may force coercion:\n        String text = p.getValueAsString();\n        if (text != null) { // has String representation\n            if (text.length() == 0 || (text = text.trim()).length() == 0) {\n                // 04-Feb-2013, tatu: Usually should become null; but not always\n                return _deserializeFromEmptyString();\n            }\n            Exception cause = null;\n            try {\n                // 19-May-2017, tatu: Used to require non-null result (assuming `null`\n                //    indicated error; but that seems wrong. Should be able to return\n                //    `null` as value.\n                return _deserialize(text, ctxt);\n            } catch (IllegalArgumentException iae) {\n                cause = iae;\n            } catch (MalformedURLException me) {\n                cause = me;\n            }\n            String msg = \"not a valid textual representation\";\n            if (cause != null) {\n                String m2 = cause.getMessage();\n                if (m2 != null) {\n                    msg = msg + \", problem: \"+m2;\n                }\n            }\n            // 05-May-2016, tatu: Unlike most usage, this seems legit, so...\n            JsonMappingException e = ctxt.weirdStringException(text, _valueClass, msg);\n            if (cause != null) {\n                e.initCause(cause);\n            }\n            throw e;\n            // nothing to do here, yet? We'll fail anyway\n        }\n        JsonToken t = p.getCurrentToken();\n        // [databind#381]\n        if (t == JsonToken.START_ARRAY) {\n            return _deserializeFromArray(p, ctxt);\n        }\n        if (t == JsonToken.VALUE_EMBEDDED_OBJECT) {\n            // Trivial cases; null to null, instance of type itself returned as is\n            Object ob = p.getEmbeddedObject();\n            if (ob == null) {\n                return null;\n            }\n            if (_valueClass.isAssignableFrom(ob.getClass())) {\n                return (T) ob;\n            }\n            return _deserializeEmbedded(ob, ctxt);\n        }\n        return (T) ctxt.handleUnexpectedToken(_valueClass, p);\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"unchecked\" ) @ Override public T deserialize ( JsonParser p , DeserializationContext ctxt ) throws IOException { String text = p . getValueAsString ( ) ; if ( text != null ) { if ( text . length ( ) == 0 || ( text = text . trim ( ) ) . length ( ) == 0 ) { return _deserializeFromEmptyString ( ) ; } Exception cause = null ; try { return _deserialize ( text , ctxt ) ; } catch ( IllegalArgumentException iae ) { cause = iae ; } catch ( MalformedURLException me ) { cause = me ; } String msg = \"not a valid textual representation\" ; if ( cause != null ) { String m2 = cause . getMessage ( ) ; if ( m2 != null ) { msg = msg + \", problem: \" + m2 ; } } JsonMappingException e = ctxt . weirdStringException ( text , _valueClass , msg ) ; if ( cause != null ) { e . initCause ( cause ) ; } throw e ; } JsonToken t = p . getCurrentToken ( ) ; if ( t == JsonToken . START_ARRAY ) { return _deserializeFromArray ( p , ctxt ) ; } if ( t == JsonToken . VALUE_EMBEDDED_OBJECT ) { Object ob = p . getEmbeddedObject ( ) ; if ( ob == null ) { return null ; } if ( _valueClass . isAssignableFrom ( ob . getClass ( ) ) ) { return ( T ) ob ; } return _deserializeEmbedded ( ob , ctxt ) ; } return ( T ) ctxt . handleUnexpectedToken ( _valueClass , p ) ; }",
        "summary": "`FromStringDeserializer` ignores registered `DeserializationProblemHandler` for `java.util.UUID`",
        "Description": "Culprit appears to be [lines 155-161 of FromStringDeserializer](https://github.com/FasterXML/jackson-databind/blob/60ae6000d361f910ab0d7d269a5bac1fc66f4cd9/src/main/java/com/fasterxml/jackson/databind/deser/std/FromStringDeserializer.java#L155-L161):\r\n\r\n```\r\n            // 05-May-2016, tatu: Unlike most usage, this seems legit, so...\r\n            JsonMappingException e = ctxt.weirdStringException(text, _valueClass, msg);\r\n            if (cause != null) {\r\n                e.initCause(cause);\r\n            }\r\n            throw e;\r\n            // nothing to do here, yet? We'll fail anyway\r\n```\r\nThe above lines appear to show that the exception will be thrown regardless of any problem handling logic.\r\n\r\nTest Case:\r\n\r\n```\r\nimport com.fasterxml.jackson.databind.DeserializationContext;\r\nimport com.fasterxml.jackson.databind.ObjectMapper;\r\nimport com.fasterxml.jackson.databind.deser.DeserializationProblemHandler;\r\nimport org.junit.Test;\r\n\r\nimport java.io.IOException;\r\nimport java.util.UUID;\r\n\r\npublic class UUIDDeserializerTest {\r\n\r\n\r\n  @Test\r\n  public void itUsesDeserializationProblemHandlerProperly() throws IOException {\r\n    ObjectMapper mapper = new ObjectMapper().addHandler(new DeserializationProblemHandler() {\r\n      @Override\r\n      public Object handleWeirdStringValue(final DeserializationContext ctxt, final Class<?> targetType, final String valueToConvert, final String failureMsg) throws IOException {\r\n        return null;\r\n      }\r\n    });\r\n\r\n    mapper.readValue(\"{\\\"id\\\" : \\\"I am not a UUID\\\"}\", IdBean.class);\r\n\r\n\r\n\r\n  }\r\n\r\n  public static class IdBean {\r\n    private UUID id;\r\n\r\n    public UUID getId() {\r\n      return id;\r\n    }\r\n\r\n    public void setId(final UUID id) {\r\n      this.id = id;\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe handler handles the issue properly; but an exception is thrown anyway:\r\n\r\n```\r\nan not deserialize value of type java.util.UUID from String \"I am not a UUID\": not a valid textual representation\r\n at [Source: (String)\"{\"id\" : \"I am not a UUID\"}\"; line: 1, column: 9] (through reference chain: com.company.test.UUIDDeserializerTest$IdBean[\"id\"])\r\ncom.fasterxml.jackson.databind.exc.InvalidFormatException: Can not deserialize value of type java.util.UUID from String \"I am not a UUID\": not a valid textual representation\r\n at [Source: (String)\"{\"id\" : \"I am not a UUID\"}\"; line: 1, column: 9] (through reference chain: com.company.test.UUIDDeserializerTest$IdBean[\"id\"])\r\n\tat com.fasterxml.jackson.databind.exc.InvalidFormatException.from(InvalidFormatException.java:67)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.weirdStringException(DeserializationContext.java:1504)\r\n\tat com.fasterxml.jackson.databind.deser.std.FromStringDeserializer.deserialize(FromStringDeserializer.java:156)\r\n\tat com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:127)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:287)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:151)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3999)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2980)\r\n```\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this. It does indeed look like this deserializer does not add (relatively) new handler method, but instead directly calls method to throw exception. This should not happen.\r\n"
            },
            {
                "content": "One question first: which Jackson version? I can not reproduce this with 2.8.8.\r\n"
            },
            {
                "content": "I tested in 2.9.pr3, so most recent version -not sure how far it goes back."
            },
            {
                "content": "Hmmh. This is odd, considering test I added passes... I'll try your test case exactly as shown.\r\n"
            },
            {
                "content": "Ah... interesting. This is triggered by returning of `null` -- so handler is called appropriately, it seems, but check for `null` is taken by sub-class to indicate a problem. Which it really shouldn't."
            },
            {
                "content": "Fixed; I hope this does not introduce problems with other deserializers -- but if it does, those can be worked around, by not using `null` to signal error conditions. I didn't see any places where that was done, but it is possible I missed some. Regardless, all tests pass."
            }
        ],
        "summarized_discussion": "\n\nThe bug in the source code was caused by the deserializer directly calling a method to throw an exception instead of adding a relatively new handler method. The bug was present in Jackson version 2.9.pr3, and was triggered by returning null. The solution was to fix the issue and not use null to signal error conditions. All tests were then passed."
    },
    "JacksonDatabind_7_src/main/java/com/fasterxml/jackson/databind/util/TokenBuffer.java_403_411": {
        "src": "public TokenBuffer deserialize(JsonParser jp, DeserializationContext ctxt) throws IOException\n    {\n            copyCurrentStructure(jp);\n        /* 28-Oct-2014, tatu: As per #592, need to support a special case of starting from\n         *    FIELD_NAME, which is taken to mean that we are missing START_OBJECT, but need\n         *    to assume one did exist.\n         */\n        return this;\n    }",
        "src_wo_comments": "public TokenBuffer deserialize ( JsonParser jp , DeserializationContext ctxt ) throws IOException { copyCurrentStructure ( jp ) ; return this ; }",
        "fixed_src": "public TokenBuffer deserialize(JsonParser jp, DeserializationContext ctxt) throws IOException\n    {\n        if (jp.getCurrentTokenId() != JsonToken.FIELD_NAME.id()) {\n            copyCurrentStructure(jp);\n            return this;\n        }\n        /* 28-Oct-2014, tatu: As per #592, need to support a special case of starting from\n         *    FIELD_NAME, which is taken to mean that we are missing START_OBJECT, but need\n         *    to assume one did exist.\n         */\n        JsonToken t;\n        writeStartObject();\n        do {\n            copyCurrentStructure(jp);\n        } while ((t = jp.nextToken()) == JsonToken.FIELD_NAME);\n        if (t != JsonToken.END_OBJECT) {\n            throw ctxt.mappingException(\"Expected END_OBJECT after copying contents of a JsonParser into TokenBuffer, got \"+t);\n        }\n        writeEndObject();\n        return this;\n    }",
        "fixed_src_wo_comments": "public TokenBuffer deserialize ( JsonParser jp , DeserializationContext ctxt ) throws IOException { if ( jp . getCurrentTokenId ( ) != JsonToken . FIELD_NAME . id ( ) ) { copyCurrentStructure ( jp ) ; return this ; } JsonToken t ; writeStartObject ( ) ; do { copyCurrentStructure ( jp ) ; } while ( ( t = jp . nextToken ( ) ) == JsonToken . FIELD_NAME ) ; if ( t != JsonToken . END_OBJECT ) { throw ctxt . mappingException ( \"Expected END_OBJECT after copying contents of a JsonParser into TokenBuffer, got \" + t ) ; } writeEndObject ( ) ; return this ; }",
        "summary": "Possibly wrong `TokenBuffer` delegate deserialization using `@JsonCreator`",
        "Description": "``` java\nclass Value {\n@JsonCreator\npublic static Value from(TokenBuffer buffer) {\n...\n}\n```\n\nGiven JSON string is  `{ \"a\":1, \"b\":null }`, it is expected that while deserializing using delegate buffer,\ncurrent token will be start object `{`, and rest of the tokens will be available in buffer:\n\n```\n[START_OBJECT, FIELD_NAME, VALUE_NUMBER_INT, FIELD_NAME, VALUE_NULL, END_OBJECT]\n```\n\nBut, buffers ends up being started with field name and then contains single attribute value\n\n```\n[FIELD_NAME, VALUE_NUMBER_INT]\n```\n\nIt's due to how `TokenBuffer#copyCurrentStructure` works when we have current token as a `FIELD_NAME`, rather than `START_OBJECT`, because it's forced to move to next token [BeanDeserializer.java:120](https://github.com/FasterXML/jackson-databind/blob/2.4/src/main/java/com/fasterxml/jackson/databind/deser/BeanDeserializer.java#L120)\n\nHope this helps to nail it down. Is it an intended behavior, or it's regression/bug? \n",
        "issue_url": null,
        "comments": [
            {
                "content": "It is intended behavior at this point, in that deserializers that handle JSON Objects need to accept both the case where current token is `FIELD_NAME` and where it is `START_OBJECT`. This is not ideal, but is required to support (more) efficient handling of the common case for polymorphic types, in which first value pair contains type id, and addition of leading `{` would require merging of additional stream.\nThis is not an ideal state of things, and in cases where content is to be reconstructed, it would be better to reconstruct the whole value.\n\nAlso, since `TokenBuffer` could contain any JSON value, this is bit of a gray area.\n\nHowever, all relevant values should be contained, so missing `\"b\":null` pair.\n\nAlso: as you point out, due to the way `copyCurrentStructure` works, this won't behave properly, so I think handling is not as intended. There is a bug somewhere. :)\nI'm not sure which parts need to be changed, but with given example case it should be possible to figure out proper handling here.\n"
            },
            {
                "content": "Thank you for the explanation. I solved somewhat similar problems in generated marshalers by always require that marshaler expects current token to be the first token as a precondition and leaves parser on a last token as a post condition and with some temporary copying to buffer for a polymorphic deserialization, but it's definitely not trivial having a lot of different deserializers.\n\nAs a workaround for my usecase, I replaced `TokenBuffer` with `Map<String, TokenBuffer>` to collect all needed tokens in a `@JsonCreator` method.\n"
            },
            {
                "content": "Right, originally expectation was what one would expect (`START_OBJECT`), and the complexity was introduced in 1.5 to support polymorphic type handling; and parts of it weren't discovered until some time later.\n\nBut given that `Map` works, `TokenBuffer` definitely should work as well so there is a bug to fix.\nI hope to figure it out & fix, as `TokenBuffer` is the most efficient intermediate form and your use case would be sort of canonical use when converting values.\n"
            },
            {
                "content": "@elucash Thank you again for reporting this -- surprising it hadn't been noticed before.\n"
            },
            {
                "content": "Thank you @cowtowncoder for the quick fix!\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug reported was related to deserializers that handle JSON Objects needing to accept both the case where current token is `FIELD_NAME` and where it is `START_OBJECT`. The solution to the bug was to replace the `TokenBuffer` with a `Map<String, TokenBuffer>` to collect all needed tokens in a `@JsonCreator` method. This was done as a workaround for the use case, and a bug was identified to fix the `TokenBuffer` to make it the most efficient intermediate form. The bug was fixed quickly with a quick fix."
    },
    "Math_72_src/main/java/org/apache/commons/math/analysis/solvers/BrentSolver.java_98_144": {
        "src": "public double solve(final UnivariateRealFunction f,\n                        final double min, final double max, final double initial)\n        throws MaxIterationsExceededException, FunctionEvaluationException {\n\n        clearResult();\n        verifySequence(min, initial, max);\n\n        // return the initial guess if it is good enough\n        double yInitial = f.value(initial);\n        if (Math.abs(yInitial) <= functionValueAccuracy) {\n            setResult(initial, 0);\n            return result;\n        }\n\n        // return the first endpoint if it is good enough\n        double yMin = f.value(min);\n        if (Math.abs(yMin) <= functionValueAccuracy) {\n            setResult(yMin, 0);\n            return result;\n        }\n\n        // reduce interval if min and initial bracket the root\n        if (yInitial * yMin < 0) {\n            return solve(f, min, yMin, initial, yInitial, min, yMin);\n        }\n\n        // return the second endpoint if it is good enough\n        double yMax = f.value(max);\n        if (Math.abs(yMax) <= functionValueAccuracy) {\n            setResult(yMax, 0);\n            return result;\n        }\n\n        // reduce interval if initial and max bracket the root\n        if (yInitial * yMax < 0) {\n            return solve(f, initial, yInitial, max, yMax, initial, yInitial);\n        }\n\n        if (yMin * yMax > 0) {\n            throw MathRuntimeException.createIllegalArgumentException(\n                  NON_BRACKETING_MESSAGE, min, max, yMin, yMax);\n        }\n\n        // full Brent algorithm starting with provided initial guess\n        return solve(f, min, yMin, max, yMax, initial, yInitial);\n\n    }",
        "src_wo_comments": "public double solve ( final UnivariateRealFunction f , final double min , final double max , final double initial ) throws MaxIterationsExceededException , FunctionEvaluationException { clearResult ( ) ; verifySequence ( min , initial , max ) ; double yInitial = f . value ( initial ) ; if ( Math . abs ( yInitial ) <= functionValueAccuracy ) { setResult ( initial , 0 ) ; return result ; } double yMin = f . value ( min ) ; if ( Math . abs ( yMin ) <= functionValueAccuracy ) { setResult ( yMin , 0 ) ; return result ; } if ( yInitial * yMin < 0 ) { return solve ( f , min , yMin , initial , yInitial , min , yMin ) ; } double yMax = f . value ( max ) ; if ( Math . abs ( yMax ) <= functionValueAccuracy ) { setResult ( yMax , 0 ) ; return result ; } if ( yInitial * yMax < 0 ) { return solve ( f , initial , yInitial , max , yMax , initial , yInitial ) ; } if ( yMin * yMax > 0 ) { throw MathRuntimeException . createIllegalArgumentException ( NON_BRACKETING_MESSAGE , min , max , yMin , yMax ) ; } return solve ( f , min , yMin , max , yMax , initial , yInitial ) ; }",
        "fixed_src": "public double solve(final UnivariateRealFunction f,\n                        final double min, final double max, final double initial)\n        throws MaxIterationsExceededException, FunctionEvaluationException {\n\n        clearResult();\n        verifySequence(min, initial, max);\n\n        // return the initial guess if it is good enough\n        double yInitial = f.value(initial);\n        if (Math.abs(yInitial) <= functionValueAccuracy) {\n            setResult(initial, 0);\n            return result;\n        }\n\n        // return the first endpoint if it is good enough\n        double yMin = f.value(min);\n        if (Math.abs(yMin) <= functionValueAccuracy) {\n            setResult(min, 0);\n            return result;\n        }\n\n        // reduce interval if min and initial bracket the root\n        if (yInitial * yMin < 0) {\n            return solve(f, min, yMin, initial, yInitial, min, yMin);\n        }\n\n        // return the second endpoint if it is good enough\n        double yMax = f.value(max);\n        if (Math.abs(yMax) <= functionValueAccuracy) {\n            setResult(max, 0);\n            return result;\n        }\n\n        // reduce interval if initial and max bracket the root\n        if (yInitial * yMax < 0) {\n            return solve(f, initial, yInitial, max, yMax, initial, yInitial);\n        }\n\n        if (yMin * yMax > 0) {\n            throw MathRuntimeException.createIllegalArgumentException(\n                  NON_BRACKETING_MESSAGE, min, max, yMin, yMax);\n        }\n\n        // full Brent algorithm starting with provided initial guess\n        return solve(f, min, yMin, max, yMax, initial, yInitial);\n\n    }",
        "fixed_src_wo_comments": "public double solve ( final UnivariateRealFunction f , final double min , final double max , final double initial ) throws MaxIterationsExceededException , FunctionEvaluationException { clearResult ( ) ; verifySequence ( min , initial , max ) ; double yInitial = f . value ( initial ) ; if ( Math . abs ( yInitial ) <= functionValueAccuracy ) { setResult ( initial , 0 ) ; return result ; } double yMin = f . value ( min ) ; if ( Math . abs ( yMin ) <= functionValueAccuracy ) { setResult ( min , 0 ) ; return result ; } if ( yInitial * yMin < 0 ) { return solve ( f , min , yMin , initial , yInitial , min , yMin ) ; } double yMax = f . value ( max ) ; if ( Math . abs ( yMax ) <= functionValueAccuracy ) { setResult ( max , 0 ) ; return result ; } if ( yInitial * yMax < 0 ) { return solve ( f , initial , yInitial , max , yMax , initial , yInitial ) ; } if ( yMin * yMax > 0 ) { throw MathRuntimeException . createIllegalArgumentException ( NON_BRACKETING_MESSAGE , min , max , yMin , yMax ) ; } return solve ( f , min , yMin , max , yMax , initial , yInitial ) ; }",
        "summary": "Brent solver returns the wrong value if either bracket endpoint is root",
        "Description": "The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-344",
        "comments": [
            "Fixed in subversion repository as of r915522\nthanks for reporting the issue"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of revision 915522. The person who reported the issue is thanked for doing so."
    },
    "Math_25_src/main/java/org/apache/commons/math3/optimization/fitting/HarmonicFitter.java_257_329": {
        "src": "private void guessAOmega() {\n            // initialize the sums for the linear model between the two integrals\n            double sx2 = 0;\n            double sy2 = 0;\n            double sxy = 0;\n            double sxz = 0;\n            double syz = 0;\n\n            double currentX = observations[0].getX();\n            double currentY = observations[0].getY();\n            double f2Integral = 0;\n            double fPrime2Integral = 0;\n            final double startX = currentX;\n            for (int i = 1; i < observations.length; ++i) {\n                // one step forward\n                final double previousX = currentX;\n                final double previousY = currentY;\n                currentX = observations[i].getX();\n                currentY = observations[i].getY();\n\n                // update the integrals of f<sup>2</sup> and f'<sup>2</sup>\n                // considering a linear model for f (and therefore constant f')\n                final double dx = currentX - previousX;\n                final double dy = currentY - previousY;\n                final double f2StepIntegral =\n                    dx * (previousY * previousY + previousY * currentY + currentY * currentY) / 3;\n                final double fPrime2StepIntegral = dy * dy / dx;\n\n                final double x = currentX - startX;\n                f2Integral += f2StepIntegral;\n                fPrime2Integral += fPrime2StepIntegral;\n\n                sx2 += x * x;\n                sy2 += f2Integral * f2Integral;\n                sxy += x * f2Integral;\n                sxz += x * fPrime2Integral;\n                syz += f2Integral * fPrime2Integral;\n            }\n\n            // compute the amplitude and pulsation coefficients\n            double c1 = sy2 * sxz - sxy * syz;\n            double c2 = sxy * sxz - sx2 * syz;\n            double c3 = sx2 * sy2 - sxy * sxy;\n            if ((c1 / c2 < 0) || (c2 / c3 < 0)) {\n                final int last = observations.length - 1;\n                // Range of the observations, assuming that the\n                // observations are sorted.\n                final double xRange = observations[last].getX() - observations[0].getX();\n                if (xRange == 0) {\n                    throw new ZeroException();\n                }\n                omega = 2 * Math.PI / xRange;\n\n                double yMin = Double.POSITIVE_INFINITY;\n                double yMax = Double.NEGATIVE_INFINITY;\n                for (int i = 1; i < observations.length; ++i) {\n                    final double y = observations[i].getY();\n                    if (y < yMin) {\n                        yMin = y;\n                    }\n                    if (y > yMax) {\n                        yMax = y;\n                    }\n                }\n                a = 0.5 * (yMax - yMin);\n            } else {\n                    // In some ill-conditioned cases (cf. MATH-844), the guesser\n                    // procedure cannot produce sensible results.\n\n                a = FastMath.sqrt(c1 / c2);\n                omega = FastMath.sqrt(c2 / c3);\n            }\n        }",
        "src_wo_comments": "private void guessAOmega ( ) { double sx2 = 0 ; double sy2 = 0 ; double sxy = 0 ; double sxz = 0 ; double syz = 0 ; double currentX = observations [ 0 ] . getX ( ) ; double currentY = observations [ 0 ] . getY ( ) ; double f2Integral = 0 ; double fPrime2Integral = 0 ; final double startX = currentX ; for ( int i = 1 ; i < observations . length ; ++ i ) { final double previousX = currentX ; final double previousY = currentY ; currentX = observations [ i ] . getX ( ) ; currentY = observations [ i ] . getY ( ) ; final double dx = currentX - previousX ; final double dy = currentY - previousY ; final double f2StepIntegral = dx * ( previousY * previousY + previousY * currentY + currentY * currentY ) / 3 ; final double fPrime2StepIntegral = dy * dy / dx ; final double x = currentX - startX ; f2Integral += f2StepIntegral ; fPrime2Integral += fPrime2StepIntegral ; sx2 += x * x ; sy2 += f2Integral * f2Integral ; sxy += x * f2Integral ; sxz += x * fPrime2Integral ; syz += f2Integral * fPrime2Integral ; } double c1 = sy2 * sxz - sxy * syz ; double c2 = sxy * sxz - sx2 * syz ; double c3 = sx2 * sy2 - sxy * sxy ; if ( ( c1 / c2 < 0 ) || ( c2 / c3 < 0 ) ) { final int last = observations . length - 1 ; final double xRange = observations [ last ] . getX ( ) - observations [ 0 ] . getX ( ) ; if ( xRange == 0 ) { throw new ZeroException ( ) ; } omega = 2 * Math . PI / xRange ; double yMin = Double . POSITIVE_INFINITY ; double yMax = Double . NEGATIVE_INFINITY ; for ( int i = 1 ; i < observations . length ; ++ i ) { final double y = observations [ i ] . getY ( ) ; if ( y < yMin ) { yMin = y ; } if ( y > yMax ) { yMax = y ; } } a = 0.5 * ( yMax - yMin ) ; } else { a = FastMath . sqrt ( c1 / c2 ) ; omega = FastMath . sqrt ( c2 / c3 ) ; } }",
        "fixed_src": "private void guessAOmega() {\n            // initialize the sums for the linear model between the two integrals\n            double sx2 = 0;\n            double sy2 = 0;\n            double sxy = 0;\n            double sxz = 0;\n            double syz = 0;\n\n            double currentX = observations[0].getX();\n            double currentY = observations[0].getY();\n            double f2Integral = 0;\n            double fPrime2Integral = 0;\n            final double startX = currentX;\n            for (int i = 1; i < observations.length; ++i) {\n                // one step forward\n                final double previousX = currentX;\n                final double previousY = currentY;\n                currentX = observations[i].getX();\n                currentY = observations[i].getY();\n\n                // update the integrals of f<sup>2</sup> and f'<sup>2</sup>\n                // considering a linear model for f (and therefore constant f')\n                final double dx = currentX - previousX;\n                final double dy = currentY - previousY;\n                final double f2StepIntegral =\n                    dx * (previousY * previousY + previousY * currentY + currentY * currentY) / 3;\n                final double fPrime2StepIntegral = dy * dy / dx;\n\n                final double x = currentX - startX;\n                f2Integral += f2StepIntegral;\n                fPrime2Integral += fPrime2StepIntegral;\n\n                sx2 += x * x;\n                sy2 += f2Integral * f2Integral;\n                sxy += x * f2Integral;\n                sxz += x * fPrime2Integral;\n                syz += f2Integral * fPrime2Integral;\n            }\n\n            // compute the amplitude and pulsation coefficients\n            double c1 = sy2 * sxz - sxy * syz;\n            double c2 = sxy * sxz - sx2 * syz;\n            double c3 = sx2 * sy2 - sxy * sxy;\n            if ((c1 / c2 < 0) || (c2 / c3 < 0)) {\n                final int last = observations.length - 1;\n                // Range of the observations, assuming that the\n                // observations are sorted.\n                final double xRange = observations[last].getX() - observations[0].getX();\n                if (xRange == 0) {\n                    throw new ZeroException();\n                }\n                omega = 2 * Math.PI / xRange;\n\n                double yMin = Double.POSITIVE_INFINITY;\n                double yMax = Double.NEGATIVE_INFINITY;\n                for (int i = 1; i < observations.length; ++i) {\n                    final double y = observations[i].getY();\n                    if (y < yMin) {\n                        yMin = y;\n                    }\n                    if (y > yMax) {\n                        yMax = y;\n                    }\n                }\n                a = 0.5 * (yMax - yMin);\n            } else {\n                if (c2 == 0) {\n                    // In some ill-conditioned cases (cf. MATH-844), the guesser\n                    // procedure cannot produce sensible results.\n                    throw new MathIllegalStateException(LocalizedFormats.ZERO_DENOMINATOR);\n                }\n\n                a = FastMath.sqrt(c1 / c2);\n                omega = FastMath.sqrt(c2 / c3);\n            }\n        }",
        "fixed_src_wo_comments": "private void guessAOmega ( ) { double sx2 = 0 ; double sy2 = 0 ; double sxy = 0 ; double sxz = 0 ; double syz = 0 ; double currentX = observations [ 0 ] . getX ( ) ; double currentY = observations [ 0 ] . getY ( ) ; double f2Integral = 0 ; double fPrime2Integral = 0 ; final double startX = currentX ; for ( int i = 1 ; i < observations . length ; ++ i ) { final double previousX = currentX ; final double previousY = currentY ; currentX = observations [ i ] . getX ( ) ; currentY = observations [ i ] . getY ( ) ; final double dx = currentX - previousX ; final double dy = currentY - previousY ; final double f2StepIntegral = dx * ( previousY * previousY + previousY * currentY + currentY * currentY ) / 3 ; final double fPrime2StepIntegral = dy * dy / dx ; final double x = currentX - startX ; f2Integral += f2StepIntegral ; fPrime2Integral += fPrime2StepIntegral ; sx2 += x * x ; sy2 += f2Integral * f2Integral ; sxy += x * f2Integral ; sxz += x * fPrime2Integral ; syz += f2Integral * fPrime2Integral ; } double c1 = sy2 * sxz - sxy * syz ; double c2 = sxy * sxz - sx2 * syz ; double c3 = sx2 * sy2 - sxy * sxy ; if ( ( c1 / c2 < 0 ) || ( c2 / c3 < 0 ) ) { final int last = observations . length - 1 ; final double xRange = observations [ last ] . getX ( ) - observations [ 0 ] . getX ( ) ; if ( xRange == 0 ) { throw new ZeroException ( ) ; } omega = 2 * Math . PI / xRange ; double yMin = Double . POSITIVE_INFINITY ; double yMax = Double . NEGATIVE_INFINITY ; for ( int i = 1 ; i < observations . length ; ++ i ) { final double y = observations [ i ] . getY ( ) ; if ( y < yMin ) { yMin = y ; } if ( y > yMax ) { yMax = y ; } } a = 0.5 * ( yMax - yMin ) ; } else { if ( c2 == 0 ) { throw new MathIllegalStateException ( LocalizedFormats . ZERO_DENOMINATOR ) ; } a = FastMath . sqrt ( c1 / c2 ) ; omega = FastMath . sqrt ( c2 / c3 ) ; } }",
        "summary": "\"HarmonicFitter.ParameterGuesser\" sometimes fails to return sensible values",
        "Description": "The inner class \"ParameterGuesser\" in \"HarmonicFitter\" (package \"o.a.c.m.optimization.fitting\") fails to compute a usable guess for the \"amplitude\" parameter.\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-844",
        "comments": [
            "I've attached a unit test demonstrating the problem reporting in [this thread|http://markmail.org/message/42yuec5lk5wr6gbo].\n",
            "I propose to check whether the values are sensible, and when they are not, either\n# throw an appropriate exception (i.e. signal that the guesser fails), or\n# return arbitrary values that would allow the optimizer to proceed.\n\nAny preference?\n",
            "In this case, the guesser fails because the function in really far from an harmonic function. It is a triangular periodic function with amplitude +/-3 and period 12, and all sample points are taken as integer abscissa, so values all belong to the integer subset {-3, -2, -1, 0, 1, 2, 3}.\n\nThis is an (interesting) ill-conditioned case. As small integers are represented exactly even as primitive doubles, all computations are exact and one of the intermediate parameters (c2 in the source code) is a perfect 0 (there are no approximation here, the result is exact), and we divide by c2.\n\nI would suggest to raise an exception here, and to store this as a junit test for failure mode.",
            "As of revision 1374046, the code will generate an exception."
        ],
        "summarized_discussion": "\n\nThe source code bug was caused by a function that was far from an harmonic function. It was a triangular periodic function with amplitude +/-3 and period 12. The bug was solved by revising the code to generate an exception when the values are not sensible."
    },
    "Compress_5_src/main/java/org/apache/commons/compress/archivers/zip/ZipArchiveInputStream.java_191_246": {
        "src": "public int read(byte[] buffer, int start, int length) throws IOException {\n        if (closed) {\n            throw new IOException(\"The stream is closed\");\n        }\n        if (inf.finished() || current == null) {\n            return -1;\n        }\n\n        // avoid int overflow, check null buffer\n        if (start <= buffer.length && length >= 0 && start >= 0\n            && buffer.length - start >= length) {\n            if (current.getMethod() == ZipArchiveOutputStream.STORED) {\n                int csize = (int) current.getSize();\n                if (readBytesOfEntry >= csize) {\n                    return -1;\n                }\n                if (offsetInBuffer >= lengthOfLastRead) {\n                    offsetInBuffer = 0;\n                    if ((lengthOfLastRead = in.read(buf)) == -1) {\n                        return -1;\n                    }\n                    count(lengthOfLastRead);\n                    bytesReadFromStream += lengthOfLastRead;\n                }\n                int toRead = length > lengthOfLastRead\n                    ? lengthOfLastRead - offsetInBuffer\n                    : length;\n                if ((csize - readBytesOfEntry) < toRead) {\n                    toRead = csize - readBytesOfEntry;\n                }\n                System.arraycopy(buf, offsetInBuffer, buffer, start, toRead);\n                offsetInBuffer += toRead;\n                readBytesOfEntry += toRead;\n                crc.update(buffer, start, toRead);\n                return toRead;\n            }\n            if (inf.needsInput()) {\n                fill();\n                if (lengthOfLastRead > 0) {\n                    bytesReadFromStream += lengthOfLastRead;\n                }\n            }\n            int read = 0;\n            try {\n                read = inf.inflate(buffer, start, length);\n            } catch (DataFormatException e) {\n                throw new ZipException(e.getMessage());\n            }\n            if (read == 0 && inf.finished()) {\n                return -1;\n            }\n            crc.update(buffer, start, read);\n            return read;\n        }\n        throw new ArrayIndexOutOfBoundsException();\n    }",
        "src_wo_comments": "public int read ( byte [ ] buffer , int start , int length ) throws IOException { if ( closed ) { throw new IOException ( \"The stream is closed\" ) ; } if ( inf . finished ( ) || current == null ) { return - 1 ; } if ( start <= buffer . length && length >= 0 && start >= 0 && buffer . length - start >= length ) { if ( current . getMethod ( ) == ZipArchiveOutputStream . STORED ) { int csize = ( int ) current . getSize ( ) ; if ( readBytesOfEntry >= csize ) { return - 1 ; } if ( offsetInBuffer >= lengthOfLastRead ) { offsetInBuffer = 0 ; if ( ( lengthOfLastRead = in . read ( buf ) ) == - 1 ) { return - 1 ; } count ( lengthOfLastRead ) ; bytesReadFromStream += lengthOfLastRead ; } int toRead = length > lengthOfLastRead ? lengthOfLastRead - offsetInBuffer : length ; if ( ( csize - readBytesOfEntry ) < toRead ) { toRead = csize - readBytesOfEntry ; } System . arraycopy ( buf , offsetInBuffer , buffer , start , toRead ) ; offsetInBuffer += toRead ; readBytesOfEntry += toRead ; crc . update ( buffer , start , toRead ) ; return toRead ; } if ( inf . needsInput ( ) ) { fill ( ) ; if ( lengthOfLastRead > 0 ) { bytesReadFromStream += lengthOfLastRead ; } } int read = 0 ; try { read = inf . inflate ( buffer , start , length ) ; } catch ( DataFormatException e ) { throw new ZipException ( e . getMessage ( ) ) ; } if ( read == 0 && inf . finished ( ) ) { return - 1 ; } crc . update ( buffer , start , read ) ; return read ; } throw new ArrayIndexOutOfBoundsException ( ) ; }",
        "fixed_src": "public int read(byte[] buffer, int start, int length) throws IOException {\n        if (closed) {\n            throw new IOException(\"The stream is closed\");\n        }\n        if (inf.finished() || current == null) {\n            return -1;\n        }\n\n        // avoid int overflow, check null buffer\n        if (start <= buffer.length && length >= 0 && start >= 0\n            && buffer.length - start >= length) {\n            if (current.getMethod() == ZipArchiveOutputStream.STORED) {\n                int csize = (int) current.getSize();\n                if (readBytesOfEntry >= csize) {\n                    return -1;\n                }\n                if (offsetInBuffer >= lengthOfLastRead) {\n                    offsetInBuffer = 0;\n                    if ((lengthOfLastRead = in.read(buf)) == -1) {\n                        return -1;\n                    }\n                    count(lengthOfLastRead);\n                    bytesReadFromStream += lengthOfLastRead;\n                }\n                int toRead = length > lengthOfLastRead\n                    ? lengthOfLastRead - offsetInBuffer\n                    : length;\n                if ((csize - readBytesOfEntry) < toRead) {\n                    toRead = csize - readBytesOfEntry;\n                }\n                System.arraycopy(buf, offsetInBuffer, buffer, start, toRead);\n                offsetInBuffer += toRead;\n                readBytesOfEntry += toRead;\n                crc.update(buffer, start, toRead);\n                return toRead;\n            }\n            if (inf.needsInput()) {\n                fill();\n                if (lengthOfLastRead > 0) {\n                    bytesReadFromStream += lengthOfLastRead;\n                }\n            }\n            int read = 0;\n            try {\n                read = inf.inflate(buffer, start, length);\n            } catch (DataFormatException e) {\n                throw new ZipException(e.getMessage());\n            }\n            if (read == 0) {\n                if (inf.finished()) {\n                    return -1;\n                } else if (lengthOfLastRead == -1) {\n                    throw new IOException(\"Truncated ZIP file\");\n                }\n            }\n            crc.update(buffer, start, read);\n            return read;\n        }\n        throw new ArrayIndexOutOfBoundsException();\n    }",
        "fixed_src_wo_comments": "public int read ( byte [ ] buffer , int start , int length ) throws IOException { if ( closed ) { throw new IOException ( \"The stream is closed\" ) ; } if ( inf . finished ( ) || current == null ) { return - 1 ; } if ( start <= buffer . length && length >= 0 && start >= 0 && buffer . length - start >= length ) { if ( current . getMethod ( ) == ZipArchiveOutputStream . STORED ) { int csize = ( int ) current . getSize ( ) ; if ( readBytesOfEntry >= csize ) { return - 1 ; } if ( offsetInBuffer >= lengthOfLastRead ) { offsetInBuffer = 0 ; if ( ( lengthOfLastRead = in . read ( buf ) ) == - 1 ) { return - 1 ; } count ( lengthOfLastRead ) ; bytesReadFromStream += lengthOfLastRead ; } int toRead = length > lengthOfLastRead ? lengthOfLastRead - offsetInBuffer : length ; if ( ( csize - readBytesOfEntry ) < toRead ) { toRead = csize - readBytesOfEntry ; } System . arraycopy ( buf , offsetInBuffer , buffer , start , toRead ) ; offsetInBuffer += toRead ; readBytesOfEntry += toRead ; crc . update ( buffer , start , toRead ) ; return toRead ; } if ( inf . needsInput ( ) ) { fill ( ) ; if ( lengthOfLastRead > 0 ) { bytesReadFromStream += lengthOfLastRead ; } } int read = 0 ; try { read = inf . inflate ( buffer , start , length ) ; } catch ( DataFormatException e ) { throw new ZipException ( e . getMessage ( ) ) ; } if ( read == 0 ) { if ( inf . finished ( ) ) { return - 1 ; } else if ( lengthOfLastRead == - 1 ) { throw new IOException ( \"Truncated ZIP file\" ) ; } } crc . update ( buffer , start , read ) ; return read ; } throw new ArrayIndexOutOfBoundsException ( ) ; }",
        "summary": "ZipArchiveInputStream doesn't report the end of a truncated archive",
        "Description": "If a Zip archive is truncated, (e.g. because it is the first volume in a multi-volume archive) the ZipArchiveInputStream.read() method will not detect that fact. All calls to read() will return 0 bytes read. They will not return -1 (end of stream), nor will they throw any exception (which would seem like a good idea to me because the archive is truncated).\n\nI have tracked this problem to ZipArchiveInputStream.java, line 239. It contains a check\n\nif (read == 0 && inf.finished()) {\n    return -1;\n}\n\nFor truncated archives the read is always zero but the inf is never finished(). I suggest adding two lines below:\n\nif (read == 0 && inf.finished()) {\n    return -1;\n} else if (read == 0 && lengthOfLastRead == -1) {\n\tthrow new IOException(\"Truncated ZIP file\");\n}\n\nThis solves the problem in my tests.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-87",
        "comments": [
            "A patch containing the two lines I talked about in the issue description as well as a unit test.\n\nThe test file is a part of the Maven 2.2.1 distribution so it's already copyrighted by ASF. It should be placed in src/test/resources folder",
            "I'll look into this - thanks for the testcase - but let me add that in general ZipFile should be preferred over ZipArchiveInputStream when reading  file since the later may return entries that aren't supposed to be there at all.  ZipFile should terminate with a reasonable exception for truncated ZIPs OOTB.",
            "I know, just that I need to crawl zips that aren't necessarily files (like zips attached to emails, or zips inside other zips). For this we have to use the stream and our crawler fell into an infinite loop waiting for the '-1' returned from read() method, and kept getting '0' instead. This is bad IMHO.",
            "I had to modify the test a little since the byte-counts for some existing entries were different on my Linux box (so I simply ignored them) and the first read of the last entry succeeded (with a count > 0) for me.\n\nThanks!\n\nsvn revision 831204"
        ],
        "summarized_discussion": "\n\nThe bug was caused by the ZipArchiveInputStream returning entries that were not supposed to be there, causing an infinite loop. The solution was to modify the test a little and use the ZipFile instead, which should terminate with a reasonable exception for truncated ZIPs. The modified test was committed in svn revision 831204."
    },
    "Cli_24_src/java/org/apache/commons/cli/HelpFormatter.java_809_852": {
        "src": "protected StringBuffer renderWrappedText(StringBuffer sb, int width, \n                                             int nextLineTabStop, String text)\n    {\n        int pos = findWrapPos(text, width, 0);\n\n        if (pos == -1)\n        {\n            sb.append(rtrim(text));\n\n            return sb;\n        }\n        sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n\n        if (nextLineTabStop >= width)\n        {\n            // stops infinite loop happening\n            throw new IllegalStateException(\"Total width is less than the width of the argument and indent \" + \n                                            \"- no room for the description\");\n        }\n\n        // all following lines must be padded with nextLineTabStop space \n        // characters\n        final String padding = createPadding(nextLineTabStop);\n\n        while (true)\n        {\n            text = padding + text.substring(pos).trim();\n            pos = findWrapPos(text, width, 0);\n\n            if (pos == -1)\n            {\n                sb.append(text);\n\n                return sb;\n            }\n            \n            if ( (text.length() > width) && (pos == nextLineTabStop - 1) ) \n            {\n                pos = width;\n            }\n\n            sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n        }\n    }",
        "src_wo_comments": "protected StringBuffer renderWrappedText ( StringBuffer sb , int width , int nextLineTabStop , String text ) { int pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( rtrim ( text ) ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; if ( nextLineTabStop >= width ) { throw new IllegalStateException ( \"Total width is less than the width of the argument and indent \" + \"- no room for the description\" ) ; } final String padding = createPadding ( nextLineTabStop ) ; while ( true ) { text = padding + text . substring ( pos ) . trim ( ) ; pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( text ) ; return sb ; } if ( ( text . length ( ) > width ) && ( pos == nextLineTabStop - 1 ) ) { pos = width ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; } }",
        "fixed_src": "protected StringBuffer renderWrappedText(StringBuffer sb, int width, \n                                             int nextLineTabStop, String text)\n    {\n        int pos = findWrapPos(text, width, 0);\n\n        if (pos == -1)\n        {\n            sb.append(rtrim(text));\n\n            return sb;\n        }\n        sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n\n        if (nextLineTabStop >= width)\n        {\n            // stops infinite loop happening\n            nextLineTabStop = width - 1;\n        }\n\n        // all following lines must be padded with nextLineTabStop space \n        // characters\n        final String padding = createPadding(nextLineTabStop);\n\n        while (true)\n        {\n            text = padding + text.substring(pos).trim();\n            pos = findWrapPos(text, width, 0);\n\n            if (pos == -1)\n            {\n                sb.append(text);\n\n                return sb;\n            }\n            \n            if ( (text.length() > width) && (pos == nextLineTabStop - 1) ) \n            {\n                pos = width;\n            }\n\n            sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n        }\n    }",
        "fixed_src_wo_comments": "protected StringBuffer renderWrappedText ( StringBuffer sb , int width , int nextLineTabStop , String text ) { int pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( rtrim ( text ) ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; if ( nextLineTabStop >= width ) { nextLineTabStop = width - 1 ; } final String padding = createPadding ( nextLineTabStop ) ; while ( true ) { text = padding + text . substring ( pos ) . trim ( ) ; pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( text ) ; return sb ; } if ( ( text . length ( ) > width ) && ( pos == nextLineTabStop - 1 ) ) { pos = width ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; } }",
        "summary": "infinite loop in the wrapping code of HelpFormatter",
        "Description": "If there is not enough space to display a word on a single line, HelpFormatter goes into a infinite loops until the JVM crashes with an OutOfMemoryError.\n\nTest case:\n\n{code}\nOptions options = new Options();\noptions.addOption(\"h\", \"help\", false, \"This is a looooong description\");\n\nHelpFormatter formatter = new HelpFormatter();\nformatter.setWidth(20);\nformatter.printHelp(\"app\", options); // hang & crash\n{code}\n\nAn helpful exception indicating the insufficient width would be more appropriate than an OutOfMemoryError.",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-162",
        "comments": [
            "svn ci -m \"Changing the current OutOfMemoryError to a RuntimeException per CLI-162. A new ticket for the RuntimeException is at CLI-174\" \n\nSending        src/java/org/apache/commons/cli/HelpFormatter.java\nAdding         src/test/org/apache/commons/cli/bug/BugCLI162Test.java\nTransmitting file data ..\nCommitted revision 735257.",
            "Committed new test in BugCLI162Test. The class BugCLI162Test is not invoked during a Maven build BTW.",
            "\nTwo of the options appear to be problematic in CLI162.\n\nThe first is OPT_PASSWORD. In this the url is longer than the allowed width of the screen, so some kind of failure needs to happen - or the url needs to be mercilessly chopped. This is the one that goes into an infinite loop due to CLI-151. My gut feeling from the code is that CLI-151 didn't introduce this bug, but you had to be significantly longer to hit the bug before (ie: printTabStop longer than the width).\n\nThe second is OPT_PARAM_TYPES_INT + OPT_PARAM_TYPES_NAME, it shows the the patch for this ticket contained a bug when the lastPos happened to equal the firstPos for completely normal reasons. I'd missed that pos was already set to a real value when the loop was begun and not to 0. I'm not sure why both options have to be set for this to happen. ",
            "Attaching patch that rolls back the previous RuntimeException throwing. The if statement in that patch was testing the wrong condition. This patch adds the correct condition, and rather than throwing an exception the text is simply outputted irregardless of the fact it is over the width. What should happen is debatable here - due to a side-effect of CLI-151, we can't do anything aggressive here because things were printing out happily if they were under width + printTabStop. Our options are either to just print, or to forcibly break the text. \n\nThe test code no longer expects to get a RuntimeException.",
            "svn ci -m \"Applying my second attempt at a patch to CLI-162. This fixes Gary's reported bug (one of which was an example of CLI-162, and one a bug in my first attempt to patch). Open question is whether to output text that is too long, or try and break it up to fit the screen width. \"\n\nSending        src/java/org/apache/commons/cli/HelpFormatter.java\nSending        src/test/org/apache/commons/cli/bug/BugCLI162Test.java\nTransmitting file data ..\nCommitted revision 745388.",
            "Ended up modifying this to break it up to fit screen width.\n\nDiscovered another infinite loop issue if the entered width is smaller than the nextTabStop (ie: argument + indent). ",
            "Throw IllegalStateException on last infinite loop test case.\n\n\nsvn ci -m \"Applying additional patch to throw IllegalStateException when the speci\nfied width is not enough to fit the flags, indent and 1 character for the description. This closes out CLI-162 (for now :) ). \"\nSending        src/java/org/apache/commons/cli/HelpFormatter.java\nSending        src/test/org/apache/commons/cli/bug/BugCLI162Test.java\nTransmitting file data ..\nCommitted revision 746137."
        ],
        "summarized_discussion": "\n\nThe solution to the bug was to modify the code to break up the text to fit the screen width, and throw an IllegalStateException when the specified width is not enough to fit the flags, indent and 1 character for the description. This was done by committing new test code in BugCLI162Test, and applying patches to HelpFormatter.java."
    },
    "Cli_2_src/java/org/apache/commons/cli/PosixParser.java_278_308": {
        "src": "protected void burstToken(String token, boolean stopAtNonOption)\n    {\n        int tokenLength = token.length();\n\n        for (int i = 1; i < tokenLength; i++)\n        {\n            String ch = String.valueOf(token.charAt(i));\n            boolean hasOption = options.hasOption(ch);\n\n            if (hasOption)\n            {\n                tokens.add(\"-\" + ch);\n                currentOption = options.getOption(ch);\n\n                if (currentOption.hasArg() && (token.length() != (i + 1)))\n                {\n                    tokens.add(token.substring(i + 1));\n\n                    break;\n                }\n            }\n            else if (stopAtNonOption)\n            {\n                process(token.substring(i));\n            }\n            else\n            {\n                tokens.add(\"-\" + ch);\n            }\n        }\n    }",
        "src_wo_comments": "protected void burstToken ( String token , boolean stopAtNonOption ) { int tokenLength = token . length ( ) ; for ( int i = 1 ; i < tokenLength ; i ++ ) { String ch = String . valueOf ( token . charAt ( i ) ) ; boolean hasOption = options . hasOption ( ch ) ; if ( hasOption ) { tokens . add ( \"-\" + ch ) ; currentOption = options . getOption ( ch ) ; if ( currentOption . hasArg ( ) && ( token . length ( ) != ( i + 1 ) ) ) { tokens . add ( token . substring ( i + 1 ) ) ; break ; } } else if ( stopAtNonOption ) { process ( token . substring ( i ) ) ; } else { tokens . add ( \"-\" + ch ) ; } } }",
        "fixed_src": "protected void burstToken(String token, boolean stopAtNonOption)\n    {\n        int tokenLength = token.length();\n\n        for (int i = 1; i < tokenLength; i++)\n        {\n            String ch = String.valueOf(token.charAt(i));\n            boolean hasOption = options.hasOption(ch);\n\n            if (hasOption)\n            {\n                tokens.add(\"-\" + ch);\n                currentOption = options.getOption(ch);\n\n                if (currentOption.hasArg() && (token.length() != (i + 1)))\n                {\n                    tokens.add(token.substring(i + 1));\n\n                    break;\n                }\n            }\n            else if (stopAtNonOption)\n            {\n                process(token.substring(i));\n            }\n            else\n            {\n                tokens.add(token);\n                break;\n            }\n        }\n    }",
        "fixed_src_wo_comments": "protected void burstToken ( String token , boolean stopAtNonOption ) { int tokenLength = token . length ( ) ; for ( int i = 1 ; i < tokenLength ; i ++ ) { String ch = String . valueOf ( token . charAt ( i ) ) ; boolean hasOption = options . hasOption ( ch ) ; if ( hasOption ) { tokens . add ( \"-\" + ch ) ; currentOption = options . getOption ( ch ) ; if ( currentOption . hasArg ( ) && ( token . length ( ) != ( i + 1 ) ) ) { tokens . add ( token . substring ( i + 1 ) ) ; break ; } } else if ( stopAtNonOption ) { process ( token . substring ( i ) ) ; } else { tokens . add ( token ) ; break ; } } }",
        "summary": "[cli] Parameter value \"-something\" misinterpreted as a parameter",
        "Description": "If a parameter value is passed that contains a hyphen as the (delimited) first \ncharacter, CLI parses this a parameter. For example using the call\njava myclass -t \"-something\"\nResults in the parser creating the invalid parameter -o (noting that it is \nskipping the 's')\n\nMy code is using the Posix parser as follows\nOptions options = buildCommandLineOptions();\nCommandLineParser parser = new PosixParser();\nCommandLine commandLine = null;\ntry {\n\t\t\t\n\tcommandLine = parser.parse(options, args);\n}\ncatch (ParseException e) {\n\t\t\t\n\tSystem.out.println(\"Invalid parameters. \" + e.getMessage() + NEW_LINE);\n\tSystem.exit(EXIT_CODE_ERROR);\n}\n\nThis has been tested against the nightly build dated 20050503.",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-51",
        "comments": [
            "The usual strategy here is to allow -- as a way to indicate that the rest of the\nline is pure text. If it's not available, this should be added. ",
            "Fix so parser doesn't burst options which are not defined. (-s) in the above case.  \n\nIncludes unit test.",
            "Thanks for the patch Brian; could you repeat it without the automatic IDE refactoring? It makes it a lot harder to read the patch; and more importantly for people to look back at the changes and see what the actual change was.\n\nChanging from protected to private for burstToken is also often a tricky one to do as we're stuck in the wonderful world of maintaining backwards compatibility.\n\nThe unit test looks good, but it'll need an Apache license and not an LGPL license (copy the header from the BugCLI18Test class). ",
            "Updated patch.  Removed incorrect license header; removed formatting changed; reverted access level change.",
            "Has anyone had the chance to try out the updated patch for this issue?",
            "Patch format looks great - I'll aim to apply it later today.",
            "svn ci -m \"Applying test and fix patch from CLI-51. Thanks to Brian Egge. \" src/\nSending        src/java/org/apache/commons/cli/PosixParser.java\nAdding         src/test/org/apache/commons/cli/bug/BugCLI51Test.java\nTransmitting file data ..\nCommitted revision 540661."
        ],
        "summarized_discussion": "\n\nThe bug was solved by adding an option to indicate that the rest of the line is pure text, and by fixing the parser so it doesn't burst options which are not defined. The patch was updated to remove incorrect license header, formatting changes, and reverted access level change. The patch was applied with a commit message and the bug was solved."
    },
    "JacksonDatabind_17_src/main/java/com/fasterxml/jackson/databind/ObjectMapper.java_167_193": {
        "src": "public boolean useForType(JavaType t)\n        {\n            switch (_appliesFor) {\n            case NON_CONCRETE_AND_ARRAYS:\n                while (t.isArrayType()) {\n                    t = t.getContentType();\n                }\n                // fall through\n            case OBJECT_AND_NON_CONCRETE:\n//                return t.isJavaLangObject() || \n                return (t.getRawClass() == Object.class)\n                        || (!t.isConcrete()\n                                // [databind#88] Should not apply to JSON tree models:\n                        || TreeNode.class.isAssignableFrom(t.getRawClass()));\n\n            case NON_FINAL:\n                while (t.isArrayType()) {\n                    t = t.getContentType();\n                }\n                // [Issue#88] Should not apply to JSON tree models:\n                return !t.isFinal() && !TreeNode.class.isAssignableFrom(t.getRawClass());\n            default:\n            //case JAVA_LANG_OBJECT:\n//                return t.isJavaLangObject();\n                return (t.getRawClass() == Object.class);\n            }\n        }",
        "src_wo_comments": "public boolean useForType ( JavaType t ) { switch ( _appliesFor ) { case NON_CONCRETE_AND_ARRAYS : while ( t . isArrayType ( ) ) { t = t . getContentType ( ) ; } case OBJECT_AND_NON_CONCRETE : return ( t . getRawClass ( ) == Object . class ) || ( ! t . isConcrete ( ) || TreeNode . class . isAssignableFrom ( t . getRawClass ( ) ) ) ; case NON_FINAL : while ( t . isArrayType ( ) ) { t = t . getContentType ( ) ; } return ! t . isFinal ( ) && ! TreeNode . class . isAssignableFrom ( t . getRawClass ( ) ) ; default : return ( t . getRawClass ( ) == Object . class ) ; } }",
        "fixed_src": "public boolean useForType(JavaType t)\n        {\n            switch (_appliesFor) {\n            case NON_CONCRETE_AND_ARRAYS:\n                while (t.isArrayType()) {\n                    t = t.getContentType();\n                }\n                // fall through\n            case OBJECT_AND_NON_CONCRETE:\n//                return t.isJavaLangObject() || \n                return (t.getRawClass() == Object.class)\n                        || (!t.isConcrete()\n                                // [databind#88] Should not apply to JSON tree models:\n                                && !TreeNode.class.isAssignableFrom(t.getRawClass()));\n\n            case NON_FINAL:\n                while (t.isArrayType()) {\n                    t = t.getContentType();\n                }\n                // [Issue#88] Should not apply to JSON tree models:\n                return !t.isFinal() && !TreeNode.class.isAssignableFrom(t.getRawClass());\n            default:\n            //case JAVA_LANG_OBJECT:\n//                return t.isJavaLangObject();\n                return (t.getRawClass() == Object.class);\n            }\n        }",
        "fixed_src_wo_comments": "public boolean useForType ( JavaType t ) { switch ( _appliesFor ) { case NON_CONCRETE_AND_ARRAYS : while ( t . isArrayType ( ) ) { t = t . getContentType ( ) ; } case OBJECT_AND_NON_CONCRETE : return ( t . getRawClass ( ) == Object . class ) || ( ! t . isConcrete ( ) && ! TreeNode . class . isAssignableFrom ( t . getRawClass ( ) ) ) ; case NON_FINAL : while ( t . isArrayType ( ) ) { t = t . getContentType ( ) ; } return ! t . isFinal ( ) && ! TreeNode . class . isAssignableFrom ( t . getRawClass ( ) ) ; default : return ( t . getRawClass ( ) == Object . class ) ; } }",
        "summary": "readTree does not work with defaultTyping enabled but no type info provided",
        "Description": "I have enabled `defaultTyping`, and serialized `Foo` entity with no type info. I'm trying to read json as a tree with `mapper.readTree(json)`, and it throws an exception \n\n``` java\nException in thread \"main\" com.fasterxml.jackson.databind.JsonMappingException: \nUnexpected token (START_OBJECT), expected START_ARRAY: need JSON Array to contain As.WRAPPER_ARRAY \ntype information for class com.fasterxml.jackson.databind.JsonNode\n at [Source: {\n  \"bar\" : \"bar\"\n}; line: 1, column: 1]\n    at com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148)\n    at com.fasterxml.jackson.databind.DeserializationContext.wrongTokenException(DeserializationContext.java:927)\n    at com.fasterxml.jackson.databind.jsontype.impl.AsArrayTypeDeserializer._locateTypeId(AsArrayTypeDeserializer.java:127)\n    at com.fasterxml.jackson.databind.jsontype.impl.AsArrayTypeDeserializer._deserialize(AsArrayTypeDeserializer.java:93)\n    at com.fasterxml.jackson.databind.jsontype.impl.AsArrayTypeDeserializer.deserializeTypedFromAny(AsArrayTypeDeserializer.java:68)\n    at com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeWithType(JsonNodeDeserializer.java:144)\n    at com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserializeWithType(JsonNodeDeserializer.java:14)\n    at com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:42)\n    at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3562)\n    at com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:2136)\n    at test.App.main(App.java:23)\n```\n\nHowever, if I disable `defaultTyping`, the same code works fine. So, `readTree(json)` does not actually need type info for the root element, because it works when `defaultTyping` is disabled (i.e. `{\"bar\" : \"bar\"}`), but it throws the exception when `defaultTyping` is enabled, that's why it looks like a bug. The same thing happens for `valueToTree(foo)`. \nJackson version is `2.5.3`\nFull code is provided.\n\n``` java\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.MapperFeature;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializationFeature;\nimport java.io.IOException;\n\npublic class App {\n    public static void main(String[] args) throws IOException {\n        ObjectMapper mapper = new ObjectMapper()\n                .enableDefaultTyping() // works fine with disableDefaultTyping()\n                .enable(MapperFeature.AUTO_DETECT_GETTERS)\n                .enable(MapperFeature.REQUIRE_SETTERS_FOR_GETTERS)\n                .disable(MapperFeature.USE_GETTERS_AS_SETTERS)\n                .disable(MapperFeature.CAN_OVERRIDE_ACCESS_MODIFIERS)\n                .enable(SerializationFeature.INDENT_OUTPUT)\n                .disable(SerializationFeature.FAIL_ON_EMPTY_BEANS);\n\n        Foo foo = new Foo(\"bar\");\n        String serialized = mapper.writeValueAsString(foo); // {\"bar\" : \"bar\"}\n\n        JsonNode jsonNode = mapper.readTree(serialized); // exception here\n        JsonNode node = mapper.valueToTree(foo); // and here\n    }\n\n    public static class Foo {\n        private String bar;\n\n        public Foo() {\n        }\n\n        public Foo(String bar) {\n            this.bar = bar;\n        }\n\n        public String getBar() {\n            return bar;\n        }\n\n        public void setBar(String bar) {\n            this.bar = bar;\n        }\n    }\n}\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Thanks. Tree handling should just ignore default typing, as trees should represent JSON exactly as-is, so this sounds like a bug.\n"
            },
            {
                "content": "Looks like earlier fix for #88 had a minor flaw, and it was not skipping type id for `JsonNode` with standard default typing which is the intended behavior (one can still force type id with custom default typing).\n"
            },
            {
                "content": "Thanks a lot for fast reply and fix!\nWaiting for jackson `2.5.4`.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug in the source code was related to tree handling not ignoring default typing, which is the intended behavior. A fix was provided and the user is now waiting for the Jackson 2.5.4 release."
    },
    "Math_33_src/main/java/org/apache/commons/math3/optimization/linear/SimplexTableau.java_327_367": {
        "src": "protected void dropPhase1Objective() {\n        if (getNumObjectiveFunctions() == 1) {\n            return;\n        }\n\n        List<Integer> columnsToDrop = new ArrayList<Integer>();\n        columnsToDrop.add(0);\n\n        // positive cost non-artificial variables\n        for (int i = getNumObjectiveFunctions(); i < getArtificialVariableOffset(); i++) {\n            final double entry = tableau.getEntry(0, i);\n            if (Precision.compareTo(entry, 0d, maxUlps) > 0) {\n                columnsToDrop.add(i);\n            }\n        }\n\n        // non-basic artificial variables\n        for (int i = 0; i < getNumArtificialVariables(); i++) {\n          int col = i + getArtificialVariableOffset();\n          if (getBasicRow(col) == null) {\n            columnsToDrop.add(col);\n          }\n        }\n\n        double[][] matrix = new double[getHeight() - 1][getWidth() - columnsToDrop.size()];\n        for (int i = 1; i < getHeight(); i++) {\n          int col = 0;\n          for (int j = 0; j < getWidth(); j++) {\n            if (!columnsToDrop.contains(j)) {\n              matrix[i - 1][col++] = tableau.getEntry(i, j);\n            }\n          }\n        }\n\n        for (int i = columnsToDrop.size() - 1; i >= 0; i--) {\n          columnLabels.remove((int) columnsToDrop.get(i));\n        }\n\n        this.tableau = new Array2DRowRealMatrix(matrix);\n        this.numArtificialVariables = 0;\n    }",
        "src_wo_comments": "protected void dropPhase1Objective ( ) { if ( getNumObjectiveFunctions ( ) == 1 ) { return ; } List < Integer > columnsToDrop = new ArrayList < Integer > ( ) ; columnsToDrop . add ( 0 ) ; for ( int i = getNumObjectiveFunctions ( ) ; i < getArtificialVariableOffset ( ) ; i ++ ) { final double entry = tableau . getEntry ( 0 , i ) ; if ( Precision . compareTo ( entry , 0d , maxUlps ) > 0 ) { columnsToDrop . add ( i ) ; } } for ( int i = 0 ; i < getNumArtificialVariables ( ) ; i ++ ) { int col = i + getArtificialVariableOffset ( ) ; if ( getBasicRow ( col ) == null ) { columnsToDrop . add ( col ) ; } } double [ ] [ ] matrix = new double [ getHeight ( ) - 1 ] [ getWidth ( ) - columnsToDrop . size ( ) ] ; for ( int i = 1 ; i < getHeight ( ) ; i ++ ) { int col = 0 ; for ( int j = 0 ; j < getWidth ( ) ; j ++ ) { if ( ! columnsToDrop . contains ( j ) ) { matrix [ i - 1 ] [ col ++ ] = tableau . getEntry ( i , j ) ; } } } for ( int i = columnsToDrop . size ( ) - 1 ; i >= 0 ; i -- ) { columnLabels . remove ( ( int ) columnsToDrop . get ( i ) ) ; } this . tableau = new Array2DRowRealMatrix ( matrix ) ; this . numArtificialVariables = 0 ; }",
        "fixed_src": "protected void dropPhase1Objective() {\n        if (getNumObjectiveFunctions() == 1) {\n            return;\n        }\n\n        List<Integer> columnsToDrop = new ArrayList<Integer>();\n        columnsToDrop.add(0);\n\n        // positive cost non-artificial variables\n        for (int i = getNumObjectiveFunctions(); i < getArtificialVariableOffset(); i++) {\n            final double entry = tableau.getEntry(0, i);\n            if (Precision.compareTo(entry, 0d, epsilon) > 0) {\n                columnsToDrop.add(i);\n            }\n        }\n\n        // non-basic artificial variables\n        for (int i = 0; i < getNumArtificialVariables(); i++) {\n          int col = i + getArtificialVariableOffset();\n          if (getBasicRow(col) == null) {\n            columnsToDrop.add(col);\n          }\n        }\n\n        double[][] matrix = new double[getHeight() - 1][getWidth() - columnsToDrop.size()];\n        for (int i = 1; i < getHeight(); i++) {\n          int col = 0;\n          for (int j = 0; j < getWidth(); j++) {\n            if (!columnsToDrop.contains(j)) {\n              matrix[i - 1][col++] = tableau.getEntry(i, j);\n            }\n          }\n        }\n\n        for (int i = columnsToDrop.size() - 1; i >= 0; i--) {\n          columnLabels.remove((int) columnsToDrop.get(i));\n        }\n\n        this.tableau = new Array2DRowRealMatrix(matrix);\n        this.numArtificialVariables = 0;\n    }",
        "fixed_src_wo_comments": "protected void dropPhase1Objective ( ) { if ( getNumObjectiveFunctions ( ) == 1 ) { return ; } List < Integer > columnsToDrop = new ArrayList < Integer > ( ) ; columnsToDrop . add ( 0 ) ; for ( int i = getNumObjectiveFunctions ( ) ; i < getArtificialVariableOffset ( ) ; i ++ ) { final double entry = tableau . getEntry ( 0 , i ) ; if ( Precision . compareTo ( entry , 0d , epsilon ) > 0 ) { columnsToDrop . add ( i ) ; } } for ( int i = 0 ; i < getNumArtificialVariables ( ) ; i ++ ) { int col = i + getArtificialVariableOffset ( ) ; if ( getBasicRow ( col ) == null ) { columnsToDrop . add ( col ) ; } } double [ ] [ ] matrix = new double [ getHeight ( ) - 1 ] [ getWidth ( ) - columnsToDrop . size ( ) ] ; for ( int i = 1 ; i < getHeight ( ) ; i ++ ) { int col = 0 ; for ( int j = 0 ; j < getWidth ( ) ; j ++ ) { if ( ! columnsToDrop . contains ( j ) ) { matrix [ i - 1 ] [ col ++ ] = tableau . getEntry ( i , j ) ; } } } for ( int i = columnsToDrop . size ( ) - 1 ; i >= 0 ; i -- ) { columnLabels . remove ( ( int ) columnsToDrop . get ( i ) ) ; } this . tableau = new Array2DRowRealMatrix ( matrix ) ; this . numArtificialVariables = 0 ; }",
        "summary": "SimplexSolver gives bad results",
        "Description": "Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0\nin a simple test problem. It works well in commons-math-2.2. ",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-781",
        "comments": [
            "Could you give some more information ?\n\nWhat is the simple problem ? What are the expected results ? What are the results returned by Apache Commons Math 3.0 ?\nCould we have some example code so we can reproduce the problem ?",
            "Hi,\n\nI attached the test codes LinearProgCM.java for commons-math3-3.0 and \nLinearProgCM2.java for commons-math-2.2.\n\nBest regards,\nE.Scheiber\n\n\n\n",
            "I did a quick check and found no obvious reason for the different behavior. I will do a more thorough check when I am back in 2 weeks.",
            "in MATH-434, floating-point comparisons have been changed to use either an:\n\n * epsilon for any comparisons related to algorithm convergence\n * ulp for any other comparisons\n\nNow, when dropping the objective function of the first phase, the comparison is done using ulp which is wrong imho as it is basically a convergence check. When changing this back to an epsilon check like before, the test runs through as expected.",
            "The algorithm flow is as follows:\n\n * perform phase 1:\n ** iterate while !optimal\n ** isOptimal uses epsilon\n * after phase 1, drop phase 1 objective function\n ** drop columns using the same criteria as in isOptimal but with ulp instead of epsilon\n * perform phase 2\n ** ..\n\nAfter finishing phase 1, we end up dropping columns based on a different epsilon (ulp) as in the convergence check of the iteration for phase 1.",
            "Fixed in r1333146."
        ],
        "summarized_discussion": "\n\nThe bug in the source code was related to the comparison of floating-point numbers, which were using an epsilon for any comparisons related to algorithm convergence, but using an ulp for any other comparisons. The solution was to change the comparison back to an epsilon check, which resolved the issue. The fix was implemented in r1333146."
    },
    "JacksonDatabind_112_src/main/java/com/fasterxml/jackson/databind/deser/std/StringCollectionDeserializer.java_99_134": {
        "src": "@Override\n    public JsonDeserializer<?> createContextual(DeserializationContext ctxt,\n            BeanProperty property) throws JsonMappingException\n    {\n        // May need to resolve types for delegate-based creators:\n        JsonDeserializer<Object> delegate = null;\n        if (_valueInstantiator != null) {\n            // [databind#2324]: check both array-delegating and delegating\n            AnnotatedWithParams delegateCreator = _valueInstantiator.getDelegateCreator();\n            if (delegateCreator != null) {\n                JavaType delegateType = _valueInstantiator.getDelegateType(ctxt.getConfig());\n                delegate = findDeserializer(ctxt, delegateType, property);\n            }\n        }\n        JsonDeserializer<?> valueDeser = _valueDeserializer;\n        final JavaType valueType = _containerType.getContentType();\n        if (valueDeser == null) {\n            // [databind#125]: May have a content converter\n            valueDeser = findConvertingContentDeserializer(ctxt, property, valueDeser);\n            if (valueDeser == null) {\n            // And we may also need to get deserializer for String\n                valueDeser = ctxt.findContextualValueDeserializer(valueType, property);\n            }\n        } else { // if directly assigned, probably not yet contextual, so:\n            valueDeser = ctxt.handleSecondaryContextualization(valueDeser, property, valueType);\n        }\n        // 11-Dec-2015, tatu: Should we pass basic `Collection.class`, or more refined? Mostly\n        //   comes down to \"List vs Collection\" I suppose... for now, pass Collection\n        Boolean unwrapSingle = findFormatFeature(ctxt, property, Collection.class,\n                JsonFormat.Feature.ACCEPT_SINGLE_VALUE_AS_ARRAY);\n        NullValueProvider nuller = findContentNullProvider(ctxt, property, valueDeser);\n        if (isDefaultDeserializer(valueDeser)) {\n            valueDeser = null;\n        }\n        return withResolved(delegate, valueDeser, nuller, unwrapSingle);\n    }",
        "src_wo_comments": "@ Override public JsonDeserializer < ? > createContextual ( DeserializationContext ctxt , BeanProperty property ) throws JsonMappingException { JsonDeserializer < Object > delegate = null ; if ( _valueInstantiator != null ) { AnnotatedWithParams delegateCreator = _valueInstantiator . getDelegateCreator ( ) ; if ( delegateCreator != null ) { JavaType delegateType = _valueInstantiator . getDelegateType ( ctxt . getConfig ( ) ) ; delegate = findDeserializer ( ctxt , delegateType , property ) ; } } JsonDeserializer < ? > valueDeser = _valueDeserializer ; final JavaType valueType = _containerType . getContentType ( ) ; if ( valueDeser == null ) { valueDeser = findConvertingContentDeserializer ( ctxt , property , valueDeser ) ; if ( valueDeser == null ) { valueDeser = ctxt . findContextualValueDeserializer ( valueType , property ) ; } } else { valueDeser = ctxt . handleSecondaryContextualization ( valueDeser , property , valueType ) ; } Boolean unwrapSingle = findFormatFeature ( ctxt , property , Collection . class , JsonFormat . Feature . ACCEPT_SINGLE_VALUE_AS_ARRAY ) ; NullValueProvider nuller = findContentNullProvider ( ctxt , property , valueDeser ) ; if ( isDefaultDeserializer ( valueDeser ) ) { valueDeser = null ; } return withResolved ( delegate , valueDeser , nuller , unwrapSingle ) ; }",
        "fixed_src": "@Override\n    public JsonDeserializer<?> createContextual(DeserializationContext ctxt,\n            BeanProperty property) throws JsonMappingException\n    {\n        // May need to resolve types for delegate-based creators:\n        JsonDeserializer<Object> delegate = null;\n        if (_valueInstantiator != null) {\n            // [databind#2324]: check both array-delegating and delegating\n            AnnotatedWithParams delegateCreator = _valueInstantiator.getArrayDelegateCreator();\n            if (delegateCreator != null) {\n                JavaType delegateType = _valueInstantiator.getArrayDelegateType(ctxt.getConfig());\n                delegate = findDeserializer(ctxt, delegateType, property);\n            } else if ((delegateCreator = _valueInstantiator.getDelegateCreator()) != null) {\n                JavaType delegateType = _valueInstantiator.getDelegateType(ctxt.getConfig());\n                delegate = findDeserializer(ctxt, delegateType, property);\n            }\n        }\n        JsonDeserializer<?> valueDeser = _valueDeserializer;\n        final JavaType valueType = _containerType.getContentType();\n        if (valueDeser == null) {\n            // [databind#125]: May have a content converter\n            valueDeser = findConvertingContentDeserializer(ctxt, property, valueDeser);\n            if (valueDeser == null) {\n            // And we may also need to get deserializer for String\n                valueDeser = ctxt.findContextualValueDeserializer(valueType, property);\n            }\n        } else { // if directly assigned, probably not yet contextual, so:\n            valueDeser = ctxt.handleSecondaryContextualization(valueDeser, property, valueType);\n        }\n        // 11-Dec-2015, tatu: Should we pass basic `Collection.class`, or more refined? Mostly\n        //   comes down to \"List vs Collection\" I suppose... for now, pass Collection\n        Boolean unwrapSingle = findFormatFeature(ctxt, property, Collection.class,\n                JsonFormat.Feature.ACCEPT_SINGLE_VALUE_AS_ARRAY);\n        NullValueProvider nuller = findContentNullProvider(ctxt, property, valueDeser);\n        if (isDefaultDeserializer(valueDeser)) {\n            valueDeser = null;\n        }\n        return withResolved(delegate, valueDeser, nuller, unwrapSingle);\n    }",
        "fixed_src_wo_comments": "@ Override public JsonDeserializer < ? > createContextual ( DeserializationContext ctxt , BeanProperty property ) throws JsonMappingException { JsonDeserializer < Object > delegate = null ; if ( _valueInstantiator != null ) { AnnotatedWithParams delegateCreator = _valueInstantiator . getArrayDelegateCreator ( ) ; if ( delegateCreator != null ) { JavaType delegateType = _valueInstantiator . getArrayDelegateType ( ctxt . getConfig ( ) ) ; delegate = findDeserializer ( ctxt , delegateType , property ) ; } else if ( ( delegateCreator = _valueInstantiator . getDelegateCreator ( ) ) != null ) { JavaType delegateType = _valueInstantiator . getDelegateType ( ctxt . getConfig ( ) ) ; delegate = findDeserializer ( ctxt , delegateType , property ) ; } } JsonDeserializer < ? > valueDeser = _valueDeserializer ; final JavaType valueType = _containerType . getContentType ( ) ; if ( valueDeser == null ) { valueDeser = findConvertingContentDeserializer ( ctxt , property , valueDeser ) ; if ( valueDeser == null ) { valueDeser = ctxt . findContextualValueDeserializer ( valueType , property ) ; } } else { valueDeser = ctxt . handleSecondaryContextualization ( valueDeser , property , valueType ) ; } Boolean unwrapSingle = findFormatFeature ( ctxt , property , Collection . class , JsonFormat . Feature . ACCEPT_SINGLE_VALUE_AS_ARRAY ) ; NullValueProvider nuller = findContentNullProvider ( ctxt , property , valueDeser ) ; if ( isDefaultDeserializer ( valueDeser ) ) { valueDeser = null ; } return withResolved ( delegate , valueDeser , nuller , unwrapSingle ) ; }",
        "summary": "`StringCollectionDeserializer` fails with custom collection",
        "Description": "Seeing this with Jackson 2.9.8.\r\n\r\nWe have a custom collection implementation, which is wired to use its \"immutable\" version for deserialization. The rationale is that we don't want accidental modifications to the data structures that come from the wire, so they all are forced to be immutable.\r\n\r\nAfter upgrade from 2.6.3 to 2.9.8, the deserialization started breaking with the message:\r\n\r\n>Cannot construct instance of `XXX` (although at least one Creator exists): no default no-arguments constructor found\r\n\r\n\r\nThis happens ONLY when you deserialize a custom collection of strings as a property of the other object. Deserializing the custom collection of strings directly works fine, and so does the deserialization of custom collection of non-strings. I believe either the `StringCollectionDeserializer` should not be invoked for custom collections, or perhaps it does not handle the delegation as expected.\r\n\r\nPlease see comments for repro and workaround.\r\n\r\nThanks!",
        "issue_url": null,
        "comments": [
            {
                "content": "Here is the unit test code that reproduces the problem. Only `testDeserializeBagOfStrings` fails for me.\r\n```\r\n    /** Assume you have a custom collection.\r\n     */\r\n    @JsonDeserialize(as=ImmutableBag.class)\r\n    public interface Bag<T> extends Collection<T> {\r\n\r\n    }\r\n\r\n    /** It is deserialized as an immutable version (i.e. no add / remove, etc.).\r\n     */\r\n    public static class ImmutableBag<T> extends AbstractCollection<T> implements Bag<T>  {\r\n        @Override\r\n        public Iterator<T> iterator() { return elements.iterator(); }\r\n\r\n        @Override\r\n        public int size() { return elements.size(); }\r\n\r\n        @JsonCreator(mode=JsonCreator.Mode.DELEGATING)\r\n        private ImmutableBag( Collection<T> elements ) {\r\n            this.elements = Collections.unmodifiableCollection(elements);\r\n        }\r\n\r\n        private final Collection<T> elements;\r\n    }\r\n\r\n    public static class Value {\r\n        public Value( String value ) { this.value = value; }\r\n        private String value;\r\n    }\r\n\r\n    /** This does not work - see {@link #testDeserializeBagOfStrings}.\r\n     */\r\n    public static class WithBagOfStrings {\r\n\r\n        public Bag<String> getStrings() { return this.bagOfStrings; }\r\n        public void setStrings(Bag<String> bagOfStrings) { this.bagOfStrings = bagOfStrings; }\r\n        private Bag<String> bagOfStrings;\r\n    }\r\n\r\n    /** This DOES work - see {@link #testDeserializeBagOfObjects}.\r\n     */\r\n    public static class WithBagOfValues {\r\n        public Bag<Value> getValues() { return this.bagOfValues; }\r\n        public void setValues(Bag<Value> bagOfValues) { this.bagOfValues = bagOfValues; }\r\n        private Bag<Value> bagOfValues;\r\n    }\r\n\r\n\r\n    @Test public void testDeserializeBagOfStrings() throws IOException {\r\n        ObjectMapper mapper = new ObjectMapper();\r\n        WithBagOfStrings result = mapper.readerFor(WithBagOfStrings.class)\r\n                .readValue(\"{\\\"strings\\\": [ \\\"a\\\", \\\"b\\\", \\\"c\\\"]}\");\r\n        // Fails with:\r\n        // com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of \r\n        // `...$ImmutableBag` (although at least one Creator exists): no default no-arguments constructor found\r\n        // at [Source: (String)\"{\"strings\": [ \"a\", \"b\", \"c\"]}\"; line: 1, column: 13] (through reference chain: \r\n        // ...$WithBagOfStrings[\"strings\"])\r\n        //\r\n        //\tat com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:63)\r\n        //\tat com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1343)\r\n        //\tat com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1032)\r\n        //\tat com.fasterxml.jackson.databind.deser.ValueInstantiator.createUsingDefault(ValueInstantiator.java:189)\r\n        //\tat com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createUsingDefault(StdValueInstantiator.java:267)\r\n        //\tat com.fasterxml.jackson.databind.deser.std.StringCollectionDeserializer.deserialize(StringCollectionDeserializer.java:168)\r\n        //\tat com.fasterxml.jackson.databind.deser.std.StringCollectionDeserializer.deserialize(StringCollectionDeserializer.java:21)\r\n        //\tat com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:127)\r\n        //\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:288)\r\n        //\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:151)\r\n        //\tat com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:1611)\r\n        //\tat com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1219)\r\n        Assert.assertEquals(3, result.getStrings().size());\r\n    }\r\n\r\n    @Test public void testDeserializeBagOfObjects() throws IOException {\r\n        ObjectMapper mapper = new ObjectMapper();\r\n        WithBagOfValues result = mapper.readerFor(WithBagOfValues.class)\r\n                .readValue(\"{\\\"values\\\": [ \\\"a\\\", \\\"b\\\", \\\"c\\\"]}\");\r\n        Assert.assertEquals(3, result.getValues().size());\r\n    }\r\n    \r\n    @Test public void testBagOfStringsAlone() throws IOException {\r\n        // curiously, this DOES work too!\r\n        ObjectMapper mapper = new ObjectMapper();\r\n        Bag<String> result = mapper.readerFor(Bag.class)\r\n                .readValue(\"[ \\\"a\\\", \\\"b\\\", \\\"c\\\"]\");\r\n        Assert.assertEquals(3, result.size());\r\n    }\r\n```"
            },
            {
                "content": "Workaround (understandably, less than desirable): nail the custom deserializer through the `ObjectMapper` configuration. Perhaps there are more elegant ways to do it, but I just customized the delegating deserializer.\r\n\r\n```\r\nObjectMapper mapper = new ObjectMapper();\r\n        mapper.registerModule( new SimpleModule() {\r\n          {\r\n              this.addDeserializer(\r\n                  ImmutableBag.class,\r\n                  new StdDelegatingDeserializer<ImmutableBag>(\r\n                      new Converter<Collection,ImmutableBag>() {\r\n                          @Override\r\n                          public ImmutableBag convert(Collection value) {\r\n                              return new ImmutableBag(value);\r\n                          }\r\n\r\n                          @Override\r\n                          public JavaType getInputType(TypeFactory typeFactory) {\r\n                              return typeFactory.constructCollectionLikeType(Collection.class, Object.class);\r\n                          }\r\n\r\n                          @Override\r\n                          public JavaType getOutputType(TypeFactory typeFactory) {\r\n                              return typeFactory.constructCollectionLikeType(ImmutableBag.class, Object.class);\r\n                          }\r\n                      }\r\n                  ) {\r\n                      @Override\r\n                      public JsonDeserializer<?> createContextual(DeserializationContext ctxt, BeanProperty property) \r\n                              throws JsonMappingException {\r\n                          // Slightly modified version of the original implementation\r\n                          // First: if already got deserializer to delegate to, contextualize it:\r\n                          if (_delegateDeserializer != null) {\r\n                              JsonDeserializer<?> deser = ctxt.handleSecondaryContextualization(_delegateDeserializer,\r\n                                      property, _delegateType);\r\n                              if (deser != _delegateDeserializer) {\r\n                                  return withDelegate(_converter, _delegateType, deser);\r\n                              }\r\n                              return this;\r\n                          }\r\n                          // Otherwise: figure out what is the fully generic delegate type, then find deserializer\r\n                          JavaType delegateType = ctxt.getTypeFactory().constructCollectionLikeType(Collection.class, \r\n                                  property.getType().getBindings().getBoundType(0));\r\n                          return withDelegate(_converter, delegateType,\r\n                                  ctxt.findContextualValueDeserializer(delegateType, property));\r\n                      }\r\n\r\n                      @Override\r\n                      protected StdDelegatingDeserializer<ImmutableBag> withDelegate(\r\n                              Converter<Object, ImmutableBag> converter, JavaType delegateType, \r\n                              JsonDeserializer<?> delegateDeserializer) {\r\n                          return new StdDelegatingDeserializer<>(converter, delegateType, delegateDeserializer);\r\n                      }\r\n                  });\r\n\r\n          }\r\n      });\r\n```"
            },
            {
                "content": "Thank you for reporting this: seems like a bug.\r\n"
            },
            {
                "content": "Interesting. I can reproduce the issue as presented, and the problem seems to be with Jackson's special handling for `String`-valued `Collection`s -- there's a minor performance benefit for case where default `String` deserializer call can be avoided. But in this case it probably need to skip that since there is custom Creator involved."
            },
            {
                "content": "Interesting. This may be due to #1010 which added separation between \"regular\" and \"array delegator\" creators -- that would have been 2.7.0. We occasionally find places where only former is supported (since earlier there wasn't array variant)."
            },
            {
                "content": "@cowtowncoder, thanks, I appreciate the quick turnaround. Please let me know if you need any help with validating the fix. \r\n\r\nMeanwhile here is slightly improved (although longer) version of the workaround that handles edge cases (like raw types and situations where the collection argument type cannot infer from context) better:\r\n\r\n```\r\npublic abstract class GenericCollectionDeserializer<T, C extends Collection<? super T>>\r\n        extends StdDeserializer<C>\r\n        implements ResolvableDeserializer, ContextualDeserializer {\r\n\r\n        protected GenericCollectionDeserializer(Class<? extends Collection> classOfC) {\r\n            super(classOfC);\r\n            this.delegatedCollectionInfo = null;\r\n        }\r\n\r\n        @Override\r\n        public JsonDeserializer<?> createContextual(DeserializationContext ctxt, BeanProperty property) throws JsonMappingException {\r\n            if (this.delegatedCollectionInfo == null || !this.delegatedCollectionInfo.isSpecific()) {\r\n                // recalculate the delegated collection info, attempt to infer the types from the property\r\n                this.delegatedCollectionInfo = new DelegatedCollectionInfo(ctxt, property);\r\n            }\r\n            final JavaType theCollectionType = this.delegatedCollectionInfo.collectionType;\r\n            return new StdDelegatingDeserializer<>(new Converter<Object, C>() {\r\n                @Override\r\n                public C convert(Object value) {\r\n                    return GenericCollectionDeserializer.this.convert((Collection) value);\r\n                }\r\n\r\n                @Override\r\n                public JavaType getInputType(TypeFactory typeFactory) {\r\n                    return rawCollectionType;\r\n                }\r\n\r\n                @Override\r\n                public JavaType getOutputType(TypeFactory typeFactory) {\r\n                    return theCollectionType;\r\n                }\r\n            }, theCollectionType, this.delegatedCollectionInfo.collectionDeserializer);\r\n        }\r\n\r\n        @Override\r\n        public C deserialize(JsonParser p, DeserializationContext ctxt) throws IOException {\r\n            // Since we always return contextually created deserializer, this should never\r\n            // be called, consider this a fallback implementation.\r\n            final Collection inner;\r\n            if (delegatedCollectionInfo != null) {\r\n                inner = delegatedCollectionInfo.collectionDeserializer.deserialize(p, ctxt);\r\n            } else {\r\n                inner = ctxt.readValue(p, rawCollectionType);\r\n            }\r\n            return convert(inner);\r\n        }\r\n\r\n        @Override\r\n        public void resolve(DeserializationContext ctxt) throws JsonMappingException {\r\n            // attempt to infer the underlying collection type from context.\r\n            rawCollectionType = ctxt.getTypeFactory().constructRawCollectionType(Collection.class);\r\n            delegatedCollectionInfo = new DelegatedCollectionInfo(ctxt, null);\r\n        }\r\n\r\n        protected abstract C convert(Collection input);\r\n\r\n        private class DelegatedCollectionInfo {\r\n            public final JsonDeserializer<? extends Collection> collectionDeserializer;\r\n            public final JavaType collectionType;\r\n            public final JavaType collectionArgType;\r\n\r\n            DelegatedCollectionInfo(DeserializationContext ctxt, BeanProperty property) throws JsonMappingException {\r\n                JavaType collectionType = null;\r\n                JavaType argType = null;\r\n                JsonDeserializer deserializer = null;\r\n                if (property != null) {\r\n                    argType = getFirstTypeParameter(property.getType());\r\n                } else {\r\n                    argType = getFirstTypeParameter(ctxt.getContextualType());\r\n                }\r\n\r\n                if (argType != null) {\r\n                    collectionType = ctxt.getTypeFactory().constructCollectionType(Collection.class, argType);\r\n                    deserializer = ctxt.findRootValueDeserializer(collectionType);\r\n                } else {\r\n                    collectionType = rawCollectionType;\r\n                    deserializer = ctxt.findRootValueDeserializer(collectionType);\r\n                }\r\n\r\n                if (deserializer == null) {\r\n                    throw JsonMappingException.from(ctxt, String.format(\r\n                            \"Cannot find deserializer to read collection type '%s'.\", collectionType)\r\n                    );\r\n                }\r\n                this.collectionType = collectionType;\r\n                this.collectionArgType = argType;\r\n                this.collectionDeserializer = deserializer;\r\n\r\n                assert(this.collectionType != null);\r\n                assert(this.collectionDeserializer != null);\r\n            }\r\n\r\n            public boolean isSpecific() {\r\n                return this.collectionArgType != null;\r\n            }\r\n\r\n            private JavaType getFirstTypeParameter(JavaType expr) {\r\n                if (expr != null) {\r\n                    TypeBindings argBindings = expr.getBindings();\r\n                    if (argBindings != null && !argBindings.isEmpty()) {\r\n                        return argBindings.getBoundType(0);\r\n                    }\r\n                }\r\n                return null;\r\n            }\r\n        }\r\n\r\n        private DelegatedCollectionInfo delegatedCollectionInfo;\r\n        private JavaType rawCollectionType;\r\n    }\r\n```\r\n\r\nThis can be used as follows:\r\n```\r\n   @JsonDeserialize(using = ImmutableBag.Deserializer.class)\r\n    public static class ImmutableBag<T> extends AbstractCollection<T> implements Bag<T>  {\r\n\r\n        public static class Deserializer<T> extends GenericCollectionDeserializer<T,ImmutableBag<T>> {\r\n            public Deserializer() { super (ImmutableBag.class); }\r\n            @Override public ImmutableBag<T> convert(Collection value) {\r\n                return new ImmutableBag<T>(value);\r\n            }\r\n        }\r\n       ...\r\n   }\r\n```"
            },
            {
                "content": "Ok. Just to make sure: I don't think you should need to do any of this, as long as deserializer is accessed via `Deserializers`, method `findCollectionDeserializer()`: `CollectionType` given will have fully resolved type accessible. It should be necessary to try to access type via `BeanProperty` from `createContextual(): that should only be needed if access to other annotations is needed.\r\n"
            }
        ],
        "summarized_discussion": ""
    },
    "Cli_32_src/main/java/org/apache/commons/cli/HelpFormatter.java_902_943": {
        "src": "protected int findWrapPos(String text, int width, int startPos)\n    {\n        int pos;\n        \n        // the line ends before the max wrap pos or a new line char found\n        if (((pos = text.indexOf('\\n', startPos)) != -1 && pos <= width)\n                || ((pos = text.indexOf('\\t', startPos)) != -1 && pos <= width))\n        {\n            return pos + 1;\n        }\n        else if (startPos + width >= text.length())\n        {\n            return -1;\n        }\n\n\n        // look for the last whitespace character before startPos+width\n        pos = startPos + width;\n\n        char c;\n\n        while ((pos >= startPos) && ((c = text.charAt(pos)) != ' ')\n                && (c != '\\n') && (c != '\\r'))\n        {\n            --pos;\n        }\n\n        // if we found it - just return\n        if (pos > startPos)\n        {\n            return pos;\n        }\n        \n        // if we didn't find one, simply chop at startPos+width\n        pos = startPos + width;\n        while ((pos <= text.length()) && ((c = text.charAt(pos)) != ' ')\n               && (c != '\\n') && (c != '\\r'))\n        {\n            ++pos;\n        }        \n        return pos == text.length() ? -1 : pos;\n    }",
        "src_wo_comments": "protected int findWrapPos ( String text , int width , int startPos ) { int pos ; if ( ( ( pos = text . indexOf ( '\\n' , startPos ) ) != - 1 && pos <= width ) || ( ( pos = text . indexOf ( '\\t' , startPos ) ) != - 1 && pos <= width ) ) { return pos + 1 ; } else if ( startPos + width >= text . length ( ) ) { return - 1 ; } pos = startPos + width ; char c ; while ( ( pos >= startPos ) && ( ( c = text . charAt ( pos ) ) != ' ' ) && ( c != '\\n' ) && ( c != '\\r' ) ) { -- pos ; } if ( pos > startPos ) { return pos ; } pos = startPos + width ; while ( ( pos <= text . length ( ) ) && ( ( c = text . charAt ( pos ) ) != ' ' ) && ( c != '\\n' ) && ( c != '\\r' ) ) { ++ pos ; } return pos == text . length ( ) ? - 1 : pos ; }",
        "fixed_src": "protected int findWrapPos(String text, int width, int startPos)\n    {\n        int pos;\n        \n        // the line ends before the max wrap pos or a new line char found\n        if (((pos = text.indexOf('\\n', startPos)) != -1 && pos <= width)\n                || ((pos = text.indexOf('\\t', startPos)) != -1 && pos <= width))\n        {\n            return pos + 1;\n        }\n        else if (startPos + width >= text.length())\n        {\n            return -1;\n        }\n\n\n        // look for the last whitespace character before startPos+width\n        pos = startPos + width;\n\n        char c;\n\n        while ((pos >= startPos) && ((c = text.charAt(pos)) != ' ')\n                && (c != '\\n') && (c != '\\r'))\n        {\n            --pos;\n        }\n\n        // if we found it - just return\n        if (pos > startPos)\n        {\n            return pos;\n        }\n        \n        // if we didn't find one, simply chop at startPos+width\n        pos = startPos + width;\n        \n        return pos == text.length() ? -1 : pos;\n    }",
        "fixed_src_wo_comments": "protected int findWrapPos ( String text , int width , int startPos ) { int pos ; if ( ( ( pos = text . indexOf ( '\\n' , startPos ) ) != - 1 && pos <= width ) || ( ( pos = text . indexOf ( '\\t' , startPos ) ) != - 1 && pos <= width ) ) { return pos + 1 ; } else if ( startPos + width >= text . length ( ) ) { return - 1 ; } pos = startPos + width ; char c ; while ( ( pos >= startPos ) && ( ( c = text . charAt ( pos ) ) != ' ' ) && ( c != '\\n' ) && ( c != '\\r' ) ) { -- pos ; } if ( pos > startPos ) { return pos ; } pos = startPos + width ; return pos == text . length ( ) ? - 1 : pos ; }",
        "summary": "StringIndexOutOfBoundsException in HelpFormatter.findWrapPos",
        "Description": "In the last while loop in HelpFormatter.findWrapPos, it can pass text.length() to text.charAt(int), which throws a StringIndexOutOfBoundsException. The first expression in that while loop condition should use a <, not a <=.\n\nThis is on line 908 in r779646:\n  http://svn.apache.org/viewvc/commons/proper/cli/trunk/src/java/org/apache/commons/cli/HelpFormatter.java?revision=779646&view=markup",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-193",
        "comments": [
            "Thank you for the report Travis. Do you have a test case that triggers the exception?",
            "Here is a patch that adds a test case and applies the fix suggested by Travis.",
            "You should be able to get it to throw with a call like this:\n\n  findWrapPos( \"hello\", 3, 0 );  // should return -1\n\nThe exception gets thrown whenever the passed-in string does not contain any spaces, tabs, or newlines. It looks like line 914 was also expecting line 908 to have a <.\n\n\nAlso, I think there are other bugs. In this call:\n\n  findWrapPos( \"helloooo\\noo\\ngoodbye\", 6, 7 );  // should return 8\n\nwe want it to short circuit and return the first newline if there is one in the substring starting at 7 with width 6 (\"o\\noo\\ng\"), and so it should return the index of the first '\\n', which is 8. However, on lines 876 and 877, it's doing \"pos <= width\", instead of \"pos <= startPos+width\", so it won't return, and it will end up (incorrectly, I think) returning the index of the second newline's position. And once you fix lines 876 and 877, it's going to return 9 instead of 8, which I think is incorrect. The other return statements return pos, not pos+1, and I think line 879 should do the same.",
            "The proposed change makes sense but it doesn't solve completely the issue with \"unbreakable\" text.\n\nFor example, considering the following test:\n\n{code}\npublic void testRenderWrappedTextWordCut()\n{\n    int width = 7;\n    int padding = 0;\n    String text = \"Thisisatest.\";\n    String expected = \"Thisisa\" + EOL + \n                      \"test.\";\n    \n    StringBuffer sb = new StringBuffer();\n    new HelpFormatter().renderWrappedText(sb, width, padding, text);\n    System.out.println(sb.toString());\n    assertEquals(\"cut and wrap\", expected, sb.toString());\n}\n{code}\n\nWith the current code it throws a StringIndexOutOfBoundsException. With the modification it fails because the text is not cut.\n",
            "Well, that would be easy enough to fix, but that's at odds with how the method is documented now. It currently says it will ONLY break on a newline or space (\"The wrap point is the last postion before startPos+width having a whitespace character (space, \\n, \\r).\").\n\nIn addition to that, it's currently looking for whitespace AFTER startPos+width (also at odds with documentation). How should it actually behave?\n\nI've attached another patch that will preserve the existing functionality (looking for spaces after startPos+width) but reverts back to returning startPos+width if none is found.\n\nIf, instead, it should *never* allow for a string longer than startPos+width, then current lines 908-912 should probably just be removed. This will cause \"wrap position 3\" unit test to fail though.",
            "I believe the absolute rule for the wrapping should be to not exceed the maximum width, then to do the best effort to wrap on space characters, and if it is not possible, break a word.",
            "Makes sense to me too. Doesn't help to have characters run only a *little* bit off the screen :)\n\nHere is an updated patch that will never exceed startPos+width.",
            "Thank you for the patch Chris. I applied the change you suggested and it seems to work fine."
        ],
        "summarized_discussion": "\n\nThe bug in the source code was related to the findWrapPos method not returning the correct value when the passed-in string did not contain any spaces, tabs, or newlines. Chris suggested that the absolute rule for the wrapping should be to not exceed the maximum width, then to do the best effort to wrap on space characters, and if it is not possible, break a word. This was accepted and an updated patch was provided by Chris. The patch was applied and it seems to have solved the issue."
    },
    "JacksonCore_20_src/main/java/com/fasterxml/jackson/core/JsonGenerator.java_1328_1332": {
        "src": "public void writeEmbeddedObject(Object object) throws IOException {\n        // 01-Sep-2016, tatu: As per [core#318], handle small number of cases\n        throw new JsonGenerationException(\"No native support for writing embedded objects\",\n                this);\n    }",
        "src_wo_comments": "public void writeEmbeddedObject ( Object object ) throws IOException { throw new JsonGenerationException ( \"No native support for writing embedded objects\" , this ) ; }",
        "fixed_src": "public void writeEmbeddedObject(Object object) throws IOException {\n        // 01-Sep-2016, tatu: As per [core#318], handle small number of cases\n        if (object == null) {\n            writeNull();\n            return;\n        }\n        if (object instanceof byte[]) {\n            writeBinary((byte[]) object);\n            return;\n        }\n        throw new JsonGenerationException(\"No native support for writing embedded objects of type \"\n                +object.getClass().getName(),\n                this);\n    }",
        "fixed_src_wo_comments": "public void writeEmbeddedObject ( Object object ) throws IOException { if ( object == null ) { writeNull ( ) ; return ; } if ( object instanceof byte [ ] ) { writeBinary ( ( byte [ ] ) object ) ; return ; } throw new JsonGenerationException ( \"No native support for writing embedded objects of type \" + object . getClass ( ) . getName ( ) , this ) ; }",
        "summary": "Add support for writing `byte[]` via `JsonGenerator.writeEmbeddedObject()`",
        "Description": "(note: should be safe for patch, that is, 2.8.3)\n\nDefault implementation of 2.8-added `writeEmbeddedObject()` throws exception (unsupported operation) for all values, since JSON does not have any native object types.\nThis is different from handling of `writeObject()`, which tries to either delegate to `ObjectCodec` (if one registered), or even encode \"simple\" values.\n\nHowever: since support for binary data is already handled in some cases using `VALUE_EMBEDDED_OBJECT`, it would actually make sense to handle case of `byte[]` (and, if feasible, perhaps `ByteBuffer` for extra points), and also ensure `null` can be written.\n\nThis is likely necessary to support https://github.com/FasterXML/jackson-databind/issues/1361 and should in general make system more robust.\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug was caused by an incorrect variable being used in the code. The solution was to identify the incorrect variable and replace it with the correct one."
    },
    "Math_64_src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java_240_464": {
        "src": "@Override\n    protected VectorialPointValuePair doOptimize()\n        throws FunctionEvaluationException, OptimizationException, IllegalArgumentException {\n\n        // arrays shared with the other private methods\n        solvedCols  = Math.min(rows, cols);\n        diagR       = new double[cols];\n        jacNorm     = new double[cols];\n        beta        = new double[cols];\n        permutation = new int[cols];\n        lmDir       = new double[cols];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[cols];\n        double[] oldX    = new double[cols];\n        double[] oldRes  = new double[rows];\n        double[] work1   = new double[cols];\n        double[] work2   = new double[cols];\n        double[] work3   = new double[cols];\n\n        // evaluate the function at the starting point and calculate its norm\n        updateResidualsAndCost();\n\n        // outer loop\n        lmPar = 0;\n        boolean firstIteration = true;\n        VectorialPointValuePair current = new VectorialPointValuePair(point, objective);\n        while (true) {\n            incrementIterationsCounter();\n\n            // compute the Q.R. decomposition of the jacobian matrix\n            VectorialPointValuePair previous = current;\n            updateJacobian();\n            qrDecomposition();\n\n            // compute Qt.res\n            qTy(residuals);\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                jacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < cols; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * point[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = Math.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (cost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += jacobian[i][pj] * residuals[i];\n                        }\n                        maxCosine = Math.max(maxCosine, Math.abs(sum) / (s * cost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // convergence has been reached\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < cols; ++j) {\n                diag[j] = Math.max(diag[j], jacNorm[j]);\n            }\n\n            // inner loop\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = point[pj];\n                }\n                double previousCost = cost;\n                double[] tmpVec = residuals;\n                residuals = oldRes;\n                oldRes    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(oldRes, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    point[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = Math.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = Math.min(delta, lmNorm);\n                }\n\n                // evaluate the function at x + p and calculate its norm\n                updateResidualsAndCost();\n                current = new VectorialPointValuePair(point, objective);\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * cost < previousCost) {\n                    double r = cost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += jacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * cost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * Math.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < cols; ++k) {\n                        double xK = diag[k] * point[k];\n                        xNorm    += xK * xK;\n                    }\n                    xNorm = Math.sqrt(xNorm);\n\n                    // tests for convergence.\n                    // we use the vectorial convergence checker\n                } else {\n                    // failed iteration, reset the previous values\n                    cost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        point[pj] = oldX[pj];\n                    }\n                    tmpVec    = residuals;\n                    residuals = oldRes;\n                    oldRes    = tmpVec;\n                }\n                if (checker==null) {\n                \tif (((Math.abs(actRed) <= costRelativeTolerance) &&\n                        (preRed <= costRelativeTolerance) &&\n                        (ratio <= 2.0)) ||\n                       (delta <= parRelativeTolerance * xNorm)) {\n                       return current;\n                   }\n                } else {\n                    if (checker.converged(getIterations(), previous, current)) {\n                        return current;\n                    }\n                }\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((Math.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                            costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                            parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                            orthoTolerance);\n                }\n\n            }\n\n        }\n\n    }",
        "src_wo_comments": "@ Override protected VectorialPointValuePair doOptimize ( ) throws FunctionEvaluationException , OptimizationException , IllegalArgumentException { solvedCols = Math . min ( rows , cols ) ; diagR = new double [ cols ] ; jacNorm = new double [ cols ] ; beta = new double [ cols ] ; permutation = new int [ cols ] ; lmDir = new double [ cols ] ; double delta = 0 ; double xNorm = 0 ; double [ ] diag = new double [ cols ] ; double [ ] oldX = new double [ cols ] ; double [ ] oldRes = new double [ rows ] ; double [ ] work1 = new double [ cols ] ; double [ ] work2 = new double [ cols ] ; double [ ] work3 = new double [ cols ] ; updateResidualsAndCost ( ) ; lmPar = 0 ; boolean firstIteration = true ; VectorialPointValuePair current = new VectorialPointValuePair ( point , objective ) ; while ( true ) { incrementIterationsCounter ( ) ; VectorialPointValuePair previous = current ; updateJacobian ( ) ; qrDecomposition ( ) ; qTy ( residuals ) ; for ( int k = 0 ; k < solvedCols ; ++ k ) { int pk = permutation [ k ] ; jacobian [ k ] [ pk ] = diagR [ pk ] ; } if ( firstIteration ) { xNorm = 0 ; for ( int k = 0 ; k < cols ; ++ k ) { double dk = jacNorm [ k ] ; if ( dk == 0 ) { dk = 1.0 ; } double xk = dk * point [ k ] ; xNorm += xk * xk ; diag [ k ] = dk ; } xNorm = Math . sqrt ( xNorm ) ; delta = ( xNorm == 0 ) ? initialStepBoundFactor : ( initialStepBoundFactor * xNorm ) ; } double maxCosine = 0 ; if ( cost != 0 ) { for ( int j = 0 ; j < solvedCols ; ++ j ) { int pj = permutation [ j ] ; double s = jacNorm [ pj ] ; if ( s != 0 ) { double sum = 0 ; for ( int i = 0 ; i <= j ; ++ i ) { sum += jacobian [ i ] [ pj ] * residuals [ i ] ; } maxCosine = Math . max ( maxCosine , Math . abs ( sum ) / ( s * cost ) ) ; } } } if ( maxCosine <= orthoTolerance ) { return current ; } for ( int j = 0 ; j < cols ; ++ j ) { diag [ j ] = Math . max ( diag [ j ] , jacNorm [ j ] ) ; } for ( double ratio = 0 ; ratio < 1.0e-4 ; ) { for ( int j = 0 ; j < solvedCols ; ++ j ) { int pj = permutation [ j ] ; oldX [ pj ] = point [ pj ] ; } double previousCost = cost ; double [ ] tmpVec = residuals ; residuals = oldRes ; oldRes = tmpVec ; determineLMParameter ( oldRes , delta , diag , work1 , work2 , work3 ) ; double lmNorm = 0 ; for ( int j = 0 ; j < solvedCols ; ++ j ) { int pj = permutation [ j ] ; lmDir [ pj ] = - lmDir [ pj ] ; point [ pj ] = oldX [ pj ] + lmDir [ pj ] ; double s = diag [ pj ] * lmDir [ pj ] ; lmNorm += s * s ; } lmNorm = Math . sqrt ( lmNorm ) ; if ( firstIteration ) { delta = Math . min ( delta , lmNorm ) ; } updateResidualsAndCost ( ) ; current = new VectorialPointValuePair ( point , objective ) ; double actRed = - 1.0 ; if ( 0.1 * cost < previousCost ) { double r = cost / previousCost ; actRed = 1.0 - r * r ; } for ( int j = 0 ; j < solvedCols ; ++ j ) { int pj = permutation [ j ] ; double dirJ = lmDir [ pj ] ; work1 [ j ] = 0 ; for ( int i = 0 ; i <= j ; ++ i ) { work1 [ i ] += jacobian [ i ] [ pj ] * dirJ ; } } double coeff1 = 0 ; for ( int j = 0 ; j < solvedCols ; ++ j ) { coeff1 += work1 [ j ] * work1 [ j ] ; } double pc2 = previousCost * previousCost ; coeff1 = coeff1 / pc2 ; double coeff2 = lmPar * lmNorm * lmNorm / pc2 ; double preRed = coeff1 + 2 * coeff2 ; double dirDer = - ( coeff1 + coeff2 ) ; ratio = ( preRed == 0 ) ? 0 : ( actRed / preRed ) ; if ( ratio <= 0.25 ) { double tmp = ( actRed < 0 ) ? ( 0.5 * dirDer / ( dirDer + 0.5 * actRed ) ) : 0.5 ; if ( ( 0.1 * cost >= previousCost ) || ( tmp < 0.1 ) ) { tmp = 0.1 ; } delta = tmp * Math . min ( delta , 10.0 * lmNorm ) ; lmPar /= tmp ; } else if ( ( lmPar == 0 ) || ( ratio >= 0.75 ) ) { delta = 2 * lmNorm ; lmPar *= 0.5 ; } if ( ratio >= 1.0e-4 ) { firstIteration = false ; xNorm = 0 ; for ( int k = 0 ; k < cols ; ++ k ) { double xK = diag [ k ] * point [ k ] ; xNorm += xK * xK ; } xNorm = Math . sqrt ( xNorm ) ; } else { cost = previousCost ; for ( int j = 0 ; j < solvedCols ; ++ j ) { int pj = permutation [ j ] ; point [ pj ] = oldX [ pj ] ; } tmpVec = residuals ; residuals = oldRes ; oldRes = tmpVec ; } if ( checker == null ) { if ( ( ( Math . abs ( actRed ) <= costRelativeTolerance ) && ( preRed <= costRelativeTolerance ) && ( ratio <= 2.0 ) ) || ( delta <= parRelativeTolerance * xNorm ) ) { return current ; } } else { if ( checker . converged ( getIterations ( ) , previous , current ) ) { return current ; } } if ( ( Math . abs ( actRed ) <= 2.2204e-16 ) && ( preRed <= 2.2204e-16 ) && ( ratio <= 2.0 ) ) { throw new OptimizationException ( LocalizedFormats . TOO_SMALL_COST_RELATIVE_TOLERANCE , costRelativeTolerance ) ; } else if ( delta <= 2.2204e-16 * xNorm ) { throw new OptimizationException ( LocalizedFormats . TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE , parRelativeTolerance ) ; } else if ( maxCosine <= 2.2204e-16 ) { throw new OptimizationException ( LocalizedFormats . TOO_SMALL_ORTHOGONALITY_TOLERANCE , orthoTolerance ) ; } } } }",
        "fixed_src": "@Override\n    protected VectorialPointValuePair doOptimize()\n        throws FunctionEvaluationException, OptimizationException, IllegalArgumentException {\n\n        // arrays shared with the other private methods\n        solvedCols  = Math.min(rows, cols);\n        diagR       = new double[cols];\n        jacNorm     = new double[cols];\n        beta        = new double[cols];\n        permutation = new int[cols];\n        lmDir       = new double[cols];\n\n        // local point\n        double   delta   = 0;\n        double   xNorm   = 0;\n        double[] diag    = new double[cols];\n        double[] oldX    = new double[cols];\n        double[] oldRes  = new double[rows];\n        double[] oldObj  = new double[rows];\n        double[] qtf     = new double[rows];\n        double[] work1   = new double[cols];\n        double[] work2   = new double[cols];\n        double[] work3   = new double[cols];\n\n        // evaluate the function at the starting point and calculate its norm\n        updateResidualsAndCost();\n\n        // outer loop\n        lmPar = 0;\n        boolean firstIteration = true;\n        VectorialPointValuePair current = new VectorialPointValuePair(point, objective);\n        while (true) {\n            for (int i=0;i<rows;i++) {\n                qtf[i]=residuals[i];\n            }\n            incrementIterationsCounter();\n\n            // compute the Q.R. decomposition of the jacobian matrix\n            VectorialPointValuePair previous = current;\n            updateJacobian();\n            qrDecomposition();\n\n            // compute Qt.res\n            qTy(qtf);\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                jacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < cols; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * point[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = Math.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (cost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += jacobian[i][pj] * qtf[i];\n                        }\n                        maxCosine = Math.max(maxCosine, Math.abs(sum) / (s * cost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // convergence has been reached\n            \tupdateResidualsAndCost();\n            \tcurrent = new VectorialPointValuePair(point, objective);\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < cols; ++j) {\n                diag[j] = Math.max(diag[j], jacNorm[j]);\n            }\n\n            // inner loop\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = point[pj];\n                }\n                double previousCost = cost;\n                double[] tmpVec = residuals;\n                residuals = oldRes;\n                oldRes    = tmpVec;\n                tmpVec    = objective;\n                objective = oldObj;\n                oldObj    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(qtf, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    point[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = Math.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = Math.min(delta, lmNorm);\n                }\n\n                // evaluate the function at x + p and calculate its norm\n                updateResidualsAndCost();\n\n                // compute the scaled actual reduction\n                double actRed = -1.0;\n                if (0.1 * cost < previousCost) {\n                    double r = cost / previousCost;\n                    actRed = 1.0 - r * r;\n                }\n\n                // compute the scaled predicted reduction\n                // and the scaled directional derivative\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    double dirJ = lmDir[pj];\n                    work1[j] = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        work1[i] += jacobian[i][pj] * dirJ;\n                    }\n                }\n                double coeff1 = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    coeff1 += work1[j] * work1[j];\n                }\n                double pc2 = previousCost * previousCost;\n                coeff1 = coeff1 / pc2;\n                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n                double preRed = coeff1 + 2 * coeff2;\n                double dirDer = -(coeff1 + coeff2);\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n\n                // update the step bound\n                if (ratio <= 0.25) {\n                    double tmp =\n                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                        if ((0.1 * cost >= previousCost) || (tmp < 0.1)) {\n                            tmp = 0.1;\n                        }\n                        delta = tmp * Math.min(delta, 10.0 * lmNorm);\n                        lmPar /= tmp;\n                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                    delta = 2 * lmNorm;\n                    lmPar *= 0.5;\n                }\n\n                // test for successful iteration.\n                if (ratio >= 1.0e-4) {\n                    // successful iteration, update the norm\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < cols; ++k) {\n                        double xK = diag[k] * point[k];\n                        xNorm    += xK * xK;\n                    }\n                    xNorm = Math.sqrt(xNorm);\n                    current = new VectorialPointValuePair(point, objective);\n\n                    // tests for convergence.\n                    if (checker != null) {\n                    // we use the vectorial convergence checker\n                    \tif (checker.converged(getIterations(), previous, current)) {\n                    \t\treturn current;\n                    \t}\n                    }\n                } else {\n                    // failed iteration, reset the previous values\n                    cost = previousCost;\n                    for (int j = 0; j < solvedCols; ++j) {\n                        int pj = permutation[j];\n                        point[pj] = oldX[pj];\n                    }\n                    tmpVec    = residuals;\n                    residuals = oldRes;\n                    oldRes    = tmpVec;\n                    tmpVec    = objective;\n                    objective = oldObj;\n                    oldObj    = tmpVec;\n                }\n                if (checker==null) {\n                \tif (((Math.abs(actRed) <= costRelativeTolerance) &&\n                        (preRed <= costRelativeTolerance) &&\n                        (ratio <= 2.0)) ||\n                       (delta <= parRelativeTolerance * xNorm)) {\n                       return current;\n                   }\n                }\n                // tests for termination and stringent tolerances\n                // (2.2204e-16 is the machine epsilon for IEEE754)\n                if ((Math.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n                            costRelativeTolerance);\n                } else if (delta <= 2.2204e-16 * xNorm) {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n                            parRelativeTolerance);\n                } else if (maxCosine <= 2.2204e-16)  {\n                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n                            orthoTolerance);\n                }\n\n            }\n\n        }\n\n    }",
        "fixed_src_wo_comments": "@ Override protected VectorialPointValuePair doOptimize ( ) throws FunctionEvaluationException , OptimizationException , IllegalArgumentException { solvedCols = Math . min ( rows , cols ) ; diagR = new double [ cols ] ; jacNorm = new double [ cols ] ; beta = new double [ cols ] ; permutation = new int [ cols ] ; lmDir = new double [ cols ] ; double delta = 0 ; double xNorm = 0 ; double [ ] diag = new double [ cols ] ; double [ ] oldX = new double [ cols ] ; double [ ] oldRes = new double [ rows ] ; double [ ] oldObj = new double [ rows ] ; double [ ] qtf = new double [ rows ] ; double [ ] work1 = new double [ cols ] ; double [ ] work2 = new double [ cols ] ; double [ ] work3 = new double [ cols ] ; updateResidualsAndCost ( ) ; lmPar = 0 ; boolean firstIteration = true ; VectorialPointValuePair current = new VectorialPointValuePair ( point , objective ) ; while ( true ) { for ( int i = 0 ; i < rows ; i ++ ) { qtf [ i ] = residuals [ i ] ; } incrementIterationsCounter ( ) ; VectorialPointValuePair previous = current ; updateJacobian ( ) ; qrDecomposition ( ) ; qTy ( qtf ) ; for ( int k = 0 ; k < solvedCols ; ++ k ) { int pk = permutation [ k ] ; jacobian [ k ] [ pk ] = diagR [ pk ] ; } if ( firstIteration ) { xNorm = 0 ; for ( int k = 0 ; k < cols ; ++ k ) { double dk = jacNorm [ k ] ; if ( dk == 0 ) { dk = 1.0 ; } double xk = dk * point [ k ] ; xNorm += xk * xk ; diag [ k ] = dk ; } xNorm = Math . sqrt ( xNorm ) ; delta = ( xNorm == 0 ) ? initialStepBoundFactor : ( initialStepBoundFactor * xNorm ) ; } double maxCosine = 0 ; if ( cost != 0 ) { for ( int j = 0 ; j < solvedCols ; ++ j ) { int pj = permutation [ j ] ; double s = jacNorm [ pj ] ; if ( s != 0 ) { double sum = 0 ; for ( int i = 0 ; i <= j ; ++ i ) { sum += jacobian [ i ] [ pj ] * qtf [ i ] ; } maxCosine = Math . max ( maxCosine , Math . abs ( sum ) / ( s * cost ) ) ; } } } if ( maxCosine <= orthoTolerance ) { updateResidualsAndCost ( ) ; current = new VectorialPointValuePair ( point , objective ) ; return current ; } for ( int j = 0 ; j < cols ; ++ j ) { diag [ j ] = Math . max ( diag [ j ] , jacNorm [ j ] ) ; } for ( double ratio = 0 ; ratio < 1.0e-4 ; ) { for ( int j = 0 ; j < solvedCols ; ++ j ) { int pj = permutation [ j ] ; oldX [ pj ] = point [ pj ] ; } double previousCost = cost ; double [ ] tmpVec = residuals ; residuals = oldRes ; oldRes = tmpVec ; tmpVec = objective ; objective = oldObj ; oldObj = tmpVec ; determineLMParameter ( qtf , delta , diag , work1 , work2 , work3 ) ; double lmNorm = 0 ; for ( int j = 0 ; j < solvedCols ; ++ j ) { int pj = permutation [ j ] ; lmDir [ pj ] = - lmDir [ pj ] ; point [ pj ] = oldX [ pj ] + lmDir [ pj ] ; double s = diag [ pj ] * lmDir [ pj ] ; lmNorm += s * s ; } lmNorm = Math . sqrt ( lmNorm ) ; if ( firstIteration ) { delta = Math . min ( delta , lmNorm ) ; } updateResidualsAndCost ( ) ; double actRed = - 1.0 ; if ( 0.1 * cost < previousCost ) { double r = cost / previousCost ; actRed = 1.0 - r * r ; } for ( int j = 0 ; j < solvedCols ; ++ j ) { int pj = permutation [ j ] ; double dirJ = lmDir [ pj ] ; work1 [ j ] = 0 ; for ( int i = 0 ; i <= j ; ++ i ) { work1 [ i ] += jacobian [ i ] [ pj ] * dirJ ; } } double coeff1 = 0 ; for ( int j = 0 ; j < solvedCols ; ++ j ) { coeff1 += work1 [ j ] * work1 [ j ] ; } double pc2 = previousCost * previousCost ; coeff1 = coeff1 / pc2 ; double coeff2 = lmPar * lmNorm * lmNorm / pc2 ; double preRed = coeff1 + 2 * coeff2 ; double dirDer = - ( coeff1 + coeff2 ) ; ratio = ( preRed == 0 ) ? 0 : ( actRed / preRed ) ; if ( ratio <= 0.25 ) { double tmp = ( actRed < 0 ) ? ( 0.5 * dirDer / ( dirDer + 0.5 * actRed ) ) : 0.5 ; if ( ( 0.1 * cost >= previousCost ) || ( tmp < 0.1 ) ) { tmp = 0.1 ; } delta = tmp * Math . min ( delta , 10.0 * lmNorm ) ; lmPar /= tmp ; } else if ( ( lmPar == 0 ) || ( ratio >= 0.75 ) ) { delta = 2 * lmNorm ; lmPar *= 0.5 ; } if ( ratio >= 1.0e-4 ) { firstIteration = false ; xNorm = 0 ; for ( int k = 0 ; k < cols ; ++ k ) { double xK = diag [ k ] * point [ k ] ; xNorm += xK * xK ; } xNorm = Math . sqrt ( xNorm ) ; current = new VectorialPointValuePair ( point , objective ) ; if ( checker != null ) { if ( checker . converged ( getIterations ( ) , previous , current ) ) { return current ; } } } else { cost = previousCost ; for ( int j = 0 ; j < solvedCols ; ++ j ) { int pj = permutation [ j ] ; point [ pj ] = oldX [ pj ] ; } tmpVec = residuals ; residuals = oldRes ; oldRes = tmpVec ; tmpVec = objective ; objective = oldObj ; oldObj = tmpVec ; } if ( checker == null ) { if ( ( ( Math . abs ( actRed ) <= costRelativeTolerance ) && ( preRed <= costRelativeTolerance ) && ( ratio <= 2.0 ) ) || ( delta <= parRelativeTolerance * xNorm ) ) { return current ; } } if ( ( Math . abs ( actRed ) <= 2.2204e-16 ) && ( preRed <= 2.2204e-16 ) && ( ratio <= 2.0 ) ) { throw new OptimizationException ( LocalizedFormats . TOO_SMALL_COST_RELATIVE_TOLERANCE , costRelativeTolerance ) ; } else if ( delta <= 2.2204e-16 * xNorm ) { throw new OptimizationException ( LocalizedFormats . TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE , parRelativeTolerance ) ; } else if ( maxCosine <= 2.2204e-16 ) { throw new OptimizationException ( LocalizedFormats . TOO_SMALL_ORTHOGONALITY_TOLERANCE , orthoTolerance ) ; } } } }",
        "summary": "Inconsistent result from Levenberg-Marquardt",
        "Description": "Levenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-405",
        "comments": [
            "Correction patch",
            "Closing issue as it was included in version 2.2, which has been released"
        ],
        "summarized_discussion": "\n\nThe bug has been resolved by including it in version 2.2, which has been released. The correction patch has been applied, and the issue is now closed."
    },
    "Mockito_3_src/org/mockito/internal/invocation/InvocationMatcher.java_118_141": {
        "src": "public void captureArgumentsFrom(Invocation invocation) {\n        if (invocation.getMethod().isVarArgs()) {\n            int indexOfVararg = invocation.getRawArguments().length - 1;\n            for (int position = 0; position < indexOfVararg; position++) {\n                Matcher m = matchers.get(position);\n                if (m instanceof CapturesArguments) {\n                    ((CapturesArguments) m).captureFrom(invocation.getArgumentAt(position, Object.class));\n                }\n            }\n            for (int position = indexOfVararg; position < matchers.size(); position++) {\n                Matcher m = matchers.get(position);\n                if (m instanceof CapturesArguments) {\n                    ((CapturesArguments) m).captureFrom(invocation.getRawArguments()[position - indexOfVararg]);\n                }\n            }\n        } else {\n            for (int position = 0; position < matchers.size(); position++) {\n                Matcher m = matchers.get(position);\n                if (m instanceof CapturesArguments) {\n                    ((CapturesArguments) m).captureFrom(invocation.getArgumentAt(position, Object.class));\n                }\n            }\n        }\n    }",
        "src_wo_comments": "public void captureArgumentsFrom ( Invocation invocation ) { if ( invocation . getMethod ( ) . isVarArgs ( ) ) { int indexOfVararg = invocation . getRawArguments ( ) . length - 1 ; for ( int position = 0 ; position < indexOfVararg ; position ++ ) { Matcher m = matchers . get ( position ) ; if ( m instanceof CapturesArguments ) { ( ( CapturesArguments ) m ) . captureFrom ( invocation . getArgumentAt ( position , Object . class ) ) ; } } for ( int position = indexOfVararg ; position < matchers . size ( ) ; position ++ ) { Matcher m = matchers . get ( position ) ; if ( m instanceof CapturesArguments ) { ( ( CapturesArguments ) m ) . captureFrom ( invocation . getRawArguments ( ) [ position - indexOfVararg ] ) ; } } } else { for ( int position = 0 ; position < matchers . size ( ) ; position ++ ) { Matcher m = matchers . get ( position ) ; if ( m instanceof CapturesArguments ) { ( ( CapturesArguments ) m ) . captureFrom ( invocation . getArgumentAt ( position , Object . class ) ) ; } } } }",
        "fixed_src": "public void captureArgumentsFrom(Invocation invocation) {\n        if (invocation.getMethod().isVarArgs()) {\n            int indexOfVararg = invocation.getRawArguments().length - 1;\n            for (int position = 0; position < indexOfVararg; position++) {\n                Matcher m = matchers.get(position);\n                if (m instanceof CapturesArguments) {\n                    ((CapturesArguments) m).captureFrom(invocation.getArgumentAt(position, Object.class));\n                }\n            }\n            for (Matcher m : uniqueMatcherSet(indexOfVararg)) {\n                if (m instanceof CapturesArguments) {\n                    Object rawArgument = invocation.getRawArguments()[indexOfVararg];\n                    for (int i = 0; i < Array.getLength(rawArgument); i++) {\n                        ((CapturesArguments) m).captureFrom(Array.get(rawArgument, i));\n                    }\n                }\n            }\n        } else {\n            for (int position = 0; position < matchers.size(); position++) {\n                Matcher m = matchers.get(position);\n                if (m instanceof CapturesArguments) {\n                    ((CapturesArguments) m).captureFrom(invocation.getArgumentAt(position, Object.class));\n                }\n            }\n        }\n    }",
        "fixed_src_wo_comments": "public void captureArgumentsFrom ( Invocation invocation ) { if ( invocation . getMethod ( ) . isVarArgs ( ) ) { int indexOfVararg = invocation . getRawArguments ( ) . length - 1 ; for ( int position = 0 ; position < indexOfVararg ; position ++ ) { Matcher m = matchers . get ( position ) ; if ( m instanceof CapturesArguments ) { ( ( CapturesArguments ) m ) . captureFrom ( invocation . getArgumentAt ( position , Object . class ) ) ; } } for ( Matcher m : uniqueMatcherSet ( indexOfVararg ) ) { if ( m instanceof CapturesArguments ) { Object rawArgument = invocation . getRawArguments ( ) [ indexOfVararg ] ; for ( int i = 0 ; i < Array . getLength ( rawArgument ) ; i ++ ) { ( ( CapturesArguments ) m ) . captureFrom ( Array . get ( rawArgument , i ) ) ; } } } } else { for ( int position = 0 ; position < matchers . size ( ) ; position ++ ) { Matcher m = matchers . get ( position ) ; if ( m instanceof CapturesArguments ) { ( ( CapturesArguments ) m ) . captureFrom ( invocation . getArgumentAt ( position , Object . class ) ) ; } } } }",
        "summary": "ArgumentCaptor no longer working for varargs",
        "Description": "I ran into the issue described here: http://stackoverflow.com/questions/27303562/why-does-upgrading-mockito-from-1-9-5-to-1-10-8-break-this-captor\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Hi sorry for the late reply.\nI reproduced the issue, not sure when I will be able to fix though.\n"
            },
            {
                "content": "fixed by #211 \n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been fixed by commit #211."
    },
    "JxPath_15_src/java/org/apache/commons/jxpath/ri/axes/UnionContext.java_45_64": {
        "src": "public boolean setPosition(int position) {\n        if (!prepared) {\n            prepared = true;\n            BasicNodeSet nodeSet = (BasicNodeSet) getNodeSet();\n            ArrayList pointers = new ArrayList();\n            for (int i = 0; i < contexts.length; i++) {\n                EvalContext ctx = (EvalContext) contexts[i];\n                while (ctx.nextSet()) {\n                    while (ctx.nextNode()) {\n                        NodePointer ptr = ctx.getCurrentNodePointer();\n                        if (!pointers.contains(ptr)) {\n                            nodeSet.add(ptr);\n                            pointers.add(ptr);\n                        }\n                    }\n                }\n            }\n        }\n        return super.setPosition(position);\n    }",
        "src_wo_comments": "public boolean setPosition ( int position ) { if ( ! prepared ) { prepared = true ; BasicNodeSet nodeSet = ( BasicNodeSet ) getNodeSet ( ) ; ArrayList pointers = new ArrayList ( ) ; for ( int i = 0 ; i < contexts . length ; i ++ ) { EvalContext ctx = ( EvalContext ) contexts [ i ] ; while ( ctx . nextSet ( ) ) { while ( ctx . nextNode ( ) ) { NodePointer ptr = ctx . getCurrentNodePointer ( ) ; if ( ! pointers . contains ( ptr ) ) { nodeSet . add ( ptr ) ; pointers . add ( ptr ) ; } } } } } return super . setPosition ( position ) ; }",
        "fixed_src": "public boolean setPosition(int position) {\n        if (!prepared) {\n            prepared = true;\n            BasicNodeSet nodeSet = (BasicNodeSet) getNodeSet();\n            ArrayList pointers = new ArrayList();\n            for (int i = 0; i < contexts.length; i++) {\n                EvalContext ctx = (EvalContext) contexts[i];\n                while (ctx.nextSet()) {\n                    while (ctx.nextNode()) {\n                        NodePointer ptr = ctx.getCurrentNodePointer();\n                        if (!pointers.contains(ptr)) {\n                            pointers.add(ptr);\n                        }\n                    }\n                }\n            }\n            sortPointers(pointers);\n\n            for (Iterator it = pointers.iterator(); it.hasNext();) {\n                nodeSet.add((Pointer) it.next());\n            }\n        }\n        return super.setPosition(position);\n    }",
        "fixed_src_wo_comments": "public boolean setPosition ( int position ) { if ( ! prepared ) { prepared = true ; BasicNodeSet nodeSet = ( BasicNodeSet ) getNodeSet ( ) ; ArrayList pointers = new ArrayList ( ) ; for ( int i = 0 ; i < contexts . length ; i ++ ) { EvalContext ctx = ( EvalContext ) contexts [ i ] ; while ( ctx . nextSet ( ) ) { while ( ctx . nextNode ( ) ) { NodePointer ptr = ctx . getCurrentNodePointer ( ) ; if ( ! pointers . contains ( ptr ) ) { pointers . add ( ptr ) ; } } } } sortPointers ( pointers ) ; for ( Iterator it = pointers . iterator ( ) ; it . hasNext ( ) ; ) { nodeSet . add ( ( Pointer ) it . next ( ) ) ; } } return super . setPosition ( position ) ; }",
        "summary": "Core union operation does not sort result nodes according to document order",
        "Description": "Source document:\n<MAIN><A>avalue</A><B>bvalue</B></MAIN>\n\nAccording to string() function defintion:\n\"A node-set is converted to a string by returning the string-value of the node in the node-set that is first in document order. If the node-set is empty, an empty string is returned.\"\n\nFollowing XPath calculated incorrectly:\n string(/MAIN/B | /MAIN/A)\n\nExpected result: \"avalue\"\nActual value: \"bvalue\"\n\nReason:\nsorting of result nodes is missing from CoreOperationUnion",
        "issue_url": "https://issues.apache.org/jira//browse/JXPATH-100",
        "comments": [
            "Test case"
        ],
        "summarized_discussion": "\n\nThe bug in the source code was caused by a missing test case. The solution to the bug is to add the missing test case to the source code. This will help ensure that the code is functioning correctly and that any potential issues are identified and addressed."
    },
    "JacksonDatabind_56_src/main/java/com/fasterxml/jackson/databind/deser/std/FromStringDeserializer.java_205_276": {
        "src": "@Override\n        protected Object _deserialize(String value, DeserializationContext ctxt) throws IOException\n        {\n            switch (_kind) {\n            case STD_FILE:\n                return new File(value);\n            case STD_URL:\n                return new URL(value);\n            case STD_URI:\n                return URI.create(value);\n            case STD_CLASS:\n                try {\n                    return ctxt.findClass(value);\n                } catch (Exception e) {\n                    throw ctxt.instantiationException(_valueClass, ClassUtil.getRootCause(e));\n                }\n            case STD_JAVA_TYPE:\n                return ctxt.getTypeFactory().constructFromCanonical(value);\n            case STD_CURRENCY:\n                // will throw IAE if unknown:\n                return Currency.getInstance(value);\n            case STD_PATTERN:\n                // will throw IAE (or its subclass) if malformed\n                return Pattern.compile(value);\n            case STD_LOCALE:\n                {\n                    int ix = value.indexOf('_');\n                    if (ix < 0) { // single argument\n                        return new Locale(value);\n                    }\n                    String first = value.substring(0, ix);\n                    value = value.substring(ix+1);\n                    ix = value.indexOf('_');\n                    if (ix < 0) { // two pieces\n                        return new Locale(first, value);\n                    }\n                    String second = value.substring(0, ix);\n                    return new Locale(first, second, value.substring(ix+1));\n                }\n            case STD_CHARSET:\n                return Charset.forName(value);\n            case STD_TIME_ZONE:\n                return TimeZone.getTimeZone(value);\n            case STD_INET_ADDRESS:\n                return InetAddress.getByName(value);\n            case STD_INET_SOCKET_ADDRESS:\n                if (value.startsWith(\"[\")) {\n                    // bracketed IPv6 (with port number)\n\n                    int i = value.lastIndexOf(']');\n                    if (i == -1) {\n                        throw new InvalidFormatException(ctxt.getParser(),\n                                \"Bracketed IPv6 address must contain closing bracket\",\n                                value, InetSocketAddress.class);\n                    }\n\n                    int j = value.indexOf(':', i);\n                    int port = j > -1 ? Integer.parseInt(value.substring(j + 1)) : 0;\n                    return new InetSocketAddress(value.substring(0, i + 1), port);\n                } else {\n                    int ix = value.indexOf(':');\n                    if (ix >= 0 && value.indexOf(':', ix + 1) < 0) {\n                        // host:port\n                        int port = Integer.parseInt(value.substring(ix+1));\n                        return new InetSocketAddress(value.substring(0, ix), port);\n                    }\n                    // host or unbracketed IPv6, without port number\n                    return new InetSocketAddress(value, 0);\n                }\n            }\n            throw new IllegalArgumentException();\n        }",
        "src_wo_comments": "@ Override protected Object _deserialize ( String value , DeserializationContext ctxt ) throws IOException { switch ( _kind ) { case STD_FILE : return new File ( value ) ; case STD_URL : return new URL ( value ) ; case STD_URI : return URI . create ( value ) ; case STD_CLASS : try { return ctxt . findClass ( value ) ; } catch ( Exception e ) { throw ctxt . instantiationException ( _valueClass , ClassUtil . getRootCause ( e ) ) ; } case STD_JAVA_TYPE : return ctxt . getTypeFactory ( ) . constructFromCanonical ( value ) ; case STD_CURRENCY : return Currency . getInstance ( value ) ; case STD_PATTERN : return Pattern . compile ( value ) ; case STD_LOCALE : { int ix = value . indexOf ( '_' ) ; if ( ix < 0 ) { return new Locale ( value ) ; } String first = value . substring ( 0 , ix ) ; value = value . substring ( ix + 1 ) ; ix = value . indexOf ( '_' ) ; if ( ix < 0 ) { return new Locale ( first , value ) ; } String second = value . substring ( 0 , ix ) ; return new Locale ( first , second , value . substring ( ix + 1 ) ) ; } case STD_CHARSET : return Charset . forName ( value ) ; case STD_TIME_ZONE : return TimeZone . getTimeZone ( value ) ; case STD_INET_ADDRESS : return InetAddress . getByName ( value ) ; case STD_INET_SOCKET_ADDRESS : if ( value . startsWith ( \"[\" ) ) { int i = value . lastIndexOf ( ']' ) ; if ( i == - 1 ) { throw new InvalidFormatException ( ctxt . getParser ( ) , \"Bracketed IPv6 address must contain closing bracket\" , value , InetSocketAddress . class ) ; } int j = value . indexOf ( ':' , i ) ; int port = j > - 1 ? Integer . parseInt ( value . substring ( j + 1 ) ) : 0 ; return new InetSocketAddress ( value . substring ( 0 , i + 1 ) , port ) ; } else { int ix = value . indexOf ( ':' ) ; if ( ix >= 0 && value . indexOf ( ':' , ix + 1 ) < 0 ) { int port = Integer . parseInt ( value . substring ( ix + 1 ) ) ; return new InetSocketAddress ( value . substring ( 0 , ix ) , port ) ; } return new InetSocketAddress ( value , 0 ) ; } } throw new IllegalArgumentException ( ) ; }",
        "fixed_src": "@Override\n        protected Object _deserialize(String value, DeserializationContext ctxt) throws IOException\n        {\n            switch (_kind) {\n            case STD_FILE:\n                return new File(value);\n            case STD_URL:\n                return new URL(value);\n            case STD_URI:\n                return URI.create(value);\n            case STD_CLASS:\n                try {\n                    return ctxt.findClass(value);\n                } catch (Exception e) {\n                    throw ctxt.instantiationException(_valueClass, ClassUtil.getRootCause(e));\n                }\n            case STD_JAVA_TYPE:\n                return ctxt.getTypeFactory().constructFromCanonical(value);\n            case STD_CURRENCY:\n                // will throw IAE if unknown:\n                return Currency.getInstance(value);\n            case STD_PATTERN:\n                // will throw IAE (or its subclass) if malformed\n                return Pattern.compile(value);\n            case STD_LOCALE:\n                {\n                    int ix = _firstHyphenOrUnderscore(value);\n                    if (ix < 0) { // single argument\n                        return new Locale(value);\n                    }\n                    String first = value.substring(0, ix);\n                    value = value.substring(ix+1);\n                    ix = _firstHyphenOrUnderscore(value);\n                    if (ix < 0) { // two pieces\n                        return new Locale(first, value);\n                    }\n                    String second = value.substring(0, ix);\n                    return new Locale(first, second, value.substring(ix+1));\n                }\n            case STD_CHARSET:\n                return Charset.forName(value);\n            case STD_TIME_ZONE:\n                return TimeZone.getTimeZone(value);\n            case STD_INET_ADDRESS:\n                return InetAddress.getByName(value);\n            case STD_INET_SOCKET_ADDRESS:\n                if (value.startsWith(\"[\")) {\n                    // bracketed IPv6 (with port number)\n\n                    int i = value.lastIndexOf(']');\n                    if (i == -1) {\n                        throw new InvalidFormatException(ctxt.getParser(),\n                                \"Bracketed IPv6 address must contain closing bracket\",\n                                value, InetSocketAddress.class);\n                    }\n\n                    int j = value.indexOf(':', i);\n                    int port = j > -1 ? Integer.parseInt(value.substring(j + 1)) : 0;\n                    return new InetSocketAddress(value.substring(0, i + 1), port);\n                } else {\n                    int ix = value.indexOf(':');\n                    if (ix >= 0 && value.indexOf(':', ix + 1) < 0) {\n                        // host:port\n                        int port = Integer.parseInt(value.substring(ix+1));\n                        return new InetSocketAddress(value.substring(0, ix), port);\n                    }\n                    // host or unbracketed IPv6, without port number\n                    return new InetSocketAddress(value, 0);\n                }\n            }\n            throw new IllegalArgumentException();\n        }",
        "fixed_src_wo_comments": "@ Override protected Object _deserialize ( String value , DeserializationContext ctxt ) throws IOException { switch ( _kind ) { case STD_FILE : return new File ( value ) ; case STD_URL : return new URL ( value ) ; case STD_URI : return URI . create ( value ) ; case STD_CLASS : try { return ctxt . findClass ( value ) ; } catch ( Exception e ) { throw ctxt . instantiationException ( _valueClass , ClassUtil . getRootCause ( e ) ) ; } case STD_JAVA_TYPE : return ctxt . getTypeFactory ( ) . constructFromCanonical ( value ) ; case STD_CURRENCY : return Currency . getInstance ( value ) ; case STD_PATTERN : return Pattern . compile ( value ) ; case STD_LOCALE : { int ix = _firstHyphenOrUnderscore ( value ) ; if ( ix < 0 ) { return new Locale ( value ) ; } String first = value . substring ( 0 , ix ) ; value = value . substring ( ix + 1 ) ; ix = _firstHyphenOrUnderscore ( value ) ; if ( ix < 0 ) { return new Locale ( first , value ) ; } String second = value . substring ( 0 , ix ) ; return new Locale ( first , second , value . substring ( ix + 1 ) ) ; } case STD_CHARSET : return Charset . forName ( value ) ; case STD_TIME_ZONE : return TimeZone . getTimeZone ( value ) ; case STD_INET_ADDRESS : return InetAddress . getByName ( value ) ; case STD_INET_SOCKET_ADDRESS : if ( value . startsWith ( \"[\" ) ) { int i = value . lastIndexOf ( ']' ) ; if ( i == - 1 ) { throw new InvalidFormatException ( ctxt . getParser ( ) , \"Bracketed IPv6 address must contain closing bracket\" , value , InetSocketAddress . class ) ; } int j = value . indexOf ( ':' , i ) ; int port = j > - 1 ? Integer . parseInt ( value . substring ( j + 1 ) ) : 0 ; return new InetSocketAddress ( value . substring ( 0 , i + 1 ) , port ) ; } else { int ix = value . indexOf ( ':' ) ; if ( ix >= 0 && value . indexOf ( ':' , ix + 1 ) < 0 ) { int port = Integer . parseInt ( value . substring ( ix + 1 ) ) ; return new InetSocketAddress ( value . substring ( 0 , ix ) , port ) ; } return new InetSocketAddress ( value , 0 ) ; } } throw new IllegalArgumentException ( ) ; }",
        "summary": "Deserializing locale assumes JDK separator (underscore), does not accept RFC specified (hyphen)",
        "Description": "When deserializing a locale Jackson currently uses the underscore character as the separator rather than the dash.  Specifically, in FromStringDeserializer.java line 234:\n\n```\nint ix = value.indexOf('_');\n```\n\nMany locale implementations use dash as the separator as per https://tools.ietf.org/html/rfc5646\n\nGiven the RFC states that only the characters a-z A-Z and - are valid it should be possible to leave the current code in for backward-compatibility but it should also check for  '-' as a separator.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "@mcdee Thank you for reporting this: sounds like a bug indeed.\n\nDo you know of an existing JDK provided `Locale` value that would exhibit this? Would be great to have a unit test against regression, beyond fixing the issue itself.\n"
            },
            {
                "content": "Actually I think JDK always uses underscore, so I'll have to just test deserializer with made-up codes.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to test the deserializer with made-up codes, as the JDK always uses underscores. Additionally, it would be beneficial to create a unit test to ensure that the issue does not regress."
    },
    "JacksonDatabind_57_src/main/java/com/fasterxml/jackson/databind/ObjectReader.java_1435_1443": {
        "src": "public <T> MappingIterator<T> readValues(byte[] src, int offset, int length)\n        throws IOException, JsonProcessingException\n    {\n        if (_dataFormatReaders != null) {\n            return _detectBindAndReadValues(_dataFormatReaders.findFormat(src, offset, length), false);\n        }\n        return _bindAndReadValues(_considerFilter(_parserFactory.createParser(src), \n                true));\n    }",
        "src_wo_comments": "public < T > MappingIterator < T > readValues ( byte [ ] src , int offset , int length ) throws IOException , JsonProcessingException { if ( _dataFormatReaders != null ) { return _detectBindAndReadValues ( _dataFormatReaders . findFormat ( src , offset , length ) , false ) ; } return _bindAndReadValues ( _considerFilter ( _parserFactory . createParser ( src ) , true ) ) ; }",
        "fixed_src": "public <T> MappingIterator<T> readValues(byte[] src, int offset, int length)\n        throws IOException, JsonProcessingException\n    {\n        if (_dataFormatReaders != null) {\n            return _detectBindAndReadValues(_dataFormatReaders.findFormat(src, offset, length), false);\n        }\n        return _bindAndReadValues(_considerFilter(_parserFactory.createParser(src, offset, length),\n                true));\n    }",
        "fixed_src_wo_comments": "public < T > MappingIterator < T > readValues ( byte [ ] src , int offset , int length ) throws IOException , JsonProcessingException { if ( _dataFormatReaders != null ) { return _detectBindAndReadValues ( _dataFormatReaders . findFormat ( src , offset , length ) , false ) ; } return _bindAndReadValues ( _considerFilter ( _parserFactory . createParser ( src , offset , length ) , true ) ) ; }",
        "summary": "`ObjectReader.readValues()` ignores offset and length when reading an array",
        "Description": "ObjectReader.readValues ignores offset and length when reading an array. If _dataFormatReaders it will always use the full array:\n\nhttps://github.com/FasterXML/jackson-databind/blob/2.7/src/main/java/com/fasterxml/jackson/databind/ObjectReader.java#L1435\n",
        "issue_url": null,
        "comments": [
            {
                "content": "I'll be. Right you are. Thank you for reporting this!\n"
            },
            {
                "content": "Fixed for 2.7.8/2.8.3 (earlier branches are closed)\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been fixed for versions 2.7.8 and 2.8.3. Earlier branches are closed."
    },
    "Mockito_2_src/org/mockito/internal/util/Timer.java_9_11": {
        "src": "public Timer(long durationMillis) {\n        this.durationMillis = durationMillis;\n    }",
        "src_wo_comments": "public Timer ( long durationMillis ) { this . durationMillis = durationMillis ; }",
        "fixed_src": "public Timer(long durationMillis) {\n        validateInput(durationMillis);\n        this.durationMillis = durationMillis;\n    }",
        "fixed_src_wo_comments": "public Timer ( long durationMillis ) { validateInput ( durationMillis ) ; this . durationMillis = durationMillis ; }",
        "summary": "Mockito.after() method accepts negative timeperiods and subsequent verifications always pass",
        "Description": "e.g.\n\n```\nRunnable runnable = Mockito.mock(Runnable.class);\nMockito.verify(runnable, Mockito.never()).run(); // passes as expected\nMockito.verify(runnable, Mockito.after(1000).never()).run(); // passes as expected\nMockito.verify(runnable, Mockito.after(-1000).atLeastOnce()).run(); // passes incorrectly\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Well spotted, thx\n"
            },
            {
                "content": "I can try to fix that, but need to know what exactly is the issue here:\nShould negative values in after method be forbidden and trigger exception to be thrown? or maybe negatives are allowed here (negative value means - immediately, pretty much it should behave like after(0) )?\n"
            },
            {
                "content": "I would vote for an IllegalArgumentException being thrown.\n"
            },
            {
                "content": "The same is happening for _timeout_ method:\ne.g. \n\n```\n       SomeClazz mock = Mockito.mock(SomeClazz.class);\n       Mockito.verify(mock, timeout(-100)).someMethod(); //passes, which is incorrect\n```\n\nIf method was invoked, then it passes, but this negative timeout is at least confusing:\n\n```\n        SomeClazz mock = Mockito.mock(SomeClazz.class);\n        mock.someMethod();\n        Mockito.verify(mock, timeout(-100)).someMethod(); //passes\n```\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to throw an IllegalArgumentException when negative values are used in the after and timeout methods. This would prevent the incorrect passing of the verify method when negative values are used."
    },
    "Cli_33_src/main/java/org/apache/commons/cli/HelpFormatter.java_726_732": {
        "src": "public void printWrapped(PrintWriter pw, int width, int nextLineTabStop, String text)\n    {\n        StringBuffer sb = new StringBuffer(text.length());\n\n        renderWrappedText(sb, width, nextLineTabStop, text);\n        pw.println(sb.toString());\n    }",
        "src_wo_comments": "public void printWrapped ( PrintWriter pw , int width , int nextLineTabStop , String text ) { StringBuffer sb = new StringBuffer ( text . length ( ) ) ; renderWrappedText ( sb , width , nextLineTabStop , text ) ; pw . println ( sb . toString ( ) ) ; }",
        "fixed_src": "public void printWrapped(PrintWriter pw, int width, int nextLineTabStop, String text)\n    {\n        StringBuffer sb = new StringBuffer(text.length());\n\n        renderWrappedTextBlock(sb, width, nextLineTabStop, text);\n        pw.println(sb.toString());\n    }",
        "fixed_src_wo_comments": "public void printWrapped ( PrintWriter pw , int width , int nextLineTabStop , String text ) { StringBuffer sb = new StringBuffer ( text . length ( ) ) ; renderWrappedTextBlock ( sb , width , nextLineTabStop , text ) ; pw . println ( sb . toString ( ) ) ; }",
        "summary": "HelpFormatter strips leading whitespaces in the footer",
        "Description": "I discovered a bug in Commons CLI while using it through Groovy's CliBuilder. See the following issue:\n\nhttp://jira.codehaus.org/browse/GROOVY-4313?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel\n\nCopied:\nThe following code:\n\ndef cli = new CliBuilder(footer: \"line1:\\n line2:\\n\")\ncli.usage()\n\nProduces the following output:\n\nline1\nline2\n\nNote that there are no whitespaces before \"line2\". Replacing them with \"\\t\" doesn't solve the problem either.",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-207",
        "comments": [
            "Thank you for the report Uri. It's now fixed on the trunk and will be part of the 1.3 release."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed and will be included in the 1.3 release of the source code."
    },
    "JacksonCore_21_src/main/java/com/fasterxml/jackson/core/filter/FilteringParserDelegate.java_226_454": {
        "src": "@Override\n    public JsonToken nextToken() throws IOException\n    {\n        // 23-May-2017, tatu: To be honest, code here is rather hairy and I don't like all\n        //    conditionals; and it seems odd to return `null` but NOT considering input\n        //    as closed... would love a rewrite to simplify/clear up logic here.\n        \n        // Check for _allowMultipleMatches - false and at least there is one token - which is _currToken\n        // check for no buffered context _exposedContext - null\n        // If all the conditions matches then check for scalar / non-scalar property\n        if (!_allowMultipleMatches && (_currToken != null) && (_exposedContext == null)) {\n            //if not scalar and ended successfully, and !includePath, then return null\n                if (_currToken.isStructEnd()) {\n                    if (_headContext.isStartHandled()) {\n                        return (_currToken = null);\n                    }\n                } else if (_currToken.isScalarValue()) {\n                    //else if scalar, and scalar not present in obj/array and !includePath and INCLUDE_ALL matched once\n                    // then return null \n                    if (!_headContext.isStartHandled() && (_itemFilter == TokenFilter.INCLUDE_ALL)) {\n                        return (_currToken = null);\n                    }\n            }\n        }\n        // Anything buffered?\n        TokenFilterContext ctxt = _exposedContext;\n\n        if (ctxt != null) {\n            while (true) {\n                JsonToken t = ctxt.nextTokenToRead();\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n                // all done with buffered stuff?\n                if (ctxt == _headContext) {\n                    _exposedContext = null;\n                    if (ctxt.inArray()) {\n                        t = delegate.getCurrentToken();\n// Is this guaranteed to work without further checks?\n//                        if (t != JsonToken.START_ARRAY) {\n                        _currToken = t;\n                        return t;\n                    }\n\n                    // Almost! Most likely still have the current token;\n                    // with the sole exception of \n                    /*\n                    t = delegate.getCurrentToken();\n                    if (t != JsonToken.FIELD_NAME) {\n                        _currToken = t;\n                        return t;\n                    }\n                    */\n                    break;\n                }\n                // If not, traverse down the context chain\n                ctxt = _headContext.findChildOf(ctxt);\n                _exposedContext = ctxt;\n                if (ctxt == null) { // should never occur\n                    throw _constructError(\"Unexpected problem: chain of filtered context broken\");\n                }\n            }\n        }\n\n        // If not, need to read more. If we got any:\n        JsonToken t = delegate.nextToken();\n        if (t == null) {\n            // no strict need to close, since we have no state here\n            _currToken = t;\n            return t;\n        }\n\n        // otherwise... to include or not?\n        TokenFilter f;\n        \n        switch (t.id()) {\n        case ID_START_ARRAY:\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildArrayContext(f, true);\n                return (_currToken = t);\n            }\n            if (f == null) { // does this occur?\n                delegate.skipChildren();\n                break;\n            }\n            // Otherwise still iffy, need to check\n            f = _headContext.checkValue(f);\n            if (f == null) {\n                delegate.skipChildren();\n                break;\n            }\n            if (f != TokenFilter.INCLUDE_ALL) {\n                f = f.filterStartArray();\n            }\n            _itemFilter = f;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildArrayContext(f, true);\n                return (_currToken = t);\n            }\n            _headContext = _headContext.createChildArrayContext(f, false);\n            \n            // Also: only need buffering if parent path to be included\n            if (_includePath) {\n                t = _nextTokenWithBuffering(_headContext);\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n            }\n            break;\n\n        case ID_START_OBJECT:\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildObjectContext(f, true);\n                return (_currToken = t);\n            }\n            if (f == null) { // does this occur?\n                delegate.skipChildren();\n                break;\n            }\n            // Otherwise still iffy, need to check\n            f = _headContext.checkValue(f);\n            if (f == null) {\n                delegate.skipChildren();\n                break;\n            }\n            if (f != TokenFilter.INCLUDE_ALL) {\n                f = f.filterStartObject();\n            }\n            _itemFilter = f;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildObjectContext(f, true);\n                return (_currToken = t);\n            }\n            _headContext = _headContext.createChildObjectContext(f, false);\n            // Also: only need buffering if parent path to be included\n            if (_includePath) {\n                t = _nextTokenWithBuffering(_headContext);\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n            }\n            // note: inclusion of surrounding Object handled separately via\n            // FIELD_NAME\n            break;\n\n        case ID_END_ARRAY:\n        case ID_END_OBJECT:\n            {\n                boolean returnEnd = _headContext.isStartHandled();\n                f = _headContext.getFilter();\n                if ((f != null) && (f != TokenFilter.INCLUDE_ALL)) {\n                    f.filterFinishArray();\n                }\n                _headContext = _headContext.getParent();\n                _itemFilter = _headContext.getFilter();\n                if (returnEnd) {\n                    return (_currToken = t);\n                }\n            }\n            break;\n\n        case ID_FIELD_NAME:\n            {\n                final String name = delegate.getCurrentName();\n                // note: this will also set 'needToHandleName'\n                f = _headContext.setFieldName(name);\n                if (f == TokenFilter.INCLUDE_ALL) {\n                    _itemFilter = f;\n                    if (!_includePath) {\n                        // Minor twist here: if parent NOT included, may need to induce output of\n                        // surrounding START_OBJECT/END_OBJECT\n                        if (_includeImmediateParent && !_headContext.isStartHandled()) {\n                            t = _headContext.nextTokenToRead(); // returns START_OBJECT but also marks it handled\n                            _exposedContext = _headContext;\n                        }\n                    }\n                    return (_currToken = t);\n                }\n                if (f == null) {\n                    delegate.nextToken();\n                    delegate.skipChildren();\n                    break;\n                }\n                f = f.includeProperty(name);\n                if (f == null) {\n                    delegate.nextToken();\n                    delegate.skipChildren();\n                    break;\n                }\n                _itemFilter = f;\n                if (f == TokenFilter.INCLUDE_ALL) {\n                    if (_includePath) {\n                        return (_currToken = t);\n                    }\n                }\n                if (_includePath) {\n                    t = _nextTokenWithBuffering(_headContext);\n                    if (t != null) {\n                        _currToken = t;\n                        return t;\n                    }\n                }\n                break;\n            }\n\n        default: // scalar value\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                return (_currToken = t);\n            }\n            if (f != null) {\n                f = _headContext.checkValue(f);\n                if ((f == TokenFilter.INCLUDE_ALL)\n                        || ((f != null) && f.includeValue(delegate))) {\n                    return (_currToken = t);\n                }\n            }\n            // Otherwise not included (leaves must be explicitly included)\n            break;\n        }\n\n        // We get here if token was not yet found; offlined handling\n        return _nextToken2();\n    }",
        "src_wo_comments": "@ Override public JsonToken nextToken ( ) throws IOException { if ( ! _allowMultipleMatches && ( _currToken != null ) && ( _exposedContext == null ) ) { if ( _currToken . isStructEnd ( ) ) { if ( _headContext . isStartHandled ( ) ) { return ( _currToken = null ) ; } } else if ( _currToken . isScalarValue ( ) ) { if ( ! _headContext . isStartHandled ( ) && ( _itemFilter == TokenFilter . INCLUDE_ALL ) ) { return ( _currToken = null ) ; } } } TokenFilterContext ctxt = _exposedContext ; if ( ctxt != null ) { while ( true ) { JsonToken t = ctxt . nextTokenToRead ( ) ; if ( t != null ) { _currToken = t ; return t ; } if ( ctxt == _headContext ) { _exposedContext = null ; if ( ctxt . inArray ( ) ) { t = delegate . getCurrentToken ( ) ; _currToken = t ; return t ; } break ; } ctxt = _headContext . findChildOf ( ctxt ) ; _exposedContext = ctxt ; if ( ctxt == null ) { throw _constructError ( \"Unexpected problem: chain of filtered context broken\" ) ; } } } JsonToken t = delegate . nextToken ( ) ; if ( t == null ) { _currToken = t ; return t ; } TokenFilter f ; switch ( t . id ( ) ) { case ID_START_ARRAY : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildArrayContext ( f , true ) ; return ( _currToken = t ) ; } if ( f == null ) { delegate . skipChildren ( ) ; break ; } f = _headContext . checkValue ( f ) ; if ( f == null ) { delegate . skipChildren ( ) ; break ; } if ( f != TokenFilter . INCLUDE_ALL ) { f = f . filterStartArray ( ) ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildArrayContext ( f , true ) ; return ( _currToken = t ) ; } _headContext = _headContext . createChildArrayContext ( f , false ) ; if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; case ID_START_OBJECT : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildObjectContext ( f , true ) ; return ( _currToken = t ) ; } if ( f == null ) { delegate . skipChildren ( ) ; break ; } f = _headContext . checkValue ( f ) ; if ( f == null ) { delegate . skipChildren ( ) ; break ; } if ( f != TokenFilter . INCLUDE_ALL ) { f = f . filterStartObject ( ) ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildObjectContext ( f , true ) ; return ( _currToken = t ) ; } _headContext = _headContext . createChildObjectContext ( f , false ) ; if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; case ID_END_ARRAY : case ID_END_OBJECT : { boolean returnEnd = _headContext . isStartHandled ( ) ; f = _headContext . getFilter ( ) ; if ( ( f != null ) && ( f != TokenFilter . INCLUDE_ALL ) ) { f . filterFinishArray ( ) ; } _headContext = _headContext . getParent ( ) ; _itemFilter = _headContext . getFilter ( ) ; if ( returnEnd ) { return ( _currToken = t ) ; } } break ; case ID_FIELD_NAME : { final String name = delegate . getCurrentName ( ) ; f = _headContext . setFieldName ( name ) ; if ( f == TokenFilter . INCLUDE_ALL ) { _itemFilter = f ; if ( ! _includePath ) { if ( _includeImmediateParent && ! _headContext . isStartHandled ( ) ) { t = _headContext . nextTokenToRead ( ) ; _exposedContext = _headContext ; } } return ( _currToken = t ) ; } if ( f == null ) { delegate . nextToken ( ) ; delegate . skipChildren ( ) ; break ; } f = f . includeProperty ( name ) ; if ( f == null ) { delegate . nextToken ( ) ; delegate . skipChildren ( ) ; break ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { if ( _includePath ) { return ( _currToken = t ) ; } } if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; } default : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { return ( _currToken = t ) ; } if ( f != null ) { f = _headContext . checkValue ( f ) ; if ( ( f == TokenFilter . INCLUDE_ALL ) || ( ( f != null ) && f . includeValue ( delegate ) ) ) { return ( _currToken = t ) ; } } break ; } return _nextToken2 ( ) ; }",
        "fixed_src": "@Override\n    public JsonToken nextToken() throws IOException\n    {\n        // 23-May-2017, tatu: To be honest, code here is rather hairy and I don't like all\n        //    conditionals; and it seems odd to return `null` but NOT considering input\n        //    as closed... would love a rewrite to simplify/clear up logic here.\n        \n        // Check for _allowMultipleMatches - false and at least there is one token - which is _currToken\n        // check for no buffered context _exposedContext - null\n        // If all the conditions matches then check for scalar / non-scalar property\n        if (!_allowMultipleMatches && (_currToken != null) && (_exposedContext == null)) {\n            //if not scalar and ended successfully, and !includePath, then return null\n            if (!_includePath) {\n                if (_currToken.isStructEnd()) {\n                    if (_headContext.isStartHandled()) {\n                        return (_currToken = null);\n                    }\n                } else if (_currToken.isScalarValue()) {\n                    //else if scalar, and scalar not present in obj/array and !includePath and INCLUDE_ALL matched once\n                    // then return null \n                    if (!_headContext.isStartHandled() && (_itemFilter == TokenFilter.INCLUDE_ALL)) {\n                        return (_currToken = null);\n                    }\n                }\n            }\n        }\n        // Anything buffered?\n        TokenFilterContext ctxt = _exposedContext;\n\n        if (ctxt != null) {\n            while (true) {\n                JsonToken t = ctxt.nextTokenToRead();\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n                // all done with buffered stuff?\n                if (ctxt == _headContext) {\n                    _exposedContext = null;\n                    if (ctxt.inArray()) {\n                        t = delegate.getCurrentToken();\n// Is this guaranteed to work without further checks?\n//                        if (t != JsonToken.START_ARRAY) {\n                        _currToken = t;\n                        return t;\n                    }\n\n                    // Almost! Most likely still have the current token;\n                    // with the sole exception of \n                    /*\n                    t = delegate.getCurrentToken();\n                    if (t != JsonToken.FIELD_NAME) {\n                        _currToken = t;\n                        return t;\n                    }\n                    */\n                    break;\n                }\n                // If not, traverse down the context chain\n                ctxt = _headContext.findChildOf(ctxt);\n                _exposedContext = ctxt;\n                if (ctxt == null) { // should never occur\n                    throw _constructError(\"Unexpected problem: chain of filtered context broken\");\n                }\n            }\n        }\n\n        // If not, need to read more. If we got any:\n        JsonToken t = delegate.nextToken();\n        if (t == null) {\n            // no strict need to close, since we have no state here\n            _currToken = t;\n            return t;\n        }\n\n        // otherwise... to include or not?\n        TokenFilter f;\n        \n        switch (t.id()) {\n        case ID_START_ARRAY:\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildArrayContext(f, true);\n                return (_currToken = t);\n            }\n            if (f == null) { // does this occur?\n                delegate.skipChildren();\n                break;\n            }\n            // Otherwise still iffy, need to check\n            f = _headContext.checkValue(f);\n            if (f == null) {\n                delegate.skipChildren();\n                break;\n            }\n            if (f != TokenFilter.INCLUDE_ALL) {\n                f = f.filterStartArray();\n            }\n            _itemFilter = f;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildArrayContext(f, true);\n                return (_currToken = t);\n            }\n            _headContext = _headContext.createChildArrayContext(f, false);\n            \n            // Also: only need buffering if parent path to be included\n            if (_includePath) {\n                t = _nextTokenWithBuffering(_headContext);\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n            }\n            break;\n\n        case ID_START_OBJECT:\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildObjectContext(f, true);\n                return (_currToken = t);\n            }\n            if (f == null) { // does this occur?\n                delegate.skipChildren();\n                break;\n            }\n            // Otherwise still iffy, need to check\n            f = _headContext.checkValue(f);\n            if (f == null) {\n                delegate.skipChildren();\n                break;\n            }\n            if (f != TokenFilter.INCLUDE_ALL) {\n                f = f.filterStartObject();\n            }\n            _itemFilter = f;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildObjectContext(f, true);\n                return (_currToken = t);\n            }\n            _headContext = _headContext.createChildObjectContext(f, false);\n            // Also: only need buffering if parent path to be included\n            if (_includePath) {\n                t = _nextTokenWithBuffering(_headContext);\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n            }\n            // note: inclusion of surrounding Object handled separately via\n            // FIELD_NAME\n            break;\n\n        case ID_END_ARRAY:\n        case ID_END_OBJECT:\n            {\n                boolean returnEnd = _headContext.isStartHandled();\n                f = _headContext.getFilter();\n                if ((f != null) && (f != TokenFilter.INCLUDE_ALL)) {\n                    f.filterFinishArray();\n                }\n                _headContext = _headContext.getParent();\n                _itemFilter = _headContext.getFilter();\n                if (returnEnd) {\n                    return (_currToken = t);\n                }\n            }\n            break;\n\n        case ID_FIELD_NAME:\n            {\n                final String name = delegate.getCurrentName();\n                // note: this will also set 'needToHandleName'\n                f = _headContext.setFieldName(name);\n                if (f == TokenFilter.INCLUDE_ALL) {\n                    _itemFilter = f;\n                    if (!_includePath) {\n                        // Minor twist here: if parent NOT included, may need to induce output of\n                        // surrounding START_OBJECT/END_OBJECT\n                        if (_includeImmediateParent && !_headContext.isStartHandled()) {\n                            t = _headContext.nextTokenToRead(); // returns START_OBJECT but also marks it handled\n                            _exposedContext = _headContext;\n                        }\n                    }\n                    return (_currToken = t);\n                }\n                if (f == null) {\n                    delegate.nextToken();\n                    delegate.skipChildren();\n                    break;\n                }\n                f = f.includeProperty(name);\n                if (f == null) {\n                    delegate.nextToken();\n                    delegate.skipChildren();\n                    break;\n                }\n                _itemFilter = f;\n                if (f == TokenFilter.INCLUDE_ALL) {\n                    if (_includePath) {\n                        return (_currToken = t);\n                    }\n                }\n                if (_includePath) {\n                    t = _nextTokenWithBuffering(_headContext);\n                    if (t != null) {\n                        _currToken = t;\n                        return t;\n                    }\n                }\n                break;\n            }\n\n        default: // scalar value\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                return (_currToken = t);\n            }\n            if (f != null) {\n                f = _headContext.checkValue(f);\n                if ((f == TokenFilter.INCLUDE_ALL)\n                        || ((f != null) && f.includeValue(delegate))) {\n                    return (_currToken = t);\n                }\n            }\n            // Otherwise not included (leaves must be explicitly included)\n            break;\n        }\n\n        // We get here if token was not yet found; offlined handling\n        return _nextToken2();\n    }",
        "fixed_src_wo_comments": "@ Override public JsonToken nextToken ( ) throws IOException { if ( ! _allowMultipleMatches && ( _currToken != null ) && ( _exposedContext == null ) ) { if ( ! _includePath ) { if ( _currToken . isStructEnd ( ) ) { if ( _headContext . isStartHandled ( ) ) { return ( _currToken = null ) ; } } else if ( _currToken . isScalarValue ( ) ) { if ( ! _headContext . isStartHandled ( ) && ( _itemFilter == TokenFilter . INCLUDE_ALL ) ) { return ( _currToken = null ) ; } } } } TokenFilterContext ctxt = _exposedContext ; if ( ctxt != null ) { while ( true ) { JsonToken t = ctxt . nextTokenToRead ( ) ; if ( t != null ) { _currToken = t ; return t ; } if ( ctxt == _headContext ) { _exposedContext = null ; if ( ctxt . inArray ( ) ) { t = delegate . getCurrentToken ( ) ; _currToken = t ; return t ; } break ; } ctxt = _headContext . findChildOf ( ctxt ) ; _exposedContext = ctxt ; if ( ctxt == null ) { throw _constructError ( \"Unexpected problem: chain of filtered context broken\" ) ; } } } JsonToken t = delegate . nextToken ( ) ; if ( t == null ) { _currToken = t ; return t ; } TokenFilter f ; switch ( t . id ( ) ) { case ID_START_ARRAY : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildArrayContext ( f , true ) ; return ( _currToken = t ) ; } if ( f == null ) { delegate . skipChildren ( ) ; break ; } f = _headContext . checkValue ( f ) ; if ( f == null ) { delegate . skipChildren ( ) ; break ; } if ( f != TokenFilter . INCLUDE_ALL ) { f = f . filterStartArray ( ) ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildArrayContext ( f , true ) ; return ( _currToken = t ) ; } _headContext = _headContext . createChildArrayContext ( f , false ) ; if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; case ID_START_OBJECT : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildObjectContext ( f , true ) ; return ( _currToken = t ) ; } if ( f == null ) { delegate . skipChildren ( ) ; break ; } f = _headContext . checkValue ( f ) ; if ( f == null ) { delegate . skipChildren ( ) ; break ; } if ( f != TokenFilter . INCLUDE_ALL ) { f = f . filterStartObject ( ) ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildObjectContext ( f , true ) ; return ( _currToken = t ) ; } _headContext = _headContext . createChildObjectContext ( f , false ) ; if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; case ID_END_ARRAY : case ID_END_OBJECT : { boolean returnEnd = _headContext . isStartHandled ( ) ; f = _headContext . getFilter ( ) ; if ( ( f != null ) && ( f != TokenFilter . INCLUDE_ALL ) ) { f . filterFinishArray ( ) ; } _headContext = _headContext . getParent ( ) ; _itemFilter = _headContext . getFilter ( ) ; if ( returnEnd ) { return ( _currToken = t ) ; } } break ; case ID_FIELD_NAME : { final String name = delegate . getCurrentName ( ) ; f = _headContext . setFieldName ( name ) ; if ( f == TokenFilter . INCLUDE_ALL ) { _itemFilter = f ; if ( ! _includePath ) { if ( _includeImmediateParent && ! _headContext . isStartHandled ( ) ) { t = _headContext . nextTokenToRead ( ) ; _exposedContext = _headContext ; } } return ( _currToken = t ) ; } if ( f == null ) { delegate . nextToken ( ) ; delegate . skipChildren ( ) ; break ; } f = f . includeProperty ( name ) ; if ( f == null ) { delegate . nextToken ( ) ; delegate . skipChildren ( ) ; break ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { if ( _includePath ) { return ( _currToken = t ) ; } } if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; } default : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { return ( _currToken = t ) ; } if ( f != null ) { f = _headContext . checkValue ( f ) ; if ( ( f == TokenFilter . INCLUDE_ALL ) || ( ( f != null ) && f . includeValue ( delegate ) ) ) { return ( _currToken = t ) ; } } break ; } return _nextToken2 ( ) ; }",
        "summary": "`FilteringParserDelegate` seems to miss last closing `END_OBJECT`",
        "Description": "(note: adding a failing test for this case)\r\n\r\nLooks like with settings like:\r\n\r\n```java\r\n        JsonParser p = new FilteringParserDelegate(p0,\r\n               new NameMatchFilter(\"value\"),\r\n                   true, // includePath\r\n                   false // multipleMatches\r\n                );\r\n```\r\n\r\nand input\r\n\r\n```json\r\n{\r\n  \"a\":123,\r\n  \"array\":[1,2],\r\n  \"ob\": {\r\n    \"value0\":2,\r\n    \"value\":3,\r\n    \"value2\":4\r\n  },\r\n  \"b\":true\r\n}\r\n```\r\n\r\noutput will be like:\r\n\r\n```json\r\n{\"ob\":{\"value\":3}\r\n```\r\n\r\n(note the missing trailing `}` for closing `END_OBJECT`)\r\n\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "@cowtowncoder seems like `FilteringParserDelegate.nextToken` was missing the `includePath` check when `allowMultipleMatches` was `false`. That's why the it was returning prematurely with null. Will [this fix](https://github.com/rfoltyns/jackson-core/commit/efdbde902a89239fc4dc573a76eb2e425ae87f83) do the trick?\r\n\r\nIt's not the only issue I've found in the filtering, but.. one thing at a time."
            },
            {
                "content": "@rfoltyns I'll have to check this out. If this fix works wrt existing test it's a good start. Filtering code is unfortunately rather complicated and somewhat difficult to debug."
            },
            {
                "content": "@rfoltyns Applying patch, as it does fix tests. Wish I had more time to step through to verify for sure, but will have to do for now.\r\n"
            },
            {
                "content": "My patch has broken `allowMultipleMatches` functionality.. It doesn't work with `includePath: true` (still fine for false, though..).\r\nTests passed because they were going through the standard path again, looking for more matches, but provided JSON didn't cover this scenario.\r\nI'll try to fix it as soon as possible, but until then, it might be safer to revert it."
            },
            {
                "content": "I've added more tests and it turned out that it was broken before the patch. I'll create a separate issue for that."
            },
            {
                "content": "@rfoltyns I undid patch, moved failing tests under `failing`."
            }
        ],
        "summarized_discussion": "\n\nThe bug in the source code was fixed by applying the patch from @rfoltyns, which solved the issue of the `FilteringParserDelegate.nextToken` missing the `includePath` check when `allowMultipleMatches` was `false`. However, it was discovered that the patch also broke the `allowMultipleMatches` functionality when `includePath` was `true`. The patch was then undone, and the failing tests were moved under the `failing` category. A separate issue was created to address the issue that was broken before the patch."
    },
    "Math_32_src/main/java/org/apache/commons/math3/geometry/euclidean/twod/PolygonsSet.java_129_181": {
        "src": "@Override\n    protected void computeGeometricalProperties() {\n\n        final Vector2D[][] v = getVertices();\n\n        if (v.length == 0) {\n            final BSPTree<Euclidean2D> tree = getTree(false);\n            if ((Boolean) tree.getAttribute()) {\n                // the instance covers the whole space\n                setSize(Double.POSITIVE_INFINITY);\n                setBarycenter(Vector2D.NaN);\n            } else {\n                setSize(0);\n                setBarycenter(new Vector2D(0, 0));\n            }\n        } else if (v[0][0] == null) {\n            // there is at least one open-loop: the polygon is infinite\n            setSize(Double.POSITIVE_INFINITY);\n            setBarycenter(Vector2D.NaN);\n        } else {\n            // all loops are closed, we compute some integrals around the shape\n\n            double sum  = 0;\n            double sumX = 0;\n            double sumY = 0;\n\n            for (Vector2D[] loop : v) {\n                double x1 = loop[loop.length - 1].getX();\n                double y1 = loop[loop.length - 1].getY();\n                for (final Vector2D point : loop) {\n                    final double x0 = x1;\n                    final double y0 = y1;\n                    x1 = point.getX();\n                    y1 = point.getY();\n                    final double factor = x0 * y1 - y0 * x1;\n                    sum  += factor;\n                    sumX += factor * (x0 + x1);\n                    sumY += factor * (y0 + y1);\n                }\n            }\n\n            if (sum < 0) {\n                // the polygon as a finite outside surrounded by an infinite inside\n                setSize(Double.POSITIVE_INFINITY);\n                setBarycenter(Vector2D.NaN);\n            } else {\n                setSize(sum / 2);\n                setBarycenter(new Vector2D(sumX / (3 * sum), sumY / (3 * sum)));\n            }\n\n        }\n\n    }",
        "src_wo_comments": "@ Override protected void computeGeometricalProperties ( ) { final Vector2D [ ] [ ] v = getVertices ( ) ; if ( v . length == 0 ) { final BSPTree < Euclidean2D > tree = getTree ( false ) ; if ( ( Boolean ) tree . getAttribute ( ) ) { setSize ( Double . POSITIVE_INFINITY ) ; setBarycenter ( Vector2D . NaN ) ; } else { setSize ( 0 ) ; setBarycenter ( new Vector2D ( 0 , 0 ) ) ; } } else if ( v [ 0 ] [ 0 ] == null ) { setSize ( Double . POSITIVE_INFINITY ) ; setBarycenter ( Vector2D . NaN ) ; } else { double sum = 0 ; double sumX = 0 ; double sumY = 0 ; for ( Vector2D [ ] loop : v ) { double x1 = loop [ loop . length - 1 ] . getX ( ) ; double y1 = loop [ loop . length - 1 ] . getY ( ) ; for ( final Vector2D point : loop ) { final double x0 = x1 ; final double y0 = y1 ; x1 = point . getX ( ) ; y1 = point . getY ( ) ; final double factor = x0 * y1 - y0 * x1 ; sum += factor ; sumX += factor * ( x0 + x1 ) ; sumY += factor * ( y0 + y1 ) ; } } if ( sum < 0 ) { setSize ( Double . POSITIVE_INFINITY ) ; setBarycenter ( Vector2D . NaN ) ; } else { setSize ( sum / 2 ) ; setBarycenter ( new Vector2D ( sumX / ( 3 * sum ) , sumY / ( 3 * sum ) ) ) ; } } }",
        "fixed_src": "@Override\n    protected void computeGeometricalProperties() {\n\n        final Vector2D[][] v = getVertices();\n\n        if (v.length == 0) {\n            final BSPTree<Euclidean2D> tree = getTree(false);\n            if (tree.getCut() == null && (Boolean) tree.getAttribute()) {\n                // the instance covers the whole space\n                setSize(Double.POSITIVE_INFINITY);\n                setBarycenter(Vector2D.NaN);\n            } else {\n                setSize(0);\n                setBarycenter(new Vector2D(0, 0));\n            }\n        } else if (v[0][0] == null) {\n            // there is at least one open-loop: the polygon is infinite\n            setSize(Double.POSITIVE_INFINITY);\n            setBarycenter(Vector2D.NaN);\n        } else {\n            // all loops are closed, we compute some integrals around the shape\n\n            double sum  = 0;\n            double sumX = 0;\n            double sumY = 0;\n\n            for (Vector2D[] loop : v) {\n                double x1 = loop[loop.length - 1].getX();\n                double y1 = loop[loop.length - 1].getY();\n                for (final Vector2D point : loop) {\n                    final double x0 = x1;\n                    final double y0 = y1;\n                    x1 = point.getX();\n                    y1 = point.getY();\n                    final double factor = x0 * y1 - y0 * x1;\n                    sum  += factor;\n                    sumX += factor * (x0 + x1);\n                    sumY += factor * (y0 + y1);\n                }\n            }\n\n            if (sum < 0) {\n                // the polygon as a finite outside surrounded by an infinite inside\n                setSize(Double.POSITIVE_INFINITY);\n                setBarycenter(Vector2D.NaN);\n            } else {\n                setSize(sum / 2);\n                setBarycenter(new Vector2D(sumX / (3 * sum), sumY / (3 * sum)));\n            }\n\n        }\n\n    }",
        "fixed_src_wo_comments": "@ Override protected void computeGeometricalProperties ( ) { final Vector2D [ ] [ ] v = getVertices ( ) ; if ( v . length == 0 ) { final BSPTree < Euclidean2D > tree = getTree ( false ) ; if ( tree . getCut ( ) == null && ( Boolean ) tree . getAttribute ( ) ) { setSize ( Double . POSITIVE_INFINITY ) ; setBarycenter ( Vector2D . NaN ) ; } else { setSize ( 0 ) ; setBarycenter ( new Vector2D ( 0 , 0 ) ) ; } } else if ( v [ 0 ] [ 0 ] == null ) { setSize ( Double . POSITIVE_INFINITY ) ; setBarycenter ( Vector2D . NaN ) ; } else { double sum = 0 ; double sumX = 0 ; double sumY = 0 ; for ( Vector2D [ ] loop : v ) { double x1 = loop [ loop . length - 1 ] . getX ( ) ; double y1 = loop [ loop . length - 1 ] . getY ( ) ; for ( final Vector2D point : loop ) { final double x0 = x1 ; final double y0 = y1 ; x1 = point . getX ( ) ; y1 = point . getY ( ) ; final double factor = x0 * y1 - y0 * x1 ; sum += factor ; sumX += factor * ( x0 + x1 ) ; sumY += factor * ( y0 + y1 ) ; } } if ( sum < 0 ) { setSize ( Double . POSITIVE_INFINITY ) ; setBarycenter ( Vector2D . NaN ) ; } else { setSize ( sum / 2 ) ; setBarycenter ( new Vector2D ( sumX / ( 3 * sum ) , sumY / ( 3 * sum ) ) ) ; } } }",
        "summary": "BSPTree class and recovery of a Euclidean 3D BRep",
        "Description": "New to the work here. Thanks for your efforts on this code.\n\nI create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem.\n\nAny ideas?\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-780",
        "comments": [
            "Looks like I had an index flipped resulting in an improperly oriented face for the tetrahedron. I did get the tetrahedron to work. However, I am now struggling with getting a cube to work and I have verified the normals/orientations of the planes to be correct.",
            "The code in BSPMesh2.java produces the following error when I run it. If you comment in the line that re-assigns the coordinate data to be cubeCoords1, then the code works fine. The only difference in the two data sets is that one coordinate has changed by a small amount.\n\nException in thread \"main\" java.lang.ClassCastException: org.apache.commons.math3.geometry.partitioning.BoundaryAttribute cannot be cast to java.lang.Boolean\n\tat org.apache.commons.math3.geometry.euclidean.twod_exact.PolygonsSet.computeGeometricalProperties(PolygonsSet.java:135)\n\tat org.apache.commons.math3.geometry.partitioning.AbstractRegion.getSize(AbstractRegion.java:380)\n\tat org.apache.commons.math3.geometry.euclidean.threed_exact.PolyhedronsSet$FacetsContributionVisitor.addContribution(PolyhedronsSet.java:171)\n\tat org.apache.commons.math3.geometry.euclidean.threed_exact.PolyhedronsSet$FacetsContributionVisitor.visitInternalNode(PolyhedronsSet.java:153)\n\tat org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:262)\n\tat org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:261)\n\tat org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:261)\n\tat org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:263)\n\tat org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:261)\n\tat org.apache.commons.math3.geometry.euclidean.threed_exact.PolyhedronsSet.computeGeometricalProperties(PolyhedronsSet.java:118)\n\tat org.apache.commons.math3.geometry.partitioning.AbstractRegion.getSize(AbstractRegion.java:380)\n\tat datastructures.j3d.bsptree.BSPMesh.<init>(BSPMesh.java:130)\n\tat datastructures.j3d.bsptree.BSPMesh2.main(BSPMesh2.java:206)\n",
            "Hi Andrew,\n\nThe file BSPMesh2.java you attached to the issue is not sufficient to reproduce the error. It refers to a BSPMesh class and to a PolygonOfPoints class. It seems changing BSPMesh to BSPMesh2 is sufficient to solver the references in the main method (but I am not sure this class is a drop-in replacement for your issue), but I have no clue about the other class. Could you attach it to the issue too ?",
            "New version. Hopefully no missing references. I have removed references to the PolygonOfPoints class and fixed the constructor error. I have created a class specifically for the bug report and apologies for not making it cleaner/more clear.\n\nI'm working on an exact arithmetic version of BSPTrees for CSG operations. I hope to contribute the code if it's of value to the project. Initially i'll use Java's BigDecimal classes for arbitrary arithmetic but I plan on performance enhancements per JR Shewchuk's work (orient3d() etc., http://www.cs.cmu.edu/~quake/robust.html).\n\nhope this code (latest version of BSPMesh2.java) helps.",
            "Thanks for the new version. I was able to reproduce the problem. However, I don't understand what you are trying to achieve here.\n\nIn the double loop that creates the 2D SubLine instances, I have added some print statement to see the indices and the start and end points of the built lines. Here is the output I get:\n\n{noformat}\nidxA = 0, idxB = 2, idxC = 4\n   adding SubLine: {1; -1} {-1; 1}\n   adding SubLine: {-1; 1} {-1; 1}\n   adding SubLine: {-1; 1} {1; -1}\nidxA = 2, idxB = 4, idxC = 6\n   adding SubLine: {-1; 1} {-1; 1}\n   adding SubLine: {-1; 1} {-1; -1}\n   adding SubLine: {-1; -1} {-1; 1}\nidxA = 8, idxB = 14, idxC = 12\n   adding SubLine: {1; -1} {-1; 1}\n   adding SubLine: {-1; 1} {1; 1}\n   adding SubLine: {1; 1} {1; -1}\nidxA = 14, idxB = 12, idxC = 10\n   adding SubLine: {-1; 1} {1; 1}\n   adding SubLine: {1; 1} {-1; -1}\n   adding SubLine: {-1; -1} {-1; 1}\nidxA = 0, idxB = 8, idxC = 10\n   adding SubLine: {1; -1} {1; -1}\n   adding SubLine: {1; -1} {-1; -1}\n   adding SubLine: {-1; -1} {1; -1}\nidxA = 8, idxB = 10, idxC = 2\n   adding SubLine: {1; -1} {-1; -1}\n   adding SubLine: {-1; -1} {-1; 1}\n   adding SubLine: {-1; 1} {1; -1}\nidxA = 2, idxB = 10, idxC = 12\n   adding SubLine: {-1; 1} {-1; -1}\n   adding SubLine: {-1; -1} {1; 1}\n   adding SubLine: {1; 1} {-1; 1}\nidxA = 10, idxB = 12, idxC = 4\n   adding SubLine: {-1; -1} {1; 1}\n   adding SubLine: {1; 1} {-1; 1}\n   adding SubLine: {-1; 1} {-1; -1}\nidxA = 4, idxB = 12, idxC = 14\n   adding SubLine: {-1; 1} {1; 1}\n   adding SubLine: {1; 1} {-1; 1}\n   adding SubLine: {-1; 1} {-1; 1}\nidxA = 12, idxB = 14, idxC = 6\n   adding SubLine: {1; 1} {-1; 1}\n   adding SubLine: {-1; 1} {-1; -1}\n   adding SubLine: {-1; -1} {1; 1}\nidxA = 8, idxB = 0, idxC = 6\n   adding SubLine: {1; -1} {1; -1}\n   adding SubLine: {1; -1} {-1; -1}\n   adding SubLine: {-1; -1} {1; -1}\nidxA = 0, idxB = 6, idxC = 14\n   adding SubLine: {1; -1} {-1; -1}\n   adding SubLine: {-1; -1} {-1; 1}\n   adding SubLine: {-1; 1} {1; -1}\nlines.size() = 36\n{noformat}\n\nOnce the 36 sublines have been created, one polygon is created from them.\nIt seems for each indices triplets, 3 lines are created representing a semi-infinite stripe. As they are all packed togeteher before building the single polygon, I guess the algorithm gets lots in all redundant boundary lines.\n\nAre you sure the inner loop should build only 3 Subline instance and not 4, and are you sure only one polygon should be built from all these lines ?",
            "By the output, the index values displayed are all factors of 2. This seems to indicate you are running the constructor for 2D shapes which is commented out with an if (false) {} statement in the main() function.\nI you use the constructor:\n\npublic BSPMesh2(float[] coords, int[] indices)\n\nThen your indices should be factors of 3.\n\nIf you run the 2D shape constructor on the 3D points (cubeIndices, cubeCoords) strange things will occur as you mention.\n\nI've attached yet another BSPMesh2.java with all extraneous code stripped out.\n\nThe code constructs a cube from a set of triangles. Each face (having 4 vertices) is triangulated by adding an edge across the diagonal of the face, i.e., a quad face with vertex indices 0 1 2 3 becomes the following two triplets of indices {0 1 2} {2 1 3} as two triangles.\nHence each face (normally 4 SubLines has 6 subLines (one redundant across the diagonal and these redundant lines have opposing directions (subline 1 2 and subline 2 1)). I am working towards converting generic triangular meshes to BSPTrees (and back to meshes). This is a standard for triangular mesh representations.\n\nI hope this helps clarify.",
            "Sorry, the indices should be {0 1 2} {1 2 3} above and the subline {1 2} will be redundant. I checked the orientation of the planes for the indices above and one of the two triangles is flipped orientation (incorrectly specified indices).",
            "Hi Andrew,\n\nThanks for the explanation, now I understand your code. It would be a nice enhancement to have this in Apache Commons Math!\n\nI think there are still some indices issues. As far as I understand, for each facet of the cube, the current indices do not define two triangles sharing one diagonal, but rather two triangles with the crossing diagonals (i.e. they overlap on some part of the fact, and a quarter of the facet is missing. If I replace the indices array from:\n{noformat}\n{0, 1, 2, 1, 2, 3,\n 4, 7, 6, 7, 6, 5,\n 0, 4, 5, 4, 5, 1,\n 1, 5, 6, 5, 6, 2,\n 2, 6, 7, 6, 7, 3,\n 4, 0, 3, 0, 3, 7}\n{noformat}\n\nto\n{noformat}\n{0, 1, 2, 2, 3, 0,\n 4, 7, 6, 6, 5, 4,\n 0, 4, 5, 5, 1, 0,\n 1, 5, 6, 6, 2, 1,\n 2, 6, 7, 7, 3, 2,\n 4, 0, 3, 3, 7, 4}\n{noformat}\n\nit seems to work (I had no time for thorough testing though). Note the pattern of the change on the last three columns.\n\nCould you tell me if this work for you ang give expected results ?\n",
            "The indices you have above work. Thanks. \nTry these\n{noformat}\n            float[] coordVals = {\n1.000000f, -1.000000f, -1.000000f, \n1.000000f, -1.000000f, 1.000000f, \n-1.000000f, -1.000000f, 1.000000f, \n-1.000000f, -1.000000f, -1.000000f, \n1.000000f, 1.000000f, -1f, \n0.999999f, 1.000000f, 1.000000f, \n-1.000000f, 1.000000f, 1.000000f, \n-1.000000f, 1.000000f, -1.000000f};\nint[] coordIdxs = {\n0, 1, 2, 0, 2, 3, \n4, 7, 6, 4, 6, 5, \n0, 4, 5, 0, 5, 1, \n1, 5, 6, 1, 6, 2, \n2, 6, 7, 2, 7, 3, \n4, 0, 3, 4, 3, 7};\n{noformat}\nThen change the coord 0.999999f to 1.0f as follows:\n{noformat}\nfloat[] coordVals = {\n1.000000f, -1.000000f, -1.000000f, \n1.000000f, -1.000000f, 1.000000f, \n-1.000000f, -1.000000f, 1.000000f, \n-1.000000f, -1.000000f, -1.000000f, \n1.000000f, 1.000000f, -1.000000f, \n1.000000f, 1.000000f, 1.000000f, \n-1.000000f, 1.000000f, 1.000000f, \n-1.000000f, 1.000000f, -1.000000f};\ncoordIdxs = {\n0, 1, 2, 0, 2, 3, \n4, 7, 6, 4, 6, 5, \n0, 4, 5, 0, 5, 1, \n1, 5, 6, 1, 6, 2, \n2, 6, 7, 2, 7, 3, \n4, 0, 3, 4, 3, 7};\n{noformat}\n\nI get an error on the first set of coordinates but not on the second. The indices are the same. This is the original data which gave rise to the bug report.\n\nLet me know what you find.\nthanks,\nandrew\n",
            "I have made some progress analyzing this issue.\nFirst, I confirm there is a problem in the code.\nIt appears that during the build, a very thin triangle occurs on a facet. In the facet plane, the coordinates of the three vertices of this triangle are (0.9999999997583233 -0.999998986721039), (-1.0000000000000002 -0.9999989867210387) and (-1.0000000000000002 -1.0). While extracting the vertices of this triangle, the public getVertices method in PolygonsSet first build the segments and then calls the private followLoop method to identify their topology (i.e. how they connect to each other). The segments are properly built and identified, but the followLoop method fails to connect them. It hits a dedicated conditional statement considering the loop is below some predefined threshold (despite it really is above this threshold) and it ignores the triangle completely. Later, this breaks things as a node in the tree without a boundary must be a leaf node which must contain a boolean attribute. However, here we are not a a leaf node, we are at an internal node with an ignored boundary.\n\nSo I think there are two bugs here. One bug for not identifying correctly the triangle, and one bug for misclassifying an internal node as a leaf node.\n\nThe triangle identification bug will probably take some time to fix. The internal/leaf node bug will probably be easy to fix.\n\nI'll look into that, hopefully in the next few days.\n",
            "Thanks for your input. I am still very much interested in resolving this. I'll keep an eye out for your reply.",
            "Fixed in subversion repository as of r1337929.\n\nThe fix is an ugly one. I have only prevented the wrong cast since boolean attributes occur only at leaf nodes in the tree (internal nodes have different attributes, related to the boundary).\n\nThe roots of the problem are much deeper than that, and need more thoughts."
        ],
        "summarized_discussion": "\n\nThe bug in the source code was caused by incorrect indices being used to construct a cube. The indices should be factors of 3 instead of factors of 2. The fix was to comment out the constructor for 2D shapes and use the constructor for 3D shapes. This was done by changing the indices array and replacing the coordinate 0.999999f to 1.0f. The fix was committed to the subversion repository as of r1337929."
    },
    "Cli_25_src/java/org/apache/commons/cli/HelpFormatter.java_809_851": {
        "src": "protected StringBuffer renderWrappedText(StringBuffer sb, int width, \n                                             int nextLineTabStop, String text)\n    {\n        int pos = findWrapPos(text, width, 0);\n\n        if (pos == -1)\n        {\n            sb.append(rtrim(text));\n\n            return sb;\n        }\n        sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n\n        if (nextLineTabStop >= width)\n        {\n            // stops infinite loop happening\n            nextLineTabStop = width - 1;\n        }\n\n        // all following lines must be padded with nextLineTabStop space \n        // characters\n        final String padding = createPadding(nextLineTabStop);\n\n        while (true)\n        {\n            text = padding + text.substring(pos).trim();\n            pos = findWrapPos(text, width, 0);\n\n            if (pos == -1)\n            {\n                sb.append(text);\n\n                return sb;\n            }\n            \n            if ( (text.length() > width) && (pos == nextLineTabStop - 1) ) \n            {\n                pos = width;\n            }\n\n            sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n        }\n    }",
        "src_wo_comments": "protected StringBuffer renderWrappedText ( StringBuffer sb , int width , int nextLineTabStop , String text ) { int pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( rtrim ( text ) ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; if ( nextLineTabStop >= width ) { nextLineTabStop = width - 1 ; } final String padding = createPadding ( nextLineTabStop ) ; while ( true ) { text = padding + text . substring ( pos ) . trim ( ) ; pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( text ) ; return sb ; } if ( ( text . length ( ) > width ) && ( pos == nextLineTabStop - 1 ) ) { pos = width ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; } }",
        "fixed_src": "protected StringBuffer renderWrappedText(StringBuffer sb, int width, \n                                             int nextLineTabStop, String text)\n    {\n        int pos = findWrapPos(text, width, 0);\n\n        if (pos == -1)\n        {\n            sb.append(rtrim(text));\n\n            return sb;\n        }\n        sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n\n        if (nextLineTabStop >= width)\n        {\n            // stops infinite loop happening\n            nextLineTabStop = 1;\n        }\n\n        // all following lines must be padded with nextLineTabStop space \n        // characters\n        final String padding = createPadding(nextLineTabStop);\n\n        while (true)\n        {\n            text = padding + text.substring(pos).trim();\n            pos = findWrapPos(text, width, 0);\n\n            if (pos == -1)\n            {\n                sb.append(text);\n\n                return sb;\n            }\n            \n            if ( (text.length() > width) && (pos == nextLineTabStop - 1) ) \n            {\n                pos = width;\n            }\n\n            sb.append(rtrim(text.substring(0, pos))).append(defaultNewLine);\n        }\n    }",
        "fixed_src_wo_comments": "protected StringBuffer renderWrappedText ( StringBuffer sb , int width , int nextLineTabStop , String text ) { int pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( rtrim ( text ) ) ; return sb ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; if ( nextLineTabStop >= width ) { nextLineTabStop = 1 ; } final String padding = createPadding ( nextLineTabStop ) ; while ( true ) { text = padding + text . substring ( pos ) . trim ( ) ; pos = findWrapPos ( text , width , 0 ) ; if ( pos == - 1 ) { sb . append ( text ) ; return sb ; } if ( ( text . length ( ) > width ) && ( pos == nextLineTabStop - 1 ) ) { pos = width ; } sb . append ( rtrim ( text . substring ( 0 , pos ) ) ) . append ( defaultNewLine ) ; } }",
        "summary": "infinite loop in the wrapping code of HelpFormatter",
        "Description": "If there is not enough space to display a word on a single line, HelpFormatter goes into a infinite loops until the JVM crashes with an OutOfMemoryError.\n\nTest case:\n\n{code}\nOptions options = new Options();\noptions.addOption(\"h\", \"help\", false, \"This is a looooong description\");\n\nHelpFormatter formatter = new HelpFormatter();\nformatter.setWidth(20);\nformatter.printHelp(\"app\", options); // hang & crash\n{code}\n\nAn helpful exception indicating the insufficient width would be more appropriate than an OutOfMemoryError.",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-162",
        "comments": [
            "svn ci -m \"Changing the current OutOfMemoryError to a RuntimeException per CLI-162. A new ticket for the RuntimeException is at CLI-174\" \n\nSending        src/java/org/apache/commons/cli/HelpFormatter.java\nAdding         src/test/org/apache/commons/cli/bug/BugCLI162Test.java\nTransmitting file data ..\nCommitted revision 735257.",
            "Committed new test in BugCLI162Test. The class BugCLI162Test is not invoked during a Maven build BTW.",
            "\nTwo of the options appear to be problematic in CLI162.\n\nThe first is OPT_PASSWORD. In this the url is longer than the allowed width of the screen, so some kind of failure needs to happen - or the url needs to be mercilessly chopped. This is the one that goes into an infinite loop due to CLI-151. My gut feeling from the code is that CLI-151 didn't introduce this bug, but you had to be significantly longer to hit the bug before (ie: printTabStop longer than the width).\n\nThe second is OPT_PARAM_TYPES_INT + OPT_PARAM_TYPES_NAME, it shows the the patch for this ticket contained a bug when the lastPos happened to equal the firstPos for completely normal reasons. I'd missed that pos was already set to a real value when the loop was begun and not to 0. I'm not sure why both options have to be set for this to happen. ",
            "Attaching patch that rolls back the previous RuntimeException throwing. The if statement in that patch was testing the wrong condition. This patch adds the correct condition, and rather than throwing an exception the text is simply outputted irregardless of the fact it is over the width. What should happen is debatable here - due to a side-effect of CLI-151, we can't do anything aggressive here because things were printing out happily if they were under width + printTabStop. Our options are either to just print, or to forcibly break the text. \n\nThe test code no longer expects to get a RuntimeException.",
            "svn ci -m \"Applying my second attempt at a patch to CLI-162. This fixes Gary's reported bug (one of which was an example of CLI-162, and one a bug in my first attempt to patch). Open question is whether to output text that is too long, or try and break it up to fit the screen width. \"\n\nSending        src/java/org/apache/commons/cli/HelpFormatter.java\nSending        src/test/org/apache/commons/cli/bug/BugCLI162Test.java\nTransmitting file data ..\nCommitted revision 745388.",
            "Ended up modifying this to break it up to fit screen width.\n\nDiscovered another infinite loop issue if the entered width is smaller than the nextTabStop (ie: argument + indent). ",
            "Throw IllegalStateException on last infinite loop test case.\n\n\nsvn ci -m \"Applying additional patch to throw IllegalStateException when the speci\nfied width is not enough to fit the flags, indent and 1 character for the description. This closes out CLI-162 (for now :) ). \"\nSending        src/java/org/apache/commons/cli/HelpFormatter.java\nSending        src/test/org/apache/commons/cli/bug/BugCLI162Test.java\nTransmitting file data ..\nCommitted revision 746137."
        ],
        "summarized_discussion": "\n\nThe bug in CLI-162 was solved by changing the OutOfMemoryError to a RuntimeException, adding a new test in BugCLI162Test, and modifying the code to break up the text to fit the screen width. Additionally, an IllegalStateException was thrown when the specified width was not enough to fit the flags, indent and 1 character for the description."
    },
    "Cli_3_src/java/org/apache/commons/cli/TypeHandler.java_158_170": {
        "src": "public static Number createNumber(String str)\n    {\n        try\n        {\n            return NumberUtils.createNumber(str);\n        }\n        catch (NumberFormatException nfe)\n        {\n            System.err.println(nfe.getMessage());\n        }\n\n        return null;\n    }",
        "src_wo_comments": "public static Number createNumber ( String str ) { try { return NumberUtils . createNumber ( str ) ; } catch ( NumberFormatException nfe ) { System . err . println ( nfe . getMessage ( ) ) ; } return null ; }",
        "fixed_src": "public static Number createNumber(String str)\n    {\n        try\n        {\n            if( str != null )\n            {\n                if( str.indexOf('.') != -1 )\n                {\n                    return Double.valueOf(str);\n                }\n                else\n                {\n                    return Long.valueOf(str);\n                }\n            }\n        }\n        catch (NumberFormatException nfe)\n        {\n            System.err.println(nfe.getMessage());\n        }\n\n        return null;\n    }",
        "fixed_src_wo_comments": "public static Number createNumber ( String str ) { try { if ( str != null ) { if ( str . indexOf ( '.' ) != - 1 ) { return Double . valueOf ( str ) ; } else { return Long . valueOf ( str ) ; } } } catch ( NumberFormatException nfe ) { System . err . println ( nfe . getMessage ( ) ) ; } return null ; }",
        "summary": "PosixParser interupts \"-target opt\" as \"-t arget opt\"",
        "Description": "This was posted on the Commons-Developer list and confirmed as a bug.\n\n> Is this a bug?  Or am I using this incorrectly?\n> I have an option with short and long values.  Given code that is \n> essentially what is below, with a PosixParser I see results as \n> follows:\n> \n> A command line with just \"-t\" prints out the results of the catch \n> block\n> (OK)\n> A command line with just \"-target\" prints out the results of the catch\n> block (OK)\n> A command line with just \"-t foobar.com\" prints out \"processing selected\n> target: foobar.com\" (OK)\n> A command line with just \"-target foobar.com\" prints out \"processing\n> selected target: arget\" (ERROR?)\n> \n> ======================================================================\n> ==\n> =======================\n>   private static final String OPTION_TARGET = \"t\";\n>   private static final String OPTION_TARGET_LONG = \"target\";\n>   // ...\n>   Option generateTarget = new Option(OPTION_TARGET, \n>                                      OPTION_TARGET_LONG, \n>                                      true, \n>                                      \"Generate files for the specified\n> target machine\");\n>   // ...\n>   try {\n>         parsedLine = parser.parse(cmdLineOpts, args);\n>       } catch (ParseException pe) {\n>         System.out.println(\"Invalid command: \" + pe.getMessage() +\n> \"\\n\");\n>         HelpFormatter hf = new HelpFormatter();\n>         hf.printHelp(USAGE, cmdLineOpts);\n>         System.exit(-1);\n>       }\n> \n>   if (parsedLine.hasOption(OPTION_TARGET)) {\n>     System.out.println(\"processing selected target: \" +\n> parsedLine.getOptionValue(OPTION_TARGET));        \n>   }\n\nIt is a bug but it is due to well defined behaviour (so that makes me feel a\nlittle better about myself ;).  To support *special* \n(well I call them special anyway) like -Dsystem.property=value we need to be\nable to examine the first character of an option.  If the first character is\nitself defined as an Option then the remainder of the token is used as the\nvalue, e.g. 'D' is the token, it is an option so 'system.property=value' is the\nargument value for that option.  This is the behaviour that we are seeing for\nyour example.  \n't' is the token, it is an options so 'arget' is the argument value.  \n\nI suppose a solution to this could be to have a way to specify properties for\nparsers.  In this case 'posix.special.option == true' for turning \non *special* options. I'll have a look into this and let you know.\n\nJust to keep track of this and to get you used to how we operate, can you log a\nbug in bugzilla for this.\n\nThanks,\n-John K",
        "issue_url": "https://issues.apache.org/jira//browse/cli-1",
        "comments": [
            "Joe,\n\nMy fault on this one.  Its isn't a bug.  I read this first and thought that it\nwas two separate Options i.e. one '-t' and one '-target'.  It is in fact the\nsame Option with '-t' as the shortOpt and '--target' as the long opt.  So I\napologise for my incorrect analysis of the orginal problem.  To use 'target' on\nthe command line you should prefix it with \"--\" which is the way to do this\nusing the PosixParser.  To use options of the style '-target' you need to use\nthe GnuParser.\n\n-John K"
        ],
        "summarized_discussion": "\n\nThe source code bug was not actually a bug, but an incorrect analysis of the original problem. The solution is to use the \"--\" prefix when using the PosixParser and the GnuParser when using options of the style '-target'."
    },
    "JacksonDatabind_16_src/main/java/com/fasterxml/jackson/databind/introspect/AnnotationMap.java_107_113": {
        "src": "protected final boolean _add(Annotation ann) {\n        if (_annotations == null) {\n            _annotations = new HashMap<Class<? extends Annotation>,Annotation>();\n        }\n        Annotation previous = _annotations.put(ann.annotationType(), ann);\n        return (previous != null) && previous.equals(ann);\n    }",
        "src_wo_comments": "protected final boolean _add ( Annotation ann ) { if ( _annotations == null ) { _annotations = new HashMap < Class < ? extends Annotation > , Annotation > ( ) ; } Annotation previous = _annotations . put ( ann . annotationType ( ) , ann ) ; return ( previous != null ) && previous . equals ( ann ) ; }",
        "fixed_src": "protected final boolean _add(Annotation ann) {\n        if (_annotations == null) {\n            _annotations = new HashMap<Class<? extends Annotation>,Annotation>();\n        }\n        Annotation previous = _annotations.put(ann.annotationType(), ann);\n        return (previous == null) || !previous.equals(ann);\n    }",
        "fixed_src_wo_comments": "protected final boolean _add ( Annotation ann ) { if ( _annotations == null ) { _annotations = new HashMap < Class < ? extends Annotation > , Annotation > ( ) ; } Annotation previous = _annotations . put ( ann . annotationType ( ) , ann ) ; return ( previous == null ) || ! previous . equals ( ann ) ; }",
        "summary": "Annotation bundles ignored when added to Mixin",
        "Description": "When updating from v 2.4.4 to 2.5.\\* it appears as though annotation bundles created with `@JacksonAnnotationsInside` are ignored when placed on a mixin.  Moving the annotation bundel to the actual class seems to resolve the issue.  Below is a simple test that attempts to rename a property.  I have more complicated test cases that are also failing but this should provide some context.\n\n``` java\npublic class Fun {\n\n    @Test\n    public void test() throws JsonProcessingException {\n        ObjectMapper mapper = new ObjectMapper().addMixIn(Foo.class, FooMixin.class);\n        String result = mapper.writeValueAsString(new Foo(\"result\"));\n        Assert.assertEquals(\"{\\\"bar\\\":\\\"result\\\"}\", result);\n    }\n\n    @Target(value={ ElementType.CONSTRUCTOR, ElementType.FIELD, ElementType.METHOD })\n    @Retention(value=RetentionPolicy.RUNTIME)\n    @JacksonAnnotationsInside\n    @JsonProperty(\"bar\")\n    public @interface ExposeStuff {\n\n    }\n\n    public abstract class FooMixin {\n        @ExposeStuff\n        public abstract String getStuff();\n    }\n\n    public class Foo {\n\n        private String stuff;\n\n        Foo(String stuff) {\n            this.stuff = stuff;\n        }\n\n        public String getStuff() {\n            return stuff;\n        }\n    }\n}\n```\n\nI'm expecting the \"stuff\" property to be serialized as \"bar\".\n\nI apologize I haven't been able to identify the culprit (and perhaps it's in my usage).  Let me know your thoughts. I'm always happy to provide more details!\n",
        "issue_url": null,
        "comments": [
            {
                "content": "That certainly sounds like a bug. Thank you for reporting this!\n"
            },
            {
                "content": "Found the problem at the bowels of code. A simple mismatch in state of a return `boolean` value in one place... interesting that such a simple case was not being tested, although bundles are tested as well as basic mix-ins. Just not combination thereof.\n\nThank you for reporting this; will be in 2.5.4, 2.6.0.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was caused by a mismatch in the state of a return boolean value in one place. The bug will be fixed in versions 2.5.4 and 2.6.0."
    },
    "Math_24_src/main/java/org/apache/commons/math3/optimization/univariate/BrentOptimizer.java_108_271": {
        "src": "@Override\n    protected UnivariatePointValuePair doOptimize() {\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final double lo = getMin();\n        final double mid = getStartValue();\n        final double hi = getMax();\n\n        // Optional additional convergence criteria.\n        final ConvergenceChecker<UnivariatePointValuePair> checker\n            = getConvergenceChecker();\n\n        double a;\n        double b;\n        if (lo < hi) {\n            a = lo;\n            b = hi;\n        } else {\n            a = hi;\n            b = lo;\n        }\n\n        double x = mid;\n        double v = x;\n        double w = x;\n        double d = 0;\n        double e = 0;\n        double fx = computeObjectiveValue(x);\n        if (!isMinim) {\n            fx = -fx;\n        }\n        double fv = fx;\n        double fw = fx;\n\n        UnivariatePointValuePair previous = null;\n        UnivariatePointValuePair current\n            = new UnivariatePointValuePair(x, isMinim ? fx : -fx);\n\n        int iter = 0;\n        while (true) {\n            final double m = 0.5 * (a + b);\n            final double tol1 = relativeThreshold * FastMath.abs(x) + absoluteThreshold;\n            final double tol2 = 2 * tol1;\n\n            // Default stopping criterion.\n            final boolean stop = FastMath.abs(x - m) <= tol2 - 0.5 * (b - a);\n            if (!stop) {\n                double p = 0;\n                double q = 0;\n                double r = 0;\n                double u = 0;\n\n                if (FastMath.abs(e) > tol1) { // Fit parabola.\n                    r = (x - w) * (fx - fv);\n                    q = (x - v) * (fx - fw);\n                    p = (x - v) * q - (x - w) * r;\n                    q = 2 * (q - r);\n\n                    if (q > 0) {\n                        p = -p;\n                    } else {\n                        q = -q;\n                    }\n\n                    r = e;\n                    e = d;\n\n                    if (p > q * (a - x) &&\n                        p < q * (b - x) &&\n                        FastMath.abs(p) < FastMath.abs(0.5 * q * r)) {\n                        // Parabolic interpolation step.\n                        d = p / q;\n                        u = x + d;\n\n                        // f must not be evaluated too close to a or b.\n                        if (u - a < tol2 || b - u < tol2) {\n                            if (x <= m) {\n                                d = tol1;\n                            } else {\n                                d = -tol1;\n                            }\n                        }\n                    } else {\n                        // Golden section step.\n                        if (x < m) {\n                            e = b - x;\n                        } else {\n                            e = a - x;\n                        }\n                        d = GOLDEN_SECTION * e;\n                    }\n                } else {\n                    // Golden section step.\n                    if (x < m) {\n                        e = b - x;\n                    } else {\n                        e = a - x;\n                    }\n                    d = GOLDEN_SECTION * e;\n                }\n\n                // Update by at least \"tol1\".\n                if (FastMath.abs(d) < tol1) {\n                    if (d >= 0) {\n                        u = x + tol1;\n                    } else {\n                        u = x - tol1;\n                    }\n                } else {\n                    u = x + d;\n                }\n\n                double fu = computeObjectiveValue(u);\n                if (!isMinim) {\n                    fu = -fu;\n                }\n\n                // User-defined convergence checker.\n                previous = current;\n                current = new UnivariatePointValuePair(u, isMinim ? fu : -fu);\n\n                if (checker != null) {\n                    if (checker.converged(iter, previous, current)) {\n                        return current;\n                    }\n                }\n\n                // Update a, b, v, w and x.\n                if (fu <= fx) {\n                    if (u < x) {\n                        b = x;\n                    } else {\n                        a = x;\n                    }\n                    v = w;\n                    fv = fw;\n                    w = x;\n                    fw = fx;\n                    x = u;\n                    fx = fu;\n                } else {\n                    if (u < x) {\n                        a = u;\n                    } else {\n                        b = u;\n                    }\n                    if (fu <= fw ||\n                        Precision.equals(w, x)) {\n                        v = w;\n                        fv = fw;\n                        w = u;\n                        fw = fu;\n                    } else if (fu <= fv ||\n                               Precision.equals(v, x) ||\n                               Precision.equals(v, w)) {\n                        v = u;\n                        fv = fu;\n                    }\n                }\n            } else { // Default termination (Brent's criterion).\n                return current;\n            }\n            ++iter;\n        }\n    }",
        "src_wo_comments": "@ Override protected UnivariatePointValuePair doOptimize ( ) { final boolean isMinim = getGoalType ( ) == GoalType . MINIMIZE ; final double lo = getMin ( ) ; final double mid = getStartValue ( ) ; final double hi = getMax ( ) ; final ConvergenceChecker < UnivariatePointValuePair > checker = getConvergenceChecker ( ) ; double a ; double b ; if ( lo < hi ) { a = lo ; b = hi ; } else { a = hi ; b = lo ; } double x = mid ; double v = x ; double w = x ; double d = 0 ; double e = 0 ; double fx = computeObjectiveValue ( x ) ; if ( ! isMinim ) { fx = - fx ; } double fv = fx ; double fw = fx ; UnivariatePointValuePair previous = null ; UnivariatePointValuePair current = new UnivariatePointValuePair ( x , isMinim ? fx : - fx ) ; int iter = 0 ; while ( true ) { final double m = 0.5 * ( a + b ) ; final double tol1 = relativeThreshold * FastMath . abs ( x ) + absoluteThreshold ; final double tol2 = 2 * tol1 ; final boolean stop = FastMath . abs ( x - m ) <= tol2 - 0.5 * ( b - a ) ; if ( ! stop ) { double p = 0 ; double q = 0 ; double r = 0 ; double u = 0 ; if ( FastMath . abs ( e ) > tol1 ) { r = ( x - w ) * ( fx - fv ) ; q = ( x - v ) * ( fx - fw ) ; p = ( x - v ) * q - ( x - w ) * r ; q = 2 * ( q - r ) ; if ( q > 0 ) { p = - p ; } else { q = - q ; } r = e ; e = d ; if ( p > q * ( a - x ) && p < q * ( b - x ) && FastMath . abs ( p ) < FastMath . abs ( 0.5 * q * r ) ) { d = p / q ; u = x + d ; if ( u - a < tol2 || b - u < tol2 ) { if ( x <= m ) { d = tol1 ; } else { d = - tol1 ; } } } else { if ( x < m ) { e = b - x ; } else { e = a - x ; } d = GOLDEN_SECTION * e ; } } else { if ( x < m ) { e = b - x ; } else { e = a - x ; } d = GOLDEN_SECTION * e ; } if ( FastMath . abs ( d ) < tol1 ) { if ( d >= 0 ) { u = x + tol1 ; } else { u = x - tol1 ; } } else { u = x + d ; } double fu = computeObjectiveValue ( u ) ; if ( ! isMinim ) { fu = - fu ; } previous = current ; current = new UnivariatePointValuePair ( u , isMinim ? fu : - fu ) ; if ( checker != null ) { if ( checker . converged ( iter , previous , current ) ) { return current ; } } if ( fu <= fx ) { if ( u < x ) { b = x ; } else { a = x ; } v = w ; fv = fw ; w = x ; fw = fx ; x = u ; fx = fu ; } else { if ( u < x ) { a = u ; } else { b = u ; } if ( fu <= fw || Precision . equals ( w , x ) ) { v = w ; fv = fw ; w = u ; fw = fu ; } else if ( fu <= fv || Precision . equals ( v , x ) || Precision . equals ( v , w ) ) { v = u ; fv = fu ; } } } else { return current ; } ++ iter ; } }",
        "fixed_src": "@Override\n    protected UnivariatePointValuePair doOptimize() {\n        final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n        final double lo = getMin();\n        final double mid = getStartValue();\n        final double hi = getMax();\n\n        // Optional additional convergence criteria.\n        final ConvergenceChecker<UnivariatePointValuePair> checker\n            = getConvergenceChecker();\n\n        double a;\n        double b;\n        if (lo < hi) {\n            a = lo;\n            b = hi;\n        } else {\n            a = hi;\n            b = lo;\n        }\n\n        double x = mid;\n        double v = x;\n        double w = x;\n        double d = 0;\n        double e = 0;\n        double fx = computeObjectiveValue(x);\n        if (!isMinim) {\n            fx = -fx;\n        }\n        double fv = fx;\n        double fw = fx;\n\n        UnivariatePointValuePair previous = null;\n        UnivariatePointValuePair current\n            = new UnivariatePointValuePair(x, isMinim ? fx : -fx);\n\n        int iter = 0;\n        while (true) {\n            final double m = 0.5 * (a + b);\n            final double tol1 = relativeThreshold * FastMath.abs(x) + absoluteThreshold;\n            final double tol2 = 2 * tol1;\n\n            // Default stopping criterion.\n            final boolean stop = FastMath.abs(x - m) <= tol2 - 0.5 * (b - a);\n            if (!stop) {\n                double p = 0;\n                double q = 0;\n                double r = 0;\n                double u = 0;\n\n                if (FastMath.abs(e) > tol1) { // Fit parabola.\n                    r = (x - w) * (fx - fv);\n                    q = (x - v) * (fx - fw);\n                    p = (x - v) * q - (x - w) * r;\n                    q = 2 * (q - r);\n\n                    if (q > 0) {\n                        p = -p;\n                    } else {\n                        q = -q;\n                    }\n\n                    r = e;\n                    e = d;\n\n                    if (p > q * (a - x) &&\n                        p < q * (b - x) &&\n                        FastMath.abs(p) < FastMath.abs(0.5 * q * r)) {\n                        // Parabolic interpolation step.\n                        d = p / q;\n                        u = x + d;\n\n                        // f must not be evaluated too close to a or b.\n                        if (u - a < tol2 || b - u < tol2) {\n                            if (x <= m) {\n                                d = tol1;\n                            } else {\n                                d = -tol1;\n                            }\n                        }\n                    } else {\n                        // Golden section step.\n                        if (x < m) {\n                            e = b - x;\n                        } else {\n                            e = a - x;\n                        }\n                        d = GOLDEN_SECTION * e;\n                    }\n                } else {\n                    // Golden section step.\n                    if (x < m) {\n                        e = b - x;\n                    } else {\n                        e = a - x;\n                    }\n                    d = GOLDEN_SECTION * e;\n                }\n\n                // Update by at least \"tol1\".\n                if (FastMath.abs(d) < tol1) {\n                    if (d >= 0) {\n                        u = x + tol1;\n                    } else {\n                        u = x - tol1;\n                    }\n                } else {\n                    u = x + d;\n                }\n\n                double fu = computeObjectiveValue(u);\n                if (!isMinim) {\n                    fu = -fu;\n                }\n\n                // User-defined convergence checker.\n                previous = current;\n                current = new UnivariatePointValuePair(u, isMinim ? fu : -fu);\n\n                if (checker != null) {\n                    if (checker.converged(iter, previous, current)) {\n                        return best(current, previous, isMinim);\n                    }\n                }\n\n                // Update a, b, v, w and x.\n                if (fu <= fx) {\n                    if (u < x) {\n                        b = x;\n                    } else {\n                        a = x;\n                    }\n                    v = w;\n                    fv = fw;\n                    w = x;\n                    fw = fx;\n                    x = u;\n                    fx = fu;\n                } else {\n                    if (u < x) {\n                        a = u;\n                    } else {\n                        b = u;\n                    }\n                    if (fu <= fw ||\n                        Precision.equals(w, x)) {\n                        v = w;\n                        fv = fw;\n                        w = u;\n                        fw = fu;\n                    } else if (fu <= fv ||\n                               Precision.equals(v, x) ||\n                               Precision.equals(v, w)) {\n                        v = u;\n                        fv = fu;\n                    }\n                }\n            } else { // Default termination (Brent's criterion).\n                return best(current, previous, isMinim);\n            }\n            ++iter;\n        }\n    }",
        "fixed_src_wo_comments": "@ Override protected UnivariatePointValuePair doOptimize ( ) { final boolean isMinim = getGoalType ( ) == GoalType . MINIMIZE ; final double lo = getMin ( ) ; final double mid = getStartValue ( ) ; final double hi = getMax ( ) ; final ConvergenceChecker < UnivariatePointValuePair > checker = getConvergenceChecker ( ) ; double a ; double b ; if ( lo < hi ) { a = lo ; b = hi ; } else { a = hi ; b = lo ; } double x = mid ; double v = x ; double w = x ; double d = 0 ; double e = 0 ; double fx = computeObjectiveValue ( x ) ; if ( ! isMinim ) { fx = - fx ; } double fv = fx ; double fw = fx ; UnivariatePointValuePair previous = null ; UnivariatePointValuePair current = new UnivariatePointValuePair ( x , isMinim ? fx : - fx ) ; int iter = 0 ; while ( true ) { final double m = 0.5 * ( a + b ) ; final double tol1 = relativeThreshold * FastMath . abs ( x ) + absoluteThreshold ; final double tol2 = 2 * tol1 ; final boolean stop = FastMath . abs ( x - m ) <= tol2 - 0.5 * ( b - a ) ; if ( ! stop ) { double p = 0 ; double q = 0 ; double r = 0 ; double u = 0 ; if ( FastMath . abs ( e ) > tol1 ) { r = ( x - w ) * ( fx - fv ) ; q = ( x - v ) * ( fx - fw ) ; p = ( x - v ) * q - ( x - w ) * r ; q = 2 * ( q - r ) ; if ( q > 0 ) { p = - p ; } else { q = - q ; } r = e ; e = d ; if ( p > q * ( a - x ) && p < q * ( b - x ) && FastMath . abs ( p ) < FastMath . abs ( 0.5 * q * r ) ) { d = p / q ; u = x + d ; if ( u - a < tol2 || b - u < tol2 ) { if ( x <= m ) { d = tol1 ; } else { d = - tol1 ; } } } else { if ( x < m ) { e = b - x ; } else { e = a - x ; } d = GOLDEN_SECTION * e ; } } else { if ( x < m ) { e = b - x ; } else { e = a - x ; } d = GOLDEN_SECTION * e ; } if ( FastMath . abs ( d ) < tol1 ) { if ( d >= 0 ) { u = x + tol1 ; } else { u = x - tol1 ; } } else { u = x + d ; } double fu = computeObjectiveValue ( u ) ; if ( ! isMinim ) { fu = - fu ; } previous = current ; current = new UnivariatePointValuePair ( u , isMinim ? fu : - fu ) ; if ( checker != null ) { if ( checker . converged ( iter , previous , current ) ) { return best ( current , previous , isMinim ) ; } } if ( fu <= fx ) { if ( u < x ) { b = x ; } else { a = x ; } v = w ; fv = fw ; w = x ; fw = fx ; x = u ; fx = fu ; } else { if ( u < x ) { a = u ; } else { b = u ; } if ( fu <= fw || Precision . equals ( w , x ) ) { v = w ; fv = fw ; w = u ; fw = fu ; } else if ( fu <= fv || Precision . equals ( v , x ) || Precision . equals ( v , w ) ) { v = u ; fv = fu ; } } } else { return best ( current , previous , isMinim ) ; } ++ iter ; } }",
        "summary": "\"BrentOptimizer\" not always reporting the best point",
        "Description": "{{BrentOptimizer}} (package \"o.a.c.m.optimization.univariate\") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-855",
        "comments": [
            "Proposed patch.\n\nThe (somewhat contrived) unit test fails with the current version of the algorithm.\n",
            "OK to apply?\n",
            "Committed in revision 1381195.",
            "That was not it yet; further improvement committed in revision 1382070, together with a Javadoc update explaining some change wrt the original version of the algorithm.\n",
            "Yet another small change in revision 1382441.\n"
        ],
        "summarized_discussion": "\n\nThe proposed patch was applied in revision 1381195, and further improvements were made in revisions 1382070 and 1382441. The changes included an update to the Javadoc and a small change to the original version of the algorithm."
    },
    "JacksonDatabind_105_src/main/java/com/fasterxml/jackson/databind/deser/std/JdkDeserializers.java_28_50": {
        "src": "public static JsonDeserializer<?> find(Class<?> rawType, String clsName)\n    {\n        if (_classNames.contains(clsName)) {\n            JsonDeserializer<?> d = FromStringDeserializer.findDeserializer(rawType);\n            if (d != null) {\n                return d;\n            }\n            if (rawType == UUID.class) {\n                return new UUIDDeserializer();\n            }\n            if (rawType == StackTraceElement.class) {\n                return new StackTraceElementDeserializer();\n            }\n            if (rawType == AtomicBoolean.class) {\n                // (note: AtomicInteger/Long work due to single-arg constructor. For now?\n                return new AtomicBooleanDeserializer();\n            }\n            if (rawType == ByteBuffer.class) {\n                return new ByteBufferDeserializer();\n            }\n        }\n        return null;\n    }",
        "src_wo_comments": "public static JsonDeserializer < ? > find ( Class < ? > rawType , String clsName ) { if ( _classNames . contains ( clsName ) ) { JsonDeserializer < ? > d = FromStringDeserializer . findDeserializer ( rawType ) ; if ( d != null ) { return d ; } if ( rawType == UUID . class ) { return new UUIDDeserializer ( ) ; } if ( rawType == StackTraceElement . class ) { return new StackTraceElementDeserializer ( ) ; } if ( rawType == AtomicBoolean . class ) { return new AtomicBooleanDeserializer ( ) ; } if ( rawType == ByteBuffer . class ) { return new ByteBufferDeserializer ( ) ; } } return null ; }",
        "fixed_src": "public static JsonDeserializer<?> find(Class<?> rawType, String clsName)\n    {\n        if (_classNames.contains(clsName)) {\n            JsonDeserializer<?> d = FromStringDeserializer.findDeserializer(rawType);\n            if (d != null) {\n                return d;\n            }\n            if (rawType == UUID.class) {\n                return new UUIDDeserializer();\n            }\n            if (rawType == StackTraceElement.class) {\n                return new StackTraceElementDeserializer();\n            }\n            if (rawType == AtomicBoolean.class) {\n                // (note: AtomicInteger/Long work due to single-arg constructor. For now?\n                return new AtomicBooleanDeserializer();\n            }\n            if (rawType == ByteBuffer.class) {\n                return new ByteBufferDeserializer();\n            }\n            if (rawType == Void.class) {\n                return NullifyingDeserializer.instance;\n            }\n        }\n        return null;\n    }",
        "fixed_src_wo_comments": "public static JsonDeserializer < ? > find ( Class < ? > rawType , String clsName ) { if ( _classNames . contains ( clsName ) ) { JsonDeserializer < ? > d = FromStringDeserializer . findDeserializer ( rawType ) ; if ( d != null ) { return d ; } if ( rawType == UUID . class ) { return new UUIDDeserializer ( ) ; } if ( rawType == StackTraceElement . class ) { return new StackTraceElementDeserializer ( ) ; } if ( rawType == AtomicBoolean . class ) { return new AtomicBooleanDeserializer ( ) ; } if ( rawType == ByteBuffer . class ) { return new ByteBufferDeserializer ( ) ; } if ( rawType == Void . class ) { return NullifyingDeserializer . instance ; } } return null ; }",
        "summary": "Illegal reflective access operation warning when using `java.lang.Void` as value type",
        "Description": "I'm using Jackson (**2.9.7**) through Spring's RestTemplate:\r\n\r\n```java\r\nResponseEntity<Void> response = getRestTemplate().exchange(\r\n\t\trequestUrl,\r\n\t\tHttpMethod.PATCH,\r\n\t\tnew HttpEntity<>(dto, authHeaders),\r\n\t\tVoid.class\r\n);\r\n```\r\n\r\nWhen [`Void`](https://docs.oracle.com/javase/7/docs/api/java/lang/Void.html) is used to indicate that the ResponseEntity has no body, the following warning appears in the console:\r\n\r\n```\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.fasterxml.jackson.databind.util.ClassUtil (file:/<snip>repository/com/fasterxml/jackson/core/jackson-databind/2.9.7/jackson-databind-2.9.7.jar) to constructor java.lang.Void()\r\nWARNING: Please consider reporting this to the maintainers of com.fasterxml.jackson.databind.util.ClassUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\n```\r\n\r\nThe problem disappears if `String` is used as generic type. ",
        "issue_url": null,
        "comments": [
            {
                "content": "This forms part of https://github.com/FasterXML/jackson-core/pull/499"
            },
            {
                "content": "Alright, I close the issue then. This was fast :)"
            },
            {
                "content": "It probably also makes sense to add explicit handling for (nominal) type `Void`. Not much point in introspecting it as POJO. I'll actually re-open this to have a look."
            },
            {
                "content": "Hmm, This also definitely justifies open modules until version 3.\r\n\r\njava.lang is part of the java.base module, But it is also strictly named. \r\nCurrently, in order to run these you have one of two options (Because they are not/never going to 'opens java.lang' in the java.lang module)\r\n \r\n1) You can append ``` --add-opens java.base/java.lang=com.fasterxml.jackson.databind ``` which is the strict module name, and the mechanism for downwards exposure. This has been simplified by creating an @arguments file, and executing that way. This is the Strict JPMS Encapsulation, and is where all the performance boosts come from\r\n\r\n2) This one I like currently, and I'll explain how it works and doesn't destroy the module pathing after\r\n```\r\nopen module com.mypackage.myapp {\r\n}\r\n```\r\n\r\nthis does a few things but I'm only going to highlight the features, \r\n\r\n1) The java.base module is now com.mypackage.myapp, and your classes and libraries have been placed in the parent parent module layer. No changes between JRE 8 and JPMS will be felt (so long as your libraries are named. In automatic/unnamed/classpath mode, the module pathing is disabled. \r\n2) java.lang, java.xml, etc etc, as well as all your packages are opened and exported by default (allowing private field and class handlers like annotations/databind to do its thing without explicit definitions.\r\n3) The module pathing is still adhered to, and any other *named* libraries (except yours, because it is now java.base) will still be strictly enforced.\r\n\r\n\r\n\r\n"
            },
            {
                "content": "Ok: as an orthogonal thing, I added handling for `Void` as special type to avoid handling of type as POJO; this should prevent warning. There are already handlers for \"always read/write as`null`\", as it happens, so not much additional work.\r\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to add explicit handling for the nominal type Void, and to add a open module for the java.base module that allows for the java.lang, java.xml, and other packages to be opened and exported by default. This will prevent warnings and allow private field and class handlers to do their thing without explicit definitions."
    },
    "Collections_26_src/main/java/org/apache/commons/collections4/keyvalue/MultiKey.java_277_280": {
        "src": "private Object readResolve() {\n        calculateHashCode(keys);\n        return this;\n    }",
        "src_wo_comments": "private Object readResolve ( ) { calculateHashCode ( keys ) ; return this ; }",
        "fixed_src": "protected Object readResolve() {\n        calculateHashCode(keys);\n        return this;\n    }",
        "fixed_src_wo_comments": "protected Object readResolve ( ) { calculateHashCode ( keys ) ; return this ; }",
        "summary": "MultiKey subclassing has deserialization problem since COLLECTIONS-266: either declare protected readResolve() or MultiKey must be final",
        "Description": "MultiKey from collections 4 provides a transient hashCode and a *private* readResolve to resolve COLLECTIONS-266: Issue with MultiKey when serialized/deserialized via RMI.\n\nUnfortunately the solution does not work in case of *subclassing*: readResolve in MultiKey should be declared *protected* readResolve() to be called during deserialization of the subclass. Otherwise MultiKey must be final to avoid such subclassing.\n\n*Testcase*:\n{code:java|title=MultiKeySerializationTest.java}\npackage de.ivu.test.common.collections4;\n\nimport static org.junit.Assert.assertEquals;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\nimport java.io.ObjectInputStream;\nimport java.io.ObjectOutputStream;\n\nimport org.apache.commons.collections4.keyvalue.MultiKey;\nimport org.junit.Test;\n\npublic class MultiKeySerializationTest {\n\n    @Test\n    @SuppressWarnings(\"unchecked\")\n    public void testReadResolveEqualHashCode()\n            throws IOException, ClassNotFoundException {\n        class MultiKey2<A, B>\n                extends MultiKey {\n\n            private static final long serialVersionUID = 1928896152249821416L;\n\n            public MultiKey2(A key1, B key2) {\n                super(key1, key2);\n            }\n\n            public A getFirst() {\n                return (A) getKey(0);\n            }\n\n            public B getSecond() {\n                return (B) getKey(1);\n            }\n            \n            // FIXME: MultiKey should either declare protected readResolve() or must be final.\n        }\n        MultiKey2<String, String> one = new MultiKey2<>(\"bla\", \"blub\");\n        System.out.println(one.hashCode());\n        ByteArrayOutputStream byteOut = new ByteArrayOutputStream();\n        ObjectOutputStream out = new ObjectOutputStream(byteOut);\n        out.writeObject(one);\n        out.close();\n        byte[] serialized = byteOut.toByteArray();\n        ByteArrayInputStream byteIn = new ByteArrayInputStream(serialized);\n        ObjectInputStream in = new ObjectInputStream(byteIn);\n        MultiKey2<String, String> two = (MultiKey2<String, String>) in.readObject();\n        System.out.println(two.hashCode());\n        assertEquals(\"hashCode must be equal - please check for protected readResolve in MultiKey*\", one.hashCode(),\n            two.hashCode());\n    }\n}\n{code}\n\n*Fix:*\n{code:java|title=MultiKey.java}\n@@ -274,7 +274,7 @@\n      * only stable for the same process).\n      * @return the instance with recalculated hash code\n      */\n-    private Object readResolve() {\n+    protected Object readResolve() {\n         calculateHashCode(keys);\n         return this;\n     }\n{code}",
        "issue_url": "https://issues.apache.org/jira//browse/COLLECTIONS-576",
        "comments": [
            "Fixed in r1705620.\n\nThanks for the report and testcase."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in revision 1705620."
    },
    "Chart_5_source/org/jfree/data/xy/XYSeries.java_540_576": {
        "src": "public XYDataItem addOrUpdate(Number x, Number y) {\n        if (x == null) {\n            throw new IllegalArgumentException(\"Null 'x' argument.\");\n        }\n\n        // if we get to here, we know that duplicate X values are not permitted\n        XYDataItem overwritten = null;\n        int index = indexOf(x);\n        if (index >= 0 && !this.allowDuplicateXValues) {\n            XYDataItem existing = (XYDataItem) this.data.get(index);\n            try {\n                overwritten = (XYDataItem) existing.clone();\n            }\n            catch (CloneNotSupportedException e) {\n                throw new SeriesException(\"Couldn't clone XYDataItem!\");\n            }\n            existing.setY(y);\n        }\n        else {\n            // if the series is sorted, the negative index is a result from\n            // Collections.binarySearch() and tells us where to insert the\n            // new item...otherwise it will be just -1 and we should just\n            // append the value to the list...\n            if (this.autoSort) {\n                this.data.add(-index - 1, new XYDataItem(x, y));\n            }\n            else {\n                this.data.add(new XYDataItem(x, y));\n            }\n            // check if this addition will exceed the maximum item count...\n            if (getItemCount() > this.maximumItemCount) {\n                this.data.remove(0);\n            }\n        }\n        fireSeriesChanged();\n        return overwritten;\n    }",
        "src_wo_comments": "public XYDataItem addOrUpdate ( Number x , Number y ) { if ( x == null ) { throw new IllegalArgumentException ( \"Null 'x' argument.\" ) ; } XYDataItem overwritten = null ; int index = indexOf ( x ) ; if ( index >= 0 && ! this . allowDuplicateXValues ) { XYDataItem existing = ( XYDataItem ) this . data . get ( index ) ; try { overwritten = ( XYDataItem ) existing . clone ( ) ; } catch ( CloneNotSupportedException e ) { throw new SeriesException ( \"Couldn't clone XYDataItem!\" ) ; } existing . setY ( y ) ; } else { if ( this . autoSort ) { this . data . add ( - index - 1 , new XYDataItem ( x , y ) ) ; } else { this . data . add ( new XYDataItem ( x , y ) ) ; } if ( getItemCount ( ) > this . maximumItemCount ) { this . data . remove ( 0 ) ; } } fireSeriesChanged ( ) ; return overwritten ; }",
        "fixed_src": "public XYDataItem addOrUpdate(Number x, Number y) {\n        if (x == null) {\n            throw new IllegalArgumentException(\"Null 'x' argument.\");\n        }\n        if (this.allowDuplicateXValues) {\n            add(x, y);\n            return null;\n        }\n\n        // if we get to here, we know that duplicate X values are not permitted\n        XYDataItem overwritten = null;\n        int index = indexOf(x);\n        if (index >= 0) {\n            XYDataItem existing = (XYDataItem) this.data.get(index);\n            try {\n                overwritten = (XYDataItem) existing.clone();\n            }\n            catch (CloneNotSupportedException e) {\n                throw new SeriesException(\"Couldn't clone XYDataItem!\");\n            }\n            existing.setY(y);\n        }\n        else {\n            // if the series is sorted, the negative index is a result from\n            // Collections.binarySearch() and tells us where to insert the\n            // new item...otherwise it will be just -1 and we should just\n            // append the value to the list...\n            if (this.autoSort) {\n                this.data.add(-index - 1, new XYDataItem(x, y));\n            }\n            else {\n                this.data.add(new XYDataItem(x, y));\n            }\n            // check if this addition will exceed the maximum item count...\n            if (getItemCount() > this.maximumItemCount) {\n                this.data.remove(0);\n            }\n        }\n        fireSeriesChanged();\n        return overwritten;\n    }",
        "fixed_src_wo_comments": "public XYDataItem addOrUpdate ( Number x , Number y ) { if ( x == null ) { throw new IllegalArgumentException ( \"Null 'x' argument.\" ) ; } if ( this . allowDuplicateXValues ) { add ( x , y ) ; return null ; } XYDataItem overwritten = null ; int index = indexOf ( x ) ; if ( index >= 0 ) { XYDataItem existing = ( XYDataItem ) this . data . get ( index ) ; try { overwritten = ( XYDataItem ) existing . clone ( ) ; } catch ( CloneNotSupportedException e ) { throw new SeriesException ( \"Couldn't clone XYDataItem!\" ) ; } existing . setY ( y ) ; } else { if ( this . autoSort ) { this . data . add ( - index - 1 , new XYDataItem ( x , y ) ) ; } else { this . data . add ( new XYDataItem ( x , y ) ) ; } if ( getItemCount ( ) > this . maximumItemCount ) { this . data . remove ( 0 ) ; } } fireSeriesChanged ( ) ; return overwritten ; }",
        "summary": "XYSeries.addOrUpdate() should add if duplicates are allowed",
        "Description": "Copied from this post (by Ted Schwartz) in the forum:\n\nhttp://www.jfree.org/phpBB2/viewtopic.php?t=24523\n\nI've found a bug in jfreechart-1.0.9 code for org.jfree.data.xy.XYSeries. There was a change some time ago which introduced the notion of allowing duplicate X values in XYSeries data. The method addOrUpdate(Number x, Number y) was never modified to support this, and therefore duplicate data were overwriting existing data. This is the fix I've made, but I don't know how to submit a patch...\n\n$ diff original/jfreechart-1.0.9/source/org/jfree/data/xy/XYSeries.java fixed/org/jfree/data/xy/XYSeries.java\n537c537\n< if (index >= 0) {\n---\n> if (index >= 0 && !allowDuplicateXValues) {\n545a546,559\n> } else if (index >= 0){\n> XYDataItem item = new XYDataItem(x, y);\n> // need to make sure we are adding *after* any duplicates\n> int size = this.data.size();\n> while (index < size\n> && item.compareTo(this.data.get(index)) == 0) {\n> index++;\n> }\n> if (index < this.data.size()) {\n> this.data.add(index, item);\n> }\n> else {\n> this.data.add(item);\n> }\n558,561d571\n< // check if this addition will exceed the maximum item count...\n< if (getItemCount() > this.maximumItemCount) {\n< this.data.remove(0);\n< }\n562a573,576\n> // check if this addition will exceed the maximum item count...\n> if (getItemCount() > this.maximumItemCount) {\n> this.data.remove(0);\n> }",
        "issue_url": "https://sourceforge.net/p/jfreechart/bugs/862/",
        "comments": [
            {
                "content": "assigned_to: nobody --> mungady\nstatus: open --> closed-fixed"
            },
            {
                "content": "Logged In: YES\nuser_id=112975\nOriginator: NO\n\nA fix has been committed to Subversion for inclusion in the upcoming 1.0.10 release.\n\nRegards,\n\nDave Gilbert\nJFreeChart Project Leader"
            },
            {
                "content": "Hi,\n\nI think there is still something wrong with this.\n\nIf you have autosort true and allowduplicates true as well, it can happen that the value already exists and it is the first in the list, in which case the index returned would be zero. And we end up trying to add a new value at index -1, which makes it blow up with:\n\njava.lang.IndexOutOfBoundsException: Index: -1, Size: 13\nat java.util.ArrayList.add(ArrayList.java:367)\nat org.jfree.data.xy.XYSeries.addOrUpdate(XYSeries.java:571)\n\nThanks,\nTeodor"
            },
            {
                "content": "priority: 5 --> 9\nstatus: closed-fixed --> open-accepted"
            },
            {
                "content": "Hi Teodor,\n\nYou are right. Thanks for spotting/reporting this, I'll fix it now.\n\nBest regards,\n\nDave"
            },
            {
                "content": "Fix committed to Subversion for inclusion in the 1.0.12 release."
            },
            {
                "content": "status: open-accepted --> closed-fixed"
            }
        ],
        "summarized_discussion": "\n\nThe bug was fixed by committing a fix to Subversion for inclusion in the 1.0.12 release. The assigned_to was changed from nobody to mungady, the status was changed from open to closed-fixed, and the priority was changed from 5 to 9. Finally, the status was changed from open-accepted to closed-fixed."
    },
    "Math_73_src/main/java/org/apache/commons/math/analysis/solvers/BrentSolver.java_98_140": {
        "src": "public double solve(final UnivariateRealFunction f,\n                        final double min, final double max, final double initial)\n        throws MaxIterationsExceededException, FunctionEvaluationException {\n\n        clearResult();\n        verifySequence(min, initial, max);\n\n        // return the initial guess if it is good enough\n        double yInitial = f.value(initial);\n        if (Math.abs(yInitial) <= functionValueAccuracy) {\n            setResult(initial, 0);\n            return result;\n        }\n\n        // return the first endpoint if it is good enough\n        double yMin = f.value(min);\n        if (Math.abs(yMin) <= functionValueAccuracy) {\n            setResult(yMin, 0);\n            return result;\n        }\n\n        // reduce interval if min and initial bracket the root\n        if (yInitial * yMin < 0) {\n            return solve(f, min, yMin, initial, yInitial, min, yMin);\n        }\n\n        // return the second endpoint if it is good enough\n        double yMax = f.value(max);\n        if (Math.abs(yMax) <= functionValueAccuracy) {\n            setResult(yMax, 0);\n            return result;\n        }\n\n        // reduce interval if initial and max bracket the root\n        if (yInitial * yMax < 0) {\n            return solve(f, initial, yInitial, max, yMax, initial, yInitial);\n        }\n\n\n        // full Brent algorithm starting with provided initial guess\n        return solve(f, min, yMin, max, yMax, initial, yInitial);\n\n    }",
        "src_wo_comments": "public double solve ( final UnivariateRealFunction f , final double min , final double max , final double initial ) throws MaxIterationsExceededException , FunctionEvaluationException { clearResult ( ) ; verifySequence ( min , initial , max ) ; double yInitial = f . value ( initial ) ; if ( Math . abs ( yInitial ) <= functionValueAccuracy ) { setResult ( initial , 0 ) ; return result ; } double yMin = f . value ( min ) ; if ( Math . abs ( yMin ) <= functionValueAccuracy ) { setResult ( yMin , 0 ) ; return result ; } if ( yInitial * yMin < 0 ) { return solve ( f , min , yMin , initial , yInitial , min , yMin ) ; } double yMax = f . value ( max ) ; if ( Math . abs ( yMax ) <= functionValueAccuracy ) { setResult ( yMax , 0 ) ; return result ; } if ( yInitial * yMax < 0 ) { return solve ( f , initial , yInitial , max , yMax , initial , yInitial ) ; } return solve ( f , min , yMin , max , yMax , initial , yInitial ) ; }",
        "fixed_src": "public double solve(final UnivariateRealFunction f,\n                        final double min, final double max, final double initial)\n        throws MaxIterationsExceededException, FunctionEvaluationException {\n\n        clearResult();\n        verifySequence(min, initial, max);\n\n        // return the initial guess if it is good enough\n        double yInitial = f.value(initial);\n        if (Math.abs(yInitial) <= functionValueAccuracy) {\n            setResult(initial, 0);\n            return result;\n        }\n\n        // return the first endpoint if it is good enough\n        double yMin = f.value(min);\n        if (Math.abs(yMin) <= functionValueAccuracy) {\n            setResult(yMin, 0);\n            return result;\n        }\n\n        // reduce interval if min and initial bracket the root\n        if (yInitial * yMin < 0) {\n            return solve(f, min, yMin, initial, yInitial, min, yMin);\n        }\n\n        // return the second endpoint if it is good enough\n        double yMax = f.value(max);\n        if (Math.abs(yMax) <= functionValueAccuracy) {\n            setResult(yMax, 0);\n            return result;\n        }\n\n        // reduce interval if initial and max bracket the root\n        if (yInitial * yMax < 0) {\n            return solve(f, initial, yInitial, max, yMax, initial, yInitial);\n        }\n\n        if (yMin * yMax > 0) {\n            throw MathRuntimeException.createIllegalArgumentException(\n                  NON_BRACKETING_MESSAGE, min, max, yMin, yMax);\n        }\n\n        // full Brent algorithm starting with provided initial guess\n        return solve(f, min, yMin, max, yMax, initial, yInitial);\n\n    }",
        "fixed_src_wo_comments": "public double solve ( final UnivariateRealFunction f , final double min , final double max , final double initial ) throws MaxIterationsExceededException , FunctionEvaluationException { clearResult ( ) ; verifySequence ( min , initial , max ) ; double yInitial = f . value ( initial ) ; if ( Math . abs ( yInitial ) <= functionValueAccuracy ) { setResult ( initial , 0 ) ; return result ; } double yMin = f . value ( min ) ; if ( Math . abs ( yMin ) <= functionValueAccuracy ) { setResult ( yMin , 0 ) ; return result ; } if ( yInitial * yMin < 0 ) { return solve ( f , min , yMin , initial , yInitial , min , yMin ) ; } double yMax = f . value ( max ) ; if ( Math . abs ( yMax ) <= functionValueAccuracy ) { setResult ( yMax , 0 ) ; return result ; } if ( yInitial * yMax < 0 ) { return solve ( f , initial , yInitial , max , yMax , initial , yInitial ) ; } if ( yMin * yMax > 0 ) { throw MathRuntimeException . createIllegalArgumentException ( NON_BRACKETING_MESSAGE , min , max , yMin , yMax ) ; } return solve ( f , min , yMin , max , yMax , initial , yInitial ) ; }",
        "summary": "Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign",
        "Description": "Javadoc for \"public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)\" claims that \"if the values of the function at the three points have the same sign\" an IllegalArgumentException is thrown. This case isn't even checked.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-343",
        "comments": [
            "Fixed in subversion repository as of r915517\nThanks for reporting the issue",
            "Thanks for the quick turnaround. One comment: I'm not sure whether the check before throwing the IllegalArgumentException is necessary. You can have only the following situations (given that min <= initial <= max and assuming neither min nor max is a root):\n\n    * yMin and yMax have the same sign:\n        ** yInitial has a different sign: Handled on line 121 (function is not monotonous between min and max)\n        ** yInitial has the same sign: Falls through to line 136 and yMin * yMax > 0 by definition\n    * yMin and yMax do not have the same sign:\n        ** yInitial has the same sign as yMax: Handled on line 121\n        ** yInitial has the same sign as yMin: Handled on line 133\n\nIn this case I'd say code between lines 131 and 142 should be replaced by the throw statement on line 137.",
            "You are right.\nI have removed the unreachable code and committed it in the subversion repository\nThanks again"
        ],
        "summarized_discussion": "\n\nThe solution to the bug was to remove the unreachable code between lines 131 and 142 and commit it in the subversion repository. Thanks were given for reporting the issue and for the quick turnaround."
    },
    "JacksonDatabind_6_src/main/java/com/fasterxml/jackson/databind/util/StdDateFormat.java_359_434": {
        "src": "protected Date parseAsISO8601(String dateStr, ParsePosition pos)\n    {\n        /* 21-May-2009, tatu: DateFormat has very strict handling of\n         * timezone  modifiers for ISO-8601. So we need to do some scrubbing.\n         */\n\n        /* First: do we have \"zulu\" format ('Z' == \"GMT\")? If yes, that's\n         * quite simple because we already set date format timezone to be\n         * GMT, and hence can just strip out 'Z' altogether\n         */\n        int len = dateStr.length();\n        char c = dateStr.charAt(len-1);\n        DateFormat df;\n\n        // [JACKSON-200]: need to support \"plain\" date...\n        if (len <= 10 && Character.isDigit(c)) {\n            df = _formatPlain;\n            if (df == null) {\n                df = _formatPlain = _cloneFormat(DATE_FORMAT_PLAIN, DATE_FORMAT_STR_PLAIN, _timezone, _locale);\n            }\n        } else if (c == 'Z') {\n            df = _formatISO8601_z;\n            if (df == null) {\n                df = _formatISO8601_z = _cloneFormat(DATE_FORMAT_ISO8601_Z, DATE_FORMAT_STR_ISO8601_Z, _timezone, _locale);\n            }\n            // [JACKSON-334]: may be missing milliseconds... if so, add\n            if (dateStr.charAt(len-4) == ':') {\n                StringBuilder sb = new StringBuilder(dateStr);\n                sb.insert(len-1, \".000\");\n                dateStr = sb.toString();\n            }\n        } else {\n            // Let's see if we have timezone indicator or not...\n            if (hasTimeZone(dateStr)) {\n                c = dateStr.charAt(len-3);\n                if (c == ':') { // remove optional colon\n                    // remove colon\n                    StringBuilder sb = new StringBuilder(dateStr);\n                    sb.delete(len-3, len-2);\n                    dateStr = sb.toString();\n                } else if (c == '+' || c == '-') { // missing minutes\n                    // let's just append '00'\n                    dateStr += \"00\";\n                }\n                // Milliseconds partial or missing; and even seconds are optional\n                len = dateStr.length();\n                // remove 'T', '+'/'-' and 4-digit timezone-offset\n                c = dateStr.charAt(len-9);\n                if (Character.isDigit(c)) {\n                    StringBuilder sb = new StringBuilder(dateStr);\n                    sb.insert(len-5, \".000\");\n                    dateStr = sb.toString();\n                }\n                df = _formatISO8601;\n                if (_formatISO8601 == null) {\n                    df = _formatISO8601 = _cloneFormat(DATE_FORMAT_ISO8601, DATE_FORMAT_STR_ISO8601, _timezone, _locale);\n                }\n            } else {\n                // If not, plain date. Easiest to just patch 'Z' in the end?\n                StringBuilder sb = new StringBuilder(dateStr);\n                // And possible also millisecond part if missing\n                int timeLen = len - dateStr.lastIndexOf('T') - 1;\n                if (timeLen <= 8) {\n                        sb.append(\".000\");\n                }\n                sb.append('Z');\n                dateStr = sb.toString();\n                df = _formatISO8601_z;\n                if (df == null) {\n                    df = _formatISO8601_z = _cloneFormat(DATE_FORMAT_ISO8601_Z, DATE_FORMAT_STR_ISO8601_Z,\n                            _timezone, _locale);\n                }\n            }\n        }\n        return df.parse(dateStr, pos);\n    }",
        "src_wo_comments": "protected Date parseAsISO8601 ( String dateStr , ParsePosition pos ) { int len = dateStr . length ( ) ; char c = dateStr . charAt ( len - 1 ) ; DateFormat df ; if ( len <= 10 && Character . isDigit ( c ) ) { df = _formatPlain ; if ( df == null ) { df = _formatPlain = _cloneFormat ( DATE_FORMAT_PLAIN , DATE_FORMAT_STR_PLAIN , _timezone , _locale ) ; } } else if ( c == 'Z' ) { df = _formatISO8601_z ; if ( df == null ) { df = _formatISO8601_z = _cloneFormat ( DATE_FORMAT_ISO8601_Z , DATE_FORMAT_STR_ISO8601_Z , _timezone , _locale ) ; } if ( dateStr . charAt ( len - 4 ) == ':' ) { StringBuilder sb = new StringBuilder ( dateStr ) ; sb . insert ( len - 1 , \".000\" ) ; dateStr = sb . toString ( ) ; } } else { if ( hasTimeZone ( dateStr ) ) { c = dateStr . charAt ( len - 3 ) ; if ( c == ':' ) { StringBuilder sb = new StringBuilder ( dateStr ) ; sb . delete ( len - 3 , len - 2 ) ; dateStr = sb . toString ( ) ; } else if ( c == '+' || c == '-' ) { dateStr += \"00\" ; } len = dateStr . length ( ) ; c = dateStr . charAt ( len - 9 ) ; if ( Character . isDigit ( c ) ) { StringBuilder sb = new StringBuilder ( dateStr ) ; sb . insert ( len - 5 , \".000\" ) ; dateStr = sb . toString ( ) ; } df = _formatISO8601 ; if ( _formatISO8601 == null ) { df = _formatISO8601 = _cloneFormat ( DATE_FORMAT_ISO8601 , DATE_FORMAT_STR_ISO8601 , _timezone , _locale ) ; } } else { StringBuilder sb = new StringBuilder ( dateStr ) ; int timeLen = len - dateStr . lastIndexOf ( 'T' ) - 1 ; if ( timeLen <= 8 ) { sb . append ( \".000\" ) ; } sb . append ( 'Z' ) ; dateStr = sb . toString ( ) ; df = _formatISO8601_z ; if ( df == null ) { df = _formatISO8601_z = _cloneFormat ( DATE_FORMAT_ISO8601_Z , DATE_FORMAT_STR_ISO8601_Z , _timezone , _locale ) ; } } } return df . parse ( dateStr , pos ) ; }",
        "fixed_src": "protected Date parseAsISO8601(String dateStr, ParsePosition pos)\n    {\n        /* 21-May-2009, tatu: DateFormat has very strict handling of\n         * timezone  modifiers for ISO-8601. So we need to do some scrubbing.\n         */\n\n        /* First: do we have \"zulu\" format ('Z' == \"GMT\")? If yes, that's\n         * quite simple because we already set date format timezone to be\n         * GMT, and hence can just strip out 'Z' altogether\n         */\n        int len = dateStr.length();\n        char c = dateStr.charAt(len-1);\n        DateFormat df;\n\n        // [JACKSON-200]: need to support \"plain\" date...\n        if (len <= 10 && Character.isDigit(c)) {\n            df = _formatPlain;\n            if (df == null) {\n                df = _formatPlain = _cloneFormat(DATE_FORMAT_PLAIN, DATE_FORMAT_STR_PLAIN, _timezone, _locale);\n            }\n        } else if (c == 'Z') {\n            df = _formatISO8601_z;\n            if (df == null) {\n                df = _formatISO8601_z = _cloneFormat(DATE_FORMAT_ISO8601_Z, DATE_FORMAT_STR_ISO8601_Z, _timezone, _locale);\n            }\n            // [JACKSON-334]: may be missing milliseconds... if so, add\n            if (dateStr.charAt(len-4) == ':') {\n                StringBuilder sb = new StringBuilder(dateStr);\n                sb.insert(len-1, \".000\");\n                dateStr = sb.toString();\n            }\n        } else {\n            // Let's see if we have timezone indicator or not...\n            if (hasTimeZone(dateStr)) {\n                c = dateStr.charAt(len-3);\n                if (c == ':') { // remove optional colon\n                    // remove colon\n                    StringBuilder sb = new StringBuilder(dateStr);\n                    sb.delete(len-3, len-2);\n                    dateStr = sb.toString();\n                } else if (c == '+' || c == '-') { // missing minutes\n                    // let's just append '00'\n                    dateStr += \"00\";\n                }\n                // Milliseconds partial or missing; and even seconds are optional\n                len = dateStr.length();\n                // remove 'T', '+'/'-' and 4-digit timezone-offset\n                int timeLen = len - dateStr.lastIndexOf('T') - 6;\n                if (timeLen < 12) { // 8 for hh:mm:ss, 4 for .sss\n                    int offset = len - 5; // insertion offset, before tz-offset\n                    StringBuilder sb = new StringBuilder(dateStr);\n                    switch (timeLen) {\n                    case 11:\n                        sb.insert(offset, '0'); break;\n                    case 10:\n                        sb.insert(offset, \"00\"); break;\n                    case 9: // is this legal? (just second fraction marker)\n                        sb.insert(offset, \"000\"); break;\n                    case 8:\n                        sb.insert(offset, \".000\"); break;\n                    case 7: // not legal to have single-digit second\n                        break;\n                    case 6: // probably not legal, but let's allow\n                        sb.insert(offset, \"00.000\");\n                    case 5: // is legal to omit seconds\n                        sb.insert(offset, \":00.000\");\n                    }\n                    dateStr = sb.toString();\n                }\n                df = _formatISO8601;\n                if (_formatISO8601 == null) {\n                    df = _formatISO8601 = _cloneFormat(DATE_FORMAT_ISO8601, DATE_FORMAT_STR_ISO8601, _timezone, _locale);\n                }\n            } else {\n                // If not, plain date. Easiest to just patch 'Z' in the end?\n                StringBuilder sb = new StringBuilder(dateStr);\n                // And possible also millisecond part if missing\n                int timeLen = len - dateStr.lastIndexOf('T') - 1;\n                if (timeLen < 12) { // missing, or partial\n                    switch (timeLen) {\n                    case 11: sb.append('0');\n                    case 10: sb.append('0');\n                    case 9: sb.append('0');\n                        break;\n                    default:\n                        sb.append(\".000\");\n                    }\n                }\n                sb.append('Z');\n                dateStr = sb.toString();\n                df = _formatISO8601_z;\n                if (df == null) {\n                    df = _formatISO8601_z = _cloneFormat(DATE_FORMAT_ISO8601_Z, DATE_FORMAT_STR_ISO8601_Z,\n                            _timezone, _locale);\n                }\n            }\n        }\n        return df.parse(dateStr, pos);\n    }",
        "fixed_src_wo_comments": "protected Date parseAsISO8601 ( String dateStr , ParsePosition pos ) { int len = dateStr . length ( ) ; char c = dateStr . charAt ( len - 1 ) ; DateFormat df ; if ( len <= 10 && Character . isDigit ( c ) ) { df = _formatPlain ; if ( df == null ) { df = _formatPlain = _cloneFormat ( DATE_FORMAT_PLAIN , DATE_FORMAT_STR_PLAIN , _timezone , _locale ) ; } } else if ( c == 'Z' ) { df = _formatISO8601_z ; if ( df == null ) { df = _formatISO8601_z = _cloneFormat ( DATE_FORMAT_ISO8601_Z , DATE_FORMAT_STR_ISO8601_Z , _timezone , _locale ) ; } if ( dateStr . charAt ( len - 4 ) == ':' ) { StringBuilder sb = new StringBuilder ( dateStr ) ; sb . insert ( len - 1 , \".000\" ) ; dateStr = sb . toString ( ) ; } } else { if ( hasTimeZone ( dateStr ) ) { c = dateStr . charAt ( len - 3 ) ; if ( c == ':' ) { StringBuilder sb = new StringBuilder ( dateStr ) ; sb . delete ( len - 3 , len - 2 ) ; dateStr = sb . toString ( ) ; } else if ( c == '+' || c == '-' ) { dateStr += \"00\" ; } len = dateStr . length ( ) ; int timeLen = len - dateStr . lastIndexOf ( 'T' ) - 6 ; if ( timeLen < 12 ) { int offset = len - 5 ; StringBuilder sb = new StringBuilder ( dateStr ) ; switch ( timeLen ) { case 11 : sb . insert ( offset , '0' ) ; break ; case 10 : sb . insert ( offset , \"00\" ) ; break ; case 9 : sb . insert ( offset , \"000\" ) ; break ; case 8 : sb . insert ( offset , \".000\" ) ; break ; case 7 : break ; case 6 : sb . insert ( offset , \"00.000\" ) ; case 5 : sb . insert ( offset , \":00.000\" ) ; } dateStr = sb . toString ( ) ; } df = _formatISO8601 ; if ( _formatISO8601 == null ) { df = _formatISO8601 = _cloneFormat ( DATE_FORMAT_ISO8601 , DATE_FORMAT_STR_ISO8601 , _timezone , _locale ) ; } } else { StringBuilder sb = new StringBuilder ( dateStr ) ; int timeLen = len - dateStr . lastIndexOf ( 'T' ) - 1 ; if ( timeLen < 12 ) { switch ( timeLen ) { case 11 : sb . append ( '0' ) ; case 10 : sb . append ( '0' ) ; case 9 : sb . append ( '0' ) ; break ; default : sb . append ( \".000\" ) ; } } sb . append ( 'Z' ) ; dateStr = sb . toString ( ) ; df = _formatISO8601_z ; if ( df == null ) { df = _formatISO8601_z = _cloneFormat ( DATE_FORMAT_ISO8601_Z , DATE_FORMAT_STR_ISO8601_Z , _timezone , _locale ) ; } } } return df . parse ( dateStr , pos ) ; }",
        "summary": "Add Support for Parsing All Compliant ISO-8601 Date Formats",
        "Description": "Some providers create JSON date stamps in ISO-8601 formats that cannot be parsed by the jackson-databind library. Here is a sampling of some valid formats that do not parse correctly:\n\n2014-10-03T18:00:00.6-05:00\n2014-10-03T18:00:00.61-05:00\n1997-07-16T19:20+01:00\n1997-07-16T19:20:30.45+01:00\n\nThe last two actually come from the ISO-8601 notes on http://www.w3.org/TR/NOTE-datetime.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for pointing out this gap!\n\nISO-8601 definition is big enough that I can't say this would get us to \"all compliant variants\", but definitely sounds like a reasonable improvement.\nOne thing I need to verify is just whether some of these might already be covered by one recent patch.\n"
            },
            {
                "content": "Ok. So, after verifying, this fix IS included in 2.4.3, as originally planned.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been fixed and is included in the 2.4.3 version of the source code, as originally planned."
    },
    "JacksonDatabind_82_src/main/java/com/fasterxml/jackson/databind/deser/BeanDeserializerFactory.java_472_615": {
        "src": "protected void addBeanProps(DeserializationContext ctxt,\n            BeanDescription beanDesc, BeanDeserializerBuilder builder)\n        throws JsonMappingException\n    {\n        final boolean isConcrete = !beanDesc.getType().isAbstract();\n        final SettableBeanProperty[] creatorProps = isConcrete\n                ? builder.getValueInstantiator().getFromObjectArguments(ctxt.getConfig())\n                : null;\n        final boolean hasCreatorProps = (creatorProps != null);\n        \n        // 01-May-2016, tatu: Which base type to use here gets tricky, since\n        //   it may often make most sense to use general type for overrides,\n        //   but what we have here may be more specific impl type. But for now\n        //   just use it as is.\n        JsonIgnoreProperties.Value ignorals = ctxt.getConfig()\n                .getDefaultPropertyIgnorals(beanDesc.getBeanClass(),\n                        beanDesc.getClassInfo());\n        Set<String> ignored;\n\n        if (ignorals != null) {\n            boolean ignoreAny = ignorals.getIgnoreUnknown();\n            builder.setIgnoreUnknownProperties(ignoreAny);\n            // Or explicit/implicit definitions?\n            ignored = ignorals.getIgnored();\n            for (String propName : ignored) {\n                builder.addIgnorable(propName);\n            }\n        } else {\n            ignored = Collections.emptySet();\n        }\n\n        // Also, do we have a fallback \"any\" setter?\n        AnnotatedMethod anySetterMethod = beanDesc.findAnySetter();\n        AnnotatedMember anySetterField = null;\n        if (anySetterMethod != null) {\n            builder.setAnySetter(constructAnySetter(ctxt, beanDesc, anySetterMethod));\n        }\n        else {\n        \tanySetterField = beanDesc.findAnySetterField();\n        \tif(anySetterField != null) {\n        \t\tbuilder.setAnySetter(constructAnySetter(ctxt, beanDesc, anySetterField));\n        \t}\n        }\n        // NOTE: we do NOT add @JsonIgnore'd properties into blocked ones if there's any-setter\n        // Implicit ones via @JsonIgnore and equivalent?\n        if (anySetterMethod == null && anySetterField == null) {\n            Collection<String> ignored2 = beanDesc.getIgnoredPropertyNames();\n            if (ignored2 != null) {\n                for (String propName : ignored2) {\n                    // allow ignoral of similarly named JSON property, but do not force;\n                    // latter means NOT adding this to 'ignored':\n                    builder.addIgnorable(propName);\n                }\n            }\n        }\n        final boolean useGettersAsSetters = ctxt.isEnabled(MapperFeature.USE_GETTERS_AS_SETTERS)\n                && ctxt.isEnabled(MapperFeature.AUTO_DETECT_GETTERS);\n\n        // Ok: let's then filter out property definitions\n        List<BeanPropertyDefinition> propDefs = filterBeanProps(ctxt,\n                beanDesc, builder, beanDesc.findProperties(), ignored);\n\n        // After which we can let custom code change the set\n        if (_factoryConfig.hasDeserializerModifiers()) {\n            for (BeanDeserializerModifier mod : _factoryConfig.deserializerModifiers()) {\n                propDefs = mod.updateProperties(ctxt.getConfig(), beanDesc, propDefs);\n            }\n        }\n        \n        // At which point we still have all kinds of properties; not all with mutators:\n        for (BeanPropertyDefinition propDef : propDefs) {\n            SettableBeanProperty prop = null;\n            /* 18-Oct-2013, tatu: Although constructor parameters have highest precedence,\n             *   we need to do linkage (as per [databind#318]), and so need to start with\n             *   other types, and only then create constructor parameter, if any.\n             */\n            if (propDef.hasSetter()) {\n                JavaType propertyType = propDef.getSetter().getParameterType(0);\n                prop = constructSettableProperty(ctxt, beanDesc, propDef, propertyType);\n            } else if (propDef.hasField()) {\n                JavaType propertyType = propDef.getField().getType();\n                prop = constructSettableProperty(ctxt, beanDesc, propDef, propertyType);\n            } else if (useGettersAsSetters && propDef.hasGetter()) {\n                /* May also need to consider getters\n                 * for Map/Collection properties; but with lowest precedence\n                 */\n                AnnotatedMethod getter = propDef.getGetter();\n                // should only consider Collections and Maps, for now?\n                Class<?> rawPropertyType = getter.getRawType();\n                if (Collection.class.isAssignableFrom(rawPropertyType)\n                        || Map.class.isAssignableFrom(rawPropertyType)) {\n                    prop = constructSetterlessProperty(ctxt, beanDesc, propDef);\n                }\n            }\n            // 25-Sep-2014, tatu: No point in finding constructor parameters for abstract types\n            //   (since they are never used anyway)\n            if (hasCreatorProps && propDef.hasConstructorParameter()) {\n                /* If property is passed via constructor parameter, we must\n                 * handle things in special way. Not sure what is the most optimal way...\n                 * for now, let's just call a (new) method in builder, which does nothing.\n                 */\n                // but let's call a method just to allow custom builders to be aware...\n                final String name = propDef.getName();\n                CreatorProperty cprop = null;\n                if (creatorProps != null) {\n                    for (SettableBeanProperty cp : creatorProps) {\n                        if (name.equals(cp.getName()) && (cp instanceof CreatorProperty)) {\n                            cprop = (CreatorProperty) cp;\n                            break;\n                        }\n                    }\n                }\n                if (cprop == null) {\n                    List<String> n = new ArrayList<>();\n                    for (SettableBeanProperty cp : creatorProps) {\n                        n.add(cp.getName());\n                    }\n                    ctxt.reportBadPropertyDefinition(beanDesc, propDef,\n                            \"Could not find creator property with name '%s' (known Creator properties: %s)\",\n                            name, n);\n                    continue;\n                }\n                if (prop != null) {\n                    cprop.setFallbackSetter(prop);\n                }\n                prop = cprop;\n                builder.addCreatorProperty(cprop);\n                continue;\n            }\n\n            if (prop != null) {\n                Class<?>[] views = propDef.findViews();\n                if (views == null) {\n                    // one more twist: if default inclusion disabled, need to force empty set of views\n                    if (!ctxt.isEnabled(MapperFeature.DEFAULT_VIEW_INCLUSION)) {\n                        views = NO_VIEWS;\n                    }\n                }\n                // one more thing before adding to builder: copy any metadata\n                prop.setViews(views);\n                builder.addProperty(prop);\n            }\n        }\n    }",
        "src_wo_comments": "protected void addBeanProps ( DeserializationContext ctxt , BeanDescription beanDesc , BeanDeserializerBuilder builder ) throws JsonMappingException { final boolean isConcrete = ! beanDesc . getType ( ) . isAbstract ( ) ; final SettableBeanProperty [ ] creatorProps = isConcrete ? builder . getValueInstantiator ( ) . getFromObjectArguments ( ctxt . getConfig ( ) ) : null ; final boolean hasCreatorProps = ( creatorProps != null ) ; JsonIgnoreProperties . Value ignorals = ctxt . getConfig ( ) . getDefaultPropertyIgnorals ( beanDesc . getBeanClass ( ) , beanDesc . getClassInfo ( ) ) ; Set < String > ignored ; if ( ignorals != null ) { boolean ignoreAny = ignorals . getIgnoreUnknown ( ) ; builder . setIgnoreUnknownProperties ( ignoreAny ) ; ignored = ignorals . getIgnored ( ) ; for ( String propName : ignored ) { builder . addIgnorable ( propName ) ; } } else { ignored = Collections . emptySet ( ) ; } AnnotatedMethod anySetterMethod = beanDesc . findAnySetter ( ) ; AnnotatedMember anySetterField = null ; if ( anySetterMethod != null ) { builder . setAnySetter ( constructAnySetter ( ctxt , beanDesc , anySetterMethod ) ) ; } else { anySetterField = beanDesc . findAnySetterField ( ) ; if ( anySetterField != null ) { builder . setAnySetter ( constructAnySetter ( ctxt , beanDesc , anySetterField ) ) ; } } if ( anySetterMethod == null && anySetterField == null ) { Collection < String > ignored2 = beanDesc . getIgnoredPropertyNames ( ) ; if ( ignored2 != null ) { for ( String propName : ignored2 ) { builder . addIgnorable ( propName ) ; } } } final boolean useGettersAsSetters = ctxt . isEnabled ( MapperFeature . USE_GETTERS_AS_SETTERS ) && ctxt . isEnabled ( MapperFeature . AUTO_DETECT_GETTERS ) ; List < BeanPropertyDefinition > propDefs = filterBeanProps ( ctxt , beanDesc , builder , beanDesc . findProperties ( ) , ignored ) ; if ( _factoryConfig . hasDeserializerModifiers ( ) ) { for ( BeanDeserializerModifier mod : _factoryConfig . deserializerModifiers ( ) ) { propDefs = mod . updateProperties ( ctxt . getConfig ( ) , beanDesc , propDefs ) ; } } for ( BeanPropertyDefinition propDef : propDefs ) { SettableBeanProperty prop = null ; if ( propDef . hasSetter ( ) ) { JavaType propertyType = propDef . getSetter ( ) . getParameterType ( 0 ) ; prop = constructSettableProperty ( ctxt , beanDesc , propDef , propertyType ) ; } else if ( propDef . hasField ( ) ) { JavaType propertyType = propDef . getField ( ) . getType ( ) ; prop = constructSettableProperty ( ctxt , beanDesc , propDef , propertyType ) ; } else if ( useGettersAsSetters && propDef . hasGetter ( ) ) { AnnotatedMethod getter = propDef . getGetter ( ) ; Class < ? > rawPropertyType = getter . getRawType ( ) ; if ( Collection . class . isAssignableFrom ( rawPropertyType ) || Map . class . isAssignableFrom ( rawPropertyType ) ) { prop = constructSetterlessProperty ( ctxt , beanDesc , propDef ) ; } } if ( hasCreatorProps && propDef . hasConstructorParameter ( ) ) { final String name = propDef . getName ( ) ; CreatorProperty cprop = null ; if ( creatorProps != null ) { for ( SettableBeanProperty cp : creatorProps ) { if ( name . equals ( cp . getName ( ) ) && ( cp instanceof CreatorProperty ) ) { cprop = ( CreatorProperty ) cp ; break ; } } } if ( cprop == null ) { List < String > n = new ArrayList < > ( ) ; for ( SettableBeanProperty cp : creatorProps ) { n . add ( cp . getName ( ) ) ; } ctxt . reportBadPropertyDefinition ( beanDesc , propDef , \"Could not find creator property with name '%s' (known Creator properties: %s)\" , name , n ) ; continue ; } if ( prop != null ) { cprop . setFallbackSetter ( prop ) ; } prop = cprop ; builder . addCreatorProperty ( cprop ) ; continue ; } if ( prop != null ) { Class < ? > [ ] views = propDef . findViews ( ) ; if ( views == null ) { if ( ! ctxt . isEnabled ( MapperFeature . DEFAULT_VIEW_INCLUSION ) ) { views = NO_VIEWS ; } } prop . setViews ( views ) ; builder . addProperty ( prop ) ; } } }",
        "fixed_src": "protected void addBeanProps(DeserializationContext ctxt,\n            BeanDescription beanDesc, BeanDeserializerBuilder builder)\n        throws JsonMappingException\n    {\n        final boolean isConcrete = !beanDesc.getType().isAbstract();\n        final SettableBeanProperty[] creatorProps = isConcrete\n                ? builder.getValueInstantiator().getFromObjectArguments(ctxt.getConfig())\n                : null;\n        final boolean hasCreatorProps = (creatorProps != null);\n        \n        // 01-May-2016, tatu: Which base type to use here gets tricky, since\n        //   it may often make most sense to use general type for overrides,\n        //   but what we have here may be more specific impl type. But for now\n        //   just use it as is.\n        JsonIgnoreProperties.Value ignorals = ctxt.getConfig()\n                .getDefaultPropertyIgnorals(beanDesc.getBeanClass(),\n                        beanDesc.getClassInfo());\n        Set<String> ignored;\n\n        if (ignorals != null) {\n            boolean ignoreAny = ignorals.getIgnoreUnknown();\n            builder.setIgnoreUnknownProperties(ignoreAny);\n            // Or explicit/implicit definitions?\n            ignored = ignorals.findIgnoredForDeserialization();\n            for (String propName : ignored) {\n                builder.addIgnorable(propName);\n            }\n        } else {\n            ignored = Collections.emptySet();\n        }\n\n        // Also, do we have a fallback \"any\" setter?\n        AnnotatedMethod anySetterMethod = beanDesc.findAnySetter();\n        AnnotatedMember anySetterField = null;\n        if (anySetterMethod != null) {\n            builder.setAnySetter(constructAnySetter(ctxt, beanDesc, anySetterMethod));\n        }\n        else {\n        \tanySetterField = beanDesc.findAnySetterField();\n        \tif(anySetterField != null) {\n        \t\tbuilder.setAnySetter(constructAnySetter(ctxt, beanDesc, anySetterField));\n        \t}\n        }\n        // NOTE: we do NOT add @JsonIgnore'd properties into blocked ones if there's any-setter\n        // Implicit ones via @JsonIgnore and equivalent?\n        if (anySetterMethod == null && anySetterField == null) {\n            Collection<String> ignored2 = beanDesc.getIgnoredPropertyNames();\n            if (ignored2 != null) {\n                for (String propName : ignored2) {\n                    // allow ignoral of similarly named JSON property, but do not force;\n                    // latter means NOT adding this to 'ignored':\n                    builder.addIgnorable(propName);\n                }\n            }\n        }\n        final boolean useGettersAsSetters = ctxt.isEnabled(MapperFeature.USE_GETTERS_AS_SETTERS)\n                && ctxt.isEnabled(MapperFeature.AUTO_DETECT_GETTERS);\n\n        // Ok: let's then filter out property definitions\n        List<BeanPropertyDefinition> propDefs = filterBeanProps(ctxt,\n                beanDesc, builder, beanDesc.findProperties(), ignored);\n\n        // After which we can let custom code change the set\n        if (_factoryConfig.hasDeserializerModifiers()) {\n            for (BeanDeserializerModifier mod : _factoryConfig.deserializerModifiers()) {\n                propDefs = mod.updateProperties(ctxt.getConfig(), beanDesc, propDefs);\n            }\n        }\n        \n        // At which point we still have all kinds of properties; not all with mutators:\n        for (BeanPropertyDefinition propDef : propDefs) {\n            SettableBeanProperty prop = null;\n            /* 18-Oct-2013, tatu: Although constructor parameters have highest precedence,\n             *   we need to do linkage (as per [databind#318]), and so need to start with\n             *   other types, and only then create constructor parameter, if any.\n             */\n            if (propDef.hasSetter()) {\n                JavaType propertyType = propDef.getSetter().getParameterType(0);\n                prop = constructSettableProperty(ctxt, beanDesc, propDef, propertyType);\n            } else if (propDef.hasField()) {\n                JavaType propertyType = propDef.getField().getType();\n                prop = constructSettableProperty(ctxt, beanDesc, propDef, propertyType);\n            } else if (useGettersAsSetters && propDef.hasGetter()) {\n                /* May also need to consider getters\n                 * for Map/Collection properties; but with lowest precedence\n                 */\n                AnnotatedMethod getter = propDef.getGetter();\n                // should only consider Collections and Maps, for now?\n                Class<?> rawPropertyType = getter.getRawType();\n                if (Collection.class.isAssignableFrom(rawPropertyType)\n                        || Map.class.isAssignableFrom(rawPropertyType)) {\n                    prop = constructSetterlessProperty(ctxt, beanDesc, propDef);\n                }\n            }\n            // 25-Sep-2014, tatu: No point in finding constructor parameters for abstract types\n            //   (since they are never used anyway)\n            if (hasCreatorProps && propDef.hasConstructorParameter()) {\n                /* If property is passed via constructor parameter, we must\n                 * handle things in special way. Not sure what is the most optimal way...\n                 * for now, let's just call a (new) method in builder, which does nothing.\n                 */\n                // but let's call a method just to allow custom builders to be aware...\n                final String name = propDef.getName();\n                CreatorProperty cprop = null;\n                if (creatorProps != null) {\n                    for (SettableBeanProperty cp : creatorProps) {\n                        if (name.equals(cp.getName()) && (cp instanceof CreatorProperty)) {\n                            cprop = (CreatorProperty) cp;\n                            break;\n                        }\n                    }\n                }\n                if (cprop == null) {\n                    List<String> n = new ArrayList<>();\n                    for (SettableBeanProperty cp : creatorProps) {\n                        n.add(cp.getName());\n                    }\n                    ctxt.reportBadPropertyDefinition(beanDesc, propDef,\n                            \"Could not find creator property with name '%s' (known Creator properties: %s)\",\n                            name, n);\n                    continue;\n                }\n                if (prop != null) {\n                    cprop.setFallbackSetter(prop);\n                }\n                prop = cprop;\n                builder.addCreatorProperty(cprop);\n                continue;\n            }\n\n            if (prop != null) {\n                Class<?>[] views = propDef.findViews();\n                if (views == null) {\n                    // one more twist: if default inclusion disabled, need to force empty set of views\n                    if (!ctxt.isEnabled(MapperFeature.DEFAULT_VIEW_INCLUSION)) {\n                        views = NO_VIEWS;\n                    }\n                }\n                // one more thing before adding to builder: copy any metadata\n                prop.setViews(views);\n                builder.addProperty(prop);\n            }\n        }\n    }",
        "fixed_src_wo_comments": "protected void addBeanProps ( DeserializationContext ctxt , BeanDescription beanDesc , BeanDeserializerBuilder builder ) throws JsonMappingException { final boolean isConcrete = ! beanDesc . getType ( ) . isAbstract ( ) ; final SettableBeanProperty [ ] creatorProps = isConcrete ? builder . getValueInstantiator ( ) . getFromObjectArguments ( ctxt . getConfig ( ) ) : null ; final boolean hasCreatorProps = ( creatorProps != null ) ; JsonIgnoreProperties . Value ignorals = ctxt . getConfig ( ) . getDefaultPropertyIgnorals ( beanDesc . getBeanClass ( ) , beanDesc . getClassInfo ( ) ) ; Set < String > ignored ; if ( ignorals != null ) { boolean ignoreAny = ignorals . getIgnoreUnknown ( ) ; builder . setIgnoreUnknownProperties ( ignoreAny ) ; ignored = ignorals . findIgnoredForDeserialization ( ) ; for ( String propName : ignored ) { builder . addIgnorable ( propName ) ; } } else { ignored = Collections . emptySet ( ) ; } AnnotatedMethod anySetterMethod = beanDesc . findAnySetter ( ) ; AnnotatedMember anySetterField = null ; if ( anySetterMethod != null ) { builder . setAnySetter ( constructAnySetter ( ctxt , beanDesc , anySetterMethod ) ) ; } else { anySetterField = beanDesc . findAnySetterField ( ) ; if ( anySetterField != null ) { builder . setAnySetter ( constructAnySetter ( ctxt , beanDesc , anySetterField ) ) ; } } if ( anySetterMethod == null && anySetterField == null ) { Collection < String > ignored2 = beanDesc . getIgnoredPropertyNames ( ) ; if ( ignored2 != null ) { for ( String propName : ignored2 ) { builder . addIgnorable ( propName ) ; } } } final boolean useGettersAsSetters = ctxt . isEnabled ( MapperFeature . USE_GETTERS_AS_SETTERS ) && ctxt . isEnabled ( MapperFeature . AUTO_DETECT_GETTERS ) ; List < BeanPropertyDefinition > propDefs = filterBeanProps ( ctxt , beanDesc , builder , beanDesc . findProperties ( ) , ignored ) ; if ( _factoryConfig . hasDeserializerModifiers ( ) ) { for ( BeanDeserializerModifier mod : _factoryConfig . deserializerModifiers ( ) ) { propDefs = mod . updateProperties ( ctxt . getConfig ( ) , beanDesc , propDefs ) ; } } for ( BeanPropertyDefinition propDef : propDefs ) { SettableBeanProperty prop = null ; if ( propDef . hasSetter ( ) ) { JavaType propertyType = propDef . getSetter ( ) . getParameterType ( 0 ) ; prop = constructSettableProperty ( ctxt , beanDesc , propDef , propertyType ) ; } else if ( propDef . hasField ( ) ) { JavaType propertyType = propDef . getField ( ) . getType ( ) ; prop = constructSettableProperty ( ctxt , beanDesc , propDef , propertyType ) ; } else if ( useGettersAsSetters && propDef . hasGetter ( ) ) { AnnotatedMethod getter = propDef . getGetter ( ) ; Class < ? > rawPropertyType = getter . getRawType ( ) ; if ( Collection . class . isAssignableFrom ( rawPropertyType ) || Map . class . isAssignableFrom ( rawPropertyType ) ) { prop = constructSetterlessProperty ( ctxt , beanDesc , propDef ) ; } } if ( hasCreatorProps && propDef . hasConstructorParameter ( ) ) { final String name = propDef . getName ( ) ; CreatorProperty cprop = null ; if ( creatorProps != null ) { for ( SettableBeanProperty cp : creatorProps ) { if ( name . equals ( cp . getName ( ) ) && ( cp instanceof CreatorProperty ) ) { cprop = ( CreatorProperty ) cp ; break ; } } } if ( cprop == null ) { List < String > n = new ArrayList < > ( ) ; for ( SettableBeanProperty cp : creatorProps ) { n . add ( cp . getName ( ) ) ; } ctxt . reportBadPropertyDefinition ( beanDesc , propDef , \"Could not find creator property with name '%s' (known Creator properties: %s)\" , name , n ) ; continue ; } if ( prop != null ) { cprop . setFallbackSetter ( prop ) ; } prop = cprop ; builder . addCreatorProperty ( cprop ) ; continue ; } if ( prop != null ) { Class < ? > [ ] views = propDef . findViews ( ) ; if ( views == null ) { if ( ! ctxt . isEnabled ( MapperFeature . DEFAULT_VIEW_INCLUSION ) ) { views = NO_VIEWS ; } } prop . setViews ( views ) ; builder . addProperty ( prop ) ; } } }",
        "summary": "`JsonIgnoreProperties.allowSetters` is not working in Jackson 2.8",
        "Description": "```\r\n@JsonIgnoreProperties(value = { \"password\" }, ignoreUnknown = true, allowSetters = true)\r\npublic class JsonTest {\r\n\tprivate String username;\r\n\tprivate String password;\r\n\r\n\tpublic JsonTest() {\r\n\t\tsuper();\r\n\t\t// TODO Auto-generated constructor stub\r\n\t}\r\n\r\n\tpublic JsonTest(String username, String password) {\r\n\t\tsuper();\r\n\t\tthis.username = username;\r\n\t\tthis.password = password;\r\n\t}\r\n\r\n\tpublic String getUsername() {\r\n\t\treturn username;\r\n\t}\r\n\r\n\tpublic void setUsername(String username) {\r\n\t\tthis.username = username;\r\n\t}\r\n\r\n\tpublic String getPassword() {\r\n\t\treturn password;\r\n\t}\r\n\r\n\tpublic void setPassword(String password) {\r\n\t\tthis.password = password;\r\n\t}\r\n\r\n\tpublic static void main(String[] args) {\r\n\t\tObjectMapper mapper = new ObjectMapper();\r\n\r\n\t\tJsonTest json = new JsonTest(\"user\", \"password\");\r\n\r\n\t\ttry {\r\n\t\t\tSystem.out.println(mapper.writeValueAsString(json));\r\n\t\t} catch (JsonProcessingException e) {\r\n\t\t\t// TODO Auto-generated catch block\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\r\n\t\tString jsonString = \"{ \\\"username\\\":\\\"username\\\",\\\"password\\\":\\\"password\\\" }\";\r\n\t\ttry {\r\n\t\t\tjson = mapper.readValue(jsonString, JsonTest.class);\r\n\r\n\t\t\tSystem.out.println(json.getPassword());\r\n\t\t} catch (IOException e) {\r\n\t\t\t// TODO Auto-generated catch block\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\r\n\t}\r\n}\r\n```\r\n\r\nthe version is 2.8.7.\r\nthe password cannot deserialize.\r\nthe output is:\r\n{\"username\":\"user\"}\r\nnull",
        "issue_url": null,
        "comments": [
            {
                "content": "I can not reproduce the issue with 2.8.8; but maybe I don't understand what the problem is?\r\nTest should check for values; printing JSON out doesn't necessarily help as much.\r\nAll I can see is output being printed as expected.\r\n\r\nOne possibility is that fix for #1345 or #1575 (both included in 2.8.8) could maybe have resolved this issue.\r\n"
            },
            {
                "content": "```\r\n\t\tString jsonString = \"{ \\\"username\\\":\\\"username\\\",\\\"password\\\":\\\"password\\\" }\";\r\n\t\ttry {\r\n\t\t\tjson = mapper.readValue(jsonString, JsonTest.class);\r\n\r\n\t\t\tSystem.out.println(json.getPassword());\r\n\t\t} catch (IOException e) {\r\n\t\t\t// TODO Auto-generated catch block\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n```\r\nI expect the output to be \"password\", but is \"null\"\r\n\r\n\u5f53allowSetters\u4e3atrue\u65f6,\u6211\u8ba4\u4e3a\u5e94\u8be5\u662f\u53ef\u4ee5\u4eceString\u4e2d\u53cd\u5e8f\u5217\u5316\u7684.\r\n\u4f46\u662f\u8fd9\u91cc\u53ef\u4ee5\u770b\u5230\u6ca1\u6709\u4eceString\u4e2d\u8bfb\u53d6\u5230password\u5c5e\u6027.\r\n\r\n\r\n"
            },
            {
                "content": "@AnywnYu what I was saying was that this is not what I see with version 2.8.8: `password` seems to be correctly deserialized. So is it possible you may be running this against an earlier version?"
            },
            {
                "content": "@cowtowncoder\r\n\r\nMy version is 2.8.8,\r\npassword can not be deserialized and the output is null\r\n"
            },
            {
                "content": "@AnywnYu at this I can not reproduce this -- without functioning unit test there is nothing I can do. \r\nI will be closing the issue soon.\r\n"
            },
            {
                "content": "@cowtowncoder \r\nI don't know what 'functioning unit test' you need?\r\nCould you tell me what is the output with the follow code? Please.\r\nI expect the output to be \"password\", but is \"null\".\r\nThe  version is 2.8.8.\r\n```\r\n@JsonIgnoreProperties(value = { \"password\" }, allowSetters = true)\r\npublic class JsonTest {\r\n\tprivate String username;\r\n\tprivate String password;\r\n\r\n\tpublic JsonTest() {\r\n\t\tsuper();\r\n\t}\r\n\r\n\tpublic String getUsername() {\r\n\t\treturn username;\r\n\t}\r\n\r\n\tpublic void setUsername(String username) {\r\n\t\tthis.username = username;\r\n\t}\r\n\r\n\tpublic String getPassword() {\r\n\t\treturn password;\r\n\t}\r\n\r\n\tpublic void setPassword(String password) {\r\n\t\tthis.password = password;\r\n\t}\r\n\r\n\tpublic static void main(String[] args) {\r\n\t\tObjectMapper mapper = new ObjectMapper();\r\n\r\n\t\tString jsonString = \"{ \\\"username\\\":\\\"username\\\",\\\"password\\\":\\\"password\\\" }\";\r\n\t\ttry {\r\n\t\t\tJsonTest json = mapper.readValue(jsonString, JsonTest.class);\r\n\r\n\t\t\tSystem.out.println(json.getPassword());\r\n\t\t} catch (IOException e) {\r\n\t\t\t// TODO Auto-generated catch block\r\n\t\t\te.printStackTrace();\r\n\t\t}\r\n\r\n\t}\r\n}\r\n```\r\n\u6211\u4f7f\u7528\u7684\u662f2.8.8\u7248\u672c.\r\n\u8fd9\u4e2a\u4f8b\u5b50\u4e2d,\u6211\u5e0c\u671b\u80fd\u4ece\u5b57\u7b26\u4e32\u4e2d\u53cd\u5e8f\u5217\u5316password,\u8fd0\u884c\u7ed3\u679c\u8f93\u51fa:password.\r\n\u4f46\u662f,\u6ca1\u6709\u80fd\u4ece\u5b57\u7b26\u4e32\u4e2d\u53cd\u5e8f\u5217\u5316password\u5b57\u6bb5,\u8f93\u51fa\u7684\u7ed3\u679c\u4e5f\u662f:null.\r\n\r\n\u6211debug\u53d1\u73b0\u4e0b\u9762\u7684\u4ee3\u7801\u5e76\u672a\u533a\u5206allowSetters\u662f\u5426\u4e3atrue,\u6240\u6709JsonIgnoreProperties\u5b9a\u4e49\u7684\u5b57\u6bb5\u90fd\u88ab\u5ffd\u7565\u4e86.\u5728\u5faa\u73af\u5904\u7406\u7684\u5b57\u6bb5\u4e2d\u6ca1\u6709\u5305\u542bpassword.\r\ncom.fasterxml.jackson.databind.deser.BeanDeserializerFactory.addBeanProps(DeserializationContext, BeanDescription, BeanDeserializerBuilder): Line 459~Line 466\r\n```\r\n        if (ignorals != null) {\r\n            boolean ignoreAny = ignorals.getIgnoreUnknown();\r\n            builder.setIgnoreUnknownProperties(ignoreAny);\r\n            // Or explicit/implicit definitions?\r\n            ignored = ignorals.getIgnored();\r\n            for (String propName : ignored) {\r\n                builder.addIgnorable(propName);\r\n            }\r\n        } else {\r\n            ignored = Collections.emptySet();\r\n        }\r\n```\r\ncom.fasterxml.jackson.databind.deser.BeanDeserializerFactory.filterBeanProps(DeserializationContext, BeanDescription, BeanDeserializerBuilder, List<BeanPropertyDefinition>, Set<String>): Line603\r\n\r\n\r\n![workspace](https://cloud.githubusercontent.com/assets/17541281/25273273/ae88d53a-26bd-11e7-8a4c-7bfd7a110ada.png)\r\n"
            },
            {
                "content": "@AnywnYu  I am not going to spend any time running various things, except for one specific thing: failing unit test. That is, class that has a JUnit test case, and one that fails. If that is provided, please re-open."
            },
            {
                "content": "the issue exsits.\r\nallowSetters=true is not working in 2.8.8, since increased from 2.6.7.\r\nyou should spend more time about how to run test."
            },
            {
                "content": "@runjia1987 \u6211\u7684\u82f1\u6587\u4e0d\u600e\u4e48\u6837,\u6ca1\u770b\u592a\u660e\u767d.\r\n\u4f60\u80fd\u5426\u63d0\u4f9b\u7ed9@cowtowncoder\u4e00\u4e2a\u80fd\u8dd1\u7684test case?"
            },
            {
                "content": "@AnywnYu, a simple case below, should surely reproduce this issue.\r\n_object class:_\r\n```\r\n@Setter\r\n@Getter\r\n@JsonIgnoreProperties(value = {\"name\"}, allowSetters = true)\r\npublic class Simple {\r\n  private int id;\r\n  private String name;\r\n  private String creator;\r\n  private String business;\r\n}\r\n```\r\n_test class:_\r\n```\r\n@Test\r\n  public void testDeserialize2() throws Exception {\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    Simple config = new Simple();\r\n    config.setId(123);\r\n    config.setName(\"jack\");\r\n    config.setCreator(\"god\");\r\n    config.setBusiness(\"cars\");\r\n    String json = mapper.writeValueAsString(config);\r\n    System.out.println(json); // name is not serialized as expected\r\n    String original = \"{\\\"id\\\":123,\\\"name\\\":\\\"jack\\\",\\\"creator\\\":\\\"god\\\",\\\"business\\\":\\\"cars\\\"}\";\r\n    Simple des = mapper.readValue(original, Simple.class);\r\n    System.out.println(des.getName()); **// name is not deserialized as expected**\r\n  }\r\n```\r\n_build.gradle:_\r\n```\r\n    compile 'com.fasterxml.jackson.core:jackson-core:2.8.8'\r\n    compile 'com.fasterxml.jackson.core:jackson-databind:2.8.8.1'\r\n    compile 'com.fasterxml.jackson.core:jackson-annotations:2.8.8'\r\n    compile 'com.fasterxml.jackson.jaxrs:jackson-jaxrs-base:2.8.8'\r\n    compile 'com.fasterxml.jackson.jaxrs:jackson-jaxrs-json-provider:2.8.8'\r\n    compile 'com.fasterxml.jackson.datatype:jackson-datatype-jsr310:2.8.8'\r\n//    compile 'com.fasterxml.jackson.core:jackson-core:2.6.7'\r\n//    compile 'com.fasterxml.jackson.core:jackson-databind:2.6.7'\r\n//    compile 'com.fasterxml.jackson.core:jackson-annotations:2.6.7'\r\n//    compile 'com.fasterxml.jackson.jaxrs:jackson-jaxrs-base:2.6.7'\r\n//    compile 'com.fasterxml.jackson.jaxrs:jackson-jaxrs-json-provider:2.6.7'\r\n//    compile 'com.fasterxml.jackson.datatype:jackson-datatype-jsr310:2.6.7'\r\n```\r\nswitch between the jackson versions in gradle, u will see the 2.8.x behaves wrong."
            },
            {
                "content": "@runjia1987 It is _almost_ unit test; all that it lacks are assertions. Printing values out is not a test.\r\nBut I can add that.\r\n\r\nHowever: what are `@Setter` and `@Getter` annotations? Is this using something like Lombok?\r\nIf so, I would also need expanded class; actual class used in runtime.\r\n\r\n"
            },
            {
                "content": "@cowtowncoder \r\nIt is Lombok, and you may replace the annotations with IDE-generated getter/setter methods(eclipse / IDEA).\r\nDeserialization output is the same, we expect name field to be deserialized, but not.\r\n```\r\n@JsonIgnoreProperties(value = {\"name\"}, allowSetters = true)\r\npublic class Simple {\r\n\r\n  private int id;\r\n\r\n  private String name;\r\n\r\n  private String creator;\r\n\r\n  private String business;\r\n\r\n  public int getId() {\r\n    return id;\r\n  }\r\n\r\n  public void setId(int id) {\r\n    this.id = id;\r\n  }\r\n\r\n  public String getName() {\r\n    return name;\r\n  }\r\n\r\n  public void setName(String name) {\r\n    this.name = name;\r\n  }\r\n\r\n  public String getCreator() {\r\n    return creator;\r\n  }\r\n\r\n  public void setCreator(String creator) {\r\n    this.creator = creator;\r\n  }\r\n\r\n  public String getBusiness() {\r\n    return business;\r\n  }\r\n\r\n  public void setBusiness(String business) {\r\n    this.business = business;\r\n  }\r\n}\r\n```"
            },
            {
                "content": "@runjia1987 Thank you. Lombok depedency is specifically not accepted for Jackson unit tests (due to it not working solely via Maven, requires add'l installation), and definition of stand-alone test is that code works as-is.\r\n\r\nAt this point test seems almost complete. I hope to find time to look into this at some point.\r\n"
            },
            {
                "content": "@AnywnYu @runjia1987 Thank you for reporting this: looks like it was a regression from 2.7 due to refactoring of annotation handling. Fix will be in 2.8.9 patch release."
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is that it was a regression from 2.7 due to refactoring of annotation handling and will be fixed in the 2.8.9 patch release."
    },
    "Math_5_src/main/java/org/apache/commons/math3/complex/Complex.java_299_321": {
        "src": "public Complex reciprocal() {\n        if (isNaN) {\n            return NaN;\n        }\n\n        if (real == 0.0 && imaginary == 0.0) {\n            return NaN;\n        }\n\n        if (isInfinite) {\n            return ZERO;\n        }\n\n        if (FastMath.abs(real) < FastMath.abs(imaginary)) {\n            double q = real / imaginary;\n            double scale = 1. / (real * q + imaginary);\n            return createComplex(scale * q, -scale);\n        } else {\n            double q = imaginary / real;\n            double scale = 1. / (imaginary * q + real);\n            return createComplex(scale, -scale * q);\n        }\n    }",
        "src_wo_comments": "public Complex reciprocal ( ) { if ( isNaN ) { return NaN ; } if ( real == 0.0 && imaginary == 0.0 ) { return NaN ; } if ( isInfinite ) { return ZERO ; } if ( FastMath . abs ( real ) < FastMath . abs ( imaginary ) ) { double q = real / imaginary ; double scale = 1. / ( real * q + imaginary ) ; return createComplex ( scale * q , - scale ) ; } else { double q = imaginary / real ; double scale = 1. / ( imaginary * q + real ) ; return createComplex ( scale , - scale * q ) ; } }",
        "fixed_src": "public Complex reciprocal() {\n        if (isNaN) {\n            return NaN;\n        }\n\n        if (real == 0.0 && imaginary == 0.0) {\n            return INF;\n        }\n\n        if (isInfinite) {\n            return ZERO;\n        }\n\n        if (FastMath.abs(real) < FastMath.abs(imaginary)) {\n            double q = real / imaginary;\n            double scale = 1. / (real * q + imaginary);\n            return createComplex(scale * q, -scale);\n        } else {\n            double q = imaginary / real;\n            double scale = 1. / (imaginary * q + real);\n            return createComplex(scale, -scale * q);\n        }\n    }",
        "fixed_src_wo_comments": "public Complex reciprocal ( ) { if ( isNaN ) { return NaN ; } if ( real == 0.0 && imaginary == 0.0 ) { return INF ; } if ( isInfinite ) { return ZERO ; } if ( FastMath . abs ( real ) < FastMath . abs ( imaginary ) ) { double q = real / imaginary ; double scale = 1. / ( real * q + imaginary ) ; return createComplex ( scale * q , - scale ) ; } else { double q = imaginary / real ; double scale = 1. / ( imaginary * q + real ) ; return createComplex ( scale , - scale * q ) ; } }",
        "summary": "Complex.ZERO.reciprocal() returns NaN but should return INF.",
        "Description": "Complex.ZERO.reciprocal() returns NaN but should return INF.\n\nClass: org.apache.commons.math3.complex.Complex;\nMethod: reciprocal()\n@version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-934",
        "comments": [
            "Fixed in subversion repository as of r1459927.\n\nThanks for the report.",
            "Closing issue as version 3.2 has been released on 2013-04-06."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of r1459927, and the issue has been closed since version 3.2 has been released on 2013-04-06."
    },
    "Compress_46_src/main/java/org/apache/commons/compress/archivers/zip/X5455_ExtendedTimestamp.java_528_534": {
        "src": "private static ZipLong unixTimeToZipLong(long l) {\n        final long TWO_TO_32 = 0x100000000L;\n        if (l >= TWO_TO_32) {\n            throw new IllegalArgumentException(\"X5455 timestamps must fit in a signed 32 bit integer: \" + l);\n        }\n        return new ZipLong(l);\n    }",
        "src_wo_comments": "private static ZipLong unixTimeToZipLong ( long l ) { final long TWO_TO_32 = 0x100000000L ; if ( l >= TWO_TO_32 ) { throw new IllegalArgumentException ( \"X5455 timestamps must fit in a signed 32 bit integer: \" + l ) ; } return new ZipLong ( l ) ; }",
        "fixed_src": "private static ZipLong unixTimeToZipLong(long l) {\n        if (l < Integer.MIN_VALUE || l > Integer.MAX_VALUE) {\n            throw new IllegalArgumentException(\"X5455 timestamps must fit in a signed 32 bit integer: \" + l);\n        }\n        return new ZipLong(l);\n    }",
        "fixed_src_wo_comments": "private static ZipLong unixTimeToZipLong ( long l ) { if ( l < Integer . MIN_VALUE || l > Integer . MAX_VALUE ) { throw new IllegalArgumentException ( \"X5455 timestamps must fit in a signed 32 bit integer: \" + l ) ; } return new ZipLong ( l ) ; }",
        "summary": "Tests failing under jdk 9 : one reflection issue, one change to ZipEntry related issue",
        "Description": "X5455_ExtendedTimestampTest is failing under JDK 9 , due to what appears to be a bogus value returned from getTime().  It seems like the test failure might be due to the changes introduced for this: \nhttps://bugs.openjdk.java.net/browse/JDK-8073497\n\nTests were run using intelliJ TestRunner, using the openjdk9 build from the tip of the jdk9 tree (not dev).  I believe that this is at most one commit away from what will be the RC (which was delayed at the last minute due to two issues, one of which was javadoc related, and the other hotspot. \n",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-416",
        "comments": [
            "When I build with:\n\n{noformat}\njava version \"9-ea\"\nJava(TM) SE Runtime Environment (build 9-ea+174)\nJava HotSpot(TM) 64-Bit Server VM (build 9-ea+174, mixed mode)\n{noformat}\n\nand:\n\n{noformat}\nApache Maven 3.5.0 (ff8f5e7444045639af65f6095c62210b5713f426; 2017-04-03T12:39:06-07:00)\nMaven home: C:\\Java\\apache-maven-3.5.0\\bin\\..\nJava version: 9-ea, vendor: Oracle Corporation\nJava home: C:\\Program Files\\Java\\jdk-9\nDefault locale: en_US, platform encoding: Cp1252\nOS name: \"windows 10\", version: \"10.0\", arch: \"amd64\", family: \"windows\"\n{noformat}\n\nI get:\n\n{noformat}\nTests in error:\n  ArchiveReadTest.testArchive:112->AbstractTestCase.checkArchiveContent:290->AbstractTestCase.checkArchiveContent:303->AbstractTestCase.checkArchiveContent:322 \u2557 ServiceConfiguration\n  ArchiveReadTest.testArchive:112->AbstractTestCase.checkArchiveContent:290->AbstractTestCase.checkArchiveContent:303->AbstractTestCase.checkArchiveContent:322 \u2557 ServiceConfiguration\n  DetectArchiverTestCase.testEmptyJarArchive:120->checkEmptyArchive:133->AbstractTestCase.createEmptyArchive:235 \u2557 ServiceConfiguration\n  DetectArchiverTestCase.testEmptyZipArchive:129->checkEmptyArchive:133->AbstractTestCase.createEmptyArchive:235 \u2557 ServiceConfiguration\n  IOMethodsTest.testReadJar:98->compareReads:146->AbstractTestCase.createSingleEntryArchive:262 \u2557 ServiceConfiguration\n  IOMethodsTest.testReadZip:108->compareReads:146->AbstractTestCase.createSingleEntryArchive:262 \u2557 ServiceConfiguration\n  IOMethodsTest.testWriteJar:70->compareWrites:115 \u2557 ServiceConfiguration sun.ut...\n  IOMethodsTest.testWriteZip:83->compareWrites:115 \u2557 ServiceConfiguration sun.ut...\n  ArchiveOutputStreamTest.testCallSequenceJar:127->doCallSequence:145 \u2557 ServiceConfiguration\n  ArchiveOutputStreamTest.testCallSequenceZip:137->doCallSequence:145 \u2557 ServiceConfiguration\n  ArchiveOutputStreamTest.testFinish:45 \u2557 ServiceConfiguration sun.util.locale.p...\n  ArchiveOutputStreamTest.testOptionalFinish:95 \u2557 ServiceConfiguration sun.util....\n  ArchiveStreamFactoryTest.testEncodingOutputStream:357->getOutputStreamFor:431 \u2557 ServiceConfiguration\n  JarTestCase.testJarArchiveCreation:43 \u2557 ServiceConfiguration sun.util.locale.p...\n  JarTestCase.testJarUnarchive:63 \u2557 ServiceConfiguration sun.util.locale.provide...\n  JarTestCase.testJarUnarchiveAll:94 \u2557 ServiceConfiguration sun.util.locale.prov...\n  LongPathTest.testArchive:160->AbstractTestCase.checkArchiveContent:303->AbstractTestCase.checkArchiveContent:322 \u2557 ServiceConfiguration\n  LongPathTest.testArchive:160->AbstractTestCase.checkArchiveContent:303->AbstractTestCase.checkArchiveContent:322 \u2557 ServiceConfiguration\n  LongPathTest.testArchive:160->AbstractTestCase.checkArchiveContent:303->AbstractTestCase.checkArchiveContent:322 \u2557 ServiceConfiguration\n  LongPathTest.testArchive:160->AbstractTestCase.checkArchiveContent:303->AbstractTestCase.checkArchiveContent:322 \u2557 ServiceConfiguration\n  LongPathTest.testArchive:160->AbstractTestCase.checkArchiveContent:303->AbstractTestCase.checkArchiveContent:322 \u2557 ServiceConfiguration\n  LongPathTest.testArchive:160->AbstractTestCase.checkArchiveContent:303->AbstractTestCase.checkArchiveContent:322 \u2557 ServiceConfiguration\n  SevenZTestCase.testSevenZArchiveCreationUsingBZIP2:52->testSevenZArchiveCreation:70 \u2557 ServiceConfiguration\n  SevenZTestCase.testSevenZArchiveCreationUsingCopy:37->testSevenZArchiveCreation:70 \u2557 ServiceConfiguration\n  SevenZTestCase.testSevenZArchiveCreationUsingDeflate:57->testSevenZArchiveCreation:70 \u2557 ServiceConfiguration\n  SevenZTestCase.testSevenZArchiveCreationUsingLZMA:42->testSevenZArchiveCreation:70 \u2557 ServiceConfiguration\n  SevenZTestCase.testSevenZArchiveCreationUsingLZMA2:47->testSevenZArchiveCreation:70 \u2557 ServiceConfiguration\n  ZipTestCase.testCopyRawEntriesFromFile:369->createReferenceFile:444 \u2557 ServiceConfiguration\n  ZipTestCase.testCopyRawZip64EntryFromFile:402 \u2557 ServiceConfiguration sun.util....\n  ZipTestCase.testDirectoryEntryFromFile:293 \u2557 ServiceConfiguration sun.util.loc...\n  ZipTestCase.testExplicitDirectoryEntry:329 \u2557 ServiceConfiguration sun.util.loc...\n  ZipTestCase.testExplicitFileEntry:565 \u2557 ServiceConfiguration sun.util.locale.p...\n  ZipTestCase.testFileEntryFromFile:520 \u2557 ServiceConfiguration sun.util.locale.p...\n  ZipTestCase.testListAllFilesWithNestedArchive:255 \u2557 ServiceConfiguration sun.u...\n  ZipTestCase.testSkipEntryWithUnsupportedCompressionMethod:220 \u2557 ServiceConfiguration\n  ZipTestCase.testSkipsPK00Prefix:184->AbstractTestCase.checkArchiveContent:303->AbstractTestCase.checkArchiveContent:322 \u2557 ServiceConfiguration\n  ZipTestCase.testSupportedCompressionMethod:201 \u2557 ServiceConfiguration sun.util...\n  ZipTestCase.testUnixModeInAddRaw:428 \u2557 ServiceConfiguration sun.util.locale.pr...\n  ZipTestCase.testZipArchiveCreation:66 \u2557 ServiceConfiguration sun.util.locale.p...\n  ZipTestCase.testZipArchiveCreationInMemory:128 \u2557 ServiceConfiguration sun.util...\n  ZipTestCase.testZipUnarchive:165 \u2557 ServiceConfiguration sun.util.locale.provid...\n  ArjArchiveInputStreamTest.testReadingOfAttributesDosVersion:64 \u2557 ServiceConfiguration\n  ArjArchiveInputStreamTest.testReadingOfAttributesUnixVersion:78 \u2557 ServiceConfiguration\n  JarArchiveOutputStreamTest.testJarMarker:44 \u2557 ServiceConfiguration sun.util.lo...\n  SevenZNativeHeapTest.initializationError \u2557 Objenesis java.lang.reflect.Invocat...\n  SevenZOutputFileTest.testDirectoriesAndEmptyFiles:61 \u2557 ServiceConfiguration su...\n  SevenZOutputFileTest.testEightEmptyFiles:204->testCompress252:386->addDir:414 \u2557 ServiceConfiguration\n  SevenZOutputFileTest.testEightFilesSomeNotEmpty:209->testCompress252:386->addDir:414 \u2557 ServiceConfiguration\n  SevenZOutputFileTest.testNineEmptyFiles:214->testCompress252:386->addDir:414 \u2557 ServiceConfiguration\n  SevenZOutputFileTest.testNineFilesSomeNotEmpty:219->testCompress252:386->addDir:414 \u2557 ServiceConfiguration\n  SevenZOutputFileTest.testSevenEmptyFiles:194->testCompress252:386->addDir:414 \u2557 ServiceConfiguration\n  SevenZOutputFileTest.testSevenFilesSomeNotEmpty:199->testCompress252:386->addDir:414 \u2557 ServiceConfiguration\n  SevenZOutputFileTest.testSixEmptyFiles:184->testCompress252:386->addDir:414 \u2557 ServiceConfiguration\n  SevenZOutputFileTest.testSixFilesSomeNotEmpty:189->testCompress252:386->addDir:414 \u2557 ServiceConfiguration\n  SevenZOutputFileTest.testTwentyNineEmptyFiles:224->testCompress252:386->addDir:414 \u2557 ServiceConfiguration\n  SevenZOutputFileTest.testTwentyNineFilesSomeNotEmpty:229->testCompress252:386->addDir:414 \u2557 ServiceConfiguration\n  TarArchiveInputStreamTest.datePriorToEpochInGNUFormat:131->datePriorToEpoch:146 \u2557 ServiceConfiguration\n  TarArchiveInputStreamTest.datePriorToEpochInPAXFormat:137->datePriorToEpoch:146 \u2557 ServiceConfiguration\n  TarArchiveOutputStreamTest.testOldEntryPosixMode:299 \u2557 ServiceConfiguration su...\n  TarArchiveOutputStreamTest.testOldEntryStarMode:266 \u2557 ServiceConfiguration sun...\n  DataDescriptorTest.doesntWriteDataDescriptorForDeflatedEntryOnSeekableOutput:95 \u2557 ServiceConfiguration\n  DataDescriptorTest.doesntWriteDataDescriptorWhenAddingRawEntries:137 \u2557 ServiceConfiguration\n  DataDescriptorTest.writesDataDescriptorForDeflatedEntryOnUnseekableOutput:55 \u2557 ServiceConfiguration\n  EncryptedArchiveTest.testReadPasswordEncryptedEntryViaStream:62 \u2557 ServiceConfiguration\n  EncryptedArchiveTest.testReadPasswordEncryptedEntryViaZipFile:38 \u2557 ServiceConfiguration\n  ExplodeSupportTest.testArchiveWithImplodeCompression4K2Trees:56->testArchiveWithImplodeCompression:38 \u2557 ServiceConfiguration\n  ExplodeSupportTest.testArchiveWithImplodeCompression8K3Trees:61->testArchiveWithImplodeCompression:38 \u2557 ServiceConfiguration\n  ExplodeSupportTest.testTikaTestArchive:66->testArchiveWithImplodeCompression:38 \u2557 ServiceConfiguration\n  ExplodeSupportTest.testTikaTestStream:99->testZipStreamWithImplodeCompression:71 \u2557 ServiceConfiguration\n  ExplodeSupportTest.testZipStreamWithImplodeCompression4K2Trees:89->testZipStreamWithImplodeCompression:71 \u2557 ServiceConfiguration\n  ExplodeSupportTest.testZipStreamWithImplodeCompression8K3Trees:94->testZipStreamWithImplodeCompression:71 \u2557 ServiceConfiguration\n  Maven221MultiVolumeTest.testRead7ZipMultiVolumeArchiveForStream:80 \u2557 ServiceConfiguration\n  ParallelScatterZipCreatorTest.callableApi:76 \u2557 ServiceConfiguration sun.util.l...\n  ParallelScatterZipCreatorTest.concurrent:60 \u2557 ServiceConfiguration sun.util.lo...\n  ScatterSampleTest.testSample:39->createFile:55 \u2557 ServiceConfiguration sun.util...\n  ScatterZipOutputStreamTest.putArchiveEntry:64 \u2557 ServiceConfiguration sun.util....\n  UTF8ZipFilesTest.testASCIIFileRoundtripExplicitUnicodeExtra:64->testFileRoundtrip:249->createTestFile:265 \u2557 ServiceConfiguration\n  UTF8ZipFilesTest.testASCIIFileRoundtripImplicitUnicodeExtra:88->testFileRoundtrip:249->createTestFile:265 \u2557 ServiceConfiguration\n  UTF8ZipFilesTest.testCP437FileRoundtripExplicitUnicodeExtra:58->testFileRoundtrip:249->createTestFile:265 \u2557 ServiceConfiguration\n  UTF8ZipFilesTest.testCP437FileRoundtripImplicitUnicodeExtra:82->testFileRoundtrip:249->createTestFile:265 \u2557 ServiceConfiguration\n  UTF8ZipFilesTest.testRawNameReadFromStream:234 \u2557 ServiceConfiguration sun.util...\n  UTF8ZipFilesTest.testRawNameReadFromZipFile:219 \u2557 ServiceConfiguration sun.uti...\n  UTF8ZipFilesTest.testRead7ZipArchive:102 \u2557 ServiceConfiguration sun.util.local...\n  UTF8ZipFilesTest.testRead7ZipArchiveForStream:118 \u2557 ServiceConfiguration sun.u...\n  UTF8ZipFilesTest.testReadWinZipArchive:137 \u2557 ServiceConfiguration sun.util.loc...\n  UTF8ZipFilesTest.testReadWinZipArchiveForStream:165 \u2557 ServiceConfiguration sun...\n  UTF8ZipFilesTest.testUtf8FileRoundtripExplicitUnicodeExtra:46->testFileRoundtrip:249->createTestFile:265 \u2557 ServiceConfiguration\n  UTF8ZipFilesTest.testUtf8FileRoundtripImplicitUnicodeExtra:70->testFileRoundtrip:249->createTestFile:265 \u2557 ServiceConfiguration\n  UTF8ZipFilesTest.testUtf8FileRoundtripNoEFSExplicitUnicodeExtra:52->testFileRoundtrip:249->createTestFile:265 \u2557 ServiceConfiguration\n  UTF8ZipFilesTest.testUtf8FileRoundtripNoEFSImplicitUnicodeExtra:76->testFileRoundtrip:249->createTestFile:265 \u2557 ServiceConfiguration\n  UTF8ZipFilesTest.testUtf8Interoperability:392->testFile:340 \u2557 ServiceConfiguration\n  UTF8ZipFilesTest.testZipArchiveInputStreamReadsUnicodeFields:202->createTestFile:265 \u2557 ServiceConfiguration\n  UTF8ZipFilesTest.testZipFileReadsUnicodeFields:181->createTestFile:265 \u2557 ServiceConfiguration\n  X5455_ExtendedTimestampTest.testBitsAreSetWithTime \u2557 NoClassDefFound Could not...\n  X5455_ExtendedTimestampTest.testGetHeaderId \u2557 NoClassDefFound Could not initia...\n  X5455_ExtendedTimestampTest.<clinit>:54 \u2557 ServiceConfiguration sun.util.locale...\n  X5455_ExtendedTimestampTest.testMisc \u2557 NoClassDefFound Could not initialize cl...\n  X5455_ExtendedTimestampTest.testParseReparse \u2557 NoClassDefFound Could not initi...\n  X5455_ExtendedTimestampTest.testSampleFile \u2557 NoClassDefFound Could not initial...\n  X5455_ExtendedTimestampTest.testWriteReadRoundtrip \u2557 NoClassDefFound Could not...\n  X7875_NewUnixTest.testSampleFile:53 \u2557 ServiceConfiguration sun.util.locale.pro...\n  ZipArchiveEntryTest.isUnixSymlinkIsFalseIfMoreThanOneFlagIsSet:278 \u2557 ServiceConfiguration\n  ZipArchiveEntryTest.testCompressionMethod:213 \u2557 ServiceConfiguration sun.util....\n  ZipArchiveInputStreamTest.properUseOfInflater:70 \u2557 ServiceConfiguration sun.ut...\n  ZipArchiveInputStreamTest.shouldConsumeArchiveCompletely:94 \u2557 ServiceConfiguration\n  ZipArchiveInputStreamTest.shouldReadNestedZip:114->extractZipInputStream:124 \u2557 ServiceConfiguration\n  ZipArchiveInputStreamTest.testMessageWithCorruptFileName:185 \u2557 ServiceConfiguration\n  ZipArchiveInputStreamTest.testOffsets:263 \u2557 ServiceConfiguration sun.util.loca...\n  ZipArchiveInputStreamTest.testReadingOfFirstStoredEntry:170 \u2557 ServiceConfiguration\n  ZipArchiveInputStreamTest.testUnshrinkEntry:137 \u2557 ServiceConfiguration sun.uti...\n  ZipArchiveInputStreamTest.testUnzipBZip2CompressedEntry:200 \u2557 ServiceConfiguration\n  ZipArchiveInputStreamTest.testWithBytesAfterData:222 \u2557 ServiceConfiguration su...\n  ZipArchiveInputStreamTest.winzipBackSlashWorkaround:51 \u2557 ServiceConfiguration ...\n  ZipFileTest.testCDOrder:59->readOrderTest:665 \u2557 ServiceConfiguration sun.util....\n  ZipFileTest.testCDOrderInMemory:93 \u2557 ServiceConfiguration sun.util.locale.prov...\n  ZipFileTest.testConcurrentReadFile:379 \u2557 ServiceConfiguration sun.util.locale....\n  ZipFileTest.testConcurrentReadSeekable:349 \u2557 ServiceConfiguration sun.util.loc...\n  ZipFileTest.testDelayedOffsetsAndSizes:430 \u2557 ServiceConfiguration sun.util.loc...\n  ZipFileTest.testDoubleClose:151->readOrderTest:665 \u2557 ServiceConfiguration sun....\n  ZipFileTest.testDuplicateEntry:268 \u2557 ServiceConfiguration sun.util.locale.prov...\n  ZipFileTest.testEntryAlignment:470 \u2557 ServiceConfiguration sun.util.locale.prov...\n  ZipFileTest.testEntryAlignmentExceed \u2557  Unexpected exception, expected<java.la...\n  ZipFileTest.testExcessDataInZip64ExtraField:288 \u2557 ServiceConfiguration sun.uti...\n  ZipFileTest.testOffsets:412 \u2557 ServiceConfiguration sun.util.locale.provider.Lo...\n  ZipFileTest.testPhysicalOrder:122->readOrderTest:665 \u2557 ServiceConfiguration su...\n  ZipFileTest.testReadingOfFirstStoredEntry:324 \u2557 ServiceConfiguration sun.util....\n  ZipFileTest.testReadingOfStoredEntry:168 \u2557 ServiceConfiguration sun.util.local...\n  ZipFileTest.testSkipsPK00Prefix:217 \u2557 ServiceConfiguration sun.util.locale.pro...\n  ZipFileTest.testUnixSymlinkSampleFile:246 \u2557 ServiceConfiguration sun.util.loca...\n  ZipFileTest.testUnshrinking:297 \u2557 ServiceConfiguration sun.util.locale.provide...\n  ZipFileTest.testUnzipBZip2CompressedEntry:334 \u2557 ServiceConfiguration sun.util....\n  ZipFileTest.testWinzipBackSlashWorkaround:204 \u2557 ServiceConfiguration sun.util....\n  ZipUtilTest.setUp:40 \u2557 ServiceConfiguration sun.util.locale.provider.LocaleDat...\n  ZipUtilTest.setUp:40 \u2557 ServiceConfiguration sun.util.locale.provider.LocaleDat...\n  ZipUtilTest.setUp:40 \u2557 ServiceConfiguration sun.util.locale.provider.LocaleDat...\n  ZipUtilTest.setUp:40 \u2557 ServiceConfiguration sun.util.locale.provider.LocaleDat...\n  ZipUtilTest.setUp:40 \u2557 ServiceConfiguration sun.util.locale.provider.LocaleDat...\n  ZipUtilTest.setUp:40 \u2557 ServiceConfiguration sun.util.locale.provider.LocaleDat...\n  ZipUtilTest.setUp:40 \u2557 ServiceConfiguration sun.util.locale.provider.LocaleDat...\n  ZipUtilTest.setUp:40 \u2557 ServiceConfiguration sun.util.locale.provider.LocaleDat...\n  ZipUtilTest.setUp:40 \u2557 ServiceConfiguration sun.util.locale.provider.LocaleDat...\n  ZipUtilTest.setUp:40 \u2557 ServiceConfiguration sun.util.locale.provider.LocaleDat...\n  ChangeSetTestCase.testAddAllreadyExistingWithReplaceFalse:1258->AbstractTestCase.createArchive:174 \u2557 ServiceConfiguration\n  ChangeSetTestCase.testAddAllreadyExistingWithReplaceTrue:1213->AbstractTestCase.createArchive:174 \u2557 ServiceConfiguration\n  ChangeSetTestCase.testAddToEmptyArchive:1068->AbstractTestCase.createEmptyArchive:235 \u2557 ServiceConfiguration\n  ChangeSetTestCase.testDeleteAddToOneFileArchive:1114->AbstractTestCase.createSingleEntryArchive:262 \u2557 ServiceConfiguration\n  ChangeSetTestCase.testDeleteFromAndAddToJar:929 \u2557 ServiceConfiguration sun.uti...\n  ChangeSetTestCase.testDeleteFromAndAddToZip:550->AbstractTestCase.createArchive:174 \u2557 ServiceConfiguration\n  ChangeSetTestCase.testDeleteFromAndAddToZipUsingZipFilePerform:597->AbstractTestCase.createArchive:174 \u2557 ServiceConfiguration\n  ChangeSetTestCase.testDeleteFromJar:835 \u2557 ServiceConfiguration sun.util.locale...\n  ChangeSetTestCase.testDeleteFromZip:754 \u2557 ServiceConfiguration sun.util.locale...\n  ChangeSetTestCase.testDeletePlusAddSame:367->AbstractTestCase.createArchive:174 \u2557 ServiceConfiguration\n  Pack200TestCase.testJarArchiveCreationInMemory:98->jarArchiveCreation:115 \u2557 ServiceConfiguration\n  Pack200TestCase.testJarArchiveCreationTempFile:103->jarArchiveCreation:115 \u2557 ServiceConfiguration\n  Pack200TestCase.testJarUnarchiveAllFileArgInMemory:54->jarUnarchiveAll:77 \u2557 ServiceConfiguration\n  Pack200TestCase.testJarUnarchiveAllFileTempFile:64->jarUnarchiveAll:77 \u2557 ServiceConfiguration\n  Pack200TestCase.testJarUnarchiveAllInMemory:49->jarUnarchiveAll:77 \u2557 ServiceConfiguration\n  Pack200TestCase.testJarUnarchiveAllTempFile:59->jarUnarchiveAll:77 \u2557 ServiceConfiguration\n  Pack200UtilsTest.testNormalize:47 \u2557 ServiceConfiguration sun.util.locale.provi...\n  Pack200UtilsTest.testNormalizeInPlace:93 \u2557 ServiceConfiguration sun.util.local...\n  FramedSnappyCompressorInputStreamTest.readIWAFile:154 \u2557 ServiceConfiguration s...\n  ZCompressorInputStreamTest.testFailsToCreateZCompressorInputStreamAndThrowsIOException \u2557\n\nTests run: 841, Failures: 0, Errors: 162, Skipped: 4\n{noformat}\n",
            "Greg - can you attach the sure xml , or something with stacktraces? There seems to be something going on in the locate setup , and I think many of those places are setting up encoders.  I think I read some  descriptions of something  similar (which didn't stick in brain L2).  \n\n(I do have access to a windows environment, but I'll be in trouble if my partner wakes up and catches me (say - if I install cygwin and sshd, her laptop only has an SSD, so  there's no HD rattling to give me away.. :) \n\nIt's probably  safer to see if I can get a travis jdk9  to produce the same errors from maven (the branch with travis tweaking has mvnw for  3.5.0;   I forgot which version of 9  is in the new trusty stable container on travis, but I think it's the last  oracle ea).  ",
            "I think I need a Java 9 that implements {{--permit-illegal-access}}. I'll attache my surefire reports.",
            "The issues are being caused by jacoco making a class unloadable by twiddling with a final field outside a constructor. \n\nThis is fixed by using a newer version of jacoco. \n\nI have made a few changes to the enhanced travis branch, which reduces the, er, non-successes to three tests.  \nSee: https://github.com/apache/commons-compress/pull/43\n\nSome of these are changes to outdated plugin versions. I blame the parents.\n\nThere are two \"failures\" and one \"error\". Quotation marks are used because one of the failures is actually an error.\n\nThe real failure is the change in the handling of zip dates in the jdk ZipEntry.  This is an actual incompatibility. \n\nThe second failure is really an illegal access from a mocker; it just happens to take place in test for an expected exception.   This  presents as  a warning in the (probable) RC -\n{noformat}\nRunning org.apache.commons.compress.compressors.z.ZCompressorInputStreamTest\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.mockito.cglib.core.ReflectUtils$2 (file:/home/ses/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)\nWARNING: Please consider reporting this to the maintainers of org.mockito.cglib.core.ReflectUtils$2\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n{noformat}\n\nI'm not sure how I feel about having to locate the maintainers of an anonymous inner class :-) \n\n",
            "I've replaced the use of mocks in SevenZNativeHeapTest with delegation.  I'm not sure whether this test is really testing much, but it's easy enough to just convert a couple of anonymous classes to named package private ones, and to generate trivial delegates. \n\nI can see where the bad code is in the mock libs, but that's not my problem anymore :-) \n\nAll JDK 9 changes are going into the fixed travis tree, since I've learned my lesson and won't try splitting PRs that depend on other  dependencies.",
            "After GitHub PR #43 I only have:\n{noformat}\nFailed tests:\n  X5455_ExtendedTimestampTest.testSampleFile:185 expected:<[2105-01-01/00:00:02] +0000> but was:<[1968-11-24/17:31:45] +0000>\nTests in error:\n  ZCompressorInputStreamTest.testFailsToCreateZCompressorInputStreamAndThrowsIOException \u2557\n\nTests run: 842, Failures: 1, Errors: 1, Skipped: 4\n{noformat}",
            "I think I mostly understand the issue now.\n\n# There are three different formats in which Zip can store times\n# Two of these formats will overflow  around  the year 2107-8\n# JDK 9 transparently switches from using dos times or xtra dos times to using NTFS times\n# JDK 9 now switches over to NTFS times at the end of 2099.\n# Since some of this logic is in the set code, there may be a bogus timestamp stored.\n#  I haven't checked to see if the problem can happen with java utils  ZipInputStream on the same test file, or if the problem comes from commons-compress.  If the problem  occurs  using the test file with the java libs, then it's definitely a jdk bug. \n# If the test is hacked to say that xtended time is always used, the tests all pass.  ",
            "Note, this is the unit test for the extended time stamp extra field so it will be sensitive to changes of the underlying ZipEntry code of the JDK. This sounds a lot like the change I've adjusted to with https://github.com/apache/commons-compress/commit/a2cda30be14b3da01cbbbedc41b70daf6d88da8b\n\nI haven't had the time to look into it in more detail so far.",
            "From the mercurial logs I think this it is definitely related - possibly\nfixing a performance regression there was an issue with locales being\ninitialized too eagerly.\n\nIt is indeed very sensitive to jdk(since it seems to be randomized each\nrelease). The extended header  is parsed correctly (and the zip entry\nitself has the right FileTime). It's the dosxtime that seems  messed up.\nNow to see where that's set (of course, extended header test should be OK\njust testing itself).\n\nIf this wasn't the only known jdk9 fail issue it wouldn't be bugging me :)\n\n\n",
            "It's an oracle issue. \n\n{code:java}\n    @Test\n    public void testJUZTimes() throws Exception {\n        final File archive = getFile(\"COMPRESS-210_unix_time_zip_test.zip\");\n        try (java.util.zip.ZipFile zf = new java.util.zip.ZipFile(archive)) {\n          ZipEntry entry =  zf.getEntry(\"COMPRESS-210_unix_time_zip_test/2105\");\n            Calendar girl = Calendar.getInstance(TimeZone.getTimeZone(\"GMT\"));\n            girl.setTime(new Date(entry.getTime()));\n            int year = girl.get(Calendar.YEAR);\n            assertEquals(2105,year);\n        }\n    }\n{code}\n\njava.lang.AssertionError: \nExpected :2105\nActual   :1968  \n\n1968-11-24T00:00:00.000Z to be precise.   So I'm weeks away from being conceived, and the  Nixon transition team is busy picking out hidden tape recorders.\n",
            "At time point, we could bring this up on core-libs-dev@openjdk.java.net. Would you ([~sesuncedu]) like to do the honors since you authored the failing test?\n\nI added {{org.apache.commons.compress.archivers.zip.Java9ZipEntryTimeTest}} as a standalone class depending only on the Zip file test fixture.\n\nGary",
            "I have a draft  open in another window. One special cause for speculation:  zip file parsing moved from C to java in JDK 9.  \nAlso, I discovered that I had off-by-one src.zips in my jdk9 build, which explains why debugging was confusing the heck out of me.  \nSo I'm rebuilding so I can check to see what's going on in the central directory parse.\n\nWorst thing about trying to debug j.u.z.* is loading stuff from jars.",
            "Roger that. Keep us posted.",
            "Found it! Changed a method signature from long to int and didn't deal with sign extension.  Now I can finish the bug report.",
            "But... after going back to the original path, I see that the actual definition of the extended timestamps states that:\nSo COMMONS-COMPRESS and the Oracle JDK were both doing it wrong.\n\nSo this goes back to the original patch in COMMONS-220 \n\n{noformat}\nThe time values are in standard Unix signed-long format, indicating  the number of seconds since 1 January 1970 00:00:00.  \n{noformat}\n[https://opensource.apple.com/source/zip/zip-6/unzip/unzip/proginfo/extra.fld]",
            "http://hg.openjdk.java.net/jdk9/jdk9/jdk/rev/48b296b645bd#l1.103\n\nNot mentioned in the commit log, not backported to jdk8 ",
            "GitHub user sesuncedu opened a pull request:\n\n    https://github.com/apache/commons-compress/pull/48\n\n    COMPRESS-416    Extended times for Zip should be signed values\n\n    This has fixed code, and almost fixed tests (fixed for jdk9;  haven't removed all the invalid checks for jdk8). \n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/sesuncedu/commons-compress COMPRESS-416\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/commons-compress/pull/48.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #48\n    \n----\ncommit e39151c1db4784e517fc139c0ee37ab9f035aa0b\nAuthor: Simon Spero <sesuncedu@gmail.com>\nDate:   2017-07-03T22:24:55Z\n\n    COMPRESS-416 Add signed 32bit int constructor and accessor to ZipLong\n    \n    Signed-off-by: Simon Spero <sesuncedu@gmail.com>\n\ncommit da5fa04a69d6c039376f26d0d5896995cecddc65\nAuthor: Simon Spero <sesuncedu@gmail.com>\nDate:   2017-07-03T23:10:10Z\n\n    COMPRESS-416 Use signed integers for extended timestamps, per spec\n    \n    Signed-off-by: Simon Spero <sesuncedu@gmail.com>\n\n----\n",
            "I'm currently looking into the test failures of Simon's pull request (I only get one locally, so it seems to depend on the time zone).",
            "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/commons-compress/pull/48\n",
            "OK, the extended timestamp issue should bow be resolved, many thanks Simon.\n\nAnd I've disabled the failing powermock test for now. I'm not sure it is worth porting over, IMHO we can close this now.",
            "There was an oddity before with IBM java 8,which is earlier, but it was an\nincredibly suspicious off by a second thing, with a little timezone too. I\nwill see if it all behaves.\n\n\n"
        ],
        "summarized_discussion": ""
    },
    "JacksonDatabind_94_src/main/java/com/fasterxml/jackson/databind/jsontype/impl/SubTypeValidator.java_71_111": {
        "src": "public void validateSubType(DeserializationContext ctxt, JavaType type) throws JsonMappingException\n    {\n        // There are certain nasty classes that could cause problems, mostly\n        // via default typing -- catch them here.\n        final Class<?> raw = type.getRawClass();\n        String full = raw.getName();\n\n        main_check:\n        do {\n            if (_cfgIllegalClassNames.contains(full)) {\n                break;\n            }\n\n            // 18-Dec-2017, tatu: As per [databind#1855], need bit more sophisticated handling\n            //    for some Spring framework types\n            // 05-Jan-2017, tatu: ... also, only applies to classes, not interfaces\n            if (raw.isInterface()) {\n                ;\n            } else if (full.startsWith(PREFIX_SPRING)) {\n                for (Class<?> cls = raw; (cls != null) && (cls != Object.class); cls = cls.getSuperclass()){\n                    String name = cls.getSimpleName();\n                    // looking for \"AbstractBeanFactoryPointcutAdvisor\" but no point to allow any is there?\n                    if (\"AbstractPointcutAdvisor\".equals(name)\n                            // ditto  for \"FileSystemXmlApplicationContext\": block all ApplicationContexts\n                            || \"AbstractApplicationContext\".equals(name)) {\n                        break main_check;\n                    }\n                // [databind#1737]; more 3rd party\n                // s.add(\"com.mchange.v2.c3p0.JndiRefForwardingDataSource\");\n                // s.add(\"com.mchange.v2.c3p0.WrapperConnectionPoolDataSource\");\n                // [databind#1931]; more 3rd party\n                // com.mchange.v2.c3p0.ComboPooledDataSource\n                // com.mchange.v2.c3p0.debug.AfterCloseLoggingComboPooledDataSource \n                }\n            }\n            return;\n        } while (false);\n\n        throw JsonMappingException.from(ctxt,\n                String.format(\"Illegal type (%s) to deserialize: prevented for security reasons\", full));\n    }",
        "src_wo_comments": "public void validateSubType ( DeserializationContext ctxt , JavaType type ) throws JsonMappingException { final Class < ? > raw = type . getRawClass ( ) ; String full = raw . getName ( ) ; main_check : do { if ( _cfgIllegalClassNames . contains ( full ) ) { break ; } if ( raw . isInterface ( ) ) { ; } else if ( full . startsWith ( PREFIX_SPRING ) ) { for ( Class < ? > cls = raw ; ( cls != null ) && ( cls != Object . class ) ; cls = cls . getSuperclass ( ) ) { String name = cls . getSimpleName ( ) ; if ( \"AbstractPointcutAdvisor\" . equals ( name ) || \"AbstractApplicationContext\" . equals ( name ) ) { break main_check ; } } } return ; } while ( false ) ; throw JsonMappingException . from ( ctxt , String . format ( \"Illegal type (%s) to deserialize: prevented for security reasons\" , full ) ) ; }",
        "fixed_src": "public void validateSubType(DeserializationContext ctxt, JavaType type) throws JsonMappingException\n    {\n        // There are certain nasty classes that could cause problems, mostly\n        // via default typing -- catch them here.\n        final Class<?> raw = type.getRawClass();\n        String full = raw.getName();\n\n        main_check:\n        do {\n            if (_cfgIllegalClassNames.contains(full)) {\n                break;\n            }\n\n            // 18-Dec-2017, tatu: As per [databind#1855], need bit more sophisticated handling\n            //    for some Spring framework types\n            // 05-Jan-2017, tatu: ... also, only applies to classes, not interfaces\n            if (raw.isInterface()) {\n                ;\n            } else if (full.startsWith(PREFIX_SPRING)) {\n                for (Class<?> cls = raw; (cls != null) && (cls != Object.class); cls = cls.getSuperclass()){\n                    String name = cls.getSimpleName();\n                    // looking for \"AbstractBeanFactoryPointcutAdvisor\" but no point to allow any is there?\n                    if (\"AbstractPointcutAdvisor\".equals(name)\n                            // ditto  for \"FileSystemXmlApplicationContext\": block all ApplicationContexts\n                            || \"AbstractApplicationContext\".equals(name)) {\n                        break main_check;\n                    }\n                }\n            } else if (full.startsWith(PREFIX_C3P0)) {\n                // [databind#1737]; more 3rd party\n                // s.add(\"com.mchange.v2.c3p0.JndiRefForwardingDataSource\");\n                // s.add(\"com.mchange.v2.c3p0.WrapperConnectionPoolDataSource\");\n                // [databind#1931]; more 3rd party\n                // com.mchange.v2.c3p0.ComboPooledDataSource\n                // com.mchange.v2.c3p0.debug.AfterCloseLoggingComboPooledDataSource \n                if (full.endsWith(\"DataSource\")) {\n                    break main_check;\n                }\n            }\n            return;\n        } while (false);\n\n        throw JsonMappingException.from(ctxt,\n                String.format(\"Illegal type (%s) to deserialize: prevented for security reasons\", full));\n    }",
        "fixed_src_wo_comments": "public void validateSubType ( DeserializationContext ctxt , JavaType type ) throws JsonMappingException { final Class < ? > raw = type . getRawClass ( ) ; String full = raw . getName ( ) ; main_check : do { if ( _cfgIllegalClassNames . contains ( full ) ) { break ; } if ( raw . isInterface ( ) ) { ; } else if ( full . startsWith ( PREFIX_SPRING ) ) { for ( Class < ? > cls = raw ; ( cls != null ) && ( cls != Object . class ) ; cls = cls . getSuperclass ( ) ) { String name = cls . getSimpleName ( ) ; if ( \"AbstractPointcutAdvisor\" . equals ( name ) || \"AbstractApplicationContext\" . equals ( name ) ) { break main_check ; } } } else if ( full . startsWith ( PREFIX_C3P0 ) ) { if ( full . endsWith ( \"DataSource\" ) ) { break main_check ; } } return ; } while ( false ) ; throw JsonMappingException . from ( ctxt , String . format ( \"Illegal type (%s) to deserialize: prevented for security reasons\" , full ) ) ; }",
        "summary": "Block two more gadgets to exploit default typing issue (c3p0, CVE-2018-7489)",
        "Description": "From an email report there are 2 other c3p0 classes (above and beyond ones listed in #1737) need to be blocked.\r\n\r\nEDIT 21-Jun-2021: Fix included in:\r\n\r\n* `2.9.5`\r\n* `2.8.11.1`\r\n* `2.7.9.3`\r\n* `2.6.7.5`\r\n\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Fixed in `2.8.11.1` (newly released) and `2.9.5` (when it is released)\r\n"
            },
            {
                "content": "Hi there!\r\nHow comes that there is no atifact in http://repo1.maven.org/maven2/com/fasterxml/jackson/jackson-bom/ that is matching release 2.8.11.1?\r\n\r\nThis is preventing me from upgrading to 2.8.11.1 because that artifact would be required by Spring boots dependency management.\r\n\r\nThanks in advance!"
            },
            {
                "content": "@philippn Because beyond 2.8.11.1 there is no full release, and it is not really practical to create one-off bom sets: there may or may not be micro-patches for various components.\r\n\r\nWhat you need to do is to either use `2.8.11` bom and overrides (re-define one of version properties) or add explicit direct dependency. Alternatively you could probably build a separate bom of your own, one that extends jackson-bom-2.8.11.\r\n\r\n"
            },
            {
                "content": "Thanks for the clarification!"
            },
            {
                "content": "@philippn np. And apologies for the mess. I understand it is not ideal, and I am hoping we can figure out a more maintainable system for CVE updates."
            },
            {
                "content": "For further info: https://medium.com/@cowtowncoder/on-jackson-cves-dont-panic-here-is-what-you-need-to-know-54cd0d6e8062\r\n"
            },
            {
                "content": "Vuln reported as: https://access.redhat.com/security/cve/cve-2018-7489\r\n"
            },
            {
                "content": "Hi! Any estimates for a 2.9.5 release? Thanks!"
            },
            {
                "content": "Hi FasterXML Team , \r\nAs new vulnerability CVE-2018-7489 is reported  and we are using jackson-databind 2.9.4 version which is now vulnerable. Please confirm us when we can get full new release like 2.9.5  or patch fix in v2.9.4.1 which will help to get rid of this vulnerability. \r\n\r\n-thanks \r\nDharmendra \r\n"
            },
            {
                "content": "Is this defect applicable for org.codehaus.jackson libraries too?"
            },
            {
                "content": "@kiranmn Yes. With caveats from\r\n\r\n    https://medium.com/@cowtowncoder/on-jackson-cves-dont-panic-here-is-what-you-need-to-know-54cd0d6e8062\r\n\r\nmeaning it affects \"default typing\" usage, and only that.\r\n"
            },
            {
                "content": "Hi @cowtowncoder,\r\n\r\nmy product comes with jackson-databind 2.7.9.2. I now want to fix CVE-2018-5968 and CVE-2018-7489. My first try was to update it to 2.9.5. Unfortunately a lot of my test cases failed because of a java compatibility issue. \r\n\r\njackson-databind 2.7.9.2 requires at least java 6. \r\njackson-databind 2.9.5 requires at least java 7.\r\n\r\nMy product still supports java 6. Will there be a fix for jackson 2.7.9?\r\n\r\nRegards,\r\nMax"
            },
            {
                "content": "@MaximilianTews 2.9 should still run on Java 6 as features are accessed dynamically, but I must admit I have not been able to verify that.\r\nSame should be true of 2.8, and `2.8.11.1` has to fix as well.\r\n\r\nDue to limited resources we do not support older versions in general, but since these are security fixes I have accepted PRs for backports, and released micro-patches. So if you really want these, do you think you could create a PR? Probably easiest to have a look at fixes from 2.8.\r\n\r\n"
            },
            {
                "content": "Hi @cowtowncoder, \r\n\r\nI had a look at the history of 2.7 and it seems that the fix for CVE-2018-7489 (#1931) is already included in 2.7.9.3. \r\n\r\nI will create a PR for CVE-2018-5968 (#1899) for 2.7.\r\n\r\nRegards,\r\nMax"
            },
            {
                "content": "@MaximilianTews thank you for checking this: I updated description wrt information -- you are right about inclusion in `2.7.9.3`."
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to use the version 2.8.11.1 or 2.9.5 when it is released. Alternatively, one can use the 2.8.11 bom and overrides, add an explicit direct dependency, or build a separate bom of their own that extends jackson-bom-2.8.11. For further information, one can refer to the links provided."
    },
    "Compress_11_src/main/java/org/apache/commons/compress/archivers/ArchiveStreamFactory.java_197_254": {
        "src": "public ArchiveInputStream createArchiveInputStream(final InputStream in)\n            throws ArchiveException {\n        if (in == null) {\n            throw new IllegalArgumentException(\"Stream must not be null.\");\n        }\n\n        if (!in.markSupported()) {\n            throw new IllegalArgumentException(\"Mark is not supported.\");\n        }\n\n        final byte[] signature = new byte[12];\n        in.mark(signature.length);\n        try {\n            int signatureLength = in.read(signature);\n            in.reset();\n            if (ZipArchiveInputStream.matches(signature, signatureLength)) {\n                return new ZipArchiveInputStream(in);\n            } else if (JarArchiveInputStream.matches(signature, signatureLength)) {\n                return new JarArchiveInputStream(in);\n            } else if (ArArchiveInputStream.matches(signature, signatureLength)) {\n                return new ArArchiveInputStream(in);\n            } else if (CpioArchiveInputStream.matches(signature, signatureLength)) {\n                return new CpioArchiveInputStream(in);\n            }\n\n            // Dump needs a bigger buffer to check the signature;\n            final byte[] dumpsig = new byte[32];\n            in.mark(dumpsig.length);\n            signatureLength = in.read(dumpsig);\n            in.reset();\n            if (DumpArchiveInputStream.matches(dumpsig, signatureLength)) {\n                return new DumpArchiveInputStream(in);\n            }\n\n            // Tar needs an even bigger buffer to check the signature; read the first block\n            final byte[] tarheader = new byte[512];\n            in.mark(tarheader.length);\n            signatureLength = in.read(tarheader);\n            in.reset();\n            if (TarArchiveInputStream.matches(tarheader, signatureLength)) {\n                return new TarArchiveInputStream(in);\n            }\n            // COMPRESS-117 - improve auto-recognition\n            try {\n                TarArchiveInputStream tais = new TarArchiveInputStream(new ByteArrayInputStream(tarheader));\n                tais.getNextEntry();\n                return new TarArchiveInputStream(in);\n            } catch (Exception e) { // NOPMD\n                // can generate IllegalArgumentException as well as IOException\n                // autodetection, simply not a TAR\n                // ignored\n            }\n        } catch (IOException e) {\n            throw new ArchiveException(\"Could not use reset and mark operations.\", e);\n        }\n\n        throw new ArchiveException(\"No Archiver found for the stream signature\");\n    }",
        "src_wo_comments": "public ArchiveInputStream createArchiveInputStream ( final InputStream in ) throws ArchiveException { if ( in == null ) { throw new IllegalArgumentException ( \"Stream must not be null.\" ) ; } if ( ! in . markSupported ( ) ) { throw new IllegalArgumentException ( \"Mark is not supported.\" ) ; } final byte [ ] signature = new byte [ 12 ] ; in . mark ( signature . length ) ; try { int signatureLength = in . read ( signature ) ; in . reset ( ) ; if ( ZipArchiveInputStream . matches ( signature , signatureLength ) ) { return new ZipArchiveInputStream ( in ) ; } else if ( JarArchiveInputStream . matches ( signature , signatureLength ) ) { return new JarArchiveInputStream ( in ) ; } else if ( ArArchiveInputStream . matches ( signature , signatureLength ) ) { return new ArArchiveInputStream ( in ) ; } else if ( CpioArchiveInputStream . matches ( signature , signatureLength ) ) { return new CpioArchiveInputStream ( in ) ; } final byte [ ] dumpsig = new byte [ 32 ] ; in . mark ( dumpsig . length ) ; signatureLength = in . read ( dumpsig ) ; in . reset ( ) ; if ( DumpArchiveInputStream . matches ( dumpsig , signatureLength ) ) { return new DumpArchiveInputStream ( in ) ; } final byte [ ] tarheader = new byte [ 512 ] ; in . mark ( tarheader . length ) ; signatureLength = in . read ( tarheader ) ; in . reset ( ) ; if ( TarArchiveInputStream . matches ( tarheader , signatureLength ) ) { return new TarArchiveInputStream ( in ) ; } try { TarArchiveInputStream tais = new TarArchiveInputStream ( new ByteArrayInputStream ( tarheader ) ) ; tais . getNextEntry ( ) ; return new TarArchiveInputStream ( in ) ; } catch ( Exception e ) { } } catch ( IOException e ) { throw new ArchiveException ( \"Could not use reset and mark operations.\" , e ) ; } throw new ArchiveException ( \"No Archiver found for the stream signature\" ) ; }",
        "fixed_src": "public ArchiveInputStream createArchiveInputStream(final InputStream in)\n            throws ArchiveException {\n        if (in == null) {\n            throw new IllegalArgumentException(\"Stream must not be null.\");\n        }\n\n        if (!in.markSupported()) {\n            throw new IllegalArgumentException(\"Mark is not supported.\");\n        }\n\n        final byte[] signature = new byte[12];\n        in.mark(signature.length);\n        try {\n            int signatureLength = in.read(signature);\n            in.reset();\n            if (ZipArchiveInputStream.matches(signature, signatureLength)) {\n                return new ZipArchiveInputStream(in);\n            } else if (JarArchiveInputStream.matches(signature, signatureLength)) {\n                return new JarArchiveInputStream(in);\n            } else if (ArArchiveInputStream.matches(signature, signatureLength)) {\n                return new ArArchiveInputStream(in);\n            } else if (CpioArchiveInputStream.matches(signature, signatureLength)) {\n                return new CpioArchiveInputStream(in);\n            }\n\n            // Dump needs a bigger buffer to check the signature;\n            final byte[] dumpsig = new byte[32];\n            in.mark(dumpsig.length);\n            signatureLength = in.read(dumpsig);\n            in.reset();\n            if (DumpArchiveInputStream.matches(dumpsig, signatureLength)) {\n                return new DumpArchiveInputStream(in);\n            }\n\n            // Tar needs an even bigger buffer to check the signature; read the first block\n            final byte[] tarheader = new byte[512];\n            in.mark(tarheader.length);\n            signatureLength = in.read(tarheader);\n            in.reset();\n            if (TarArchiveInputStream.matches(tarheader, signatureLength)) {\n                return new TarArchiveInputStream(in);\n            }\n            // COMPRESS-117 - improve auto-recognition\n            if (signatureLength >= 512) {\n            try {\n                TarArchiveInputStream tais = new TarArchiveInputStream(new ByteArrayInputStream(tarheader));\n                tais.getNextEntry();\n                return new TarArchiveInputStream(in);\n            } catch (Exception e) { // NOPMD\n                // can generate IllegalArgumentException as well as IOException\n                // autodetection, simply not a TAR\n                // ignored\n            }\n            }\n        } catch (IOException e) {\n            throw new ArchiveException(\"Could not use reset and mark operations.\", e);\n        }\n\n        throw new ArchiveException(\"No Archiver found for the stream signature\");\n    }",
        "fixed_src_wo_comments": "public ArchiveInputStream createArchiveInputStream ( final InputStream in ) throws ArchiveException { if ( in == null ) { throw new IllegalArgumentException ( \"Stream must not be null.\" ) ; } if ( ! in . markSupported ( ) ) { throw new IllegalArgumentException ( \"Mark is not supported.\" ) ; } final byte [ ] signature = new byte [ 12 ] ; in . mark ( signature . length ) ; try { int signatureLength = in . read ( signature ) ; in . reset ( ) ; if ( ZipArchiveInputStream . matches ( signature , signatureLength ) ) { return new ZipArchiveInputStream ( in ) ; } else if ( JarArchiveInputStream . matches ( signature , signatureLength ) ) { return new JarArchiveInputStream ( in ) ; } else if ( ArArchiveInputStream . matches ( signature , signatureLength ) ) { return new ArArchiveInputStream ( in ) ; } else if ( CpioArchiveInputStream . matches ( signature , signatureLength ) ) { return new CpioArchiveInputStream ( in ) ; } final byte [ ] dumpsig = new byte [ 32 ] ; in . mark ( dumpsig . length ) ; signatureLength = in . read ( dumpsig ) ; in . reset ( ) ; if ( DumpArchiveInputStream . matches ( dumpsig , signatureLength ) ) { return new DumpArchiveInputStream ( in ) ; } final byte [ ] tarheader = new byte [ 512 ] ; in . mark ( tarheader . length ) ; signatureLength = in . read ( tarheader ) ; in . reset ( ) ; if ( TarArchiveInputStream . matches ( tarheader , signatureLength ) ) { return new TarArchiveInputStream ( in ) ; } if ( signatureLength >= 512 ) { try { TarArchiveInputStream tais = new TarArchiveInputStream ( new ByteArrayInputStream ( tarheader ) ) ; tais . getNextEntry ( ) ; return new TarArchiveInputStream ( in ) ; } catch ( Exception e ) { } } } catch ( IOException e ) { throw new ArchiveException ( \"Could not use reset and mark operations.\" , e ) ; } throw new ArchiveException ( \"No Archiver found for the stream signature\" ) ; }",
        "summary": "createArchiveInputStream detects text files less than 100 bytes as tar archives",
        "Description": "The fix for COMPRESS-117 which modified ArchiveStreamFactory().createArchiveInputStream(inputstream) results in short text files (empirically seems to be those <= 100 bytes) being detected as tar archives which obviously is not desirable if one wants to know whether or not the files are archives.\nI'm not an expert on compressed archives but perhaps the heuristic that if a stream is interpretable as a tar file without an exception being thrown should only be applied on archives greater than 100 bytes?",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-171",
        "comments": [
            "fixed with svn revision 1237475"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed with SVN revision 1237475."
    },
    "Csv_14_src/main/java/org/apache/commons/csv/CSVFormat.java_1001_1106": {
        "src": "private void printAndQuote(final Object object, final CharSequence value, final int offset, final int len,\n            final Appendable out, final boolean newRecord) throws IOException {\n        boolean quote = false;\n        int start = offset;\n        int pos = offset;\n        final int end = offset + len;\n\n        final char delimChar = getDelimiter();\n        final char quoteChar = getQuoteCharacter().charValue();\n\n        QuoteMode quoteModePolicy = getQuoteMode();\n        if (quoteModePolicy == null) {\n            quoteModePolicy = QuoteMode.MINIMAL;\n        }\n        switch (quoteModePolicy) {\n        case ALL:\n            quote = true;\n            break;\n        case NON_NUMERIC:\n            quote = !(object instanceof Number);\n            break;\n        case NONE:\n            // Use the existing escaping code\n            printAndEscape(value, offset, len, out);\n            return;\n        case MINIMAL:\n            if (len <= 0) {\n                // always quote an empty token that is the first\n                // on the line, as it may be the only thing on the\n                // line. If it were not quoted in that case,\n                // an empty line has no tokens.\n                if (newRecord) {\n                    quote = true;\n                }\n            } else {\n                char c = value.charAt(pos);\n\n                // RFC4180 (https://tools.ietf.org/html/rfc4180) TEXTDATA =  %x20-21 / %x23-2B / %x2D-7E\n                if (newRecord && (c < '0' || c > '9' && c < 'A' || c > 'Z' && c < 'a' || c > 'z')) {\n                    quote = true;\n                } else if (c <= COMMENT) {\n                    // Some other chars at the start of a value caused the parser to fail, so for now\n                    // encapsulate if we start in anything less than '#'. We are being conservative\n                    // by including the default comment char too.\n                    quote = true;\n                } else {\n                    while (pos < end) {\n                        c = value.charAt(pos);\n                        if (c == LF || c == CR || c == quoteChar || c == delimChar) {\n                            quote = true;\n                            break;\n                        }\n                        pos++;\n                    }\n\n                    if (!quote) {\n                        pos = end - 1;\n                        c = value.charAt(pos);\n                        // Some other chars at the end caused the parser to fail, so for now\n                        // encapsulate if we end in anything less than ' '\n                        if (c <= SP) {\n                            quote = true;\n                        }\n                    }\n                }\n            }\n\n            if (!quote) {\n                // no encapsulation needed - write out the original value\n                out.append(value, start, end);\n                return;\n            }\n            break;\n        default:\n            throw new IllegalStateException(\"Unexpected Quote value: \" + quoteModePolicy);\n        }\n\n        if (!quote) {\n            // no encapsulation needed - write out the original value\n            out.append(value, start, end);\n            return;\n        }\n\n        // we hit something that needed encapsulation\n        out.append(quoteChar);\n\n        // Pick up where we left off: pos should be positioned on the first character that caused\n        // the need for encapsulation.\n        while (pos < end) {\n            final char c = value.charAt(pos);\n            if (c == quoteChar) {\n                // write out the chunk up until this point\n\n                // add 1 to the length to write out the encapsulator also\n                out.append(value, start, pos + 1);\n                // put the next starting position on the encapsulator so we will\n                // write it out again with the next string (effectively doubling it)\n                start = pos;\n            }\n            pos++;\n        }\n\n        // write the last segment\n        out.append(value, start, pos);\n        out.append(quoteChar);\n    }",
        "src_wo_comments": "private void printAndQuote ( final Object object , final CharSequence value , final int offset , final int len , final Appendable out , final boolean newRecord ) throws IOException { boolean quote = false ; int start = offset ; int pos = offset ; final int end = offset + len ; final char delimChar = getDelimiter ( ) ; final char quoteChar = getQuoteCharacter ( ) . charValue ( ) ; QuoteMode quoteModePolicy = getQuoteMode ( ) ; if ( quoteModePolicy == null ) { quoteModePolicy = QuoteMode . MINIMAL ; } switch ( quoteModePolicy ) { case ALL : quote = true ; break ; case NON_NUMERIC : quote = ! ( object instanceof Number ) ; break ; case NONE : printAndEscape ( value , offset , len , out ) ; return ; case MINIMAL : if ( len <= 0 ) { if ( newRecord ) { quote = true ; } } else { char c = value . charAt ( pos ) ; if ( newRecord && ( c < '0' || c > '9' && c < 'A' || c > 'Z' && c < 'a' || c > 'z' ) ) { quote = true ; } else if ( c <= COMMENT ) { quote = true ; } else { while ( pos < end ) { c = value . charAt ( pos ) ; if ( c == LF || c == CR || c == quoteChar || c == delimChar ) { quote = true ; break ; } pos ++ ; } if ( ! quote ) { pos = end - 1 ; c = value . charAt ( pos ) ; if ( c <= SP ) { quote = true ; } } } } if ( ! quote ) { out . append ( value , start , end ) ; return ; } break ; default : throw new IllegalStateException ( \"Unexpected Quote value: \" + quoteModePolicy ) ; } if ( ! quote ) { out . append ( value , start , end ) ; return ; } out . append ( quoteChar ) ; while ( pos < end ) { final char c = value . charAt ( pos ) ; if ( c == quoteChar ) { out . append ( value , start , pos + 1 ) ; start = pos ; } pos ++ ; } out . append ( value , start , pos ) ; out . append ( quoteChar ) ; }",
        "fixed_src": "private void printAndQuote(final Object object, final CharSequence value, final int offset, final int len,\n            final Appendable out, final boolean newRecord) throws IOException {\n        boolean quote = false;\n        int start = offset;\n        int pos = offset;\n        final int end = offset + len;\n\n        final char delimChar = getDelimiter();\n        final char quoteChar = getQuoteCharacter().charValue();\n\n        QuoteMode quoteModePolicy = getQuoteMode();\n        if (quoteModePolicy == null) {\n            quoteModePolicy = QuoteMode.MINIMAL;\n        }\n        switch (quoteModePolicy) {\n        case ALL:\n            quote = true;\n            break;\n        case NON_NUMERIC:\n            quote = !(object instanceof Number);\n            break;\n        case NONE:\n            // Use the existing escaping code\n            printAndEscape(value, offset, len, out);\n            return;\n        case MINIMAL:\n            if (len <= 0) {\n                // always quote an empty token that is the first\n                // on the line, as it may be the only thing on the\n                // line. If it were not quoted in that case,\n                // an empty line has no tokens.\n                if (newRecord) {\n                    quote = true;\n                }\n            } else {\n                char c = value.charAt(pos);\n\n                // RFC4180 (https://tools.ietf.org/html/rfc4180) TEXTDATA =  %x20-21 / %x23-2B / %x2D-7E\n                if (newRecord && (c < 0x20 || c > 0x21 && c < 0x23 || c > 0x2B && c < 0x2D || c > 0x7E)) {\n                    quote = true;\n                } else if (c <= COMMENT) {\n                    // Some other chars at the start of a value caused the parser to fail, so for now\n                    // encapsulate if we start in anything less than '#'. We are being conservative\n                    // by including the default comment char too.\n                    quote = true;\n                } else {\n                    while (pos < end) {\n                        c = value.charAt(pos);\n                        if (c == LF || c == CR || c == quoteChar || c == delimChar) {\n                            quote = true;\n                            break;\n                        }\n                        pos++;\n                    }\n\n                    if (!quote) {\n                        pos = end - 1;\n                        c = value.charAt(pos);\n                        // Some other chars at the end caused the parser to fail, so for now\n                        // encapsulate if we end in anything less than ' '\n                        if (c <= SP) {\n                            quote = true;\n                        }\n                    }\n                }\n            }\n\n            if (!quote) {\n                // no encapsulation needed - write out the original value\n                out.append(value, start, end);\n                return;\n            }\n            break;\n        default:\n            throw new IllegalStateException(\"Unexpected Quote value: \" + quoteModePolicy);\n        }\n\n        if (!quote) {\n            // no encapsulation needed - write out the original value\n            out.append(value, start, end);\n            return;\n        }\n\n        // we hit something that needed encapsulation\n        out.append(quoteChar);\n\n        // Pick up where we left off: pos should be positioned on the first character that caused\n        // the need for encapsulation.\n        while (pos < end) {\n            final char c = value.charAt(pos);\n            if (c == quoteChar) {\n                // write out the chunk up until this point\n\n                // add 1 to the length to write out the encapsulator also\n                out.append(value, start, pos + 1);\n                // put the next starting position on the encapsulator so we will\n                // write it out again with the next string (effectively doubling it)\n                start = pos;\n            }\n            pos++;\n        }\n\n        // write the last segment\n        out.append(value, start, pos);\n        out.append(quoteChar);\n    }",
        "fixed_src_wo_comments": "private void printAndQuote ( final Object object , final CharSequence value , final int offset , final int len , final Appendable out , final boolean newRecord ) throws IOException { boolean quote = false ; int start = offset ; int pos = offset ; final int end = offset + len ; final char delimChar = getDelimiter ( ) ; final char quoteChar = getQuoteCharacter ( ) . charValue ( ) ; QuoteMode quoteModePolicy = getQuoteMode ( ) ; if ( quoteModePolicy == null ) { quoteModePolicy = QuoteMode . MINIMAL ; } switch ( quoteModePolicy ) { case ALL : quote = true ; break ; case NON_NUMERIC : quote = ! ( object instanceof Number ) ; break ; case NONE : printAndEscape ( value , offset , len , out ) ; return ; case MINIMAL : if ( len <= 0 ) { if ( newRecord ) { quote = true ; } } else { char c = value . charAt ( pos ) ; if ( newRecord && ( c < 0x20 || c > 0x21 && c < 0x23 || c > 0x2B && c < 0x2D || c > 0x7E ) ) { quote = true ; } else if ( c <= COMMENT ) { quote = true ; } else { while ( pos < end ) { c = value . charAt ( pos ) ; if ( c == LF || c == CR || c == quoteChar || c == delimChar ) { quote = true ; break ; } pos ++ ; } if ( ! quote ) { pos = end - 1 ; c = value . charAt ( pos ) ; if ( c <= SP ) { quote = true ; } } } } if ( ! quote ) { out . append ( value , start , end ) ; return ; } break ; default : throw new IllegalStateException ( \"Unexpected Quote value: \" + quoteModePolicy ) ; } if ( ! quote ) { out . append ( value , start , end ) ; return ; } out . append ( quoteChar ) ; while ( pos < end ) { final char c = value . charAt ( pos ) ; if ( c == quoteChar ) { out . append ( value , start , pos + 1 ) ; start = pos ; } pos ++ ; } out . append ( value , start , pos ) ; out . append ( quoteChar ) ; }",
        "summary": "Negative numeric values in the first column are always quoted in minimal mode",
        "Description": "Negative Numeric values are always quoted in minimal mode if (and only if) they are in the first column.\n\ni.e.\nlong,lat,data\n\"-92.222\",43.333,3\n\nLooking at the code, this is by design but seem to be for an unknown reason.\n\nFrom v1.2 CSVPrinter line 230:\n\n// TODO where did this rule come from?\nif (newRecord && (c < '0' || (c > '9' && c < 'A') || (c > 'Z' && c < 'a') || (c > 'z'))) {\n    quote = true;\n} else ...\n   \n\nI propose this rule to either be remove or at a minimum be changed to:\n// TODO where did this rule come from?\nif (newRecord && (c !='-' && c < '0' || (c > '9' && c < 'A') || (c > 'Z' && c < 'a') || (c > 'z'))) {\n    quote = true;\n} else ...\n   \n\n",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-171",
        "comments": [
            "I wonder if that newRecord rule is a poorer version of the rule higher up that ensures that if there is only one single column that it is quoted.\nCompletely agree that it'd be really helpful if this worked correctly with negative numbers in particular as this causes problems when e.g. writing out a CSV file and then attempting to import it as some systems take the quotes as forcing it to be a string. ",
            "The first char test is now per RFC4180 (https://tools.ietf.org/html/rfc4180): TEXTDATA =  %x20-21 / %x23-2B / %x2D-7E.\n\nPlease verify and fix."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to change the first char test to be per RFC4180, which states that TEXTDATA should be %x20-21 / %x23-2B / %x2D-7E. This should ensure that negative numbers are correctly written out and imported as strings."
    },
    "Math_69_src/main/java/org/apache/commons/math/stat/correlation/PearsonsCorrelation.java_160_176": {
        "src": "public RealMatrix getCorrelationPValues() throws MathException {\n        TDistribution tDistribution = new TDistributionImpl(nObs - 2);\n        int nVars = correlationMatrix.getColumnDimension();\n        double[][] out = new double[nVars][nVars];\n        for (int i = 0; i < nVars; i++) {\n            for (int j = 0; j < nVars; j++) {\n                if (i == j) {\n                    out[i][j] = 0d;\n                } else {\n                    double r = correlationMatrix.getEntry(i, j);\n                    double t = Math.abs(r * Math.sqrt((nObs - 2)/(1 - r * r)));\n                    out[i][j] = 2 * (1 - tDistribution.cumulativeProbability(t));\n                }\n            }\n        }\n        return new BlockRealMatrix(out);\n    }",
        "src_wo_comments": "public RealMatrix getCorrelationPValues ( ) throws MathException { TDistribution tDistribution = new TDistributionImpl ( nObs - 2 ) ; int nVars = correlationMatrix . getColumnDimension ( ) ; double [ ] [ ] out = new double [ nVars ] [ nVars ] ; for ( int i = 0 ; i < nVars ; i ++ ) { for ( int j = 0 ; j < nVars ; j ++ ) { if ( i == j ) { out [ i ] [ j ] = 0d ; } else { double r = correlationMatrix . getEntry ( i , j ) ; double t = Math . abs ( r * Math . sqrt ( ( nObs - 2 ) / ( 1 - r * r ) ) ) ; out [ i ] [ j ] = 2 * ( 1 - tDistribution . cumulativeProbability ( t ) ) ; } } } return new BlockRealMatrix ( out ) ; }",
        "fixed_src": "public RealMatrix getCorrelationPValues() throws MathException {\n        TDistribution tDistribution = new TDistributionImpl(nObs - 2);\n        int nVars = correlationMatrix.getColumnDimension();\n        double[][] out = new double[nVars][nVars];\n        for (int i = 0; i < nVars; i++) {\n            for (int j = 0; j < nVars; j++) {\n                if (i == j) {\n                    out[i][j] = 0d;\n                } else {\n                    double r = correlationMatrix.getEntry(i, j);\n                    double t = Math.abs(r * Math.sqrt((nObs - 2)/(1 - r * r)));\n                    out[i][j] = 2 * tDistribution.cumulativeProbability(-t);\n                }\n            }\n        }\n        return new BlockRealMatrix(out);\n    }",
        "fixed_src_wo_comments": "public RealMatrix getCorrelationPValues ( ) throws MathException { TDistribution tDistribution = new TDistributionImpl ( nObs - 2 ) ; int nVars = correlationMatrix . getColumnDimension ( ) ; double [ ] [ ] out = new double [ nVars ] [ nVars ] ; for ( int i = 0 ; i < nVars ; i ++ ) { for ( int j = 0 ; j < nVars ; j ++ ) { if ( i == j ) { out [ i ] [ j ] = 0d ; } else { double r = correlationMatrix . getEntry ( i , j ) ; double t = Math . abs ( r * Math . sqrt ( ( nObs - 2 ) / ( 1 - r * r ) ) ) ; out [ i ] [ j ] = 2 * tDistribution . cumulativeProbability ( - t ) ; } } } return new BlockRealMatrix ( out ) ; }",
        "summary": "PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon",
        "Description": "Similar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that.\n\nIn MATH-201, the problem was described as such:\n> So in essence, the p-value returned by TTestImpl.tTest() is:\n> \n> 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t))\n> \n> For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When \n> cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because:\n> \n> 1.0 - 1.0 + 0.0 = 0.0\n\nThe solution in MATH-201 was to modify the p-value calculation to this:\n> p = 2.0 * cumulativeProbability(-t)\n\nHere, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():\n  p = 2 * (1 - tDistribution.cumulativeProbability(t));\n\nDirectly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:\n  p = 2 * (tDistribution.cumulativeProbability(-t));\n\n\n\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-371",
        "comments": [
            "Thanks for reporting this.",
            "Fixed in r944939.  Thanks!",
            "Closing issue as it was included in version 2.2, which has been released"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in version 2.2, which has been released and is included in r944939. The issue has been closed."
    },
    "Math_86_src/java/org/apache/commons/math/linear/CholeskyDecompositionImpl.java_93_151": {
        "src": "public CholeskyDecompositionImpl(final RealMatrix matrix,\n                                     final double relativeSymmetryThreshold,\n                                     final double absolutePositivityThreshold)\n        throws NonSquareMatrixException,\n               NotSymmetricMatrixException, NotPositiveDefiniteMatrixException {\n\n        if (!matrix.isSquare()) {\n            throw new NonSquareMatrixException(matrix.getRowDimension(),\n                                               matrix.getColumnDimension());\n        }\n\n        final int order = matrix.getRowDimension();\n        lTData   = matrix.getData();\n        cachedL  = null;\n        cachedLT = null;\n\n        // check the matrix before transformation\n        for (int i = 0; i < order; ++i) {\n\n            final double[] lI = lTData[i];\n\n            if (lTData[i][i] < absolutePositivityThreshold) {\n                throw new NotPositiveDefiniteMatrixException();\n            }\n            // check off-diagonal elements (and reset them to 0)\n            for (int j = i + 1; j < order; ++j) {\n                final double[] lJ = lTData[j];\n                final double lIJ = lI[j];\n                final double lJI = lJ[i];\n                final double maxDelta =\n                    relativeSymmetryThreshold * Math.max(Math.abs(lIJ), Math.abs(lJI));\n                if (Math.abs(lIJ - lJI) > maxDelta) {\n                    throw new NotSymmetricMatrixException();\n                }\n                lJ[i] = 0;\n           }\n        }\n\n        // transform the matrix\n        for (int i = 0; i < order; ++i) {\n\n            final double[] ltI = lTData[i];\n\n            // check diagonal element\n\n            ltI[i] = Math.sqrt(ltI[i]);\n            final double inverse = 1.0 / ltI[i];\n\n            for (int q = order - 1; q > i; --q) {\n                ltI[q] *= inverse;\n                final double[] ltQ = lTData[q];\n                for (int p = q; p < order; ++p) {\n                    ltQ[p] -= ltI[q] * ltI[p];\n                }\n            }\n\n        }\n\n    }",
        "src_wo_comments": "public CholeskyDecompositionImpl ( final RealMatrix matrix , final double relativeSymmetryThreshold , final double absolutePositivityThreshold ) throws NonSquareMatrixException , NotSymmetricMatrixException , NotPositiveDefiniteMatrixException { if ( ! matrix . isSquare ( ) ) { throw new NonSquareMatrixException ( matrix . getRowDimension ( ) , matrix . getColumnDimension ( ) ) ; } final int order = matrix . getRowDimension ( ) ; lTData = matrix . getData ( ) ; cachedL = null ; cachedLT = null ; for ( int i = 0 ; i < order ; ++ i ) { final double [ ] lI = lTData [ i ] ; if ( lTData [ i ] [ i ] < absolutePositivityThreshold ) { throw new NotPositiveDefiniteMatrixException ( ) ; } for ( int j = i + 1 ; j < order ; ++ j ) { final double [ ] lJ = lTData [ j ] ; final double lIJ = lI [ j ] ; final double lJI = lJ [ i ] ; final double maxDelta = relativeSymmetryThreshold * Math . max ( Math . abs ( lIJ ) , Math . abs ( lJI ) ) ; if ( Math . abs ( lIJ - lJI ) > maxDelta ) { throw new NotSymmetricMatrixException ( ) ; } lJ [ i ] = 0 ; } } for ( int i = 0 ; i < order ; ++ i ) { final double [ ] ltI = lTData [ i ] ; ltI [ i ] = Math . sqrt ( ltI [ i ] ) ; final double inverse = 1.0 / ltI [ i ] ; for ( int q = order - 1 ; q > i ; -- q ) { ltI [ q ] *= inverse ; final double [ ] ltQ = lTData [ q ] ; for ( int p = q ; p < order ; ++ p ) { ltQ [ p ] -= ltI [ q ] * ltI [ p ] ; } } } }",
        "fixed_src": "public CholeskyDecompositionImpl(final RealMatrix matrix,\n                                     final double relativeSymmetryThreshold,\n                                     final double absolutePositivityThreshold)\n        throws NonSquareMatrixException,\n               NotSymmetricMatrixException, NotPositiveDefiniteMatrixException {\n\n        if (!matrix.isSquare()) {\n            throw new NonSquareMatrixException(matrix.getRowDimension(),\n                                               matrix.getColumnDimension());\n        }\n\n        final int order = matrix.getRowDimension();\n        lTData   = matrix.getData();\n        cachedL  = null;\n        cachedLT = null;\n\n        // check the matrix before transformation\n        for (int i = 0; i < order; ++i) {\n\n            final double[] lI = lTData[i];\n\n            // check off-diagonal elements (and reset them to 0)\n            for (int j = i + 1; j < order; ++j) {\n                final double[] lJ = lTData[j];\n                final double lIJ = lI[j];\n                final double lJI = lJ[i];\n                final double maxDelta =\n                    relativeSymmetryThreshold * Math.max(Math.abs(lIJ), Math.abs(lJI));\n                if (Math.abs(lIJ - lJI) > maxDelta) {\n                    throw new NotSymmetricMatrixException();\n                }\n                lJ[i] = 0;\n           }\n        }\n\n        // transform the matrix\n        for (int i = 0; i < order; ++i) {\n\n            final double[] ltI = lTData[i];\n\n            // check diagonal element\n            if (ltI[i] < absolutePositivityThreshold) {\n                throw new NotPositiveDefiniteMatrixException();\n            }\n\n            ltI[i] = Math.sqrt(ltI[i]);\n            final double inverse = 1.0 / ltI[i];\n\n            for (int q = order - 1; q > i; --q) {\n                ltI[q] *= inverse;\n                final double[] ltQ = lTData[q];\n                for (int p = q; p < order; ++p) {\n                    ltQ[p] -= ltI[q] * ltI[p];\n                }\n            }\n\n        }\n\n    }",
        "fixed_src_wo_comments": "public CholeskyDecompositionImpl ( final RealMatrix matrix , final double relativeSymmetryThreshold , final double absolutePositivityThreshold ) throws NonSquareMatrixException , NotSymmetricMatrixException , NotPositiveDefiniteMatrixException { if ( ! matrix . isSquare ( ) ) { throw new NonSquareMatrixException ( matrix . getRowDimension ( ) , matrix . getColumnDimension ( ) ) ; } final int order = matrix . getRowDimension ( ) ; lTData = matrix . getData ( ) ; cachedL = null ; cachedLT = null ; for ( int i = 0 ; i < order ; ++ i ) { final double [ ] lI = lTData [ i ] ; for ( int j = i + 1 ; j < order ; ++ j ) { final double [ ] lJ = lTData [ j ] ; final double lIJ = lI [ j ] ; final double lJI = lJ [ i ] ; final double maxDelta = relativeSymmetryThreshold * Math . max ( Math . abs ( lIJ ) , Math . abs ( lJI ) ) ; if ( Math . abs ( lIJ - lJI ) > maxDelta ) { throw new NotSymmetricMatrixException ( ) ; } lJ [ i ] = 0 ; } } for ( int i = 0 ; i < order ; ++ i ) { final double [ ] ltI = lTData [ i ] ; if ( ltI [ i ] < absolutePositivityThreshold ) { throw new NotPositiveDefiniteMatrixException ( ) ; } ltI [ i ] = Math . sqrt ( ltI [ i ] ) ; final double inverse = 1.0 / ltI [ i ] ; for ( int q = order - 1 ; q > i ; -- q ) { ltI [ q ] *= inverse ; final double [ ] ltQ = lTData [ q ] ; for ( int p = q ; p < order ; ++ p ) { ltQ [ p ] -= ltI [ q ] * ltI [ p ] ; } } } }",
        "summary": "testing for symmetric positive definite matrix in CholeskyDecomposition",
        "Description": "I used this matrix:\n\n        double[][] cv = {\n            {0.40434286, 0.09376327, 0.30328980, 0.04909388},\n            {0.09376327, 0.10400408, 0.07137959, 0.04762857},\n            {0.30328980, 0.07137959, 0.30458776, 0.04882449},\n            {0.04909388, 0.04762857, 0.04882449, 0.07543265}\n        };\n\nAnd it works fine, because it is symmetric positive definite\n\nI tried this matrix:\n\n        double[][] cv = {\n            {0.40434286, -0.09376327, 0.30328980, 0.04909388},\n            {-0.09376327, 0.10400408, 0.07137959, 0.04762857},\n            {0.30328980, 0.07137959, 0.30458776, 0.04882449},\n            {0.04909388, 0.04762857, 0.04882449, 0.07543265}\n        };\n\nAnd it should throw an exception but it does not.  I tested the matrix in R and R's cholesky decomposition method returns that the matrix is not symmetric positive definite.\n\nObviously your code is not catching this appropriately.\n\nBy the way (in my opinion) the use of exceptions to check these conditions is not the best design or use for exceptions.  If you are going to force the use to try and catch these exceptions at least provide methods  to test the conditions prior to the possibility of the exception.  \n\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-274",
        "comments": [
            "fixed in subversion repository as of r 781845\nConcerning the exception, it is not possible to check the matrix without trying to decompose it, so providing an external check would be a waste as it would already do almost everything. In fact, it was exactly the reason for the bug: the check was done too early on the raw matrix, not on the matrix after some changes have been made to its elements.\nthanks for the report",
            "Luc,\n\nThanks for fixing the error.\n\nYou are missing my point.  You should provide the user the opportunity  \nto check the condition, rather than having to rely on exceptions.  The  \nclient can make the decision whether it is a waste or not.\n\nAnd, we will agree to disagree on how the library is using exceptions.\n\nRegards,\nManuel\n\n\n\n-----------------------------------------------------\nManuel D. Rossetti, Ph.D., P.E.\nAssociate Professor of Industrial Engineering\nUniversity of Arkansas\nDepartment of Industrial Engineering\n4207 Bell Engineering Center\nFayetteville, AR 72701\nPhone: (479) 575-6756\nFax: (479) 575-8431\nemail: rossetti@uark.edu\nwww: www.uark.edu/~rossetti\n\n\n\n",
            "closing resolved issue for 2.0 release"
        ],
        "summarized_discussion": "\n\nThe bug was resolved by fixing it in the subversion repository as of r 781845. The bug was caused by checking the matrix too early on the raw matrix, not on the matrix after some changes have been made to its elements. Manuel disagreed with Luc's solution of providing the user the opportunity to check the condition, rather than having to rely on exceptions, but the issue was closed for the 2.0 release."
    },
    "Csv_7_src/main/java/org/apache/commons/csv/CSVParser.java_348_376": {
        "src": "private Map<String, Integer> initializeHeader() throws IOException {\n        Map<String, Integer> hdrMap = null;\n        final String[] formatHeader = this.format.getHeader();\n        if (formatHeader != null) {\n            hdrMap = new LinkedHashMap<String, Integer>();\n\n            String[] header = null;\n            if (formatHeader.length == 0) {\n                // read the header from the first line of the file\n                final CSVRecord nextRecord = this.nextRecord();\n                if (nextRecord != null) {\n                    header = nextRecord.values();\n                }\n            } else {\n                if (this.format.getSkipHeaderRecord()) {\n                    this.nextRecord();\n                }\n                header = formatHeader;\n            }\n\n            // build the name to index mappings\n            if (header != null) {\n                for (int i = 0; i < header.length; i++) {\n                    hdrMap.put(header[i], Integer.valueOf(i));\n                }\n            }\n        }\n        return hdrMap;\n    }",
        "src_wo_comments": "private Map < String , Integer > initializeHeader ( ) throws IOException { Map < String , Integer > hdrMap = null ; final String [ ] formatHeader = this . format . getHeader ( ) ; if ( formatHeader != null ) { hdrMap = new LinkedHashMap < String , Integer > ( ) ; String [ ] header = null ; if ( formatHeader . length == 0 ) { final CSVRecord nextRecord = this . nextRecord ( ) ; if ( nextRecord != null ) { header = nextRecord . values ( ) ; } } else { if ( this . format . getSkipHeaderRecord ( ) ) { this . nextRecord ( ) ; } header = formatHeader ; } if ( header != null ) { for ( int i = 0 ; i < header . length ; i ++ ) { hdrMap . put ( header [ i ] , Integer . valueOf ( i ) ) ; } } } return hdrMap ; }",
        "fixed_src": "private Map<String, Integer> initializeHeader() throws IOException {\n        Map<String, Integer> hdrMap = null;\n        final String[] formatHeader = this.format.getHeader();\n        if (formatHeader != null) {\n            hdrMap = new LinkedHashMap<String, Integer>();\n\n            String[] header = null;\n            if (formatHeader.length == 0) {\n                // read the header from the first line of the file\n                final CSVRecord nextRecord = this.nextRecord();\n                if (nextRecord != null) {\n                    header = nextRecord.values();\n                }\n            } else {\n                if (this.format.getSkipHeaderRecord()) {\n                    this.nextRecord();\n                }\n                header = formatHeader;\n            }\n\n            // build the name to index mappings\n            if (header != null) {\n                for (int i = 0; i < header.length; i++) {\n                    if (hdrMap.containsKey(header[i])) {\n                        throw new IllegalStateException(\"The header contains duplicate names: \" + Arrays.toString(header));\n                    }\n                    hdrMap.put(header[i], Integer.valueOf(i));\n                }\n            }\n        }\n        return hdrMap;\n    }",
        "fixed_src_wo_comments": "private Map < String , Integer > initializeHeader ( ) throws IOException { Map < String , Integer > hdrMap = null ; final String [ ] formatHeader = this . format . getHeader ( ) ; if ( formatHeader != null ) { hdrMap = new LinkedHashMap < String , Integer > ( ) ; String [ ] header = null ; if ( formatHeader . length == 0 ) { final CSVRecord nextRecord = this . nextRecord ( ) ; if ( nextRecord != null ) { header = nextRecord . values ( ) ; } } else { if ( this . format . getSkipHeaderRecord ( ) ) { this . nextRecord ( ) ; } header = formatHeader ; } if ( header != null ) { for ( int i = 0 ; i < header . length ; i ++ ) { if ( hdrMap . containsKey ( header [ i ] ) ) { throw new IllegalStateException ( \"The header contains duplicate names: \" + Arrays . toString ( header ) ) ; } hdrMap . put ( header [ i ] , Integer . valueOf ( i ) ) ; } } } return hdrMap ; }",
        "summary": "HeaderMap is inconsistent when it is parsed from an input with duplicate columns names",
        "Description": "Given a parser format for csv files with a header line:\n{code}\nCSVFormat myFormat = CSVFormat.RFC4180.withDelimiter(\",\").withQuoteChar('\"').withQuotePolicy(Quote.MINIMAL)\n\t\t\t\t.withIgnoreSurroundingSpaces(true).withHeader().withSkipHeaderRecord(true);\n{code}\n\nAnd given a file with duplicate header names:\n \nCol1,Col2,Col2,Col3,Col4\n1,2,3,4,5\n4,5,6,7,8 \n\nThe HeaderMap returned by the parser misses an entry because of the Column name being used as a key, leading to wrong behavior when we rely on it.\n\nIf this is not supposed to happen in the file regarding the CSV format, at least this should raise an error. If not we should come up with a more clever way to store and access the headers.\n",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-112",
        "comments": [
            "The problem is, that we provide a key based access to the values of a {{CSVRecord}} using the {{get(String)}} method. How should that method behave if there are duplicate column names?\n\nThrowing an exception seems reasonable here.",
            "I guess handling this would imply too many changes in the API.\n\n\n",
            "Fixed in revision 1592371. An IllegalStateException is now thrown when an inconsistent header is parsed from the input."
        ],
        "summarized_discussion": "\n\nThe bug was solved by revising the code so that an IllegalStateException is thrown when an inconsistent header is parsed from the input."
    },
    "JacksonXml_1_src/main/java/com/fasterxml/jackson/dataformat/xml/deser/FromXmlParser.java_444_578": {
        "src": "@Override\n    public JsonToken nextToken() throws IOException\n    {\n        _binaryValue = null;\n        if (_nextToken != null) {\n            JsonToken t = _nextToken;\n            _currToken = t;\n            _nextToken = null;\n            switch (t) {\n            case START_OBJECT:\n                _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                break;\n            case START_ARRAY:\n                _parsingContext = _parsingContext.createChildArrayContext(-1, -1);\n                break;\n            case END_OBJECT:\n            case END_ARRAY:\n                _parsingContext = _parsingContext.getParent();\n                _namesToWrap = _parsingContext.getNamesToWrap();\n                break;\n            case FIELD_NAME:\n                _parsingContext.setCurrentName(_xmlTokens.getLocalName());\n                break;\n            default: // VALUE_STRING, VALUE_NULL\n                // should be fine as is?\n            }\n            return t;\n        }\n        int token = _xmlTokens.next();\n\n        // Need to have a loop just because we may have to eat/convert\n        // a start-element that indicates an array element.\n        while (token == XmlTokenStream.XML_START_ELEMENT) {\n            // If we thought we might get leaf, no such luck\n            if (_mayBeLeaf) {\n                // leave _mayBeLeaf set, as we start a new context\n                _nextToken = JsonToken.FIELD_NAME;\n                _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                return (_currToken = JsonToken.START_OBJECT);\n            }\n            if (_parsingContext.inArray()) {\n                // Yup: in array, so this element could be verified; but it won't be\n                // reported anyway, and we need to process following event.\n                token = _xmlTokens.next();\n                _mayBeLeaf = true;\n                continue;\n            }\n            String name = _xmlTokens.getLocalName();\n            _parsingContext.setCurrentName(name);\n\n            // Ok: virtual wrapping can be done by simply repeating current START_ELEMENT.\n            // Couple of ways to do it; but start by making _xmlTokens replay the thing...\n            if (_namesToWrap != null && _namesToWrap.contains(name)) {\n                _xmlTokens.repeatStartElement();\n            }\n\n            _mayBeLeaf = true;\n            // Ok: in array context we need to skip reporting field names.\n            // But what's the best way to find next token?\n            return (_currToken = JsonToken.FIELD_NAME);\n        }\n\n        // Ok; beyond start element, what do we get?\n        switch (token) {\n        case XmlTokenStream.XML_END_ELEMENT:\n            // Simple, except that if this is a leaf, need to suppress end:\n            if (_mayBeLeaf) {\n                _mayBeLeaf = false;\n                    // 06-Jan-2015, tatu: as per [dataformat-xml#180], need to\n                    //    expose as empty Object, not null\n                return (_currToken = JsonToken.VALUE_NULL);\n            }\n            _currToken = _parsingContext.inArray() ? JsonToken.END_ARRAY : JsonToken.END_OBJECT;\n            _parsingContext = _parsingContext.getParent();\n            _namesToWrap = _parsingContext.getNamesToWrap();\n            return _currToken;\n            \n        case XmlTokenStream.XML_ATTRIBUTE_NAME:\n            // If there was a chance of leaf node, no more...\n            if (_mayBeLeaf) {\n                _mayBeLeaf = false;\n                _nextToken = JsonToken.FIELD_NAME;\n                _currText = _xmlTokens.getText();\n                _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                return (_currToken = JsonToken.START_OBJECT);\n            }\n            _parsingContext.setCurrentName(_xmlTokens.getLocalName());\n            return (_currToken = JsonToken.FIELD_NAME);\n        case XmlTokenStream.XML_ATTRIBUTE_VALUE:\n            _currText = _xmlTokens.getText();\n            return (_currToken = JsonToken.VALUE_STRING);\n        case XmlTokenStream.XML_TEXT:\n            _currText = _xmlTokens.getText();\n            if (_mayBeLeaf) {\n                _mayBeLeaf = false;\n                /* One more refinement (pronunced like \"hack\") is that if\n                 * we had an empty String (or all white space), and we are\n                 * deserializing an array, we better hide the empty text.\n                 */\n                // Also: must skip following END_ELEMENT\n                _xmlTokens.skipEndElement();\n                if (_parsingContext.inArray()) {\n                    if (_isEmpty(_currText)) {\n                        // 06-Jan-2015, tatu: as per [dataformat-xml#180], need to\n                        //    expose as empty Object, not null (or, worse, as used to\n                        //    be done, by swallowing the token)\n                        _currToken = JsonToken.END_ARRAY;\n                        _parsingContext = _parsingContext.getParent();\n                        _namesToWrap = _parsingContext.getNamesToWrap();\n                        return _currToken;\n                    }\n                }\n                return (_currToken = JsonToken.VALUE_STRING);\n            } else {\n                // [dataformat-xml#177]: empty text may also need to be skipped\n                if (_parsingContext.inObject()\n                        && (_currToken != JsonToken.FIELD_NAME) && _isEmpty(_currText)) {\n                    _currToken = JsonToken.END_OBJECT;\n                    _parsingContext = _parsingContext.getParent();\n                    _namesToWrap = _parsingContext.getNamesToWrap();\n                    return _currToken;\n                }\n            }\n            // If not a leaf (or otherwise ignorable), need to transform into property...\n            _parsingContext.setCurrentName(_cfgNameForTextElement);\n            _nextToken = JsonToken.VALUE_STRING;\n            return (_currToken = JsonToken.FIELD_NAME);\n        case XmlTokenStream.XML_END:\n            return (_currToken = null);\n        }\n        \n        // should never get here\n        _throwInternal();\n        return null;\n    }",
        "src_wo_comments": "@ Override public JsonToken nextToken ( ) throws IOException { _binaryValue = null ; if ( _nextToken != null ) { JsonToken t = _nextToken ; _currToken = t ; _nextToken = null ; switch ( t ) { case START_OBJECT : _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; break ; case START_ARRAY : _parsingContext = _parsingContext . createChildArrayContext ( - 1 , - 1 ) ; break ; case END_OBJECT : case END_ARRAY : _parsingContext = _parsingContext . getParent ( ) ; _namesToWrap = _parsingContext . getNamesToWrap ( ) ; break ; case FIELD_NAME : _parsingContext . setCurrentName ( _xmlTokens . getLocalName ( ) ) ; break ; default : } return t ; } int token = _xmlTokens . next ( ) ; while ( token == XmlTokenStream . XML_START_ELEMENT ) { if ( _mayBeLeaf ) { _nextToken = JsonToken . FIELD_NAME ; _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; return ( _currToken = JsonToken . START_OBJECT ) ; } if ( _parsingContext . inArray ( ) ) { token = _xmlTokens . next ( ) ; _mayBeLeaf = true ; continue ; } String name = _xmlTokens . getLocalName ( ) ; _parsingContext . setCurrentName ( name ) ; if ( _namesToWrap != null && _namesToWrap . contains ( name ) ) { _xmlTokens . repeatStartElement ( ) ; } _mayBeLeaf = true ; return ( _currToken = JsonToken . FIELD_NAME ) ; } switch ( token ) { case XmlTokenStream . XML_END_ELEMENT : if ( _mayBeLeaf ) { _mayBeLeaf = false ; return ( _currToken = JsonToken . VALUE_NULL ) ; } _currToken = _parsingContext . inArray ( ) ? JsonToken . END_ARRAY : JsonToken . END_OBJECT ; _parsingContext = _parsingContext . getParent ( ) ; _namesToWrap = _parsingContext . getNamesToWrap ( ) ; return _currToken ; case XmlTokenStream . XML_ATTRIBUTE_NAME : if ( _mayBeLeaf ) { _mayBeLeaf = false ; _nextToken = JsonToken . FIELD_NAME ; _currText = _xmlTokens . getText ( ) ; _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; return ( _currToken = JsonToken . START_OBJECT ) ; } _parsingContext . setCurrentName ( _xmlTokens . getLocalName ( ) ) ; return ( _currToken = JsonToken . FIELD_NAME ) ; case XmlTokenStream . XML_ATTRIBUTE_VALUE : _currText = _xmlTokens . getText ( ) ; return ( _currToken = JsonToken . VALUE_STRING ) ; case XmlTokenStream . XML_TEXT : _currText = _xmlTokens . getText ( ) ; if ( _mayBeLeaf ) { _mayBeLeaf = false ; _xmlTokens . skipEndElement ( ) ; if ( _parsingContext . inArray ( ) ) { if ( _isEmpty ( _currText ) ) { _currToken = JsonToken . END_ARRAY ; _parsingContext = _parsingContext . getParent ( ) ; _namesToWrap = _parsingContext . getNamesToWrap ( ) ; return _currToken ; } } return ( _currToken = JsonToken . VALUE_STRING ) ; } else { if ( _parsingContext . inObject ( ) && ( _currToken != JsonToken . FIELD_NAME ) && _isEmpty ( _currText ) ) { _currToken = JsonToken . END_OBJECT ; _parsingContext = _parsingContext . getParent ( ) ; _namesToWrap = _parsingContext . getNamesToWrap ( ) ; return _currToken ; } } _parsingContext . setCurrentName ( _cfgNameForTextElement ) ; _nextToken = JsonToken . VALUE_STRING ; return ( _currToken = JsonToken . FIELD_NAME ) ; case XmlTokenStream . XML_END : return ( _currToken = null ) ; } _throwInternal ( ) ; return null ; }",
        "fixed_src": "@Override\n    public JsonToken nextToken() throws IOException\n    {\n        _binaryValue = null;\n        if (_nextToken != null) {\n            JsonToken t = _nextToken;\n            _currToken = t;\n            _nextToken = null;\n            switch (t) {\n            case START_OBJECT:\n                _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                break;\n            case START_ARRAY:\n                _parsingContext = _parsingContext.createChildArrayContext(-1, -1);\n                break;\n            case END_OBJECT:\n            case END_ARRAY:\n                _parsingContext = _parsingContext.getParent();\n                _namesToWrap = _parsingContext.getNamesToWrap();\n                break;\n            case FIELD_NAME:\n                _parsingContext.setCurrentName(_xmlTokens.getLocalName());\n                break;\n            default: // VALUE_STRING, VALUE_NULL\n                // should be fine as is?\n            }\n            return t;\n        }\n        int token = _xmlTokens.next();\n\n        // Need to have a loop just because we may have to eat/convert\n        // a start-element that indicates an array element.\n        while (token == XmlTokenStream.XML_START_ELEMENT) {\n            // If we thought we might get leaf, no such luck\n            if (_mayBeLeaf) {\n                // leave _mayBeLeaf set, as we start a new context\n                _nextToken = JsonToken.FIELD_NAME;\n                _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                return (_currToken = JsonToken.START_OBJECT);\n            }\n            if (_parsingContext.inArray()) {\n                // Yup: in array, so this element could be verified; but it won't be\n                // reported anyway, and we need to process following event.\n                token = _xmlTokens.next();\n                _mayBeLeaf = true;\n                continue;\n            }\n            String name = _xmlTokens.getLocalName();\n            _parsingContext.setCurrentName(name);\n\n            // Ok: virtual wrapping can be done by simply repeating current START_ELEMENT.\n            // Couple of ways to do it; but start by making _xmlTokens replay the thing...\n            if (_namesToWrap != null && _namesToWrap.contains(name)) {\n                _xmlTokens.repeatStartElement();\n            }\n\n            _mayBeLeaf = true;\n            // Ok: in array context we need to skip reporting field names.\n            // But what's the best way to find next token?\n            return (_currToken = JsonToken.FIELD_NAME);\n        }\n\n        // Ok; beyond start element, what do we get?\n        switch (token) {\n        case XmlTokenStream.XML_END_ELEMENT:\n            // Simple, except that if this is a leaf, need to suppress end:\n            if (_mayBeLeaf) {\n                _mayBeLeaf = false;\n                if (_parsingContext.inArray()) {\n                    // 06-Jan-2015, tatu: as per [dataformat-xml#180], need to\n                    //    expose as empty Object, not null\n                    _nextToken = JsonToken.END_OBJECT;\n                    _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                    return (_currToken = JsonToken.START_OBJECT);\n                }\n                return (_currToken = JsonToken.VALUE_NULL);\n            }\n            _currToken = _parsingContext.inArray() ? JsonToken.END_ARRAY : JsonToken.END_OBJECT;\n            _parsingContext = _parsingContext.getParent();\n            _namesToWrap = _parsingContext.getNamesToWrap();\n            return _currToken;\n            \n        case XmlTokenStream.XML_ATTRIBUTE_NAME:\n            // If there was a chance of leaf node, no more...\n            if (_mayBeLeaf) {\n                _mayBeLeaf = false;\n                _nextToken = JsonToken.FIELD_NAME;\n                _currText = _xmlTokens.getText();\n                _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                return (_currToken = JsonToken.START_OBJECT);\n            }\n            _parsingContext.setCurrentName(_xmlTokens.getLocalName());\n            return (_currToken = JsonToken.FIELD_NAME);\n        case XmlTokenStream.XML_ATTRIBUTE_VALUE:\n            _currText = _xmlTokens.getText();\n            return (_currToken = JsonToken.VALUE_STRING);\n        case XmlTokenStream.XML_TEXT:\n            _currText = _xmlTokens.getText();\n            if (_mayBeLeaf) {\n                _mayBeLeaf = false;\n                /* One more refinement (pronunced like \"hack\") is that if\n                 * we had an empty String (or all white space), and we are\n                 * deserializing an array, we better hide the empty text.\n                 */\n                // Also: must skip following END_ELEMENT\n                _xmlTokens.skipEndElement();\n                if (_parsingContext.inArray()) {\n                    if (_isEmpty(_currText)) {\n                        // 06-Jan-2015, tatu: as per [dataformat-xml#180], need to\n                        //    expose as empty Object, not null (or, worse, as used to\n                        //    be done, by swallowing the token)\n                        _nextToken = JsonToken.END_OBJECT;\n                        _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                        return (_currToken = JsonToken.START_OBJECT);\n                    }\n                }\n                return (_currToken = JsonToken.VALUE_STRING);\n            } else {\n                // [dataformat-xml#177]: empty text may also need to be skipped\n                if (_parsingContext.inObject()\n                        && (_currToken != JsonToken.FIELD_NAME) && _isEmpty(_currText)) {\n                    _currToken = JsonToken.END_OBJECT;\n                    _parsingContext = _parsingContext.getParent();\n                    _namesToWrap = _parsingContext.getNamesToWrap();\n                    return _currToken;\n                }\n            }\n            // If not a leaf (or otherwise ignorable), need to transform into property...\n            _parsingContext.setCurrentName(_cfgNameForTextElement);\n            _nextToken = JsonToken.VALUE_STRING;\n            return (_currToken = JsonToken.FIELD_NAME);\n        case XmlTokenStream.XML_END:\n            return (_currToken = null);\n        }\n        \n        // should never get here\n        _throwInternal();\n        return null;\n    }",
        "fixed_src_wo_comments": "@ Override public JsonToken nextToken ( ) throws IOException { _binaryValue = null ; if ( _nextToken != null ) { JsonToken t = _nextToken ; _currToken = t ; _nextToken = null ; switch ( t ) { case START_OBJECT : _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; break ; case START_ARRAY : _parsingContext = _parsingContext . createChildArrayContext ( - 1 , - 1 ) ; break ; case END_OBJECT : case END_ARRAY : _parsingContext = _parsingContext . getParent ( ) ; _namesToWrap = _parsingContext . getNamesToWrap ( ) ; break ; case FIELD_NAME : _parsingContext . setCurrentName ( _xmlTokens . getLocalName ( ) ) ; break ; default : } return t ; } int token = _xmlTokens . next ( ) ; while ( token == XmlTokenStream . XML_START_ELEMENT ) { if ( _mayBeLeaf ) { _nextToken = JsonToken . FIELD_NAME ; _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; return ( _currToken = JsonToken . START_OBJECT ) ; } if ( _parsingContext . inArray ( ) ) { token = _xmlTokens . next ( ) ; _mayBeLeaf = true ; continue ; } String name = _xmlTokens . getLocalName ( ) ; _parsingContext . setCurrentName ( name ) ; if ( _namesToWrap != null && _namesToWrap . contains ( name ) ) { _xmlTokens . repeatStartElement ( ) ; } _mayBeLeaf = true ; return ( _currToken = JsonToken . FIELD_NAME ) ; } switch ( token ) { case XmlTokenStream . XML_END_ELEMENT : if ( _mayBeLeaf ) { _mayBeLeaf = false ; if ( _parsingContext . inArray ( ) ) { _nextToken = JsonToken . END_OBJECT ; _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; return ( _currToken = JsonToken . START_OBJECT ) ; } return ( _currToken = JsonToken . VALUE_NULL ) ; } _currToken = _parsingContext . inArray ( ) ? JsonToken . END_ARRAY : JsonToken . END_OBJECT ; _parsingContext = _parsingContext . getParent ( ) ; _namesToWrap = _parsingContext . getNamesToWrap ( ) ; return _currToken ; case XmlTokenStream . XML_ATTRIBUTE_NAME : if ( _mayBeLeaf ) { _mayBeLeaf = false ; _nextToken = JsonToken . FIELD_NAME ; _currText = _xmlTokens . getText ( ) ; _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; return ( _currToken = JsonToken . START_OBJECT ) ; } _parsingContext . setCurrentName ( _xmlTokens . getLocalName ( ) ) ; return ( _currToken = JsonToken . FIELD_NAME ) ; case XmlTokenStream . XML_ATTRIBUTE_VALUE : _currText = _xmlTokens . getText ( ) ; return ( _currToken = JsonToken . VALUE_STRING ) ; case XmlTokenStream . XML_TEXT : _currText = _xmlTokens . getText ( ) ; if ( _mayBeLeaf ) { _mayBeLeaf = false ; _xmlTokens . skipEndElement ( ) ; if ( _parsingContext . inArray ( ) ) { if ( _isEmpty ( _currText ) ) { _nextToken = JsonToken . END_OBJECT ; _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; return ( _currToken = JsonToken . START_OBJECT ) ; } } return ( _currToken = JsonToken . VALUE_STRING ) ; } else { if ( _parsingContext . inObject ( ) && ( _currToken != JsonToken . FIELD_NAME ) && _isEmpty ( _currText ) ) { _currToken = JsonToken . END_OBJECT ; _parsingContext = _parsingContext . getParent ( ) ; _namesToWrap = _parsingContext . getNamesToWrap ( ) ; return _currToken ; } } _parsingContext . setCurrentName ( _cfgNameForTextElement ) ; _nextToken = JsonToken . VALUE_STRING ; return ( _currToken = JsonToken . FIELD_NAME ) ; case XmlTokenStream . XML_END : return ( _currToken = null ) ; } _throwInternal ( ) ; return null ; }",
        "summary": "Problem with deserialization of nested non-wrapped lists, with empty inner list",
        "Description": "Looks like there is a problem, wherein nested structures like say:\n- Definition POJO, with `records`, unwrapped List with `Record`\n- `Record` POJO having property `fields`, another unwrapped list of `Field` POJOs\n\nand case where inner `List` happens to be empty/missing, cause incorrectly \"split\" parts of outermost `List`s (here for property `records`).\n\nI will come up with a full reproduction later on, but observed this in the wild, and I think it occurs with latest 2.7.0-rc code, as well as `2.6.4-1`, so is not just something that has been fixed with a later version.\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe bug in the source code was caused by a missing semicolon at the end of a line of code. The solution to the bug is to add the missing semicolon at the end of the line of code."
    },
    "JxPath_18_src/java/org/apache/commons/jxpath/ri/axes/AttributeContext.java_71_90": {
        "src": "public boolean nextNode() {\n        super.setPosition(getCurrentPosition() + 1);\n        if (!setStarted) {\n            setStarted = true;\n            if (!(nodeTest instanceof NodeNameTest)) {\n                return false;\n            }\n            QName name = ((NodeNameTest) nodeTest).getNodeName();\n            iterator =\n                parentContext.getCurrentNodePointer().attributeIterator(name);\n        }\n        if (iterator == null) {\n            return false;\n        }\n        if (!iterator.setPosition(iterator.getPosition() + 1)) {\n            return false;\n        }\n        currentNodePointer = iterator.getNodePointer();\n        return true;\n    }",
        "src_wo_comments": "public boolean nextNode ( ) { super . setPosition ( getCurrentPosition ( ) + 1 ) ; if ( ! setStarted ) { setStarted = true ; if ( ! ( nodeTest instanceof NodeNameTest ) ) { return false ; } QName name = ( ( NodeNameTest ) nodeTest ) . getNodeName ( ) ; iterator = parentContext . getCurrentNodePointer ( ) . attributeIterator ( name ) ; } if ( iterator == null ) { return false ; } if ( ! iterator . setPosition ( iterator . getPosition ( ) + 1 ) ) { return false ; } currentNodePointer = iterator . getNodePointer ( ) ; return true ; }",
        "fixed_src": "public boolean nextNode() {\n        super.setPosition(getCurrentPosition() + 1);\n        if (!setStarted) {\n            setStarted = true;\n            NodeNameTest nodeNameTest = null;\n            if (nodeTest instanceof NodeTypeTest) {\n                if (((NodeTypeTest) nodeTest).getNodeType() == Compiler.NODE_TYPE_NODE) {\n                    nodeNameTest = WILDCARD_TEST;\n                }\n            }\n            else if (nodeTest instanceof NodeNameTest) {\n                nodeNameTest = (NodeNameTest) nodeTest;\n            }\n            if (nodeNameTest == null) {\n                return false;\n            }\n            iterator = parentContext.getCurrentNodePointer().attributeIterator(\n                    nodeNameTest.getNodeName());\n        }\n        if (iterator == null) {\n            return false;\n        }\n        if (!iterator.setPosition(iterator.getPosition() + 1)) {\n            return false;\n        }\n        currentNodePointer = iterator.getNodePointer();\n        return true;\n    }",
        "fixed_src_wo_comments": "public boolean nextNode ( ) { super . setPosition ( getCurrentPosition ( ) + 1 ) ; if ( ! setStarted ) { setStarted = true ; NodeNameTest nodeNameTest = null ; if ( nodeTest instanceof NodeTypeTest ) { if ( ( ( NodeTypeTest ) nodeTest ) . getNodeType ( ) == Compiler . NODE_TYPE_NODE ) { nodeNameTest = WILDCARD_TEST ; } } else if ( nodeTest instanceof NodeNameTest ) { nodeNameTest = ( NodeNameTest ) nodeTest ; } if ( nodeNameTest == null ) { return false ; } iterator = parentContext . getCurrentNodePointer ( ) . attributeIterator ( nodeNameTest . getNodeName ( ) ) ; } if ( iterator == null ) { return false ; } if ( ! iterator . setPosition ( iterator . getPosition ( ) + 1 ) ) { return false ; } currentNodePointer = iterator . getNodePointer ( ) ; return true ; }",
        "summary": "Issue with attribute::",
        "Description": "Checking test (Issue172_CountAttributeNode) I came with the following fix for the code in AttributeContext  line 72\nfrom \n-----\nif (!(nodeTest instanceof NodeNameTest)) {\n                return false;\n            }\n            QName name = ((NodeNameTest) nodeTest).getNodeName();\n            \n------\n'\nto \n--- (outside method)\nprivate static final QName WILDCARD = new QName(\"\", \"*\");\n--- (in method)\n    \nfinal QName name ;\nif (nodeTest instanceof NodeTypeTest)\n{\n\t if (((NodeTypeTest) nodeTest).getNodeType() == Compiler.NODE_TYPE_NODE)\n\t\t name = WILDCARD;\n\t else return false;\n}\nelse if (nodeTest instanceof NodeNameTest) {\n\tname = ((NodeNameTest) nodeTest).getNodeName();\n}\nelse\n{\n\treturn false;\n}\n\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/JXPATH-115",
        "comments": [
            "Test case:\n\nxpath\n\n//attribute::node()\n\nxml\n\n<xml foo='bar'/>",
            "committed a similar change; rev 617942.",
            "Plus even a bit more similar in rev 617943.  ;)"
        ],
        "summarized_discussion": "\n\nThe bug was solved by committing a similar change in revision 617942 and an even more similar change in revision 617943."
    },
    "Codec_2_src/java/org/apache/commons/codec/binary/Base64.java_414_473": {
        "src": "void encode(byte[] in, int inPos, int inAvail) {\n        if (eof) {\n            return;\n        }\n        // inAvail < 0 is how we're informed of EOF in the underlying data we're\n        // encoding.\n        if (inAvail < 0) {\n            eof = true;\n            if (buf == null || buf.length - pos < encodeSize) {\n                resizeBuf();\n            }\n            switch (modulus) {\n                case 1:\n                    buf[pos++] = encodeTable[(x >> 2) & MASK_6BITS];\n                    buf[pos++] = encodeTable[(x << 4) & MASK_6BITS];\n                    // URL-SAFE skips the padding to further reduce size.\n                    if (encodeTable == STANDARD_ENCODE_TABLE) {\n                        buf[pos++] = PAD;\n                        buf[pos++] = PAD;\n                    }\n                    break;\n\n                case 2:\n                    buf[pos++] = encodeTable[(x >> 10) & MASK_6BITS];\n                    buf[pos++] = encodeTable[(x >> 4) & MASK_6BITS];\n                    buf[pos++] = encodeTable[(x << 2) & MASK_6BITS];\n                    // URL-SAFE skips the padding to further reduce size.\n                    if (encodeTable == STANDARD_ENCODE_TABLE) {\n                        buf[pos++] = PAD;\n                    }\n                    break;\n            }\n            if (lineLength > 0) {\n                System.arraycopy(lineSeparator, 0, buf, pos, lineSeparator.length);\n                pos += lineSeparator.length;\n            }\n        } else {\n            for (int i = 0; i < inAvail; i++) {\n                if (buf == null || buf.length - pos < encodeSize) {\n                    resizeBuf();\n                }\n                modulus = (++modulus) % 3;\n                int b = in[inPos++];\n                if (b < 0) { b += 256; }\n                x = (x << 8) + b;\n                if (0 == modulus) {\n                    buf[pos++] = encodeTable[(x >> 18) & MASK_6BITS];\n                    buf[pos++] = encodeTable[(x >> 12) & MASK_6BITS];\n                    buf[pos++] = encodeTable[(x >> 6) & MASK_6BITS];\n                    buf[pos++] = encodeTable[x & MASK_6BITS];\n                    currentLinePos += 4;\n                    if (lineLength > 0 && lineLength <= currentLinePos) {\n                        System.arraycopy(lineSeparator, 0, buf, pos, lineSeparator.length);\n                        pos += lineSeparator.length;\n                        currentLinePos = 0;\n                    }\n                }\n            }\n        }\n    }",
        "src_wo_comments": "void encode ( byte [ ] in , int inPos , int inAvail ) { if ( eof ) { return ; } if ( inAvail < 0 ) { eof = true ; if ( buf == null || buf . length - pos < encodeSize ) { resizeBuf ( ) ; } switch ( modulus ) { case 1 : buf [ pos ++ ] = encodeTable [ ( x >> 2 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ ( x << 4 ) & MASK_6BITS ] ; if ( encodeTable == STANDARD_ENCODE_TABLE ) { buf [ pos ++ ] = PAD ; buf [ pos ++ ] = PAD ; } break ; case 2 : buf [ pos ++ ] = encodeTable [ ( x >> 10 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ ( x >> 4 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ ( x << 2 ) & MASK_6BITS ] ; if ( encodeTable == STANDARD_ENCODE_TABLE ) { buf [ pos ++ ] = PAD ; } break ; } if ( lineLength > 0 ) { System . arraycopy ( lineSeparator , 0 , buf , pos , lineSeparator . length ) ; pos += lineSeparator . length ; } } else { for ( int i = 0 ; i < inAvail ; i ++ ) { if ( buf == null || buf . length - pos < encodeSize ) { resizeBuf ( ) ; } modulus = ( ++ modulus ) % 3 ; int b = in [ inPos ++ ] ; if ( b < 0 ) { b += 256 ; } x = ( x << 8 ) + b ; if ( 0 == modulus ) { buf [ pos ++ ] = encodeTable [ ( x >> 18 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ ( x >> 12 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ ( x >> 6 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ x & MASK_6BITS ] ; currentLinePos += 4 ; if ( lineLength > 0 && lineLength <= currentLinePos ) { System . arraycopy ( lineSeparator , 0 , buf , pos , lineSeparator . length ) ; pos += lineSeparator . length ; currentLinePos = 0 ; } } } } }",
        "fixed_src": "void encode(byte[] in, int inPos, int inAvail) {\n        if (eof) {\n            return;\n        }\n        // inAvail < 0 is how we're informed of EOF in the underlying data we're\n        // encoding.\n        if (inAvail < 0) {\n            eof = true;\n            if (buf == null || buf.length - pos < encodeSize) {\n                resizeBuf();\n            }\n            switch (modulus) {\n                case 1:\n                    buf[pos++] = encodeTable[(x >> 2) & MASK_6BITS];\n                    buf[pos++] = encodeTable[(x << 4) & MASK_6BITS];\n                    // URL-SAFE skips the padding to further reduce size.\n                    if (encodeTable == STANDARD_ENCODE_TABLE) {\n                        buf[pos++] = PAD;\n                        buf[pos++] = PAD;\n                    }\n                    break;\n\n                case 2:\n                    buf[pos++] = encodeTable[(x >> 10) & MASK_6BITS];\n                    buf[pos++] = encodeTable[(x >> 4) & MASK_6BITS];\n                    buf[pos++] = encodeTable[(x << 2) & MASK_6BITS];\n                    // URL-SAFE skips the padding to further reduce size.\n                    if (encodeTable == STANDARD_ENCODE_TABLE) {\n                        buf[pos++] = PAD;\n                    }\n                    break;\n            }\n            if (lineLength > 0 && pos > 0) {\n                System.arraycopy(lineSeparator, 0, buf, pos, lineSeparator.length);\n                pos += lineSeparator.length;\n            }\n        } else {\n            for (int i = 0; i < inAvail; i++) {\n                if (buf == null || buf.length - pos < encodeSize) {\n                    resizeBuf();\n                }\n                modulus = (++modulus) % 3;\n                int b = in[inPos++];\n                if (b < 0) { b += 256; }\n                x = (x << 8) + b;\n                if (0 == modulus) {\n                    buf[pos++] = encodeTable[(x >> 18) & MASK_6BITS];\n                    buf[pos++] = encodeTable[(x >> 12) & MASK_6BITS];\n                    buf[pos++] = encodeTable[(x >> 6) & MASK_6BITS];\n                    buf[pos++] = encodeTable[x & MASK_6BITS];\n                    currentLinePos += 4;\n                    if (lineLength > 0 && lineLength <= currentLinePos) {\n                        System.arraycopy(lineSeparator, 0, buf, pos, lineSeparator.length);\n                        pos += lineSeparator.length;\n                        currentLinePos = 0;\n                    }\n                }\n            }\n        }\n    }",
        "fixed_src_wo_comments": "void encode ( byte [ ] in , int inPos , int inAvail ) { if ( eof ) { return ; } if ( inAvail < 0 ) { eof = true ; if ( buf == null || buf . length - pos < encodeSize ) { resizeBuf ( ) ; } switch ( modulus ) { case 1 : buf [ pos ++ ] = encodeTable [ ( x >> 2 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ ( x << 4 ) & MASK_6BITS ] ; if ( encodeTable == STANDARD_ENCODE_TABLE ) { buf [ pos ++ ] = PAD ; buf [ pos ++ ] = PAD ; } break ; case 2 : buf [ pos ++ ] = encodeTable [ ( x >> 10 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ ( x >> 4 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ ( x << 2 ) & MASK_6BITS ] ; if ( encodeTable == STANDARD_ENCODE_TABLE ) { buf [ pos ++ ] = PAD ; } break ; } if ( lineLength > 0 && pos > 0 ) { System . arraycopy ( lineSeparator , 0 , buf , pos , lineSeparator . length ) ; pos += lineSeparator . length ; } } else { for ( int i = 0 ; i < inAvail ; i ++ ) { if ( buf == null || buf . length - pos < encodeSize ) { resizeBuf ( ) ; } modulus = ( ++ modulus ) % 3 ; int b = in [ inPos ++ ] ; if ( b < 0 ) { b += 256 ; } x = ( x << 8 ) + b ; if ( 0 == modulus ) { buf [ pos ++ ] = encodeTable [ ( x >> 18 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ ( x >> 12 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ ( x >> 6 ) & MASK_6BITS ] ; buf [ pos ++ ] = encodeTable [ x & MASK_6BITS ] ; currentLinePos += 4 ; if ( lineLength > 0 && lineLength <= currentLinePos ) { System . arraycopy ( lineSeparator , 0 , buf , pos , lineSeparator . length ) ; pos += lineSeparator . length ; currentLinePos = 0 ; } } } } }",
        "summary": "Base64 bug with empty input (new byte[0])",
        "Description": "Base64.encode(new byte[0]) doesn't return an empty byte array back!  It returns CRLF.",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-77",
        "comments": [
            "Fix for codec-77, including JUnits.\n\nAlso fixed two minor naming glitches in the JUnits.",
            "Note:  codec-1.3 properly encodes empty-byte-array into an empty-byte-array.  So this is a regression on Trunk caused by the CODEC-69 patch I submitted.",
            "Committed to SVN."
        ],
        "summarized_discussion": "\n\nThe solution to the bug, codec-77, was to fix two minor naming glitches in the JUnits and commit them to SVN. Additionally, the CODEC-69 patch was reverted so that codec-1.3 properly encodes empty-byte-array into an empty-byte-array."
    },
    "Compress_31_src/main/java/org/apache/commons/compress/archivers/tar/TarUtils.java_101_148": {
        "src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        if (buffer[start] == 0) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Trim all trailing NULs and spaces.\n        // The ustar and POSIX tar specs require a trailing NUL or\n        // space but some implementations use the extra digit for big\n        // sizes/uids/gids ...\n        byte trailer = buffer[end - 1];\n        while (start < end && (trailer == 0 || trailer == ' ')) {\n            end--;\n            trailer = buffer[end - 1];\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            if (currentByte == 0) {\n                break;\n            }\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } if ( buffer [ start ] == 0 ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer = buffer [ end - 1 ] ; while ( start < end && ( trailer == 0 || trailer == ' ' ) ) { end -- ; trailer = buffer [ end - 1 ] ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte == 0 ) { break ; } if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "fixed_src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        if (buffer[start] == 0) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Trim all trailing NULs and spaces.\n        // The ustar and POSIX tar specs require a trailing NUL or\n        // space but some implementations use the extra digit for big\n        // sizes/uids/gids ...\n        byte trailer = buffer[end - 1];\n        while (start < end && (trailer == 0 || trailer == ' ')) {\n            end--;\n            trailer = buffer[end - 1];\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "fixed_src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } if ( buffer [ start ] == 0 ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer = buffer [ end - 1 ] ; while ( start < end && ( trailer == 0 || trailer == ' ' ) ) { end -- ; trailer = buffer [ end - 1 ] ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "summary": "Illegal argument exception when extracting .tgz file ",
        "Description": "When attempting to unpack a .tgz file, I am receiving the illegal argument exception: java.lang.IllegalArgumentException: Invalid byte 0 at offset 5 in '05412{NUL}11' len=8. This is causing a java.io.IOException: Error detected parsing the header error. \n\nThis is being thrown when the function TarArchiveInputStream.getNextTarEntry() is called. \n\nHere is the code I am using. \n\n{code:java}\n            TarArchiveInputStream tarIn = new TarArchiveInputStream(\n                    new GZIPInputStream(\n                            new BufferedInputStream(\n                                    new FileInputStream(\n                                            tempDirPath + fileName))));\n\n            TarArchiveEntry entry = tarIn.getNextTarEntry();\n\n            while (entry != null) {\n                File path = new File(tempDirPath, entry.getName());\n                if (entry.isDirectory()) {\n                    path.mkdirs();\n                } else {          \n                    path.createNewFile();\n                    byte[] read = new byte[2048];\n                    BufferedOutputStream bout = new BufferedOutputStream(new FileOutputStream(path));\n                    int len;\n                    while ((len = tarIn.read(read)) != -1) {\n                        bout.write(read, 0, len);\n                        System.out.print(new String(read, \"UTF-8\"));\n                    }\n                    bout.close();\n                    read = null;\n                }\n                entry = tarIn.getNextTarEntry();\n            }\n            tarIn.close();\n{code}\n\nHere is the full stack trace: \n\n[2015-02-12T23:17:31.944+0000] [glassfish 4.0] [SEVERE] [] [] [tid: _ThreadID=123 _ThreadName=Thread-4] [timeMillis: 1423783051944] [levelValue: 1000] [[\n  java.io.IOException: Error detected parsing the header\n        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:257)\n        at org.unavco.ws.tilt.ExtractTiltFile.extractFile(ExtractTiltFile.java:125)\n        at org.unavco.ws.tilt.ExtractTiltFile.run(ExtractTiltFile.java:59)\n        at org.unavco.ws.cache.ProcessDataFile.getFileData(ProcessDataFile.java:100)\n        at org.unavco.ws.cache.ProcessDataFile.getResultSet(ProcessDataFile.java:81)\n        at org.unavco.ws.tilt.TiltDsClient.write(TiltDsClient.java:47)\n        at org.glassfish.jersey.message.internal.StreamingOutputProvider.writeTo(StreamingOutputProvider.java:76)\n        at org.glassfish.jersey.message.internal.StreamingOutputProvider.writeTo(StreamingOutputProvider.java:58)\n        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor$TerminalWriterInterceptor.aroundWriteTo(WriterInterceptorExecutor.java:194)\n        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:139)\n        at org.glassfish.jersey.server.internal.JsonWithPaddingInterceptor.aroundWriteTo(JsonWithPaddingInterceptor.java:103)\n        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:139)\n        at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundWriteTo(MappableExceptionWrapperInterceptor.java:88)\n        at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:139)\n        at org.glassfish.jersey.message.internal.MessageBodyFactory.writeTo(MessageBodyFactory.java:1005)\n        at org.glassfish.jersey.server.ServerRuntime$Responder.writeResponse(ServerRuntime.java:471)\n        at org.glassfish.jersey.server.ServerRuntime$Responder.processResponse(ServerRuntime.java:333)\n        at org.glassfish.jersey.server.ServerRuntime$Responder.process(ServerRuntime.java:323)\n        at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:227)\n        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)\n        at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)\n        at org.glassfish.jersey.internal.Errors.process(Errors.java:315)\n        at org.glassfish.jersey.internal.Errors.process(Errors.java:297)\n        at org.glassfish.jersey.internal.Errors.process(Errors.java:267)\n        at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317)\n        at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:198)\n        at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:946)\n        at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:323)\n        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:372)\n        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:335)\n        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:218)\n        at org.apache.catalina.core.StandardWrapper.service(StandardWrapper.java:1682)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:344)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214)\n        at com.thetransactioncompany.cors.CORSFilter.doFilter(Unknown Source)\n        at com.thetransactioncompany.cors.CORSFilter.doFilter(Unknown Source)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:256)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:316)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:160)\n        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:734)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:673)\n        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:99)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:174)\n        at org.apache.catalina.connector.CoyoteAdapter.doService(CoyoteAdapter.java:357)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:260)\n        at com.sun.enterprise.v3.services.impl.ContainerMapper.service(ContainerMapper.java:188)\n        at org.glassfish.grizzly.http.server.HttpHandler.runService(HttpHandler.java:191)\n        at org.glassfish.grizzly.http.server.HttpHandler.doHandle(HttpHandler.java:168)\n        at org.glassfish.grizzly.http.server.HttpServerFilter.handleRead(HttpServerFilter.java:189)\n        at org.glassfish.grizzly.filterchain.ExecutorResolver$9.execute(ExecutorResolver.java:119)\n        at org.glassfish.grizzly.filterchain.DefaultFilterChain.executeFilter(DefaultFilterChain.java:288)\n        at org.glassfish.grizzly.filterchain.DefaultFilterChain.executeChainPart(DefaultFilterChain.java:206)\n        at org.glassfish.grizzly.filterchain.DefaultFilterChain.execute(DefaultFilterChain.java:136)\n        at org.glassfish.grizzly.filterchain.DefaultFilterChain.process(DefaultFilterChain.java:114)\n        at org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:77)\n        at org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:838)\n        at org.glassfish.grizzly.strategies.AbstractIOStrategy.fireIOEvent(AbstractIOStrategy.java:113)\n        at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.run0(WorkerThreadIOStrategy.java:115)\n        at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.access$100(WorkerThreadIOStrategy.java:55)\n        at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy$WorkerThreadRunnable.run(WorkerThreadIOStrategy.java:135)\n        at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:564)\n        at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:544)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalArgumentException: Invalid byte 0 at offset 5 in '05412{NUL}11' len=8\n        at org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:138)\n        at org.apache.commons.compress.archivers.tar.TarUtils.parseOctalOrBinary(TarUtils.java:169)\n        at org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:951)\n        at org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:940)\n        at org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:324)\n        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:255)\n        ... 63 more]]\n\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-301",
        "comments": [
            "Example .tgz file with error. ",
            "Also this .tar file is made using a python script, with the tarfile library.   ",
            "Thanks, Collin.  Can we distribute the file as a unit test resource?\n\nAccording to  the spec the number fields should be padded with NULs, looks as if the library just inserted a single NUL.  Our current implementations strips NULs starting at the right, which fails here.  It should be easy to fix by breaking out of the loop as soon as a NUL is detected.",
            "I don't see the Exception when running the code using current trunk.\n\nI tried both the code in the description (though I omitted writing any files) and the Lister class.\nBoth ran successfully.\n\n@Colin: Maybe you could try using the 1.10-SNAPSHOT build from the ASF snapshot repo:\nhttps://repository.apache.org/content/repositories/snapshots/\n\nDoes this fail for your sample program?",
            "Thank you Stefan and Sebb. It seems that my issue is when I'm downloading the file using the commons.net.FTPClient, and it seems somehow when I download the file it is being corrupted. \n\nFrustratingly enough, the corrupted file is still a valid tar file so I assumed the issue with was compress. So I will have to see if I need to change the encoding of the client, or if I need to do something else.  \n",
            "Make sure you tell FTPClient to use binary mode when downloading.\nOtherwise the file will be corrupted.\nAlso, when writing the file, ensure that you take account of the number of bytes read.\nYour example code did not always do this correctly:\n\n{code}\nbout.write(read, 0, len); // OK\nSystem.out.print(new String(read, \"UTF-8\")); // wrong, should be\nSystem.out.print(new String(read, 0, len, \"UTF-8\"));\n{code}",
            "[~sebb@apache.org], is this trunk after svn revision 1659649?  I've already committed a provisional fix for this bug with that revision.",
            "I wouldn't close the bug as \"not a problem\" if command line tar can extract the archive but Commons Compress can't.",
            "The attached tar seem to work with 1.9 as well, I'd really be interested in the corrupted tar if only to see what GNU tar does with it.",
            "I used 1659645"
        ],
        "summarized_discussion": "\n\nThe bug is caused by the tarfile library inserting a single NUL when creating the .tgz file, which causes the current implementations to strip NULs starting at the right and fail. The solution is to break out of the loop as soon as a NUL is detected. Additionally, when downloading the file using the commons.net.FTPClient, make sure to use binary mode and take account of the number of bytes read when writing the file. Finally, the bug should not be closed as \"not a problem\" if command line tar can extract the archive but Commons Compress can't."
    },
    "Math_28_src/main/java/org/apache/commons/math3/optimization/linear/SimplexSolver.java_90_154": {
        "src": "private Integer getPivotRow(SimplexTableau tableau, final int col) {\n        // create a list of all the rows that tie for the lowest score in the minimum ratio test\n        List<Integer> minRatioPositions = new ArrayList<Integer>();\n        double minRatio = Double.MAX_VALUE;\n        for (int i = tableau.getNumObjectiveFunctions(); i < tableau.getHeight(); i++) {\n            final double rhs = tableau.getEntry(i, tableau.getWidth() - 1);\n            final double entry = tableau.getEntry(i, col);\n\n            if (Precision.compareTo(entry, 0d, maxUlps) > 0) {\n                final double ratio = rhs / entry;\n                // check if the entry is strictly equal to the current min ratio\n                // do not use a ulp/epsilon check\n                final int cmp = Double.compare(ratio, minRatio);\n                if (cmp == 0) {\n                    minRatioPositions.add(i);\n                } else if (cmp < 0) {\n                    minRatio = ratio;\n                    minRatioPositions = new ArrayList<Integer>();\n                    minRatioPositions.add(i);\n                }\n            }\n        }\n\n        if (minRatioPositions.size() == 0) {\n            return null;\n        } else if (minRatioPositions.size() > 1) {\n            // there's a degeneracy as indicated by a tie in the minimum ratio test\n\n            // 1. check if there's an artificial variable that can be forced out of the basis\n                for (Integer row : minRatioPositions) {\n                    for (int i = 0; i < tableau.getNumArtificialVariables(); i++) {\n                        int column = i + tableau.getArtificialVariableOffset();\n                        final double entry = tableau.getEntry(row, column);\n                        if (Precision.equals(entry, 1d, maxUlps) && row.equals(tableau.getBasicRow(column))) {\n                            return row;\n                        }\n                    }\n                }\n\n            // 2. apply Bland's rule to prevent cycling:\n            //    take the row for which the corresponding basic variable has the smallest index\n            //\n            // see http://www.stanford.edu/class/msande310/blandrule.pdf\n            // see http://en.wikipedia.org/wiki/Bland%27s_rule (not equivalent to the above paper)\n            //\n            // Additional heuristic: if we did not get a solution after half of maxIterations\n            //                       revert to the simple case of just returning the top-most row\n            // This heuristic is based on empirical data gathered while investigating MATH-828.\n                Integer minRow = null;\n                int minIndex = tableau.getWidth();\n                for (Integer row : minRatioPositions) {\n                    int i = tableau.getNumObjectiveFunctions();\n                    for (; i < tableau.getWidth() - 1 && minRow != row; i++) {\n                        if (row == tableau.getBasicRow(i)) {\n                            if (i < minIndex) {\n                                minIndex = i;\n                                minRow = row;\n                            }\n                        }\n                    }\n                }\n                return minRow;\n        }\n        return minRatioPositions.get(0);\n    }",
        "src_wo_comments": "private Integer getPivotRow ( SimplexTableau tableau , final int col ) { List < Integer > minRatioPositions = new ArrayList < Integer > ( ) ; double minRatio = Double . MAX_VALUE ; for ( int i = tableau . getNumObjectiveFunctions ( ) ; i < tableau . getHeight ( ) ; i ++ ) { final double rhs = tableau . getEntry ( i , tableau . getWidth ( ) - 1 ) ; final double entry = tableau . getEntry ( i , col ) ; if ( Precision . compareTo ( entry , 0d , maxUlps ) > 0 ) { final double ratio = rhs / entry ; final int cmp = Double . compare ( ratio , minRatio ) ; if ( cmp == 0 ) { minRatioPositions . add ( i ) ; } else if ( cmp < 0 ) { minRatio = ratio ; minRatioPositions = new ArrayList < Integer > ( ) ; minRatioPositions . add ( i ) ; } } } if ( minRatioPositions . size ( ) == 0 ) { return null ; } else if ( minRatioPositions . size ( ) > 1 ) { for ( Integer row : minRatioPositions ) { for ( int i = 0 ; i < tableau . getNumArtificialVariables ( ) ; i ++ ) { int column = i + tableau . getArtificialVariableOffset ( ) ; final double entry = tableau . getEntry ( row , column ) ; if ( Precision . equals ( entry , 1d , maxUlps ) && row . equals ( tableau . getBasicRow ( column ) ) ) { return row ; } } } Integer minRow = null ; int minIndex = tableau . getWidth ( ) ; for ( Integer row : minRatioPositions ) { int i = tableau . getNumObjectiveFunctions ( ) ; for ( ; i < tableau . getWidth ( ) - 1 && minRow != row ; i ++ ) { if ( row == tableau . getBasicRow ( i ) ) { if ( i < minIndex ) { minIndex = i ; minRow = row ; } } } } return minRow ; } return minRatioPositions . get ( 0 ) ; }",
        "fixed_src": "private Integer getPivotRow(SimplexTableau tableau, final int col) {\n        // create a list of all the rows that tie for the lowest score in the minimum ratio test\n        List<Integer> minRatioPositions = new ArrayList<Integer>();\n        double minRatio = Double.MAX_VALUE;\n        for (int i = tableau.getNumObjectiveFunctions(); i < tableau.getHeight(); i++) {\n            final double rhs = tableau.getEntry(i, tableau.getWidth() - 1);\n            final double entry = tableau.getEntry(i, col);\n\n            if (Precision.compareTo(entry, 0d, maxUlps) > 0) {\n                final double ratio = rhs / entry;\n                // check if the entry is strictly equal to the current min ratio\n                // do not use a ulp/epsilon check\n                final int cmp = Double.compare(ratio, minRatio);\n                if (cmp == 0) {\n                    minRatioPositions.add(i);\n                } else if (cmp < 0) {\n                    minRatio = ratio;\n                    minRatioPositions = new ArrayList<Integer>();\n                    minRatioPositions.add(i);\n                }\n            }\n        }\n\n        if (minRatioPositions.size() == 0) {\n            return null;\n        } else if (minRatioPositions.size() > 1) {\n            // there's a degeneracy as indicated by a tie in the minimum ratio test\n\n            // 1. check if there's an artificial variable that can be forced out of the basis\n            if (tableau.getNumArtificialVariables() > 0) {\n                for (Integer row : minRatioPositions) {\n                    for (int i = 0; i < tableau.getNumArtificialVariables(); i++) {\n                        int column = i + tableau.getArtificialVariableOffset();\n                        final double entry = tableau.getEntry(row, column);\n                        if (Precision.equals(entry, 1d, maxUlps) && row.equals(tableau.getBasicRow(column))) {\n                            return row;\n                        }\n                    }\n                }\n            }\n\n            // 2. apply Bland's rule to prevent cycling:\n            //    take the row for which the corresponding basic variable has the smallest index\n            //\n            // see http://www.stanford.edu/class/msande310/blandrule.pdf\n            // see http://en.wikipedia.org/wiki/Bland%27s_rule (not equivalent to the above paper)\n            //\n            // Additional heuristic: if we did not get a solution after half of maxIterations\n            //                       revert to the simple case of just returning the top-most row\n            // This heuristic is based on empirical data gathered while investigating MATH-828.\n            if (getIterations() < getMaxIterations() / 2) {\n                Integer minRow = null;\n                int minIndex = tableau.getWidth();\n                for (Integer row : minRatioPositions) {\n                    int i = tableau.getNumObjectiveFunctions();\n                    for (; i < tableau.getWidth() - 1 && minRow != row; i++) {\n                        if (row == tableau.getBasicRow(i)) {\n                            if (i < minIndex) {\n                                minIndex = i;\n                                minRow = row;\n                            }\n                        }\n                    }\n                }\n                return minRow;\n            }\n        }\n        return minRatioPositions.get(0);\n    }",
        "fixed_src_wo_comments": "private Integer getPivotRow ( SimplexTableau tableau , final int col ) { List < Integer > minRatioPositions = new ArrayList < Integer > ( ) ; double minRatio = Double . MAX_VALUE ; for ( int i = tableau . getNumObjectiveFunctions ( ) ; i < tableau . getHeight ( ) ; i ++ ) { final double rhs = tableau . getEntry ( i , tableau . getWidth ( ) - 1 ) ; final double entry = tableau . getEntry ( i , col ) ; if ( Precision . compareTo ( entry , 0d , maxUlps ) > 0 ) { final double ratio = rhs / entry ; final int cmp = Double . compare ( ratio , minRatio ) ; if ( cmp == 0 ) { minRatioPositions . add ( i ) ; } else if ( cmp < 0 ) { minRatio = ratio ; minRatioPositions = new ArrayList < Integer > ( ) ; minRatioPositions . add ( i ) ; } } } if ( minRatioPositions . size ( ) == 0 ) { return null ; } else if ( minRatioPositions . size ( ) > 1 ) { if ( tableau . getNumArtificialVariables ( ) > 0 ) { for ( Integer row : minRatioPositions ) { for ( int i = 0 ; i < tableau . getNumArtificialVariables ( ) ; i ++ ) { int column = i + tableau . getArtificialVariableOffset ( ) ; final double entry = tableau . getEntry ( row , column ) ; if ( Precision . equals ( entry , 1d , maxUlps ) && row . equals ( tableau . getBasicRow ( column ) ) ) { return row ; } } } } if ( getIterations ( ) < getMaxIterations ( ) / 2 ) { Integer minRow = null ; int minIndex = tableau . getWidth ( ) ; for ( Integer row : minRatioPositions ) { int i = tableau . getNumObjectiveFunctions ( ) ; for ( ; i < tableau . getWidth ( ) - 1 && minRow != row ; i ++ ) { if ( row == tableau . getBasicRow ( i ) ) { if ( i < minIndex ) { minIndex = i ; minRow = row ; } } } } return minRow ; } } return minRatioPositions . get ( 0 ) ; }",
        "summary": "Not expected UnboundedSolutionException",
        "Description": "SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables.\n\nIn order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions.\nFirst iteration is runned with predefined set of input data with which the Solver gives back an appropriate result.\n\nThe problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values.\n\nWhat is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem.\n\nThe problem is formulated as\nmin(1*t + 0*L) (for every r-th subject)\ns.t.\n-q(r) + QL >= 0\nx(r)t - XL >= 0\nL >= 0\nwhere \nr = 1..R, \nL = {l(1), l(2), ..., l(R)} (vector of R rows and 1 column),\nQ - coefficients matrix MxR\nX - coefficients matrix NxR ",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-828",
        "comments": [
            "ApacheSimplexWrapperTest.java is the entry point\nApacheSimplexWrapper.java is the main class\n\ncommons-math3-3.0.jar is that same Apache Math 3 Lib I use\n\nSource code contatins come auxiliary method, almost all for printing. Just run ApacheSimplexWrapperTest.java with a debugger and run straight forward to the line 160 in ApacheSimplexWrapper.java where things happen",
            "And there is one more strange thing I see. Solutions of the problem that are not unbounded give value of t = 1 every single time while they are supposed to be within [0, 1]. E.g. in the predifend variables values case in the first iteration t1 is 1 and t2 is 0.25 (see Test #1)",
            "Did you test with a recent development snapshot?\n",
            "Hi Alexey,\n\nI have looked at your updated test case, and my observation is as follows:\n\nYou create lots of constraints (L >= 0) that are unnecessary as the solver is already configured to restrict variables to non-negative values.\n\nI also think you use the objective function in a wrong way. It is defined as:\n\n{noformat}\nc1*x1 + ... cn*xn + d\n{noformat}\n\nso at index 0 you have the coefficient for the first variable, .... and the last index is for the constant term. Now you use something called theta, which you put on index 0 which is wrong imho.\n\nIf I remove all the unnecessary constraints, and move the theta variable to the end of the objective function vector, the tests run through successfully.\n\nThomas",
            "I close this issue also as invalid, as there is nothing wrong with the solver itself. You may also ask questions regarding the use of the simplex solver on the commons-user mailinglist.\n\nThomas",
            "Thomas, thanks for reference that the solver is already configured to restrict variables to non-negative values. That signally decreased time the solver needed to find solution as much as decreased the number of UnboundedSolutionExceptions.\n\nAs to the objective, there is a misunderstanding I believe.\nI use the definition of the objective as it's declared in http://commons.apache.org/math/apidocs/org/apache/commons/math/optimization/linear/LinearObjectiveFunction.html\nand theta is x1 and there are R+1 variables. R is the number of objects the omtimum is to find for (ENTITIES_COUNT) that is equal to the number of lambdas l(i). And as it shown in the very first test of the JUnit test where Q = [1,2], X = [2,1] and L = T[l1, l2] the solver gives an expected result. That is the indicator that equations are written properly. In other words for every single of N entities there has to be an objective with N + 1 variables. For the case of 2 entities the 3 dimension space is used to build a surface that has it's theta or x(1) coordinate set to minimum, for the case of 3 entities the 4 dimension space is used to build a shape that has it's theta or x(1) coordinate set to minimum and etc.\n\nHere is how it looks for the simplest case (JUnit test #0)\n\nModel name: DEA problem\n                 t       L1       L2 \nMinimize         1        0        0 \nR1               0        2        1 >=        1\nR2               2       -1       -2 >=        0\nR3               0        1        0 >=        0\nR4               0        0        1 >=        0\nType          Real     Real     Real \nupbo           Inf      Inf      Inf \nlowbo            0        0        0 \n\nthe objective is to be 0.25 and theta = 0.25 and L1 = 0.5, L2 = 0\nThe solver gives the same result for the case.\nBut only I add more entities to find minimum for as the same add more lambdas the solver gives back wrong answer, unbounded solution or theta greater than 1 (that is wrong due to the problem condition)\n\nI'm sure it's been really too early to close the issue :( ",
            "Hi Alexey,\n\nyou are right, I was too quick to draw a conclusion, the way you setup the problem is indeed correct.\n\nWhat I have seen is that you use a very small maxUlps setting in your solver. The default it 10 and should work better atm. I will further look into it, it seems to be related to numerical instabilities.\n\nSolving the same problems with glpk seems to be more robust, which maybe due to the scaling that is applied there to improve numerical properties of the constraint matrix.",
            "Thank you Thomas for reopenning and your confirmation that I use math3 properly. I'll play around with maxUIps setting and will try GLPK that I've heard about but haven't tryed yet.\n\nGilles am I right thinking that I need to use SVN to get 3.1 (r12317576)\nsvn checkout http://svn.apache.org/repos/asf/commons/proper/math/trunk commons-math3\n?\nor if I'm wrong please refer me to the proper link.\nThanks",
            "bq. [...] I need to use SVN to get 3.1 [...]\n\nThe link refers to the development branch (it's not yet 3.1).\nYes, that's the way to get the up-to-date version of the code.\n\nIf you don't want to compile the code (which requires the \"maven\" software), you could also download a snapshot JAR:\n https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/\n\n",
            "Now with 3.1-SNAPSHOT things are going better. Number of unbounded exceptions and unexpected theta values (> 1) are fewer versus math 3.0.\nNow the picture of failures and successes for\n25 entities (i.e. 25 + 1 variables) with 7 constraint equations, 3 for -q(r) + QL >= 0 and 4 for x(r)*t - XL >= 0\nlooks like\n\nIteration 1 of 64\nIteration 2 of 64\nIteration 3 of 64\nIteration 4 of 64\nIteration 5 of 64\nEXCEPTION: illegal state: maximal count (32,768) exceeded\nIteration 6 of 64\nIteration 7 of 64\nEXCEPTION: illegal state: maximal count (32,768) exceeded\nIteration 8 of 64\nIteration 9 of 64\nIteration 10 of 64\nIteration 11 of 64\nIteration 12 of 64\nIteration 13 of 64\nIteration 14 of 64\nIteration 15 of 64\nIteration 16 of 64\nIteration 17 of 64\nIteration 18 of 64\nIteration 19 of 64\nIteration 20 of 64\nEXCEPTION: illegal state: maximal count (32,768) exceeded\nIteration 21 of 64\nIteration 22 of 64\nIteration 23 of 64\nEXCEPTION: illegal state: maximal count (32,768) exceeded\nIteration 24 of 64\nIteration 25 of 64\nIteration 26 of 64\nEXCEPTION: unbounded solution\nIteration 27 of 64\nIteration 28 of 64\nIteration 29 of 64\nIteration 30 of 64\nIteration 31 of 64\nIteration 32 of 64\nIteration 33 of 64\nEXCEPTION: illegal state: maximal count (32,768) exceeded\nIteration 34 of 64\nEXCEPTION: illegal state: maximal count (32,768) exceeded\nIteration 35 of 64\nIteration 36 of 64\nIteration 37 of 64\nEXCEPTION: illegal state: maximal count (32,768) exceeded\nIteration 38 of 64\nIteration 39 of 64\nEXCEPTION: illegal state: maximal count (32,768) exceeded\nIteration 40 of 64\nIteration 41 of 64\nIteration 42 of 64\nIteration 43 of 64\nIteration 44 of 64\nIteration 45 of 64\nEXCEPTION: illegal state: maximal count (32,768) exceeded\nIteration 46 of 64\nIteration 47 of 64\nIteration 48 of 64\nIteration 49 of 64\nIteration 50 of 64\nIteration 51 of 64\nIteration 52 of 64\nIteration 53 of 64\nIteration 54 of 64\nIteration 55 of 64\nIteration 56 of 64\nIteration 57 of 64\nIteration 58 of 64\nIteration 59 of 64\nIteration 60 of 64\nIteration 61 of 64\nIteration 62 of 64\nEXCEPTION: illegal state: maximal count (32,768) exceeded\nIteration 63 of 64\nIteration 64 of 64\n\nfor the code\nSimplexSolver solver = new SimplexSolver(epsilon, 15);\ntry\n{\n\tsolver.setMaxIterations(32768);\n\tPointValuePair optimum = solver.optimize(objectiveFunction, constraints, GoalType.MINIMIZE, true);\n...\n\nIt's much better but it's still to risky to use math 3 to solve problems of this kind in real-world projects.",
            "I've made several launches of the program with different values. Here is the result\nThese\nfinal int INPUT_ARGUMENTS_COUNT = 4;\nfinal int OUTPUT_ARGUMENTS_COUNT = 3;\nfinal int MIN_ARGUMENT_VALUE = 1;\nfinal int MAX_ARGUMENT_VALUE = 100;\nfinal int ITERATIONS_COUNT = 512;\nand\nmaxIterationsCount = 65536;\nstay the same over all experiments\n\n\nExperiment 1\nfinal int ENTITIES_COUNT = 20;\nfinal double EPSILON = 1E-5;\nfinal boolean IS_INTEGER = false;\n/*\nIS_INTEGER is using in the source code as\nvalue = MIN_ARGUMENT_VALUE + rand.nextInt(MAX_ARGUMENT_VALUE);\nif(!IS_INTEGER){\n     value += rand.nextDouble();\n}\nwhere value is an entity' coefficient value that is a cell of Q and X matrices\n*/\ngives\n13 unbounded solutions of 512 iterations\n0 nofeasible solutions of 512 iterations\n34 maxcount exceeded exception of 512 iterations\nTotal 47.0 failures of 512 iterations ( = 0.091796875 of 1)\n\nExperiment 2\nfinal int ENTITIES_COUNT = 20;\nfinal double EPSILON = 1E-8;\nfinal boolean IS_INTEGER = false;\ngives\n13 unbounded solutions of 512 iterations\n0 nofeasible solutions of 512 iterations\n37 maxcount exceeded exception of 512 iterations\nTotal 50.0 failures of 512 iterations ( = 0.09765625 of 1)\n\nExperiment 3\nfinal int ENTITIES_COUNT = 20;\nfinal double EPSILON = 1E-8;\nfinal boolean IS_INTEGER = true;\ngives\n11 unbounded solutions of 512 iterations\n3 nofeasible solutions of 512 iterations\n33 maxcount exceeded exception of 512 iterations\nTotal 47.0 failures of 512 iterations ( = 0.091796875 of 1)\n\nExperiment 4\nfinal int ENTITIES_COUNT = 15;\nfinal double EPSILON = 1E-8;\nfinal boolean IS_INTEGER = false;\ngives\n10 unbounded solutions of 512 iterations\n0 nofeasible solutions of 512 iterations\n18 maxcount exceeded exception of 512 iterations\nTotal 28.0 failures of 512 iterations ( = 0.0546875 of 1)\n\nExperiment 5\nfinal int ENTITIES_COUNT = 10;\nfinal double EPSILON = 1E-8;\nfinal boolean IS_INTEGER = false;\ngives\n7 unbounded solutions of 512 iterations\n1 nofeasible solutions of 512 iterations\n16 maxcount exceeded exception of 512 iterations\nTotal 24.0 failures of 512 iterations ( = 0.046875 of 1)\n\nExperiment 6\nfinal int ENTITIES_COUNT = 5;\nfinal double EPSILON = 1E-8;\nfinal boolean IS_INTEGER = false;\ngives\n3 unbounded solutions of 512 iterations\n0 nofeasible solutions of 512 iterations\n0 maxcount exceeded exception of 512 iterations\nTotal 3.0 failures of 512 iterations ( = 0.005859375 of 1)\n\n\nAs you can see the most influence to the amount of failures gives the number of variables. When there are 5 of them the amount of failure is about a half of a precent which is satisfyingly. When there are 10 or more variables the amount of failures becomes unacceptable.\n\nPlease pay attention to the dependence of the amount of failures on the number of variables that is shown through experiments 3, 4, 5, 6\nvariables     failures\n20            47\n15            28\n10            24\n5             3\nThese failures numbers would change from one experiment launch to another of course but not much, +/- 2 failures",
            "Another parameter I've playd around with is maxUlps\nAs far as I see it's Precision.equals(double x, double y, int maxUlps) where maxUlps is used\n\n  public static boolean equals(double x, double y, int maxUlps) {\n        long xInt = Double.doubleToLongBits(x);\n        long yInt = Double.doubleToLongBits(y);\n\n        // Make lexicographically ordered as a two's-complement integer.\n        if (xInt < 0) {\n            xInt = SGN_MASK - xInt;\n        }\n        if (yInt < 0) {\n            yInt = SGN_MASK - yInt;\n        }\n\n        final boolean isEqual = FastMath.abs(xInt - yInt) <= maxUlps;\n\n        return isEqual && !Double.isNaN(x) && !Double.isNaN(y);\n    }\n\nI've made several more experiments. Notice that on each iteration \n\nSimplexSolver solver = new SimplexSolver(epsilon, maxUlps);\n\t\ttry\n\t\t{\n\t\t\tsolver.setMaxIterations(maxIterationsCount);\n\t\t\tPointValuePair optimum = solver.optimize(objectiveFunction, constraints, GoalType.MINIMIZE, true);\n\ncode is called for each entity. I.e. when 1024 iterations there are 1024*ENTITIES_COUNT = 1024*15 calls of new instances of SimplexSolver\nSum of input and output arguments is equal to number of variables - 1. In this case there were 8 variables for the objective function.\n\nThese below are results of experiments with different maxUlps values\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 0\n8 unbounded solutions of 1024 iterations ( = 0.0078125 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n48 maxcount exceeded exception of 1024 iterations ( = 0.046875 of 1)\nTotal 56.0 failures of 1024 iterations ( = 0.0546875 of 1)\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 2\n13 unbounded solutions of 1024 iterations ( = 0.0126953125 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n67 maxcount exceeded exception of 1024 iterations ( = 0.0654296875 of 1)\nTotal 80.0 failures of 1024 iterations ( = 0.078125 of 1)\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 4\n16 unbounded solutions of 1024 iterations ( = 0.015625 of 1)\n1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)\n45 maxcount exceeded exception of 1024 iterations ( = 0.0439453125 of 1)\nTotal 62.0 failures of 1024 iterations ( = 0.060546875 of 1)\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 6\n21 unbounded solutions of 1024 iterations ( = 0.0205078125 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n50 maxcount exceeded exception of 1024 iterations ( = 0.048828125 of 1)\nTotal 71.0 failures of 1024 iterations ( = 0.0693359375 of 1)\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 8\n13 unbounded solutions of 1024 iterations ( = 0.0126953125 of 1)\n1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)\n39 maxcount exceeded exception of 1024 iterations ( = 0.0380859375 of 1)\nTotal 53.0 failures of 1024 iterations ( = 0.0517578125 of 1)\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 25\n24 unbounded solutions of 1024 iterations ( = 0.0234375 of 1)\n1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)\n44 maxcount exceeded exception of 1024 iterations ( = 0.04296875 of 1)\nTotal 69.0 failures of 1024 iterations ( = 0.0673828125 of 1)\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 30\n19 unbounded solutions of 1024 iterations ( = 0.0185546875 of 1)\n2 nofeasible solutions of 1024 iterations ( = 0.001953125 of 1)\n43 maxcount exceeded exception of 1024 iterations ( = 0.0419921875 of 1)\nTotal 64.0 failures of 1024 iterations ( = 0.0625 of 1)\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 35\n15 unbounded solutions of 1024 iterations ( = 0.0146484375 of 1)\n1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)\n36 maxcount exceeded exception of 1024 iterations ( = 0.03515625 of 1)\nTotal 52.0 failures of 1024 iterations ( = 0.05078125 of 1)\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 40\n33 unbounded solutions of 1024 iterations ( = 0.0322265625 of 1)\n1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)\n44 maxcount exceeded exception of 1024 iterations ( = 0.04296875 of 1)\nTotal 78.0 failures of 1024 iterations ( = 0.076171875 of 1)\n\nIt seems that maxUlps gives not much to the state of the problem.\nI used https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/commons-math3-3.1-20120726.144152-154.jar SNAPSHOT for this expirement set",
            "Lines 74-78 in {{SimplexSolver}} look strange:\n{code}\nif (Precision.compareTo(entry, minValue, maxUlps) < 0) {\n    minValue = entry;\n    minPos = i;\n}\n{code}\n\nIts seems like the \"minValue\" will not really be most negative negative coefficient, as claimed in the doc.\nThe larger \"maxUlps\", the most likely it will not be, in line with what you observe...\n",
            "In r1366707, I committed the following changes to the SimplexSolver:\n\n * do not use maxUlps in getPivotColumn and getPivotRow when trying to find a minimum\n   this is contra-productive, and the check should be strict\n\n * implement Bland's rule to prevent cycling when selecting the pivot row in case of multiple candidates\n\n * to improve numerical stability, introduce a CUTOFF_THRESHOLD (currently 1e-12) to zero-out\n   values that are smaller than this threshold in SimplexTableau.subtractRow\n\nWith these changes your tests run through without any errors. Could you please verify yourself and confirm.\n\nThanks, \n\nThomas\n",
            "Thank you very much, Thomas! Now it works well! \n\n5 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10\n0 unbounded solutions of 1024 iterations ( = 0.0 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)\nTotal 0.0 failures of 1024 iterations ( = 0.0 of 1)\n\n10 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10\n0 unbounded solutions of 1024 iterations ( = 0.0 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)\nTotal 0.0 failures of 1024 iterations ( = 0.0 of 1)\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10\n0 unbounded solutions of 1024 iterations ( = 0.0 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)\nTotal 0.0 failures of 1024 iterations ( = 0.0 of 1)\n\n20 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10\n0 unbounded solutions of 1024 iterations ( = 0.0 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n2 maxcount exceeded exception of 1024 iterations ( = 0.001953125 of 1)\nTotal 2.0 failures of 1024 iterations ( = 0.001953125 of 1)\n\n25 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10\n0 unbounded solutions of 1024 iterations ( = 0.0 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n2 maxcount exceeded exception of 1024 iterations ( = 0.001953125 of 1)\nTotal 2.0 failures of 1024 iterations ( = 0.001953125 of 1)\n\n30 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10\n0 unbounded solutions of 1024 iterations ( = 0.0 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n3 maxcount exceeded exception of 1024 iterations ( = 0.0029296875 of 1)\nTotal 3.0 failures of 1024 iterations ( = 0.0029296875 of 1)\n\nhttps://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/commons-math3-3.1-20120729.124827-157.jar",
            "Thanks for the feedback, I will further investigate the remaining maxcount exceptions.\nIn my dev environment I mainly tested with 15 entities.",
            "I further investigated the remaining problems and came up with an empirical heuristic:\n\n * If we have not found a solution after half of maxIterations, ignore Bland's rule and revert to the top-most row\n\nI did extensive tests with your provided test case and did not receive any exceptions anymore.",
            "I confirm that those new improvements erased errors were remaining.\nTests of\nhttps://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/commons-math3-3.1-20120730.205101-159.jar\ngives\n\n15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8\n0 unbounded solutions of 1024 iterations ( = 0.0 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)\nTotal 0.0 failures of 1024 iterations ( = 0.0 of 1)\n\n25 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8\n0 unbounded solutions of 1024 iterations ( = 0.0 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)\nTotal 0.0 failures of 1024 iterations ( = 0.0 of 1)\n\n35 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8\n0 unbounded solutions of 1024 iterations ( = 0.0 of 1)\n0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)\n0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)\nTotal 0.0 failures of 1024 iterations ( = 0.0 of 1)\n\nExcellent! Thank you, Thomas!",
            "Hmm, I am no so sure if the last fix is the right way to go. I will leave this issue open and will investigate more on the topic.",
            "What can go wrong with the last fix?",
            "The first fix introduced Bland's rule to prevent cycling. In fact cycling still occurs, thats why I added the additional heuristic.\nWe need to better understand why cycling still occurs with Bland's rule and better fix this problem, than trying to circumvent it with a rule like it is implemented now.\n\nIt should hopefully not affect you, as with the latest version your problems seem to work pretty well, but I want a more general solution. The current fix may break for other type of problems.",
            "Hi Thomas.\n\nWouldn't it be clearer (cf. description and subject of this issue) to open a new issue for the problem which you identified (cycling) together with a test case that induces it?\nThe new issue can reference this report so that the history can be recovered easily.\n\nThis will allow to resolve this issue, which accurately reflects its state from the perspective of the problem raised.\n",
            "Hi Gilles,\n\nthe cycling problem has been fixed (+ test case) but in a way that may be very specific to the problem definition of Alexey.\nBut I am fine with closing this issue and creating a new one to further investigate the issue.\n\nThomas",
            "Thomas, when you'll open a new thread for further investigation of the issue, post here a link to that thread, please",
            "Hi Alexey,\n\nI linked the newly created issue to this one.\n\nThomas"
        ],
        "summarized_discussion": ""
    },
    "Compress_8_src/main/java/org/apache/commons/compress/archivers/tar/TarUtils.java_51_87": {
        "src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        boolean stillPadding = true;\n        int     end = offset + length;\n        int     start = offset;\n\n        for (int i = start; i < end; i++){\n            final byte currentByte = buffer[i];\n            if (currentByte == 0) {\n                break;\n            }\n\n        // Skip leading spaces\n            if (currentByte == (byte) ' ' || currentByte == '0') {\n                if (stillPadding) {\n                   continue;\n            }\n                if (currentByte == (byte) ' ') {\n                break;\n                }\n            }\n\n        // Must have trailing NUL or space\n        // May have additional NUL or space\n\n            stillPadding = false;\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; boolean stillPadding = true ; int end = offset + length ; int start = offset ; for ( int i = start ; i < end ; i ++ ) { final byte currentByte = buffer [ i ] ; if ( currentByte == 0 ) { break ; } if ( currentByte == ( byte ) ' ' || currentByte == '0' ) { if ( stillPadding ) { continue ; } if ( currentByte == ( byte ) ' ' ) { break ; } } stillPadding = false ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "fixed_src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        boolean allNUL = true;\n        for (int i = start; i < end; i++){\n            if (buffer[i] != 0){\n                allNUL = false;\n                break;\n            }\n        }\n        if (allNUL) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Must have trailing NUL or space\n        byte trailer;\n        trailer = buffer[end-1];\n        if (trailer == 0 || trailer == ' '){\n            end--;\n        } else {\n            throw new IllegalArgumentException(\n                    exceptionMessage(buffer, offset, length, end-1, trailer));\n        }\n        // May have additional NUL or space\n        trailer = buffer[end-1];\n        if (trailer == 0 || trailer == ' '){\n            end--;\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "fixed_src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } boolean allNUL = true ; for ( int i = start ; i < end ; i ++ ) { if ( buffer [ i ] != 0 ) { allNUL = false ; break ; } } if ( allNUL ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer ; trailer = buffer [ end - 1 ] ; if ( trailer == 0 || trailer == ' ' ) { end -- ; } else { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , end - 1 , trailer ) ) ; } trailer = buffer [ end - 1 ] ; if ( trailer == 0 || trailer == ' ' ) { end -- ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "summary": "TarArchiveEntry.parseTarHeader() includes the trailing space/NUL when parsing the octal size",
        "Description": "TarArchiveEntry.parseTarHeader() includes the trailing space/NUL when parsing the octal size.\n\nAlthough the size field in the header is 12 bytes, the last byte is supposed to be space or NUL - i.e. only 11 octal digits are allowed for the size.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-113",
        "comments": [
            "The code will work OK for valid input, but an invalid file could have 12 octal digits.\n\nPossible solutions:\n- ignore the trailing byte when converting\n- change the parsing routine to insist on a trailing space or NUL for all numeric fields.",
            "Christian Grobmeier:\n>  > COMPRESS-113\n>  > TarArchiveEntry.parseTarHeader() includes the trailing space/NUL when\n>  > parsing the octal\n>  > Suggestion from Sebb available. If a trailing NUL is in the tar specs,\n>  > I wold go for option 2 otherwise 1.\n>\n\nStefan Bodewig\n> If only there was *the* tar spec.  The best source I know is the FreeBSD\n>  man page\n>  <http://www.freebsd.org/cgi/man.cgi?query=tar&sektion=5&manpath=FreeBSD+8-current>\n>  and it says \"must end in a space\" for 'old archives' and \"allows\n>  [numeric fields] to be terminated with either space or NUL\".  Similar\n>  rules (slightly twisted for 'old archives') apply to other fields than\n>  size as well, like mode or uid.\n>\n>  I'd go with \"insist on a trailing space or NUL\".\n\nIt seems that there is a strong preference for option 2.",
            "TarUtils.parseOctal() currently stops processing when it detects a non-leading space or null, so \"0777 7 \" is not rejected.\n\nIt also ignores both leading '0' and spaces, so \"0 0 0777 \" is not rejected.\n\nAFAICT, there is no format that allows embedded space or NUL, so it seems to me that these should also be rejected.\n\nLooks like there are several conventions for terminating numeric fields:\n\nspace,NUL - mode, uid, and gid\nNUL, space - checksum\nspace - size and mtime; POSIX\nNUL - POSIX\n\nMaybe the most portable approach for parsing octal numbers would be to allow:\n- leading spaces\n- 1 or 2 trailing space or NUL characters.",
            "Just discovered that one also needs to allow for fields which are not in the original tar formats.\nFor example devmajor, devminor - these fields will be all null.",
            "Fixed in r950489."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to change the parsing routine to insist on a trailing space or NUL for all numeric fields, including fields which are not in the original tar formats (e.g. devmajor, devminor). This was fixed in r950489."
    },
    "Cli_29_src/java/org/apache/commons/cli/Util.java_63_76": {
        "src": "static String stripLeadingAndTrailingQuotes(String str)\n    {\n        if (str.startsWith(\"\\\"\"))\n        {\n            str = str.substring(1, str.length());\n        }\n        int length = str.length();\n        if (str.endsWith(\"\\\"\"))\n        {\n            str = str.substring(0, length - 1);\n        }\n        \n        return str;\n    }",
        "src_wo_comments": "static String stripLeadingAndTrailingQuotes ( String str ) { if ( str . startsWith ( \"\\\"\" ) ) { str = str . substring ( 1 , str . length ( ) ) ; } int length = str . length ( ) ; if ( str . endsWith ( \"\\\"\" ) ) { str = str . substring ( 0 , length - 1 ) ; } return str ; }",
        "fixed_src": "static String stripLeadingAndTrailingQuotes(String str)\n    {\n        int length = str.length();\n        if (length > 1 && str.startsWith(\"\\\"\") && str.endsWith(\"\\\"\") && str.substring(1, length - 1).indexOf('\"') == -1)\n        {\n            str = str.substring(1, length - 1);\n        }\n        \n        return str;\n    }",
        "fixed_src_wo_comments": "static String stripLeadingAndTrailingQuotes ( String str ) { int length = str . length ( ) ; if ( length > 1 && str . startsWith ( \"\\\"\" ) && str . endsWith ( \"\\\"\" ) && str . substring ( 1 , length - 1 ) . indexOf ( '\"' ) == - 1 ) { str = str . substring ( 1 , length - 1 ) ; } return str ; }",
        "summary": "Commons CLI incorrectly stripping leading and trailing quotes",
        "Description": "org.apache.commons.cli.Parser.processArgs() calls Util.stripLeadingAndTrailingQuotes() for all argument values. IMHO this is incorrect and totally broken.\n\nIt is trivial to create a simple test for this. Output:\n\n    $ java -cp target/clitest.jar Clitest --balloo \"this is a \\\"test\\\"\"\n    Value of argument balloo is 'this is a \"test'.\n\nThe argument 'balloo' should indeed keep its trailing double quote. It is what the shell gives it, so don't try to do something clever to it.\n\nThe offending code was committed here:\n    http://svn.apache.org/viewvc?view=rev&revision=129874\nand has been there for more than 6 years (!). Why was this committed in the first place?\n\nThe fix is trivial, just get rid of Util.stripLeadingAndTrailingQuotes(), and consequently avoid calling it from Parser.processArgs().",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-185",
        "comments": [
            "This was committed to fix CLI-22. I do agree that the quotes should not be removed.",
            "This is a regression introduced with CLI 1.1, CLI 1.0 didn't trim the quotes.",
            "Any info on when 1.3 will be out?",
            "We just started working on CLI 1.3, it's not ready to be released yet. You can use a snapshot once the issue is fixed, but if you absolutely need a released component we might roll out a 1.2.1 bug fix release.",
            "If you could release a 1.2.1 bugfix release with this fix, I'd be forever grateful. I'm depending on it for a big project. Thanks!",
            "This is probably a windows vs. linux issue (or more accurately command.com vs bash I guess)\n\nI can't see how the original issues CLI-22 would have existed as bash shouldn't have passed the quote marks on to Java.\n\nA fix for this issue should probably continue to work both when quotes are passed in and when they aren't. How should CLI differentiate between \n\n./Clitest --balloo \"\\\"this is a test\\\"\"\n\nrun on bash and \n\n./Clitest --balloo \"this is a test\"\n\nrun on command.com?",
            "Even on Windows the quotes aren't passed to the application. I don't understand why John Keyes had to remove quotes in CLI-22.",
            "At least on all *nix platforms (sh/bash/zsh/tcsh/whatever...), modifying the actual arguments in this way is totally broken.\n\nIf you really *must* do something clever on Windows, make *sure* it's Windows-only by checking the host platform or something.",
            "fwiw, I have done a bit of experimenting and it seems I don't understand windows as well as I thought (which wasn't much to start with!).\n\nIt seems double quote is not passed through by cmd.exe (although single quotes are passed through and cmd.exe doesn't require double quotes to be matched). If someone types:\n\njava tmp 'foo'\n\non windows, I think it is safe to assume they know they are on windows at that they intend the single quote to be part of the argument (as if they had typed the following on bash et al)\n\njava tmp '\\foo\\'\n\n\nIn conclusion, quotes (single or double) shouldn't be stripped at all and people should be careful when using single quotes in a platform ignorant way.",
            "The single quotes are never stripped, it's only the double quotes.\n\nI tried to remove the stripping, it broke two tests:\n- test15648 in BugsTest: that's the test of CLI-22, it checks if the quotes of an argument are removed after the parsing. This can be easily changed.\n- testWorkaround2 in BugCLI148Test: this is the test for CLI-148, this one is more tricky to fix. Is the workaround introduced in CLI-148 worth keeping? Is it possible to keep the workaround without messing with other cases like the one exposed by Einar?",
            "It seems that BugCLI148Test.test2() relies on double quotes to parse this as a value and not an argument. Fine, but then it should *not* automagically remove the quotes; that should be left for the user and this should be documented as a known issue. The workaround is bad, bad, bad.\n\nThe proper fix for CLI-148 is of course to actually support command lines like program -arg -val, where 'arg' is the name of the argument and '-val' is the actual value. I do understand, however, that this is a lot more work to implement.",
            "A 1.2.1 release with this fixed  would be great, it would make Commons CLI usable for my project.",
            "Unfortunately, it's not clear that there is consensus on whether the fix creates more problems than it addresses. In other words, how many people will be surprised by this change and how many environments will it turn a working instance into a broken one?",
            "True, this *could* break working code in production. But such code *does* rely on a \"feature\" in Commons-CLI that is just faulty, so sooner or later you will *have to* flip the switch. \n\nI would say that you should do it sooner rather than later, and make sure to document the changed behavior carefully in the release notes.",
            "How about if for 1.2.1 we add a boolean that allows for people to decide whether or not they want quotes stripped or not.? This can ease the transition...\n\ncomments?",
            "Which is worse?\n - Changing behavior of a method without changing the API at all (could make client code \"silently\" just break in prod.)\n - Changing the API in a minor release (requires recompile of client code)\n\nFor me any of these would work, since I don't rely on Commons-CLI for legacy code.",
            "In my case, removing the quotes for an argument is fine, I don't think this behavior should be changed. The issue is, for an argument like \"abc\"test\"\" (nested double quotes), the removal of quotes removes the last two quotes, the argument becomes abc\"test  instead of being abc\"test\" .\n\nLooking at the original patch, this could happen if stripLeadingAndTrailingQuotes function gets called two times, first one will do the proper stripping, second stripping will incorrectly  remove the last quote. A possible fix would be to check if first and last char are both quotes. If both are, then do the stripping. This change should not break the API compatibility, unless someone depends on the incorrect behavior.\n",
            "But Ajay, you are *wrong*. If a program is given the argument \"abc\"test\"\" (which means it has been given the argument --something \"\\\"abc\\\"test\\\"\\\"\" if run through bash), then it *must* be presented with \"abc\"test\"\" through Commons-CLI. \n\nThere is no such thing as proper stripping of double quotes. Get rid of it.",
            "There are two issues here. First is the behavior of whether or not to strip quotes. The second is a bug where abc\"test\" becomes abc\"test . What I meant was that fixing the second issue should not depend on the first. The bug can be fixed even if the stripping behavior is retained as it is.\n",
            "The workaround for CLI-148 is the only justification for the removal of the quotes, I'll see if this can be addressed in the new DefaultParser.\n\nIn the meantime I will follow Ajay suggestion to remove the quotes only if there is one at the beginning *and* one at the end. And to take Einar's example into account this will only happen if the value contains exactly 2 quotes. If the value contains more than 2 quotes, no stripping is performed.\n\nThis will preserve the workaround for CLI-148 and eliminate erroneous stripping.",
            "OK, if breaking CLI-148 is not an option, then the proposed fix will at least make things much better.\n\nDo you have any news on when the next release (containing this fix) is due?"
        ],
        "summarized_discussion": "\n\nThe bug discussed is a regression introduced with CLI 1.1, where quotes are removed from arguments. The proposed solution is to remove quotes only if there is one at the beginning and one at the end, and if the value contains exactly 2 quotes. This will preserve the workaround for CLI-148 and eliminate erroneous stripping. It is not yet known when the next release containing this fix is due."
    },
    "Compress_27_src/main/java/org/apache/commons/compress/archivers/tar/TarUtils.java_102_150": {
        "src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        if (buffer[start] == 0) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Trim all trailing NULs and spaces.\n        // The ustar and POSIX tar specs require a trailing NUL or\n        // space but some implementations use the extra digit for big\n        // sizes/uids/gids ...\n        byte trailer = buffer[end - 1];\n        while (start < end && (trailer == 0 || trailer == ' ')) {\n            end--;\n            trailer = buffer[end - 1];\n        }\n        if (start == end) {\n            throw new IllegalArgumentException(\n                    exceptionMessage(buffer, offset, length, start, trailer));\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } if ( buffer [ start ] == 0 ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer = buffer [ end - 1 ] ; while ( start < end && ( trailer == 0 || trailer == ' ' ) ) { end -- ; trailer = buffer [ end - 1 ] ; } if ( start == end ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , trailer ) ) ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "fixed_src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        if (buffer[start] == 0) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Trim all trailing NULs and spaces.\n        // The ustar and POSIX tar specs require a trailing NUL or\n        // space but some implementations use the extra digit for big\n        // sizes/uids/gids ...\n        byte trailer = buffer[end - 1];\n        while (start < end && (trailer == 0 || trailer == ' ')) {\n            end--;\n            trailer = buffer[end - 1];\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "fixed_src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } if ( buffer [ start ] == 0 ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer = buffer [ end - 1 ] ; while ( start < end && ( trailer == 0 || trailer == ' ' ) ) { end -- ; trailer = buffer [ end - 1 ] ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "summary": "Incorrect handling of NUL username and group Tar.gz entries",
        "Description": "With version 1.8 of commons-compress it's no longer possible to decompress  files from an archive if the archive contains entries having null (or being empty?) set as username and/or usergroup. With version 1.7 this still worked now I get this exception:\n\n{code}\njava.io.IOException: Error detected parsing the header\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:249)\n\tat TestBed.AppTest.extractNoFileOwner(AppTest.java:30)\nCaused by: java.lang.IllegalArgumentException: Invalid byte 32 at offset 7 in '       {NUL}' len=8\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:134)\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctalOrBinary(TarUtils.java:173)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:953)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:940)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:324)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:247)\n\t... 27 more\n\n{code}\nThis exception leads to my suspision that the regression was introduced with the fix for this ticket COMPRESS-262, which has a nearly identical exception provided.\n\nSome test code you can run to verify it:\n\n{code}\npackage TestBed;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\n\nimport org.apache.commons.compress.archivers.tar.TarArchiveEntry;\nimport org.apache.commons.compress.archivers.tar.TarArchiveInputStream;\nimport org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;\nimport org.junit.Test;\n\n/**\n * Unit test for simple App.\n */\npublic class AppTest\n{\n\n    @Test\n    public void extractNoFileOwner()\n    {\n        TarArchiveInputStream tarInputStream = null;\n\n        try\n        {\n            tarInputStream =\n                new TarArchiveInputStream( new GzipCompressorInputStream( new FileInputStream( new File(\n                    \"/home/pknobel/redis-dist-2.8.3_1-linux.tar.gz\" ) ) ) );\n            TarArchiveEntry entry;\n            while ( ( entry = tarInputStream.getNextTarEntry() ) != null )\n            {\n                System.out.println( entry.getName() );\n                System.out.println(entry.getUserName()+\"/\"+entry.getGroupName());\n            }\n\n        }\n        catch ( FileNotFoundException e )\n        {\n            e.printStackTrace();\n        }\n        catch ( IOException e )\n        {\n            e.printStackTrace();\n        }\n    }\n\n}\n{code}\nWith 1.7 the TestCase outputed this:\n\n{code}\nredis-dist-2.8.3_1/bin/\n/\nredis-dist-2.8.3_1/bin/redis-server\njenkins/jenkins\nredis-dist-2.8.3_1/bin/redis-cli\njenkins/jenkins\n{code}\n\nWith 1.8 it's failing once it reaches the null valued entry, which is the first. The archive is created using maven assembly plugin, and I tried the same with maven ant task. Both generating an archive with not set username and groups for at least some entries.\n\nYou can download the archive from http://heli0s.darktech.org/redis/2.8.3_1/redis-dist-2.8.3_1-linux.tar.gz\n\nIf you run a tar -tvzf on the file you see this report:\n\n{code}\ndrwxr-xr-x 0/0               0 2014-04-18 09:43 redis-dist-2.8.3_1-SNAPSHOT/bin/\n-rwxr-xr-x pknobel/pknobel 3824588 2014-01-02 14:58 redis-dist-2.8.3_1-SNAPSHOT/bin/redis-cli\n-rwxr-xr-x pknobel/pknobel 5217234 2014-01-02 14:58 redis-dist-2.8.3_1-SNAPSHOT/bin/redis-server\n{code}\n\nThe user 0/0 probably indicates that it's not set although it's the root user id. A correctly root user file would show up as root/root",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-278",
        "comments": [
            "you are correct COMPRESS-262 introduced the change.\n\nShould be fixed with svn revision 1588618",
            "see also https://issues.apache.org/bugzilla/show_bug.cgi?id=56641"
        ],
        "summarized_discussion": "\n\nThe bug COMPRESS-262 should be fixed with SVN revision 1588618, as suggested in the discussion. Additionally, more information can be found in the Apache Bugzilla page (https://issues.apache.org/bugzilla/show_bug.cgi?id=56641)."
    },
    "Chart_9_source/org/jfree/data/time/TimeSeries.java_918_956": {
        "src": "public TimeSeries createCopy(RegularTimePeriod start, RegularTimePeriod end)\n        throws CloneNotSupportedException {\n\n        if (start == null) {\n            throw new IllegalArgumentException(\"Null 'start' argument.\");\n        }\n        if (end == null) {\n            throw new IllegalArgumentException(\"Null 'end' argument.\");\n        }\n        if (start.compareTo(end) > 0) {\n            throw new IllegalArgumentException(\n                    \"Requires start on or before end.\");\n        }\n        boolean emptyRange = false;\n        int startIndex = getIndex(start);\n        if (startIndex < 0) {\n            startIndex = -(startIndex + 1);\n            if (startIndex == this.data.size()) {\n                emptyRange = true;  // start is after last data item\n            }\n        }\n        int endIndex = getIndex(end);\n        if (endIndex < 0) {             // end period is not in original series\n            endIndex = -(endIndex + 1); // this is first item AFTER end period\n            endIndex = endIndex - 1;    // so this is last item BEFORE end\n        }\n        if (endIndex < 0) {\n            emptyRange = true;\n        }\n        if (emptyRange) {\n            TimeSeries copy = (TimeSeries) super.clone();\n            copy.data = new java.util.ArrayList();\n            return copy;\n        }\n        else {\n            return createCopy(startIndex, endIndex);\n        }\n\n    }",
        "src_wo_comments": "public TimeSeries createCopy ( RegularTimePeriod start , RegularTimePeriod end ) throws CloneNotSupportedException { if ( start == null ) { throw new IllegalArgumentException ( \"Null 'start' argument.\" ) ; } if ( end == null ) { throw new IllegalArgumentException ( \"Null 'end' argument.\" ) ; } if ( start . compareTo ( end ) > 0 ) { throw new IllegalArgumentException ( \"Requires start on or before end.\" ) ; } boolean emptyRange = false ; int startIndex = getIndex ( start ) ; if ( startIndex < 0 ) { startIndex = - ( startIndex + 1 ) ; if ( startIndex == this . data . size ( ) ) { emptyRange = true ; } } int endIndex = getIndex ( end ) ; if ( endIndex < 0 ) { endIndex = - ( endIndex + 1 ) ; endIndex = endIndex - 1 ; } if ( endIndex < 0 ) { emptyRange = true ; } if ( emptyRange ) { TimeSeries copy = ( TimeSeries ) super . clone ( ) ; copy . data = new java . util . ArrayList ( ) ; return copy ; } else { return createCopy ( startIndex , endIndex ) ; } }",
        "fixed_src": "public TimeSeries createCopy(RegularTimePeriod start, RegularTimePeriod end)\n        throws CloneNotSupportedException {\n\n        if (start == null) {\n            throw new IllegalArgumentException(\"Null 'start' argument.\");\n        }\n        if (end == null) {\n            throw new IllegalArgumentException(\"Null 'end' argument.\");\n        }\n        if (start.compareTo(end) > 0) {\n            throw new IllegalArgumentException(\n                    \"Requires start on or before end.\");\n        }\n        boolean emptyRange = false;\n        int startIndex = getIndex(start);\n        if (startIndex < 0) {\n            startIndex = -(startIndex + 1);\n            if (startIndex == this.data.size()) {\n                emptyRange = true;  // start is after last data item\n            }\n        }\n        int endIndex = getIndex(end);\n        if (endIndex < 0) {             // end period is not in original series\n            endIndex = -(endIndex + 1); // this is first item AFTER end period\n            endIndex = endIndex - 1;    // so this is last item BEFORE end\n        }\n        if ((endIndex < 0)  || (endIndex < startIndex)) {\n            emptyRange = true;\n        }\n        if (emptyRange) {\n            TimeSeries copy = (TimeSeries) super.clone();\n            copy.data = new java.util.ArrayList();\n            return copy;\n        }\n        else {\n            return createCopy(startIndex, endIndex);\n        }\n\n    }",
        "fixed_src_wo_comments": "public TimeSeries createCopy ( RegularTimePeriod start , RegularTimePeriod end ) throws CloneNotSupportedException { if ( start == null ) { throw new IllegalArgumentException ( \"Null 'start' argument.\" ) ; } if ( end == null ) { throw new IllegalArgumentException ( \"Null 'end' argument.\" ) ; } if ( start . compareTo ( end ) > 0 ) { throw new IllegalArgumentException ( \"Requires start on or before end.\" ) ; } boolean emptyRange = false ; int startIndex = getIndex ( start ) ; if ( startIndex < 0 ) { startIndex = - ( startIndex + 1 ) ; if ( startIndex == this . data . size ( ) ) { emptyRange = true ; } } int endIndex = getIndex ( end ) ; if ( endIndex < 0 ) { endIndex = - ( endIndex + 1 ) ; endIndex = endIndex - 1 ; } if ( ( endIndex < 0 ) || ( endIndex < startIndex ) ) { emptyRange = true ; } if ( emptyRange ) { TimeSeries copy = ( TimeSeries ) super . clone ( ) ; copy . data = new java . util . ArrayList ( ) ; return copy ; } else { return createCopy ( startIndex , endIndex ) ; } }",
        "summary": "Error on TimeSeries createCopy() method",
        "Description": "The test case at the end fails with :\n\njava.lang.IllegalArgumentException: Requires start <= end.\n\nThe problem is in that the int start and end indexes corresponding to given timePeriod are computed incorectly. Here I would expect an empty serie to be returned, not an exception. This is with jfreechart 1.0.7\n\npublic class foo {\nstatic public void main(String args[]) {\nTimeSeries foo = new TimeSeries(\"foo\",Day.class);\nfoo.add(new Day(19,4,2005),1);\nfoo.add(new Day(25,5,2005),1);\nfoo.add(new Day(28,5,2005),1);\nfoo.add(new Day(30,5,2005),1);\nfoo.add(new Day(1,6,2005),1);\nfoo.add(new Day(3,6,2005),1);\nfoo.add(new Day(19,8,2005),1);\nfoo.add(new Day(31,1,2006),1);\n\n    try \\{\n        TimeSeries bar = foo.createCopy\\(new Day\\(1,12,2005\\),new Day\\(18,1,2006\\)\\);\n    \\} catch \\(CloneNotSupportedException e\\) \\{\n\n        e.printStackTrace\\(\\);\n}\n}",
        "issue_url": "https://sourceforge.net/p/jfreechart/bugs/818/",
        "comments": [
            {
                "content": "\"Logged In: YES\nuser_id=112975\nOriginator: NO\n\nConfirmed."
            },
            {
                "content": "labels: --> General\nmilestone: --> 1.0.x\nassigned_to: nobody --> mungady"
            },
            {
                "content": "status: open --> closed-fixed"
            },
            {
                "content": "user_id=112975\nOriginator: NO\n\nThanks for the report. I committed a JUnit test and a bug fix for inclusion in the 1.0.10 release.\n\nRegards,\n\nDave Gilbert\nJFreeChart Project Leader"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to change the labels to \"General\", the milestone to \"1.0.x\", the assigned_to to \"mungady\" and the status to \"closed-fixed\". Additionally, a JUnit test and a bug fix were committed for inclusion in the 1.0.10 release."
    },
    "Mockito_37_src/org/mockito/internal/stubbing/answers/AnswersValidator.java_15_28": {
        "src": "public void validate(Answer<?> answer, Invocation invocation) {\n        if (answer instanceof ThrowsException) {\n            validateException((ThrowsException) answer, invocation);\n        }\n        \n        if (answer instanceof Returns) {\n            validateReturnValue((Returns) answer, invocation);\n        }\n        \n        if (answer instanceof DoesNothing) {\n            validateDoNothing((DoesNothing) answer, invocation);\n        }\n        \n    }",
        "src_wo_comments": "public void validate ( Answer < ? > answer , Invocation invocation ) { if ( answer instanceof ThrowsException ) { validateException ( ( ThrowsException ) answer , invocation ) ; } if ( answer instanceof Returns ) { validateReturnValue ( ( Returns ) answer , invocation ) ; } if ( answer instanceof DoesNothing ) { validateDoNothing ( ( DoesNothing ) answer , invocation ) ; } }",
        "fixed_src": "public void validate(Answer<?> answer, Invocation invocation) {\n        if (answer instanceof ThrowsException) {\n            validateException((ThrowsException) answer, invocation);\n        }\n        \n        if (answer instanceof Returns) {\n            validateReturnValue((Returns) answer, invocation);\n        }\n        \n        if (answer instanceof DoesNothing) {\n            validateDoNothing((DoesNothing) answer, invocation);\n        }\n        \n        if (answer instanceof CallsRealMethods) {\n            validateMockingConcreteClass((CallsRealMethods) answer, invocation);\n        }\n    }",
        "fixed_src_wo_comments": "public void validate ( Answer < ? > answer , Invocation invocation ) { if ( answer instanceof ThrowsException ) { validateException ( ( ThrowsException ) answer , invocation ) ; } if ( answer instanceof Returns ) { validateReturnValue ( ( Returns ) answer , invocation ) ; } if ( answer instanceof DoesNothing ) { validateDoNothing ( ( DoesNothing ) answer , invocation ) ; } if ( answer instanceof CallsRealMethods ) { validateMockingConcreteClass ( ( CallsRealMethods ) answer , invocation ) ; } }",
        "summary": "Make Mockito JUnit rule easier to use",
        "Description": "- Mockito JUnit rule easier to use by avoiding the need to pass test instance\n- Make it compatible with JUnit 4.7+ instead of 4.9+\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug is related to a problem with the program's ability to recognize certain inputs. The solution to the bug is to modify the code so that it can recognize the inputs correctly. This can be done by adding additional code to the program or by making changes to the existing code."
    },
    "Math_105_src/java/org/apache/commons/math/stat/regression/SimpleRegression.java_263_265": {
        "src": "public double getSumSquaredErrors() {\n        return sumYY - sumXY * sumXY / sumXX;\n    }",
        "src_wo_comments": "public double getSumSquaredErrors ( ) { return sumYY - sumXY * sumXY / sumXX ; }",
        "fixed_src": "public double getSumSquaredErrors() {\n        return Math.max(0d, sumYY - sumXY * sumXY / sumXX);\n    }",
        "fixed_src_wo_comments": "public double getSumSquaredErrors ( ) { return Math . max ( 0d , sumYY - sumXY * sumXY / sumXX ) ; }",
        "summary": "[math]  SimpleRegression getSumSquaredErrors",
        "Description": "getSumSquaredErrors returns -ve value. See test below:\n\npublic void testSimpleRegression() {\n\t\tdouble[] y = {  8915.102, 8919.302, 8923.502};\n\t\tdouble[] x = { 1.107178495, 1.107264895, 1.107351295};\n\t\tdouble[] x2 = { 1.107178495E2, 1.107264895E2, 1.107351295E2};\n\t\tSimpleRegression reg = new SimpleRegression();\n\t\tfor (int i = 0; i < x.length; i++) {\n\t\t\treg.addData(x[i],y[i]);\n\t\t}\n\t\tassertTrue(reg.getSumSquaredErrors() >= 0.0); // OK\n\t\treg.clear();\n\t\tfor (int i = 0; i < x.length; i++) {\n\t\t\treg.addData(x2[i],y[i]);\n\t\t}\n\t\tassertTrue(reg.getSumSquaredErrors() >= 0.0); // FAIL\n\t\t\n\t}",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-85",
        "comments": [
            "The problem is related to computation accuracy in a corner case.\n\nThe data (110.7178495, 8915.102), (110.7264895, 8919.302), (110.7351295, 8923.502) represent three points on a perfect straigth line, with the second point exactly at the middle of the two extreme points. In this case, the sum of the squares of the errors should be exactly 0 as all points lie exactly on the estimated line.\n\nIf instead of checking reg.getSumSquaredErrors() >= 0.0 I print the value, I get -7.105427357601002E-15 on my GNU/Linux box. This seems quite fair for me as the computation involves computing a subtraction close to 35.28 - 35.28, where both terms result from several former computations. This is consistent with double precision.\n\nWhat we observe here is simply a cancellation effect on subtraction. The result is null in the first part of the test (where the x values are 100 times smaller), slightly negative in the second part. I think the null result in the first part is only good fortune (well, it is really related to the orders of magnitude involved: x^2, y^2 and xy).\n\nI suggest to consider this is not a bug.\nI will add a patch with a slightly modified test case in a few minutes.",
            "patch adding a test case for issue MATH-85",
            "I agree this is a corner case and the negative result is due to rounding.  The question is, should we force the result to 0 when a negative value is returned by the computation? ",
            "Constrained returned result to be non-negative.  "
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to constrain the returned result to be non-negative. A patch will be added with a slightly modified test case to address the issue."
    },
    "JacksonDatabind_36_src/main/java/com/fasterxml/jackson/databind/util/StdDateFormat.java_545_558": {
        "src": "private final static DateFormat _cloneFormat(DateFormat df, String format,\n            TimeZone tz, Locale loc, Boolean lenient)\n    {\n        if (!loc.equals(DEFAULT_LOCALE)) {\n            df = new SimpleDateFormat(format, loc);\n            df.setTimeZone((tz == null) ? DEFAULT_TIMEZONE : tz);\n        } else {\n            df = (DateFormat) df.clone();\n            if (tz != null) {\n                df.setTimeZone(tz);\n            }\n        }\n        return df;\n    }",
        "src_wo_comments": "private final static DateFormat _cloneFormat ( DateFormat df , String format , TimeZone tz , Locale loc , Boolean lenient ) { if ( ! loc . equals ( DEFAULT_LOCALE ) ) { df = new SimpleDateFormat ( format , loc ) ; df . setTimeZone ( ( tz == null ) ? DEFAULT_TIMEZONE : tz ) ; } else { df = ( DateFormat ) df . clone ( ) ; if ( tz != null ) { df . setTimeZone ( tz ) ; } } return df ; }",
        "fixed_src": "private final static DateFormat _cloneFormat(DateFormat df, String format,\n            TimeZone tz, Locale loc, Boolean lenient)\n    {\n        if (!loc.equals(DEFAULT_LOCALE)) {\n            df = new SimpleDateFormat(format, loc);\n            df.setTimeZone((tz == null) ? DEFAULT_TIMEZONE : tz);\n        } else {\n            df = (DateFormat) df.clone();\n            if (tz != null) {\n                df.setTimeZone(tz);\n            }\n        }\n        if (lenient != null) {\n            df.setLenient(lenient.booleanValue());\n        }\n        return df;\n    }",
        "fixed_src_wo_comments": "private final static DateFormat _cloneFormat ( DateFormat df , String format , TimeZone tz , Locale loc , Boolean lenient ) { if ( ! loc . equals ( DEFAULT_LOCALE ) ) { df = new SimpleDateFormat ( format , loc ) ; df . setTimeZone ( ( tz == null ) ? DEFAULT_TIMEZONE : tz ) ; } else { df = ( DateFormat ) df . clone ( ) ; if ( tz != null ) { df . setTimeZone ( tz ) ; } } if ( lenient != null ) { df . setLenient ( lenient . booleanValue ( ) ) ; } return df ; }",
        "summary": "Allow use\tof `StdDateFormat.setLenient()`",
        "Description": "ObjectMapper uses the StdDateFormat for date serialization. Jackson date parsing is lenient by default, so 2015-01-32 gets parsed as 2015-02-01.  Jackson\u2019s StdDateParser is matching default behavior of DateParser.  \n\nStdDateParser wasn\u2019t really designed for extension to just enable strict date parsing.  If it were, we could just call objectMapper.setDateFormat(new StdDateFormat().setLenient(false)). But StdDateFomrat doesn't support setting lenient to false. And i.e. the reason date like 2015-01-32 gets parsed as 2015-02-01 ad Jackson date parsing is lenient by defualt.\n\nCan StdDateFormat can be enhanced to support to non lenient date parsing?\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Was about to comment that `setLenient()` exists from the base class but then realized that while true, that will not have an effect on `DateFormat` instances that are actually used.\n\nSo: it should be possible to support non-leniency setting. I can not change `setLenient` to be chainable (non-covariant change is illegal I think), but it could be changed to pass setting to contained format instances.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to change the setLenient() method to pass the setting to the contained format instances, instead of making it chainable."
    },
    "Codec_17_src/main/java/org/apache/commons/codec/binary/StringUtils.java_338_340": {
        "src": "public static String newStringIso8859_1(final byte[] bytes) {\n        return new String(bytes, Charsets.ISO_8859_1);\n    }",
        "src_wo_comments": "public static String newStringIso8859_1 ( final byte [ ] bytes ) { return new String ( bytes , Charsets . ISO_8859_1 ) ; }",
        "fixed_src": "public static String newStringIso8859_1(final byte[] bytes) {\n        return newString(bytes, Charsets.ISO_8859_1);\n    }",
        "fixed_src_wo_comments": "public static String newStringIso8859_1 ( final byte [ ] bytes ) { return newString ( bytes , Charsets . ISO_8859_1 ) ; }",
        "summary": "StringUtils.newStringxxx(null) should return null, not NPE",
        "Description": "Method calls such as StringUtils.newStringIso8859_1(null) should return null, not NPE.\n\nIt looks like this capability was lost with the fix for CODEC-136, i.e.\nhttp://svn.apache.org/viewvc?rev=1306366&view=rev\n\nSeveral methods were changed from\n\n{code}\nreturn StringUtils.newString(bytes, CharEncoding.xxx);\nto\nreturn new String(bytes, Charsets.xxx);\n{code}\n\nThe new code should have been:\n\n{code}\nreturn newString(bytes, Charsets.xxx);\n{code}\n\nThe newString method handles null input.\n\nThere were no tests for null input so the change in behaviour was missed.\n",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-229",
        "comments": [
            "URL: http://svn.apache.org/viewvc?rev=1788755&view=rev\nLog:\nCODEC-229 StringUtils.newStringxxx(null) should return null, not NPE\n\nModified:\n    commons/proper/codec/trunk/src/changes/changes.xml\n    commons/proper/codec/trunk/src/main/java/org/apache/commons/codec/binary/StringUtils.java\n    commons/proper/codec/trunk/src/test/java/org/apache/commons/codec/binary/StringUtilsTest.java\n"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to modify the StringUtils.java, changes.xml, and StringUtilsTest.java files in the Apache Commons Codec library so that StringUtils.newStringxxx(null) returns null instead of causing a NullPointerException."
    },
    "Math_9_src/main/java/org/apache/commons/math3/geometry/euclidean/threed/Line.java_86_89": {
        "src": "public Line revert() {\n        final Line reverted = new Line(zero, zero.subtract(direction));\n        return reverted;\n    }",
        "src_wo_comments": "public Line revert ( ) { final Line reverted = new Line ( zero , zero . subtract ( direction ) ) ; return reverted ; }",
        "fixed_src": "public Line revert() {\n        final Line reverted = new Line(this);\n        reverted.direction = reverted.direction.negate();\n        return reverted;\n    }",
        "fixed_src_wo_comments": "public Line revert ( ) { final Line reverted = new Line ( this ) ; reverted . direction = reverted . direction . negate ( ) ; return reverted ; }",
        "summary": "Line.revert() is imprecise",
        "Description": "Line.revert() only maintains ~10 digits for the direction. This becomes an issue when the line's position is evaluated far from the origin. A simple fix would be to use Vector3D.negate() for the direction.\n\nAlso, is there a reason why Line is not immutable? It is just comprised of two vectors.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-938",
        "comments": [
            "Test Case:\n\n{noformat} \n    @Test\n    public void testRevert() {\n        // setup\n        Line line = new Line(new Vector3D(1653345.6696423641,\n                6170370.041579291, 90000), new Vector3D(1650757.5050732433,\n                6160710.879908984, 0.9));\n        Vector3D expected = line.getDirection().negate();\n\n        // action\n        Line reverted = line.revert();\n\n        // verify\n        assertArrayEquals(expected.toArray(),\n                reverted.getDirection().toArray(), 0);\n    }\n{noformat}",
            "Fixed in subversion repository as of r1453218.\n\nLine should be immutable, and in fact we want to make all of the Hyperplane/SubHyperplane/Embedding/BSPTree instances immutable, but this is a large incompatible change. So it will occur only at a major release (hopefully 4.0).\n\nThanks for reporting the issue.",
            "Closing issue as version 3.2 has been released on 2013-04-06."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of r1453218, and the Line class should be made immutable. However, this change will not occur until the major release of version 4.0. The issue has been closed as version 3.2 has been released on 2013-04-06."
    },
    "Math_53_src/main/java/org/apache/commons/math/complex/Complex.java_150_155": {
        "src": "public Complex add(Complex rhs)\n        throws NullArgumentException {\n        MathUtils.checkNotNull(rhs);\n        return createComplex(real + rhs.getReal(),\n            imaginary + rhs.getImaginary());\n    }",
        "src_wo_comments": "public Complex add ( Complex rhs ) throws NullArgumentException { MathUtils . checkNotNull ( rhs ) ; return createComplex ( real + rhs . getReal ( ) , imaginary + rhs . getImaginary ( ) ) ; }",
        "fixed_src": "public Complex add(Complex rhs)\n        throws NullArgumentException {\n        MathUtils.checkNotNull(rhs);\n        if (isNaN || rhs.isNaN) {\n            return NaN;\n        }\n        return createComplex(real + rhs.getReal(),\n            imaginary + rhs.getImaginary());\n    }",
        "fixed_src_wo_comments": "public Complex add ( Complex rhs ) throws NullArgumentException { MathUtils . checkNotNull ( rhs ) ; if ( isNaN || rhs . isNaN ) { return NaN ; } return createComplex ( real + rhs . getReal ( ) , imaginary + rhs . getImaginary ( ) ) ; }",
        "summary": "Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same",
        "Description": "For both Complex add and subtract, the javadoc states that\n\n{code}\n     * If either this or <code>rhs</code> has a NaN value in either part,\n     * {@link #NaN} is returned; otherwise Inifinite and NaN values are\n     * returned in the parts of the result according to the rules for\n     * {@link java.lang.Double} arithmetic\n{code}\n\nSubtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-618",
        "comments": [
            "Fixed in r1146573"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is that it was fixed in r1146573."
    },
    "JxPath_22_src/java/org/apache/commons/jxpath/ri/model/dom/DOMNodePointer.java_672_697": {
        "src": "public static String getNamespaceURI(Node node) {\n        if (node instanceof Document) {\n            node = ((Document) node).getDocumentElement();\n        }\n\n        Element element = (Element) node;\n\n        String uri = element.getNamespaceURI();\n        if (uri == null) {\n            String prefix = getPrefix(node);\n            String qname = prefix == null ? \"xmlns\" : \"xmlns:\" + prefix;\n    \n            Node aNode = node;\n            while (aNode != null) {\n                if (aNode.getNodeType() == Node.ELEMENT_NODE) {\n                    Attr attr = ((Element) aNode).getAttributeNode(qname);\n                    if (attr != null) {\n                        return attr.getValue();\n                    }\n                }\n                aNode = aNode.getParentNode();\n            }\n            return null;\n        }\n        return uri;\n    }",
        "src_wo_comments": "public static String getNamespaceURI ( Node node ) { if ( node instanceof Document ) { node = ( ( Document ) node ) . getDocumentElement ( ) ; } Element element = ( Element ) node ; String uri = element . getNamespaceURI ( ) ; if ( uri == null ) { String prefix = getPrefix ( node ) ; String qname = prefix == null ? \"xmlns\" : \"xmlns:\" + prefix ; Node aNode = node ; while ( aNode != null ) { if ( aNode . getNodeType ( ) == Node . ELEMENT_NODE ) { Attr attr = ( ( Element ) aNode ) . getAttributeNode ( qname ) ; if ( attr != null ) { return attr . getValue ( ) ; } } aNode = aNode . getParentNode ( ) ; } return null ; } return uri ; }",
        "fixed_src": "public static String getNamespaceURI(Node node) {\n        if (node instanceof Document) {\n            node = ((Document) node).getDocumentElement();\n        }\n\n        Element element = (Element) node;\n\n        String uri = element.getNamespaceURI();\n        if (uri == null) {\n            String prefix = getPrefix(node);\n            String qname = prefix == null ? \"xmlns\" : \"xmlns:\" + prefix;\n    \n            Node aNode = node;\n            while (aNode != null) {\n                if (aNode.getNodeType() == Node.ELEMENT_NODE) {\n                    Attr attr = ((Element) aNode).getAttributeNode(qname);\n                    if (attr != null) {\n                        uri = attr.getValue();\n                        break;\n                    }\n                }\n                aNode = aNode.getParentNode();\n            }\n        }\n        return \"\".equals(uri) ? null : uri;\n    }",
        "fixed_src_wo_comments": "public static String getNamespaceURI ( Node node ) { if ( node instanceof Document ) { node = ( ( Document ) node ) . getDocumentElement ( ) ; } Element element = ( Element ) node ; String uri = element . getNamespaceURI ( ) ; if ( uri == null ) { String prefix = getPrefix ( node ) ; String qname = prefix == null ? \"xmlns\" : \"xmlns:\" + prefix ; Node aNode = node ; while ( aNode != null ) { if ( aNode . getNodeType ( ) == Node . ELEMENT_NODE ) { Attr attr = ( ( Element ) aNode ) . getAttributeNode ( qname ) ; if ( attr != null ) { uri = attr . getValue ( ) ; break ; } } aNode = aNode . getParentNode ( ) ; } } return \"\" . equals ( uri ) ? null : uri ; }",
        "summary": "Resetting the default namespace causes a serious endless loop when requesting .asPath() on a node.",
        "Description": "sample smaller case:\n{code}\n<...>\n <b:foo xmlns:b=\"bla\" xmlns=\"test111\">    <!--  No nodes are placed in the tree within ns \"test111\" but the attribute is still there.-->\n  <b:bar>a</b:bar>                         <!-- is in ns 'bla' -->\n  <test xmlns=\"\"></test>                   <!-- does not have a namespace -->\n </b:foo>\n</...>\n{code}\n\nwhen requesting .asPath() on the 'test' node, it loops in org.apache.commons.jxpath.ri.NamespaceResolver.getPrefix(NodePointer, String), \nand if it didn't loop it would create a wrong xpath '//b:fo/null:test' DOMNodePointer.asPath().\n\n\nSo I think that the fix should be in org.apache.commons.jxpath.ri.model.dom.DOMNodePointer.asPath()\n\n{code}\n....\n                    String ln = DOMNodePointer.getLocalName(node);\n                    String nsURI = getNamespaceURI();\n                    if (nsURI == null) {\n                        buffer.append(ln);\n                        buffer.append('[');\n                        buffer.append(getRelativePositionByName()).append(']');\n                    }\n                    else {\n                        String prefix = getNamespaceResolver().getPrefix(nsURI);\n                        if (prefix != null) {\n...\n{code}\n\nshould become\n{code}\n...\n                    String ln = DOMNodePointer.getLocalName(node);\n                    String nsURI = getNamespaceURI();\n                    if (nsURI == null || nsURI.length() == 0) { // check for empty string which means that the node doesn't have a namespace.\n                        buffer.append(ln);\n                        buffer.append('[');\n                        buffer.append(getRelativePositionByName()).append(']');\n                    }\n                    else {\n                        String prefix = getNamespaceResolver().getPrefix(nsURI);\n                        if (prefix != null) {\n...\n{code}\n",
        "issue_url": "https://issues.apache.org/jira//browse/JXPATH-154",
        "comments": [
            "It is a convention within JXPath code that the empty namespace be represented as a Java {{null}}.  Rather than check for {{\"\"}} at the point you identified, I have elected to modify DOMNodePointer to translate {{\"\"}} to {{null}} when returning its result.  In this manner JXPath's DOM handling will function in the same manner as its JDOM handling, and any other callers of the method will be fixed as well.\n\n\nCommitted revision 1234036.\n",
            "Thanks a lot, I build the current 1.4 from svn and it fixes our case."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to modify DOMNodePointer to translate {{\"\"}} to {{null}} when returning its result, which was committed in revision 1234036. This fix has been confirmed to work in the current 1.4 version from svn."
    },
    "Math_45_src/main/java/org/apache/commons/math/linear/OpenMapRealMatrix.java_48_53": {
        "src": "public OpenMapRealMatrix(int rowDimension, int columnDimension) {\n        super(rowDimension, columnDimension);\n        this.rows = rowDimension;\n        this.columns = columnDimension;\n        this.entries = new OpenIntToDoubleHashMap(0.0);\n    }",
        "src_wo_comments": "public OpenMapRealMatrix ( int rowDimension , int columnDimension ) { super ( rowDimension , columnDimension ) ; this . rows = rowDimension ; this . columns = columnDimension ; this . entries = new OpenIntToDoubleHashMap ( 0.0 ) ; }",
        "fixed_src": "public OpenMapRealMatrix(int rowDimension, int columnDimension) {\n        super(rowDimension, columnDimension);\n        long lRow = (long) rowDimension;\n        long lCol = (long) columnDimension;\n        if (lRow * lCol >= (long) Integer.MAX_VALUE) {\n            throw new NumberIsTooLargeException(lRow * lCol, Integer.MAX_VALUE, false);\n        }\n        this.rows = rowDimension;\n        this.columns = columnDimension;\n        this.entries = new OpenIntToDoubleHashMap(0.0);\n    }",
        "fixed_src_wo_comments": "public OpenMapRealMatrix ( int rowDimension , int columnDimension ) { super ( rowDimension , columnDimension ) ; long lRow = ( long ) rowDimension ; long lCol = ( long ) columnDimension ; if ( lRow * lCol >= ( long ) Integer . MAX_VALUE ) { throw new NumberIsTooLargeException ( lRow * lCol , Integer . MAX_VALUE , false ) ; } this . rows = rowDimension ; this . columns = columnDimension ; this . entries = new OpenIntToDoubleHashMap ( 0.0 ) ; }",
        "summary": "Integer overflow in OpenMapRealMatrix",
        "Description": "computeKey() has an integer overflow. Since it is a sparse matrix, this is quite easily encountered long before heap space is exhausted. The attached code demonstrates the problem, which could potentially be a security vulnerability (for example, if one was to use this matrix to store access control information).\n\nWorkaround: never create an OpenMapRealMatrix with more cells than are addressable with an int.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-679",
        "comments": [
            "Move code to an attachment",
            "Fixed in subversion repository as of r1181181.\n\nThanks for the report and the workaround."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of r1181181 by moving the code to an attachment. Thanks for the report and the workaround."
    },
    "JacksonDatabind_77_src/main/java/com/fasterxml/jackson/databind/deser/BeanDeserializerFactory.java_96_145": {
        "src": "@Override\n    public JsonDeserializer<Object> createBeanDeserializer(DeserializationContext ctxt,\n            JavaType type, BeanDescription beanDesc)\n        throws JsonMappingException\n    {\n        final DeserializationConfig config = ctxt.getConfig();\n        // We may also have custom overrides:\n        JsonDeserializer<Object> custom = _findCustomBeanDeserializer(type, config, beanDesc);\n        if (custom != null) {\n            return custom;\n        }\n        /* One more thing to check: do we have an exception type\n         * (Throwable or its sub-classes)? If so, need slightly\n         * different handling.\n         */\n        if (type.isThrowable()) {\n            return buildThrowableDeserializer(ctxt, type, beanDesc);\n        }\n        /* Or, for abstract types, may have alternate means for resolution\n         * (defaulting, materialization)\n         */\n        // 29-Nov-2015, tatu: Also, filter out calls to primitive types, they are\n        //    not something we could materialize anything for\n        if (type.isAbstract() && !type.isPrimitive()) {\n            // Let's make it possible to materialize abstract types.\n            JavaType concreteType = materializeAbstractType(ctxt, type, beanDesc);\n            if (concreteType != null) {\n                /* important: introspect actual implementation (abstract class or\n                 * interface doesn't have constructors, for one)\n                 */\n                beanDesc = config.introspect(concreteType);\n                return buildBeanDeserializer(ctxt, concreteType, beanDesc);\n            }\n        }\n\n        // Otherwise, may want to check handlers for standard types, from superclass:\n        @SuppressWarnings(\"unchecked\")\n        JsonDeserializer<Object> deser = (JsonDeserializer<Object>) findStdDeserializer(ctxt, type, beanDesc);\n        if (deser != null) {\n            return deser;\n        }\n\n        // Otherwise: could the class be a Bean class? If not, bail out\n        if (!isPotentialBeanType(type.getRawClass())) {\n            return null;\n        }\n        // For checks like [databind#1599]\n        // Use generic bean introspection to build deserializer\n        return buildBeanDeserializer(ctxt, type, beanDesc);\n    }",
        "src_wo_comments": "@ Override public JsonDeserializer < Object > createBeanDeserializer ( DeserializationContext ctxt , JavaType type , BeanDescription beanDesc ) throws JsonMappingException { final DeserializationConfig config = ctxt . getConfig ( ) ; JsonDeserializer < Object > custom = _findCustomBeanDeserializer ( type , config , beanDesc ) ; if ( custom != null ) { return custom ; } if ( type . isThrowable ( ) ) { return buildThrowableDeserializer ( ctxt , type , beanDesc ) ; } if ( type . isAbstract ( ) && ! type . isPrimitive ( ) ) { JavaType concreteType = materializeAbstractType ( ctxt , type , beanDesc ) ; if ( concreteType != null ) { beanDesc = config . introspect ( concreteType ) ; return buildBeanDeserializer ( ctxt , concreteType , beanDesc ) ; } } @ SuppressWarnings ( \"unchecked\" ) JsonDeserializer < Object > deser = ( JsonDeserializer < Object > ) findStdDeserializer ( ctxt , type , beanDesc ) ; if ( deser != null ) { return deser ; } if ( ! isPotentialBeanType ( type . getRawClass ( ) ) ) { return null ; } return buildBeanDeserializer ( ctxt , type , beanDesc ) ; }",
        "fixed_src": "@Override\n    public JsonDeserializer<Object> createBeanDeserializer(DeserializationContext ctxt,\n            JavaType type, BeanDescription beanDesc)\n        throws JsonMappingException\n    {\n        final DeserializationConfig config = ctxt.getConfig();\n        // We may also have custom overrides:\n        JsonDeserializer<Object> custom = _findCustomBeanDeserializer(type, config, beanDesc);\n        if (custom != null) {\n            return custom;\n        }\n        /* One more thing to check: do we have an exception type\n         * (Throwable or its sub-classes)? If so, need slightly\n         * different handling.\n         */\n        if (type.isThrowable()) {\n            return buildThrowableDeserializer(ctxt, type, beanDesc);\n        }\n        /* Or, for abstract types, may have alternate means for resolution\n         * (defaulting, materialization)\n         */\n        // 29-Nov-2015, tatu: Also, filter out calls to primitive types, they are\n        //    not something we could materialize anything for\n        if (type.isAbstract() && !type.isPrimitive()) {\n            // Let's make it possible to materialize abstract types.\n            JavaType concreteType = materializeAbstractType(ctxt, type, beanDesc);\n            if (concreteType != null) {\n                /* important: introspect actual implementation (abstract class or\n                 * interface doesn't have constructors, for one)\n                 */\n                beanDesc = config.introspect(concreteType);\n                return buildBeanDeserializer(ctxt, concreteType, beanDesc);\n            }\n        }\n\n        // Otherwise, may want to check handlers for standard types, from superclass:\n        @SuppressWarnings(\"unchecked\")\n        JsonDeserializer<Object> deser = (JsonDeserializer<Object>) findStdDeserializer(ctxt, type, beanDesc);\n        if (deser != null) {\n            return deser;\n        }\n\n        // Otherwise: could the class be a Bean class? If not, bail out\n        if (!isPotentialBeanType(type.getRawClass())) {\n            return null;\n        }\n        // For checks like [databind#1599]\n        checkIllegalTypes(ctxt, type, beanDesc);\n        // Use generic bean introspection to build deserializer\n        return buildBeanDeserializer(ctxt, type, beanDesc);\n    }",
        "fixed_src_wo_comments": "@ Override public JsonDeserializer < Object > createBeanDeserializer ( DeserializationContext ctxt , JavaType type , BeanDescription beanDesc ) throws JsonMappingException { final DeserializationConfig config = ctxt . getConfig ( ) ; JsonDeserializer < Object > custom = _findCustomBeanDeserializer ( type , config , beanDesc ) ; if ( custom != null ) { return custom ; } if ( type . isThrowable ( ) ) { return buildThrowableDeserializer ( ctxt , type , beanDesc ) ; } if ( type . isAbstract ( ) && ! type . isPrimitive ( ) ) { JavaType concreteType = materializeAbstractType ( ctxt , type , beanDesc ) ; if ( concreteType != null ) { beanDesc = config . introspect ( concreteType ) ; return buildBeanDeserializer ( ctxt , concreteType , beanDesc ) ; } } @ SuppressWarnings ( \"unchecked\" ) JsonDeserializer < Object > deser = ( JsonDeserializer < Object > ) findStdDeserializer ( ctxt , type , beanDesc ) ; if ( deser != null ) { return deser ; } if ( ! isPotentialBeanType ( type . getRawClass ( ) ) ) { return null ; } checkIllegalTypes ( ctxt , type , beanDesc ) ; return buildBeanDeserializer ( ctxt , type , beanDesc ) ; }",
        "summary": "Jackson Deserializer security vulnerability via default typing (CVE-2017-7525)",
        "Description": "I have send email to info@fasterxml.com",
        "issue_url": null,
        "comments": [
            {
                "content": "@mbechler +1 to all of that. We need to figure out educational part of strong recommending avoiding class name based, unless source is fully trusted.\r\nLooking forward to your paper, sounds really interesting (if there is a draft I would like to see that too, but if not published versions will do nicely).\r\n\r\nHaving said that I really hope to get `jackson-module-security` (or whetever better name there is) started. I think more configurable, modular, perhaps dynamic (allowing loading of external lists) approach makes sense. I am talking to a colleague who might be interested in ownership of this module; and I would be happy to get other experts involved as well. This is one area where keeping up with another domain (of sec vulns), timely updates, would be important; and it's quite distinct in many ways from maintaining core databinding which at this point is and needs to move bit slower.\r\n"
            },
            {
                "content": "nice"
            },
            {
                "content": "@cowtowncoder \r\nSome more delays, but finally published: https://github.com/mbechler/marshalsec"
            },
            {
                "content": "Note: Jackson 2.8.9 patch version released, full set."
            },
            {
                "content": "how to fix versions 1.5 and above\uff1f"
            },
            {
                "content": "@zaodian17 I assume the easiest route would be to register a `Module` that blocks attempts to create deserializers for suspicious types.\r\n\r\nAnd actually I think this:\r\n\r\nhttps://github.com/FasterXML/jackson-1/pull/5\r\n\r\nadds such protection for 1.9. There'd need to be build 1.9.14.\r\n"
            },
            {
                "content": "> But if anyone wants to take time for PR (from 2.7 or whatever) I could merge too, push micro-patch 2.6.7.1.\r\n\r\n@cowtowncoder We ([AWS SDK for Java](https://github.com/aws/aws-sdk-java)) are interested in backporting this to the 2.6.x version of Jackson so our customers aren't forced to upgrade. Would you still be willing to accept a backport patch for this fix and release it to MavenCentral?"
            },
            {
                "content": "Fwtw, there is `2.6.7.1` release of `jackson-databind` now."
            },
            {
                "content": "So: this is apparently this is reported as CVE-2017-7525 (https://bugzilla.redhat.com/show_bug.cgi?id=1462702).\r\nAnd explained further in https://www.github.com/mbechler/marshalsec/blob/master/marshalsec.pdf\r\n"
            },
            {
                "content": "Does anyone know when CVE-2017-7525 is going to be published in the NVD?  I see that it was created at least as early as July 16, here it is November 2 and it's still not published."
            },
            {
                "content": "For sake of completeness, list of fixed-in versions at this point is:\r\n\r\n* 2.6.7.1\r\n* 2.7.9.1\r\n* 2.8.9\r\n* 2.9.0\r\n\r\n\r\n"
            },
            {
                "content": "Do I get it right that the `default typing` isn't enabled by default?"
            },
            {
                "content": "@lukaszlenart Absolutely, it is not the default for Jackson. It is possible that some frameworks could enable it, but I really hope they do not -- I do not think it is a setting that makes sense as the baseline, both for security reasons and for basic ergonomics. To me at least it is quite specific setting that one might want to use when replacing JDK serialization, for internal storage, checkpointing or such usage.\r\nBut never for public (REST) endpoints.\r\n"
            },
            {
                "content": "Note: there will soon be `2.8.11` as well, to address further work at #1855."
            },
            {
                "content": "@cowtowncoder thank you for the explanation, I was struggling with this question for some time and cannot find a clear answer :)"
            },
            {
                "content": "One more question because this isn't clear to me: using `@JsonTypeInfo` anywhere in an app enables `default typing`, is that true?"
            },
            {
                "content": "@lukaszlenart No. You can think of it as reverse: enabling default typing is about same as adding `@JsonTypeInfo` either on all classes, or on all properties.\r\n\r\nSo adding `@JsonTypeInfo` only enables polymorphic typing for either:\r\n\r\n1. Properties of type `X` (when added to declaration of `class X`)\r\n2. Specific property annotated.\r\n\r\nand even then it will only allow subtypes of the property. So vulnerability would only apply property like:\r\n\r\n```\r\n@JsonTypeInfo(....)\r\npublic Object value;\r\n```\r\n\r\nwhere any type is accepted as a subtype. But would not apply, for example, for:\r\n\r\n```\r\n@JsonTypeInfo(....)\r\npublic MyBaseType value;\r\n```\r\n\r\nwhere only real subtypes are accepted, ever.\r\n\r\nYou can still override handling, too, to disable polymorphic handling for specific type (Class) or property, by using `@JsonTypeInfo(include = As.NONE)` (or something similar, I forget exact setting).\r\n"
            },
            {
                "content": "Nice, thank you :)"
            },
            {
                "content": "I use jackson-databind 2.9.3 which depends on jackson-annotations 2.9.0, will this cause problems? Do I need to upgrade all jackson related jars to 2.9.3?"
            },
            {
                "content": "@fasterxml-travis No: `jackson-annotations` is quite special in that it only declares annotation types, and as a general rule there should be no differences at all between all `2.9.x` versions (and same for all 2.x minor patches). In Jackson 3.x we will only have `3.0`, `3.1` and so on, to remove this one source of confusion.\r\n\r\nSo: `2.9.0` is fine; `2.9.3` is also fine. They do not have code differences (*)\r\n\r\n(*) For sake of completeness: there happens to be one actual packaging difference: 2.9.1 and above have Java 9 module declaration compiled in. Otherwise patches are identical.\r\n"
            },
            {
                "content": "does this vulnerability affect jackson-databind v2.3.3?"
            },
            {
                "content": "@ksuresh8 yes. All 2.x versions not specifically included as having the fix will have it."
            },
            {
                "content": "Have you considered disabling this feature when the target field\u2019s type is `java.lang.Object`, `Comparable`, etc.? That would quickly cut down the attack surface."
            },
            {
                "content": "@swankjesse Unfortunately ability to use that base type is a big use case (equivalent of JDK serialization, with all the warts), so those can not really be blocked without breaking some of existing usage. Especially since library can not really determine if content is trusted or might come from untrusted source.\r\n\r\nBut I think adding a `MapperFeature` that would add such limits is indeed something that might make sense -- and making that feature enabled by default for 3.0.\r\n\r\nThat is the loose plan anyway, so that although it will be possible to allow unsafe (potentially unsafe) usage it should require bit more work. As things are one has to enable default typing (or unsafe base type for `@JsonTypeInfo`), but it is not obvious that there are security considerations.\r\n\r\nThere is also some related discussion on #1866 as well as at:\r\n\r\nhttps://github.com/FasterXML/jackson3-dev/issues/21\r\n\r\nalthough I may not have added an issue for `MapperFeature` to perhaps add in 2.9 (which is against SemVer, but 2.9 is likely the last 2.x version so... may be lesser evil. Unless I'll change my mind and do 2.10 yet)"
            },
            {
                "content": "For further info: https://medium.com/@cowtowncoder/on-jackson-cves-dont-panic-here-is-what-you-need-to-know-54cd0d6e8062\r\n"
            },
            {
                "content": "> Does anyone know when [CVE-2017-7525](https://github.com/advisories/GHSA-qxxx-2pp7-5hmx) is going to be published in the NVD? I see that it was created at least as early as July 16, here it is November 2 and it's still not published.\r\n\r\nHi @cowtowncoder , may I ask a off-topic question? When I try to following all the discussion above, I notice that you mention the cve-2017-7525 before it is published. Could you let me know how do you know it? (Do you guys already discuss it on other channels?)\r\n"
            },
            {
                "content": "@yiikou Whoever files an issue typically can see it before issue becomes publicly available -- and person filing usually sends email to a fasterxml dot com email (either mine, `tatu`, or general purpose `info`). This is usually where information comes from: CVE submitter is expected to contact the \"vendor\" (in this case, author(s) of the OSS package).\r\n"
            },
            {
                "content": "> @yiikou Whoever files an issue typically can see it before issue becomes publicly available -- and person filing usually sends email to a fasterxml dot com email (either mine, `tatu`, or general purpose `info`). This is usually where information comes from: CVE submitter is expected to contact the \"vendor\" (in this case, author(s) of the OSS package).\r\n\r\nThank you for the explanation. I got your point: you are either a CVE submitter or member of the vendor team, right? And please allow me to ask an additional question, would you have any concern about discussing this vulnerability before it is disclosed? Since the potential attackers may notice this vulnerability before it is patched, and leverage it to exploit product."
            },
            {
                "content": "@yiikou right. I am involved in discussions happening outside of public CVE stream. \r\n\r\nAs to discussions on issue tracker, it is bit of a balance: I usually do not include necessary details for reproduction for polymorphic type deserialization exploits (such as specific classes involved or properties/json used).\r\nIf we are talking about this particular issue (CVE-2017-7525), it was patched years ago so discussion would be fine.\r\n\r\nAnother source of information about exploit are actual block list additions; these do give some information on exploits themselves: not complete ones (since no reproduction is added as unit tests),  but could be helpful for someone looking for reconstructing exploits.\r\n\r\nI am not aware of any actual real-world exploits against Jackson, for what that is worth. That does not mean there has not been any, could just be that those are not published in general, or that if they are no one related to Jackson has been notified.\r\n"
            },
            {
                "content": "> @yiikou right. I am involved in discussions happening outside of public CVE stream.\r\n> \r\n> As to discussions on issue tracker, it is bit of a balance: I usually do not include necessary details for reproduction for polymorphic type deserialization exploits (such as specific classes involved or properties/json used).\r\n> If we are talking about this particular issue ([CVE-2017-7525](https://github.com/advisories/GHSA-qxxx-2pp7-5hmx)), it was patched years ago so discussion would be fine.\r\n> \r\n> Another source of information about exploit are actual block list additions; these do give some information on exploits themselves: not complete ones (since no reproduction is added as unit tests), but could be helpful for someone looking for reconstructing exploits.\r\n> \r\n> I am not aware of any actual real-world exploits against Jackson, for what that is worth. That does not mean there has not been any, could just be that those are not published in general, or that if they are no one related to Jackson has been notified.\r\n\r\nThank you for the explanation, I appreciateit."
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to upgrade to versions 2.6.7.1, 2.7.9.1, 2.8.9, 2.9.0, or 2.8.11 of Jackson-databind, which have been patched to fix the vulnerability. Additionally, it is recommended to use a Module that blocks attempts to create deserializers for suspicious types, and to enable a MapperFeature that would add limits to the attack surface."
    },
    "JacksonDatabind_98_src/main/java/com/fasterxml/jackson/databind/deser/impl/ExternalTypeHandler.java_255_311": {
        "src": "public Object complete(JsonParser p, DeserializationContext ctxt,\n            PropertyValueBuffer buffer, PropertyBasedCreator creator)\n        throws IOException\n    {\n        // first things first: deserialize all data buffered:\n        final int len = _properties.length;\n        Object[] values = new Object[len];\n        for (int i = 0; i < len; ++i) {\n            String typeId = _typeIds[i];\n            final ExtTypedProperty extProp = _properties[i];\n            if (typeId == null) {\n                // let's allow missing both type and property (may already have been set, too)\n                if (_tokens[i] == null) {\n                    continue;\n                }\n                // but not just one\n                // 26-Oct-2012, tatu: As per [databind#94], must allow use of 'defaultImpl'\n                if (!extProp.hasDefaultType()) {\n                    ctxt.reportInputMismatch(_beanType,\n                            \"Missing external type id property '%s'\",\n                            extProp.getTypePropertyName());\n                } else {\n                    typeId = extProp.getDefaultTypeId();\n                }\n            } else if (_tokens[i] == null) {\n                SettableBeanProperty prop = extProp.getProperty();\n                ctxt.reportInputMismatch(_beanType,\n                        \"Missing property '%s' for external type id '%s'\",\n                        prop.getName(), _properties[i].getTypePropertyName());\n            }\n            values[i] = _deserialize(p, ctxt, i, typeId);\n\n            final SettableBeanProperty prop = extProp.getProperty();\n            // also: if it's creator prop, fill in\n            if (prop.getCreatorIndex() >= 0) {\n                buffer.assignParameter(prop, values[i]);\n\n                // [databind#999] And maybe there's creator property for type id too?\n                SettableBeanProperty typeProp = extProp.getTypeProperty();\n                // for now, should only be needed for creator properties, too\n                if ((typeProp != null) && (typeProp.getCreatorIndex() >= 0)) {\n                    // 31-May-2018, tatu: [databind#1328] if id is NOT plain `String`, need to\n                    //    apply deserializer... fun fun.\n                    buffer.assignParameter(typeProp, typeId);\n                }\n            }\n        }\n        Object bean = creator.build(ctxt, buffer);\n        // third: assign non-creator properties\n        for (int i = 0; i < len; ++i) {\n            SettableBeanProperty prop = _properties[i].getProperty();\n            if (prop.getCreatorIndex() < 0) {\n                prop.set(bean, values[i]);\n            }\n        }\n        return bean;\n    }",
        "src_wo_comments": "public Object complete ( JsonParser p , DeserializationContext ctxt , PropertyValueBuffer buffer , PropertyBasedCreator creator ) throws IOException { final int len = _properties . length ; Object [ ] values = new Object [ len ] ; for ( int i = 0 ; i < len ; ++ i ) { String typeId = _typeIds [ i ] ; final ExtTypedProperty extProp = _properties [ i ] ; if ( typeId == null ) { if ( _tokens [ i ] == null ) { continue ; } if ( ! extProp . hasDefaultType ( ) ) { ctxt . reportInputMismatch ( _beanType , \"Missing external type id property '%s'\" , extProp . getTypePropertyName ( ) ) ; } else { typeId = extProp . getDefaultTypeId ( ) ; } } else if ( _tokens [ i ] == null ) { SettableBeanProperty prop = extProp . getProperty ( ) ; ctxt . reportInputMismatch ( _beanType , \"Missing property '%s' for external type id '%s'\" , prop . getName ( ) , _properties [ i ] . getTypePropertyName ( ) ) ; } values [ i ] = _deserialize ( p , ctxt , i , typeId ) ; final SettableBeanProperty prop = extProp . getProperty ( ) ; if ( prop . getCreatorIndex ( ) >= 0 ) { buffer . assignParameter ( prop , values [ i ] ) ; SettableBeanProperty typeProp = extProp . getTypeProperty ( ) ; if ( ( typeProp != null ) && ( typeProp . getCreatorIndex ( ) >= 0 ) ) { buffer . assignParameter ( typeProp , typeId ) ; } } } Object bean = creator . build ( ctxt , buffer ) ; for ( int i = 0 ; i < len ; ++ i ) { SettableBeanProperty prop = _properties [ i ] . getProperty ( ) ; if ( prop . getCreatorIndex ( ) < 0 ) { prop . set ( bean , values [ i ] ) ; } } return bean ; }",
        "fixed_src": "public Object complete(JsonParser p, DeserializationContext ctxt,\n            PropertyValueBuffer buffer, PropertyBasedCreator creator)\n        throws IOException\n    {\n        // first things first: deserialize all data buffered:\n        final int len = _properties.length;\n        Object[] values = new Object[len];\n        for (int i = 0; i < len; ++i) {\n            String typeId = _typeIds[i];\n            final ExtTypedProperty extProp = _properties[i];\n            if (typeId == null) {\n                // let's allow missing both type and property (may already have been set, too)\n                if (_tokens[i] == null) {\n                    continue;\n                }\n                // but not just one\n                // 26-Oct-2012, tatu: As per [databind#94], must allow use of 'defaultImpl'\n                if (!extProp.hasDefaultType()) {\n                    ctxt.reportInputMismatch(_beanType,\n                            \"Missing external type id property '%s'\",\n                            extProp.getTypePropertyName());\n                } else {\n                    typeId = extProp.getDefaultTypeId();\n                }\n            } else if (_tokens[i] == null) {\n                SettableBeanProperty prop = extProp.getProperty();\n                ctxt.reportInputMismatch(_beanType,\n                        \"Missing property '%s' for external type id '%s'\",\n                        prop.getName(), _properties[i].getTypePropertyName());\n            }\n            values[i] = _deserialize(p, ctxt, i, typeId);\n\n            final SettableBeanProperty prop = extProp.getProperty();\n            // also: if it's creator prop, fill in\n            if (prop.getCreatorIndex() >= 0) {\n                buffer.assignParameter(prop, values[i]);\n\n                // [databind#999] And maybe there's creator property for type id too?\n                SettableBeanProperty typeProp = extProp.getTypeProperty();\n                // for now, should only be needed for creator properties, too\n                if ((typeProp != null) && (typeProp.getCreatorIndex() >= 0)) {\n                    // 31-May-2018, tatu: [databind#1328] if id is NOT plain `String`, need to\n                    //    apply deserializer... fun fun.\n                    final Object v;\n                    if (typeProp.getType().hasRawClass(String.class)) {\n                        v = typeId;\n                    } else {\n                        TokenBuffer tb = new TokenBuffer(p, ctxt);\n                        tb.writeString(typeId);\n                        v = typeProp.getValueDeserializer().deserialize(tb.asParserOnFirstToken(), ctxt);\n                        tb.close();\n                    }\n                    buffer.assignParameter(typeProp, v);\n                }\n            }\n        }\n        Object bean = creator.build(ctxt, buffer);\n        // third: assign non-creator properties\n        for (int i = 0; i < len; ++i) {\n            SettableBeanProperty prop = _properties[i].getProperty();\n            if (prop.getCreatorIndex() < 0) {\n                prop.set(bean, values[i]);\n            }\n        }\n        return bean;\n    }",
        "fixed_src_wo_comments": "public Object complete ( JsonParser p , DeserializationContext ctxt , PropertyValueBuffer buffer , PropertyBasedCreator creator ) throws IOException { final int len = _properties . length ; Object [ ] values = new Object [ len ] ; for ( int i = 0 ; i < len ; ++ i ) { String typeId = _typeIds [ i ] ; final ExtTypedProperty extProp = _properties [ i ] ; if ( typeId == null ) { if ( _tokens [ i ] == null ) { continue ; } if ( ! extProp . hasDefaultType ( ) ) { ctxt . reportInputMismatch ( _beanType , \"Missing external type id property '%s'\" , extProp . getTypePropertyName ( ) ) ; } else { typeId = extProp . getDefaultTypeId ( ) ; } } else if ( _tokens [ i ] == null ) { SettableBeanProperty prop = extProp . getProperty ( ) ; ctxt . reportInputMismatch ( _beanType , \"Missing property '%s' for external type id '%s'\" , prop . getName ( ) , _properties [ i ] . getTypePropertyName ( ) ) ; } values [ i ] = _deserialize ( p , ctxt , i , typeId ) ; final SettableBeanProperty prop = extProp . getProperty ( ) ; if ( prop . getCreatorIndex ( ) >= 0 ) { buffer . assignParameter ( prop , values [ i ] ) ; SettableBeanProperty typeProp = extProp . getTypeProperty ( ) ; if ( ( typeProp != null ) && ( typeProp . getCreatorIndex ( ) >= 0 ) ) { final Object v ; if ( typeProp . getType ( ) . hasRawClass ( String . class ) ) { v = typeId ; } else { TokenBuffer tb = new TokenBuffer ( p , ctxt ) ; tb . writeString ( typeId ) ; v = typeProp . getValueDeserializer ( ) . deserialize ( tb . asParserOnFirstToken ( ) , ctxt ) ; tb . close ( ) ; } buffer . assignParameter ( typeProp , v ) ; } } } Object bean = creator . build ( ctxt , buffer ) ; for ( int i = 0 ; i < len ; ++ i ) { SettableBeanProperty prop = _properties [ i ] . getProperty ( ) ; if ( prop . getCreatorIndex ( ) < 0 ) { prop . set ( bean , values [ i ] ) ; } } return bean ; }",
        "summary": "External property polymorphic deserialization does not work with enums",
        "Description": "versions: Jackson 2.8.1, Jackson-module-kotlin 2.8.1\r\n\r\nAttempting to deserialize a class using external_property. In my case, the property is an Enum type with values matching the type name. Now that issue #999 is fixed, I thought this would work, but now I'm getting a different error:\r\n\r\n```\r\nException in thread \"main\" com.fasterxml.jackson.databind.JsonMappingException: Can not construct instance of enum.Invite, problem: argument type mismatch\r\n at [Source: {\r\n  \"kind\": \"CONTACT\",\r\n  \"to\": {\r\n    \"name\": \"Foo\"\r\n  }\r\n}; line: 6, column: 1]\r\n    at com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:268)\r\n    at com.fasterxml.jackson.databind.DeserializationContext.instantiationException(DeserializationContext.java:1405)\r\n    at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.wrapAsJsonMappingException(StdValueInstantiator.java:468)\r\n    at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.rewrapCtorProblem(StdValueInstantiator.java:487)\r\n    at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:276)\r\n    at com.fasterxml.jackson.module.kotlin.KotlinValueInstantiator.createFromObjectWith(KotlinValueInstantiator.kt:30)\r\n    at com.fasterxml.jackson.databind.deser.impl.PropertyBasedCreator.build(PropertyBasedCreator.java:135)\r\n    at com.fasterxml.jackson.databind.deser.impl.ExternalTypeHandler.complete(ExternalTypeHandler.java:225)\r\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeUsingPropertyBasedWithExternalTypeId(BeanDeserializer.java:937)\r\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeWithExternalTypeId(BeanDeserializer.java:792)\r\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:312)\r\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:148)\r\n    at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3789)\r\n    at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2852)\r\n    at enum.Reproduction_KindEnumKt.main(Reproduction-KindEnum.kt:49)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke(Method.java:498)\r\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)\r\nCaused by: java.lang.IllegalArgumentException: argument type mismatch\r\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n    at com.fasterxml.jackson.databind.introspect.AnnotatedConstructor.call(AnnotatedConstructor.java:124)\r\n    at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:274)\r\n    ... 15 more\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nHere is the reproduction recipe: https://github.com/rocketraman/jackson-issue-enum-polymorphism/blob/master/src/main/kotlin/enumtype/Reproduction-KindEnum.kt\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "This one is odd, basically in the subclass of `StdValueInstantiator` in `createFromObjectWith()` function it receives in the `buffer: PropertyValueBuffer` parameter when calling `buffer.getParameters(props)` a first parameter of type String that is `CONTACT` and a second that is type `InviteToContact` that is correct and this is stored in local variable`jsonParmValueList`.  \n\nThen the `KotlinValueInstantiator` in this case just delegates to `super.createFromObjectWith(ctxt, jsonParmValueList)` which blows up with this exception, so it tries to create the object with the wrong datatype for the first parameter (String instead of the `InviteKind` enum) and causes this exception.  That doesn't appear to be a problem in the Kotlin module which doesn't handle this case and just delegates.  Although I think if it didn't it would blow up differently because it would still have the wrong type.  \n"
            },
            {
                "content": "So the bug is that the type of the polymorphic variable is coming in as `String` instead of the `Enum` type and therefore cannot be used to create the object, argument type mismatch to the constructor.\n\nFails when databind code does the instantiation, and when I have the kotlin module do the same, it fails as well.  \n\nMaybe only applies when `ValueInstantiator` is being used, or is that always used as `StdValueInstantiator`?\n\n@cowtowncoder ??\n"
            },
            {
                "content": "You can test by running the Kotlin module after re-enabling the test in:\nhttps://github.com/FasterXML/jackson-module-kotlin/blob/master/src/test/kotlin/com/fasterxml/jackson/module/kotlin/test/GithubDatabind1328.kt\n"
            },
            {
                "content": "I would need a Java-only reproduction here, since it sounds there's nothing Kotlin-specific about it.\n"
            },
            {
                "content": "Or is this basically #1366?\n"
            },
            {
                "content": "@apatrida (cc: @cowtowncoder ) I was not able to reproduce this in Java. Here is the java repro attempt:\n\nhttps://github.com/rocketraman/jackson-issue-enum-polymorphism/blob/master/src/main/java/enumtype/java/ReproductionKindEnum.java\n"
            },
            {
                "content": "@rocketraman not sure, basically I receive a value of the wrong type (string) when it should be coerced to the enum value already.  \n\nIt could still be an issue with the StdValueInstantiator... i'll check it again.\n"
            },
            {
                "content": "@cowtowncoder  The issue is `StdValueInstantiator` and is hard to reproduce in Java because the default constructor is called and then values are set after at which point they have the correct type.  But if a value instantiator is in play, then this code will break:\n\n```\n @Override public Object createFromObjectWith(DeserializationContext ctxt,\n            SettableBeanProperty[] props, PropertyValueBuffer buffer) {\n   Object[] jsonParmValueList = buffer.getParameters(props)\n   super.createFromObjectWith(ctxt, jsonParmValueList)\n}\n```\n\nand looking at the `Object[]` of parameters received it is because the enum comes as type `String` and not as an enum type.  Whereas if this is set via a property (in @rocketraman  attempt to reproduce in Java) then it comes as the enum type which doesn't crash.\n\nSo something in the `PropertyValueBuffer` class does not know what type to coerce the value into... \n\nLooking at the method signature again...  the `SettableBeanProperty[] props` parameter should have the properties with their correct types, right?    And it does, I have 2 properties that come in as:\n\n```\n[simple type, class com.fasterxml.jackson.module.kotlin.test.GithubDatabind1328$InviteKind]\n[simple type, class com.fasterxml.jackson.module.kotlin.test.GithubDatabind1328$InviteTo]\n```\n\nGreat, those are correct.  So why then does `buffer.getParameters(props)` return:\n\n```\n[0]  \"CONTACT\" as String\n[1]   InviteToContact(name=Foo) as InviteToContact\n```\n\nTherefore `buffer.getParameters` in databind does not coerce the string \"CONTACT\" into an `enum` as it should.  The issue lies there.  Not sure how in Java you can force the StdValueInstantiator to be used to reproduce this.  \n"
            },
            {
                "content": "Looking at the work from https://github.com/FasterXML/jackson-databind/pull/1224 would help to figure out how to reproduce this from Java since there were test cases written for this @rocketraman \n"
            },
            {
                "content": "And it goes deeper, whatever created the `PropertyValueBuffer` instance put the wrong type in to begin with since `_creatorParameters` contains the string instead of enum as well.\n"
            },
            {
                "content": "```\n public boolean assignParameter(SettableBeanProperty prop, Object value)\n    {\n        final int ix = prop.getCreatorIndex();\n        _creatorParameters[ix] = value;    <------ VALUE IS WRONG TYPE, but `prop` is correct type\n        ...\n```\n\nSo the value passed  into `PropertyValueBuffer.assignParameter` is wrong type.  \n\nwho's job is it to coerce the type from a string into an enum @cowtowncoder ?\n"
            },
            {
                "content": "I can work around the issue by special casing `string` => `enum` conversion but then the normal StdValueInstantiator will still fail, I'd only fix the Kotlin case.\n"
            },
            {
                "content": "@apatrida Ok hadn't noticed your second to last comment. I think I'd really need a smallish example to follow through, to know who is right, who wrong.\n"
            },
            {
                "content": "I have the same issue(I think its an issue) after upgrading from 2.7.4 -> 2.8.6  in Java spring-boot project\r\nUsing an Enum as EXTERNAL_PROPERTY, the instantiation of my polymorphic variable its right set but the type variable(Enum type) (that its correctly mapped) on object creation the args comes as String\r\n\r\nMy json\r\n`{ \r\n    type:\"A\", \r\n     obj: {...}\r\n}`\r\n\r\n```\r\npublic class MyObject {\r\n\tType type;\r\n\r\n\t@JsonTypeInfo(property = \"type\", include = EXTERNAL_PROPERTY, use = NAME)\r\n\t@JsonSubTypes({\r\n\t\t\t@JsonSubTypes.Type(value = A.class,name = \"A\"),\r\n\t\t\t@JsonSubTypes.Type(value = B.class,name = \"B\")\r\n        })\r\n\tInterface \tobj;\r\n}\r\n\r\nenum Type {\r\n        A, B\r\n}\r\n\r\nclass A implements Interface {\r\n...\r\n}\r\nclass B implements Interface {\r\n...\r\n}\r\n```\r\n\r\n"
            },
            {
                "content": "Looking at this again now I realize that it is not clear whether type id is even expected to allow anything other than `String`s. I never intended it to be used for anything other than simple strings, for what that is worth -- if it has worked, that is actually surprising...\r\n\r\nBut I'll try to see if it'd be easy to make this work. It is not an unreasonable wish.\r\n"
            },
            {
                "content": "@pmorixe Unfortunately I still don't see the issue here: no exception is thrown. What is the exact problem you see?\r\n\r\nNote that `type` is metadata-only, by default, so its value should not be visible to properties, unless `visible=true` is set for `@JsonTypeInfo`. But even if that is done, I do not see failure.\r\nSo I think I'd like to see some assertion(s) in test.\r\nI tested this with 2.8.8 / 2.9.0.pr3, but I don't assume this is different from 2.8.4."
            },
            {
                "content": "@apatrida Conversion to expected value of property should be done by `jackson-databind`, when buffering (if possible); if not possible only tokens (as `TokenBuffer`) should be buffered and no value deserialized."
            },
            {
                "content": "Unfortunately can not reproduce at this point. May be re-opened/re-filed with Java reproduction.\r\n"
            },
            {
                "content": "I also ran into this problem. I noticed that when it came time to call the all argument constructor if I set \r\n```\r\ninclude = JsonTypeInfo.As.EXTERNAL_PROPERTY,\r\n```\r\nJackson would try to pass a `String` rather than a `Enum`. I tried to break it down into something small to demonstrate this problem, and found out the issue is lombok adds\r\n```\r\n@java.beans.ConstructorProperties({\"type\", \"animal\"})\r\n```\r\nto the all args constructor.\r\n\r\nIf I manually make the constructor with that annotation it fails if I remove it it works.\r\n\r\nHere is a code that demonstrates the problem:\r\n```\r\nimport java.util.Arrays;\r\nimport java.util.List;\r\n\r\nimport org.junit.Test;\r\n\r\nimport java.io.IOException;\r\n\r\nimport com.fasterxml.jackson.annotation.JsonTypeInfo;\r\nimport com.fasterxml.jackson.annotation.JsonTypeInfo.Id;\r\nimport com.fasterxml.jackson.databind.DatabindContext;\r\nimport com.fasterxml.jackson.databind.JavaType;\r\nimport com.fasterxml.jackson.databind.ObjectMapper;\r\nimport com.fasterxml.jackson.databind.annotation.JsonTypeIdResolver;\r\nimport com.fasterxml.jackson.databind.jsontype.TypeIdResolver;\r\nimport com.fasterxml.jackson.databind.type.TypeFactory;\r\n\r\npublic class EnumFailureExample {\r\n\r\n    public interface Animal {\r\n        \r\n    }\r\n    \r\n    public static class Dog implements Animal {\r\n        \r\n        private String dogStuff;\r\n        \r\n        public Dog() {\r\n            \r\n        }\r\n        \r\n        public Dog(String dogStuff) {\r\n            this.dogStuff = dogStuff;\r\n        }\r\n\r\n        public String getDogStuff() {\r\n            return dogStuff;\r\n        }\r\n\r\n        public void setDogStuff(String dogStuff) {\r\n            this.dogStuff = dogStuff;\r\n        }\r\n        \r\n    }\r\n    \r\n    public enum AmimalType {\r\n        Dog;\r\n    }\r\n    \r\n    public static class AnimalAndType {\r\n        \r\n        private AmimalType type;\r\n        \r\n        @JsonTypeInfo(use = JsonTypeInfo.Id.CLASS, \r\n            include = JsonTypeInfo.As.EXTERNAL_PROPERTY, \r\n            property = \"type\")\r\n        @JsonTypeIdResolver(AnimalResolver.class)   \r\n        private Animal animal;\r\n        \r\n        \r\n        public AnimalAndType() {\r\n            \r\n        }\r\n        \r\n        // problem is this annotation\r\n        @java.beans.ConstructorProperties({\"type\", \"animal\"})\r\n        public AnimalAndType(final AmimalType type, final Animal animal) {\r\n            this.type = type;\r\n            this.animal = animal;\r\n        }\r\n        \r\n        \r\n        public AmimalType getType() {\r\n            return type;\r\n        }\r\n        public void setType(AmimalType type) {\r\n            this.type = type;\r\n        }\r\n        public Animal getAnimal() {\r\n            return animal;\r\n        }\r\n        public void setAnimal(Animal animal) {\r\n            this.animal = animal;\r\n        }\r\n    }\r\n    \r\n    public static class AnimalResolver implements TypeIdResolver {\r\n\r\n        private JavaType baseType;\r\n        @Override\r\n        public void init(JavaType baseType) {\r\n            this.baseType = baseType;\r\n        }\r\n\r\n        @Override\r\n        public String idFromValue(Object value) {\r\n            // Its external why is this called?\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public String idFromValueAndType(Object value, Class<?> suggestedType) {\r\n            // Its external why is this called?\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public String idFromBaseType() {\r\n            throw new UnsupportedOperationException(\"Missing action type information - Can not construct\");\r\n        }\r\n\r\n        @Override\r\n        public JavaType typeFromId(DatabindContext context, String id) throws IOException {\r\n            Class clazz = null;\r\n            if (AmimalType.Dog.toString().equals(id)) {\r\n                clazz = Dog.class;\r\n            } else {\r\n                throw new IllegalArgumentException(\"What is a \" + id);\r\n            }\r\n                       \r\n            return TypeFactory.defaultInstance()\r\n                    .constructSpecializedType(baseType, clazz);\r\n        }\r\n\r\n        @Override\r\n        public String getDescForKnownTypeIds() {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Id getMechanism() {\r\n            return Id.CUSTOM;\r\n        } \r\n        \r\n    }\r\n    \r\n    @Test\r\n    public void testExample() throws Exception {\r\n        ObjectMapper mapper = new ObjectMapper();\r\n        \r\n        byte[] bytes = mapper.writerWithDefaultPrettyPrinter()\r\n                .writeValueAsBytes(Arrays.asList(new AnimalAndType(AmimalType.Dog, new Dog())));\r\n        \r\n        System.out.println(new String(bytes));\r\n        mapper.readValue(bytes, \r\n            mapper.getTypeFactory().constructCollectionType(List.class, AnimalAndType.class));\r\n    }\r\n}\r\n\r\n``` \r\n\r\n\r\nThis is using jackson 2.9.1"
            },
            {
                "content": "Lombok unfortunately is often part of a problem. It is possible here it is due to its adding `@ConstructorParameters`, which both names the constructor parameters and indicates it as \"creator\" (active constructor to use).\r\n\r\nBut it is possible to disable latter part, by disabling\r\n\r\n    MapperFeature.INFER_CREATOR_FROM_CONSTRUCTOR_PROPERTIES\r\n\r\nwhich might prevent the issue.\r\n"
            },
            {
                "content": "Ah thanks that is useful to know.\r\n\r\nYes it is due to the `@ConstructorParameters` annotation, I was able to remove lombok and just have that annotation and the problem showed up.\r\n\r\nWhat should be done next? I think this is a bug. Would you like a new issue be created or should this be re-opened?"
            },
            {
                "content": "@LukeButtersFunnelback How about I re-open it, given that there is a Java reproduction.\r\n"
            },
            {
                "content": "Ok so this took a while. But I found the problem and fixed it for 2.9.6. As @apatrida pointed out earlier, value was of wrong type -- `String` -- and needs to be routed via deserializer (first added into temporary buffer). Doing that resolves this case at least; I did not see other code paths so I hope this is sufficient.\r\n"
            }
        ],
        "summarized_discussion": ""
    },
    "JacksonDatabind_20_src/main/java/com/fasterxml/jackson/databind/node/ObjectNode.java_324_334": {
        "src": "public JsonNode setAll(Map<String,? extends JsonNode> properties)\n    {\n        for (Map.Entry<String,? extends JsonNode> en : properties.entrySet()) {\n            JsonNode n = en.getValue();\n            if (n == null) {\n                n = nullNode();\n            }\n            _children.put(en.getKey(), n);\n        }\n        return this;\n    }",
        "src_wo_comments": "public JsonNode setAll ( Map < String , ? extends JsonNode > properties ) { for ( Map . Entry < String , ? extends JsonNode > en : properties . entrySet ( ) ) { JsonNode n = en . getValue ( ) ; if ( n == null ) { n = nullNode ( ) ; } _children . put ( en . getKey ( ) , n ) ; } return this ; }",
        "fixed_src": "@JsonIgnore // work-around for [databind#815]\n    public JsonNode setAll(Map<String,? extends JsonNode> properties)\n    {\n        for (Map.Entry<String,? extends JsonNode> en : properties.entrySet()) {\n            JsonNode n = en.getValue();\n            if (n == null) {\n                n = nullNode();\n            }\n            _children.put(en.getKey(), n);\n        }\n        return this;\n    }",
        "fixed_src_wo_comments": "@ JsonIgnore public JsonNode setAll ( Map < String , ? extends JsonNode > properties ) { for ( Map . Entry < String , ? extends JsonNode > en : properties . entrySet ( ) ) { JsonNode n = en . getValue ( ) ; if ( n == null ) { n = nullNode ( ) ; } _children . put ( en . getKey ( ) , n ) ; } return this ; }",
        "summary": "Presence of PropertyNamingStrategy Makes Deserialization Fail",
        "Description": "I originally came across this issue using Dropwizard - https://github.com/dropwizard/dropwizard/issues/1095.  But it looks like this is a Jackson issue.  Here's the rerproducer:\n\n``` java\npublic class TestPropertyNamingStrategyIssue {\n  public static class ClassWithObjectNodeField {\n    public String id;\n    public ObjectNode json;\n  }\n\n  @Test\n  public void reproducer() throws Exception {\n    ObjectMapper mapper = new ObjectMapper();\n    mapper.setPropertyNamingStrategy(PropertyNamingStrategy.LOWER_CASE);\n    ClassWithObjectNodeField deserialized =\n        mapper.readValue(\n            \"{ \\\"id\\\": \\\"1\\\", \\\"json\\\": { \\\"foo\\\": \\\"bar\\\", \\\"baz\\\": \\\"bing\\\" } }\",\n            ClassWithObjectNodeField.class);\n  }\n}\n```\n\nLooks like the presence of any PropertyNamingStrategy make deserialization to ObjectNode fail.  This works fine if I remove the property naming strategy.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Sounds like a problem. Which version of Jackson are you using? If possible, can you try it with 2.5.3, if not yet using that (DW baseline is 2.5.1 or so I think). There have been some fixes related to handling of JsonNode deserialization, although I don't recall issues with naming strategy.\n"
            },
            {
                "content": "We're using Jackson 2.5.3 (newer than the DW baseline) and the issue still exists.\n"
            },
            {
                "content": "Ok thank you for confirming this.\n"
            },
            {
                "content": "Interesting. Yes, it's bit tricky. I can patch 2.5.x by adding an annotation, but there is the bigger problem (to be tackled for 2.6) with reporting potential problems before they would be actual problem. We have observed this in some other cases already. But fix for that is bit more invasive, hence will do that for master.\n"
            },
            {
                "content": "The only open question is why does the naming convention trigger this problem. Oh well, mystery for another day. :)\n\nFix will be in 2.5.4.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug appears to be related to handling of JsonNode deserialization and is present in Jackson 2.5.3. A patch can be added to 2.5.x with an annotation, but a more invasive fix will be implemented for 2.6. The cause of the problem is unknown, but the fix will be included in 2.5.4."
    },
    "Csv_4_src/main/java/org/apache/commons/csv/CSVParser.java_287_289": {
        "src": "public Map<String, Integer> getHeaderMap() {\n        return new LinkedHashMap<String, Integer>(this.headerMap);\n    }",
        "src_wo_comments": "public Map < String , Integer > getHeaderMap ( ) { return new LinkedHashMap < String , Integer > ( this . headerMap ) ; }",
        "fixed_src": "public Map<String, Integer> getHeaderMap() {\n        return this.headerMap == null ? null : new LinkedHashMap<String, Integer>(this.headerMap);\n    }",
        "fixed_src_wo_comments": "public Map < String , Integer > getHeaderMap ( ) { return this . headerMap == null ? null : new LinkedHashMap < String , Integer > ( this . headerMap ) ; }",
        "summary": "CSVParser: getHeaderMap throws NPE ",
        "Description": "title nearly says it all :-) \n\nGiven a CSVParser parser, the following line throws an NPE:\n\n{code}\nMap<String, Integer> header = parser.getHeaderMap();\n{code}\n\nStacktrace: \n\n{noformat}\nCaused by: java.lang.NullPointerException\nat java.util.HashMap.<init>(HashMap.java:318)\nat java.util.LinkedHashMap.<init>(LinkedHashMap.java:212)\nat org.apache.commons.csv.CSVParser.getHeaderMap(CSVParser.java:288)\n{noformat}\n\nhappens if the format doesn't have a headerMap.\n\nto fix, check if the parser's headerMap is null before trying to create the returned map:\n\n{code}\npublic Map<String, Integer> getHeaderMap() {\n    return this.headerMap != null ?\n       new LinkedHashMap<String, Integer>(this.headerMap)\n       : null;\n}\n\n{code}\n",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-100",
        "comments": [
            "{noformat}\ncommit -m \"[CSV-100] CSVParser: getHeaderMap throws NPE.\" C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVParser.java C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVParserTest.java\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVParser.java\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVParserTest.java\n    Transmitting file data ...\n    Committed revision 1524435.\n{noformat}",
            "Thank you for the report!\n\nGary"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to commit a revision to the source code, as indicated by the code in the discussion. The revision was successfully committed, and the bug should now be fixed."
    },
    "Math_85_src/java/org/apache/commons/math/analysis/solvers/UnivariateRealSolverUtils.java_165_208": {
        "src": "public static double[] bracket(UnivariateRealFunction function,\n            double initial, double lowerBound, double upperBound, \n            int maximumIterations) throws ConvergenceException, \n            FunctionEvaluationException {\n        \n        if (function == null) {\n            throw MathRuntimeException.createIllegalArgumentException(\"function is null\");\n        }\n        if (maximumIterations <= 0)  {\n            throw MathRuntimeException.createIllegalArgumentException(\n                  \"bad value for maximum iterations number: {0}\", maximumIterations);\n        }\n        if (initial < lowerBound || initial > upperBound || lowerBound >= upperBound) {\n            throw MathRuntimeException.createIllegalArgumentException(\n                  \"invalid bracketing parameters:  lower bound={0},  initial={1}, upper bound={2}\",\n                  lowerBound, initial, upperBound);\n        }\n        double a = initial;\n        double b = initial;\n        double fa;\n        double fb;\n        int numIterations = 0 ;\n    \n        do {\n            a = Math.max(a - 1.0, lowerBound);\n            b = Math.min(b + 1.0, upperBound);\n            fa = function.value(a);\n            \n            fb = function.value(b);\n            numIterations++ ;\n        } while ((fa * fb > 0.0) && (numIterations < maximumIterations) && \n                ((a > lowerBound) || (b < upperBound)));\n   \n        if (fa * fb >= 0.0 ) {\n            throw new ConvergenceException(\n                      \"number of iterations={0}, maximum iterations={1}, \" +\n                      \"initial={2}, lower bound={3}, upper bound={4}, final a value={5}, \" +\n                      \"final b value={6}, f(a)={7}, f(b)={8}\",\n                      numIterations, maximumIterations, initial,\n                      lowerBound, upperBound, a, b, fa, fb);\n        }\n        \n        return new double[]{a, b};\n    }",
        "src_wo_comments": "public static double [ ] bracket ( UnivariateRealFunction function , double initial , double lowerBound , double upperBound , int maximumIterations ) throws ConvergenceException , FunctionEvaluationException { if ( function == null ) { throw MathRuntimeException . createIllegalArgumentException ( \"function is null\" ) ; } if ( maximumIterations <= 0 ) { throw MathRuntimeException . createIllegalArgumentException ( \"bad value for maximum iterations number: {0}\" , maximumIterations ) ; } if ( initial < lowerBound || initial > upperBound || lowerBound >= upperBound ) { throw MathRuntimeException . createIllegalArgumentException ( \"invalid bracketing parameters:  lower bound={0},  initial={1}, upper bound={2}\" , lowerBound , initial , upperBound ) ; } double a = initial ; double b = initial ; double fa ; double fb ; int numIterations = 0 ; do { a = Math . max ( a - 1.0 , lowerBound ) ; b = Math . min ( b + 1.0 , upperBound ) ; fa = function . value ( a ) ; fb = function . value ( b ) ; numIterations ++ ; } while ( ( fa * fb > 0.0 ) && ( numIterations < maximumIterations ) && ( ( a > lowerBound ) || ( b < upperBound ) ) ) ; if ( fa * fb >= 0.0 ) { throw new ConvergenceException ( \"number of iterations={0}, maximum iterations={1}, \" + \"initial={2}, lower bound={3}, upper bound={4}, final a value={5}, \" + \"final b value={6}, f(a)={7}, f(b)={8}\" , numIterations , maximumIterations , initial , lowerBound , upperBound , a , b , fa , fb ) ; } return new double [ ] { a , b } ; }",
        "fixed_src": "public static double[] bracket(UnivariateRealFunction function,\n            double initial, double lowerBound, double upperBound, \n            int maximumIterations) throws ConvergenceException, \n            FunctionEvaluationException {\n        \n        if (function == null) {\n            throw MathRuntimeException.createIllegalArgumentException(\"function is null\");\n        }\n        if (maximumIterations <= 0)  {\n            throw MathRuntimeException.createIllegalArgumentException(\n                  \"bad value for maximum iterations number: {0}\", maximumIterations);\n        }\n        if (initial < lowerBound || initial > upperBound || lowerBound >= upperBound) {\n            throw MathRuntimeException.createIllegalArgumentException(\n                  \"invalid bracketing parameters:  lower bound={0},  initial={1}, upper bound={2}\",\n                  lowerBound, initial, upperBound);\n        }\n        double a = initial;\n        double b = initial;\n        double fa;\n        double fb;\n        int numIterations = 0 ;\n    \n        do {\n            a = Math.max(a - 1.0, lowerBound);\n            b = Math.min(b + 1.0, upperBound);\n            fa = function.value(a);\n            \n            fb = function.value(b);\n            numIterations++ ;\n        } while ((fa * fb > 0.0) && (numIterations < maximumIterations) && \n                ((a > lowerBound) || (b < upperBound)));\n   \n        if (fa * fb > 0.0 ) {\n            throw new ConvergenceException(\n                      \"number of iterations={0}, maximum iterations={1}, \" +\n                      \"initial={2}, lower bound={3}, upper bound={4}, final a value={5}, \" +\n                      \"final b value={6}, f(a)={7}, f(b)={8}\",\n                      numIterations, maximumIterations, initial,\n                      lowerBound, upperBound, a, b, fa, fb);\n        }\n        \n        return new double[]{a, b};\n    }",
        "fixed_src_wo_comments": "public static double [ ] bracket ( UnivariateRealFunction function , double initial , double lowerBound , double upperBound , int maximumIterations ) throws ConvergenceException , FunctionEvaluationException { if ( function == null ) { throw MathRuntimeException . createIllegalArgumentException ( \"function is null\" ) ; } if ( maximumIterations <= 0 ) { throw MathRuntimeException . createIllegalArgumentException ( \"bad value for maximum iterations number: {0}\" , maximumIterations ) ; } if ( initial < lowerBound || initial > upperBound || lowerBound >= upperBound ) { throw MathRuntimeException . createIllegalArgumentException ( \"invalid bracketing parameters:  lower bound={0},  initial={1}, upper bound={2}\" , lowerBound , initial , upperBound ) ; } double a = initial ; double b = initial ; double fa ; double fb ; int numIterations = 0 ; do { a = Math . max ( a - 1.0 , lowerBound ) ; b = Math . min ( b + 1.0 , upperBound ) ; fa = function . value ( a ) ; fb = function . value ( b ) ; numIterations ++ ; } while ( ( fa * fb > 0.0 ) && ( numIterations < maximumIterations ) && ( ( a > lowerBound ) || ( b < upperBound ) ) ) ; if ( fa * fb > 0.0 ) { throw new ConvergenceException ( \"number of iterations={0}, maximum iterations={1}, \" + \"initial={2}, lower bound={3}, upper bound={4}, final a value={5}, \" + \"final b value={6}, f(a)={7}, f(b)={8}\" , numIterations , maximumIterations , initial , lowerBound , upperBound , a , b , fa , fb ) ; } return new double [ ] { a , b } ; }",
        "summary": "bug in inverseCumulativeProbability() for Normal Distribution",
        "Description": "\n * @version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $\n */\npublic class NormalDistributionImpl extends AbstractContinuousDistribution \n\n\n * @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $\n */\npublic abstract class AbstractContinuousDistribution\n\n\nThis code:\n\n        \tDistributionFactory factory = app.getDistributionFactory();\n        \tNormalDistribution normal = factory.createNormalDistribution(0,1);\n        \tdouble result = normal.inverseCumulativeProbability(0.9772498680518209);\n\ngives the exception below. It should return (approx) 2.0000...\n\nnormal.inverseCumulativeProbability(0.977249868051820); works fine\n\nThese also give errors:\n0.9986501019683698 (should return 3.0000...)\n0.9999683287581673 (should return 4.0000...)\n\norg.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0\n\tat org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103)\n\tat org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)\n\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-280",
        "comments": [
            "fixed in subversion repository as of r791766\nthanks for the report",
            "closing resolved issue for 2.0 release"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of revision 791766, and the issue has been closed for the 2.0 release."
    },
    "Mockito_22_src/org/mockito/internal/matchers/Equality.java_12_20": {
        "src": "public static boolean areEqual(Object o1, Object o2) {\n        if (o1 == null || o2 == null) {\n            return o1 == null && o2 == null;\n        } else if (isArray(o1)) {\n            return isArray(o2) && areArraysEqual(o1, o2);\n        } else {\n            return o1.equals(o2);\n        }\n    }",
        "src_wo_comments": "public static boolean areEqual ( Object o1 , Object o2 ) { if ( o1 == null || o2 == null ) { return o1 == null && o2 == null ; } else if ( isArray ( o1 ) ) { return isArray ( o2 ) && areArraysEqual ( o1 , o2 ) ; } else { return o1 . equals ( o2 ) ; } }",
        "fixed_src": "public static boolean areEqual(Object o1, Object o2) {\n        if (o1 == o2 ) {\n            return true;\n\t} else if (o1 == null || o2 == null) {\n            return o1 == null && o2 == null;\n        } else if (isArray(o1)) {\n            return isArray(o2) && areArraysEqual(o1, o2);\n        } else {\n            return o1.equals(o2);\n        }\n    }",
        "fixed_src_wo_comments": "public static boolean areEqual ( Object o1 , Object o2 ) { if ( o1 == o2 ) { return true ; } else if ( o1 == null || o2 == null ) { return o1 == null && o2 == null ; } else if ( isArray ( o1 ) ) { return isArray ( o2 ) && areArraysEqual ( o1 , o2 ) ; } else { return o1 . equals ( o2 ) ; } }",
        "summary": "Can not Return deep stubs from generic method that returns generic type",
        "Description": "Hey,\n\nif I try to mock a generic method which a generic returntype, where the returntype is derived from the generic type of the method using deep stubs I get a `ClassCastException` when calling `when` on it.\n\n```\ninterface I {\n    <T> Supplier<T> m(Class<T> type);\n}\n@Test\npublic void test() throws Exception {\n    I i = mock(I.class, RETURNS_DEEP_STUBS);\n    when(i.m(Boolean.class).get()); // <- ClassCastException\n}\n```\n\nWhen you don't use deep stubs and a raw `Supplier` mock to pass around it works:\n\n```\nI i = mock(I.class);\nSupplier s = mock(Supplier.class);\nwhen(i.m(Boolean.class)).thenReturn(s);\nwhen(i.m(Boolean.class).get());\n```\n\nThe `ClassCastException`:\n\n```\njava.lang.ClassCastException: org.mockito.internal.creation.cglib.ClassImposterizer$ClassWithSuperclassToWorkAroundCglibBug$$EnhancerByMockitoWithCGLIB$$cdb13154 cannot be cast to java.lang.String\n  at MockitoGenerics.test(MockitoGenerics.java:21)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:483)\n  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n  at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n  at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n  at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n  at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n  at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n  at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n  at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)\n  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)\n  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)\n  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)\n```\n\nTested using mockito 1.10.19, jdk 1.8.0_20 and no Powermock\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Unfortunately this is not possible in Java at this time, because Java implements Generic with erasure.\nThe only exceptions where generic metadata are kept by the compiler are :\n- class or interface that extends or implements a generic type and **sets** this generic type, then method generic type may be _inferred_ e.g.\n  \n  ``` java\n  interface C extends Iterable<A> {}\n  ```\n- when bounds are declared, either on the type or on a method\n  \n  ``` java\n  interface C<T extends Number> {\n    T get();\n    <O extends Observer> observe();\n  }\n  ```\n"
            },
            {
                "content": "In this case declare all stubs, it should be something like :\n\n``` java\nI i = mock(I.class, RETURNS_DEEP_STUBS);\nwhen(i.m(Boolean.class)).thenReturn(mock(Supplier.class));\nwhen(i.m(Boolean.class).get()).thenReturn(true);\n```\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to declare all stubs, using code similar to the example provided: `I i = mock(I.class, RETURNS_DEEP_STUBS); when(i.m(Boolean.class)).thenReturn(mock(Supplier.class)); when(i.m(Boolean.class).get()).thenReturn(true);`"
    },
    "JacksonDatabind_58_src/main/java/com/fasterxml/jackson/databind/deser/BeanDeserializerFactory.java_703_752": {
        "src": "protected SettableBeanProperty constructSettableProperty(DeserializationContext ctxt,\n            BeanDescription beanDesc, BeanPropertyDefinition propDef,\n            JavaType propType0)\n        throws JsonMappingException\n    {\n        // need to ensure method is callable (for non-public)\n        AnnotatedMember mutator = propDef.getNonConstructorMutator();\n\n        if (ctxt.canOverrideAccessModifiers()) {\n            // [databind#877]: explicitly prevent forced access to `cause` of `Throwable`;\n            // never needed and attempts may cause problems on some platforms.\n            // !!! NOTE: should be handled better for 2.8 and later\n                mutator.fixAccess(ctxt.isEnabled(MapperFeature.OVERRIDE_PUBLIC_ACCESS_MODIFIERS));\n        }\n        // note: this works since we know there's exactly one argument for methods\n        BeanProperty.Std property = new BeanProperty.Std(propDef.getFullName(),\n                propType0, propDef.getWrapperName(),\n                beanDesc.getClassAnnotations(), mutator, propDef.getMetadata());\n        JavaType type = resolveType(ctxt, beanDesc, propType0, mutator);\n        // did type change?\n        if (type != propType0) {\n            property = property.withType(type);\n        }\n\n        // First: does the Method specify the deserializer to use? If so, let's use it.\n        JsonDeserializer<Object> propDeser = findDeserializerFromAnnotation(ctxt, mutator);\n        type = modifyTypeByAnnotation(ctxt, mutator, type);\n        TypeDeserializer typeDeser = type.getTypeHandler();\n        SettableBeanProperty prop;\n        if (mutator instanceof AnnotatedMethod) {\n            prop = new MethodProperty(propDef, type, typeDeser,\n                    beanDesc.getClassAnnotations(), (AnnotatedMethod) mutator);\n        } else {\n            prop = new FieldProperty(propDef, type, typeDeser,\n                    beanDesc.getClassAnnotations(), (AnnotatedField) mutator);\n        }\n        if (propDeser != null) {\n            prop = prop.withValueDeserializer(propDeser);\n        }\n        // need to retain name of managed forward references:\n        AnnotationIntrospector.ReferenceProperty ref = propDef.findReferenceType();\n        if (ref != null && ref.isManagedReference()) {\n            prop.setManagedReferenceName(ref.getName());\n        }\n        ObjectIdInfo objectIdInfo = propDef.findObjectIdInfo();\n        if(objectIdInfo != null){\n            prop.setObjectIdInfo(objectIdInfo);\n        }\n        return prop;\n    }",
        "src_wo_comments": "protected SettableBeanProperty constructSettableProperty ( DeserializationContext ctxt , BeanDescription beanDesc , BeanPropertyDefinition propDef , JavaType propType0 ) throws JsonMappingException { AnnotatedMember mutator = propDef . getNonConstructorMutator ( ) ; if ( ctxt . canOverrideAccessModifiers ( ) ) { mutator . fixAccess ( ctxt . isEnabled ( MapperFeature . OVERRIDE_PUBLIC_ACCESS_MODIFIERS ) ) ; } BeanProperty . Std property = new BeanProperty . Std ( propDef . getFullName ( ) , propType0 , propDef . getWrapperName ( ) , beanDesc . getClassAnnotations ( ) , mutator , propDef . getMetadata ( ) ) ; JavaType type = resolveType ( ctxt , beanDesc , propType0 , mutator ) ; if ( type != propType0 ) { property = property . withType ( type ) ; } JsonDeserializer < Object > propDeser = findDeserializerFromAnnotation ( ctxt , mutator ) ; type = modifyTypeByAnnotation ( ctxt , mutator , type ) ; TypeDeserializer typeDeser = type . getTypeHandler ( ) ; SettableBeanProperty prop ; if ( mutator instanceof AnnotatedMethod ) { prop = new MethodProperty ( propDef , type , typeDeser , beanDesc . getClassAnnotations ( ) , ( AnnotatedMethod ) mutator ) ; } else { prop = new FieldProperty ( propDef , type , typeDeser , beanDesc . getClassAnnotations ( ) , ( AnnotatedField ) mutator ) ; } if ( propDeser != null ) { prop = prop . withValueDeserializer ( propDeser ) ; } AnnotationIntrospector . ReferenceProperty ref = propDef . findReferenceType ( ) ; if ( ref != null && ref . isManagedReference ( ) ) { prop . setManagedReferenceName ( ref . getName ( ) ) ; } ObjectIdInfo objectIdInfo = propDef . findObjectIdInfo ( ) ; if ( objectIdInfo != null ) { prop . setObjectIdInfo ( objectIdInfo ) ; } return prop ; }",
        "fixed_src": "protected SettableBeanProperty constructSettableProperty(DeserializationContext ctxt,\n            BeanDescription beanDesc, BeanPropertyDefinition propDef,\n            JavaType propType0)\n        throws JsonMappingException\n    {\n        // need to ensure method is callable (for non-public)\n        AnnotatedMember mutator = propDef.getNonConstructorMutator();\n\n        if (ctxt.canOverrideAccessModifiers()) {\n            // [databind#877]: explicitly prevent forced access to `cause` of `Throwable`;\n            // never needed and attempts may cause problems on some platforms.\n            // !!! NOTE: should be handled better for 2.8 and later\n            if ((mutator instanceof AnnotatedField)\n                    && \"cause\".equals(mutator.getName())) {\n                ;\n            } else {\n                mutator.fixAccess(ctxt.isEnabled(MapperFeature.OVERRIDE_PUBLIC_ACCESS_MODIFIERS));\n            }\n        }\n        // note: this works since we know there's exactly one argument for methods\n        BeanProperty.Std property = new BeanProperty.Std(propDef.getFullName(),\n                propType0, propDef.getWrapperName(),\n                beanDesc.getClassAnnotations(), mutator, propDef.getMetadata());\n        JavaType type = resolveType(ctxt, beanDesc, propType0, mutator);\n        // did type change?\n        if (type != propType0) {\n            property = property.withType(type);\n        }\n\n        // First: does the Method specify the deserializer to use? If so, let's use it.\n        JsonDeserializer<Object> propDeser = findDeserializerFromAnnotation(ctxt, mutator);\n        type = modifyTypeByAnnotation(ctxt, mutator, type);\n        TypeDeserializer typeDeser = type.getTypeHandler();\n        SettableBeanProperty prop;\n        if (mutator instanceof AnnotatedMethod) {\n            prop = new MethodProperty(propDef, type, typeDeser,\n                    beanDesc.getClassAnnotations(), (AnnotatedMethod) mutator);\n        } else {\n            prop = new FieldProperty(propDef, type, typeDeser,\n                    beanDesc.getClassAnnotations(), (AnnotatedField) mutator);\n        }\n        if (propDeser != null) {\n            prop = prop.withValueDeserializer(propDeser);\n        }\n        // need to retain name of managed forward references:\n        AnnotationIntrospector.ReferenceProperty ref = propDef.findReferenceType();\n        if (ref != null && ref.isManagedReference()) {\n            prop.setManagedReferenceName(ref.getName());\n        }\n        ObjectIdInfo objectIdInfo = propDef.findObjectIdInfo();\n        if(objectIdInfo != null){\n            prop.setObjectIdInfo(objectIdInfo);\n        }\n        return prop;\n    }",
        "fixed_src_wo_comments": "protected SettableBeanProperty constructSettableProperty ( DeserializationContext ctxt , BeanDescription beanDesc , BeanPropertyDefinition propDef , JavaType propType0 ) throws JsonMappingException { AnnotatedMember mutator = propDef . getNonConstructorMutator ( ) ; if ( ctxt . canOverrideAccessModifiers ( ) ) { if ( ( mutator instanceof AnnotatedField ) && \"cause\" . equals ( mutator . getName ( ) ) ) { ; } else { mutator . fixAccess ( ctxt . isEnabled ( MapperFeature . OVERRIDE_PUBLIC_ACCESS_MODIFIERS ) ) ; } } BeanProperty . Std property = new BeanProperty . Std ( propDef . getFullName ( ) , propType0 , propDef . getWrapperName ( ) , beanDesc . getClassAnnotations ( ) , mutator , propDef . getMetadata ( ) ) ; JavaType type = resolveType ( ctxt , beanDesc , propType0 , mutator ) ; if ( type != propType0 ) { property = property . withType ( type ) ; } JsonDeserializer < Object > propDeser = findDeserializerFromAnnotation ( ctxt , mutator ) ; type = modifyTypeByAnnotation ( ctxt , mutator , type ) ; TypeDeserializer typeDeser = type . getTypeHandler ( ) ; SettableBeanProperty prop ; if ( mutator instanceof AnnotatedMethod ) { prop = new MethodProperty ( propDef , type , typeDeser , beanDesc . getClassAnnotations ( ) , ( AnnotatedMethod ) mutator ) ; } else { prop = new FieldProperty ( propDef , type , typeDeser , beanDesc . getClassAnnotations ( ) , ( AnnotatedField ) mutator ) ; } if ( propDeser != null ) { prop = prop . withValueDeserializer ( propDeser ) ; } AnnotationIntrospector . ReferenceProperty ref = propDef . findReferenceType ( ) ; if ( ref != null && ref . isManagedReference ( ) ) { prop . setManagedReferenceName ( ref . getName ( ) ) ; } ObjectIdInfo objectIdInfo = propDef . findObjectIdInfo ( ) ; if ( objectIdInfo != null ) { prop . setObjectIdInfo ( objectIdInfo ) ; } return prop ; }",
        "summary": "`@JsonIgnoreProperties`: ignoring the \"cause\" property of `Throwable` on GAE",
        "Description": "Deserializing an exception class from json on Google App Engine causes this error:\n\n```\nCaused by: java.lang.IllegalArgumentException: Can not access private java.lang.Throwable java.lang.Throwable.cause (from class java.lang.Throwable; failed to set access: java.lang.IllegalAccessException: Reflection is not allowed on private java.lang.Throwable java.lang.Throwable.cause\n    at com.fasterxml.jackson.databind.util.ClassUtil.checkAndFixAccess(ClassUtil.java:505)\n    at com.fasterxml.jackson.databind.introspect.AnnotatedMember.fixAccess(AnnotatedMember.java:123)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.constructSettableProperty(BeanDeserializerFactory.java:704)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.addBeanProps(BeanDeserializerFactory.java:501)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.buildThrowableDeserializer(BeanDeserializerFactory.java:356)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.createBeanDeserializer(BeanDeserializerFactory.java:114)\n```\n\nI tried preventing this by using `@JsonIgnoreProperties`:\n\n``` java\n@JsonIgnoreProperties(\"cause\")\npublic class MyException extends RuntimeException { ... }\n```\n\n... but the same error still occurs. What am I doing wrong? What else could I do?\n\nI've also considered setting `MapperFeature.CAN_OVERRIDE_ACCESS_MODIFIERS` to false, but I don't like this solution because I need this setting to be `true` in some other cases (in particular, I provide no-arg constructors for Jackson, but they should't be public in my API).\n",
        "issue_url": null,
        "comments": [
            {
                "content": "I've also tried using mixins, and I still get the same error:\n\n``` java\npublic abstract class ThrowableMixin {\n    @JsonIgnore private Throwable cause;\n    @JsonIgnore public abstract Throwable getCause();\n}\n```\n\n``` java\nobjectMapper.addMixIn(MyException.class, ThrowableMixin.class);\nMyException myException = objectMapper.readValue(\"{}\", MyException.class);\n```\n"
            },
            {
                "content": "I was able to fix this for my case using a custom `JacksonAnnotationIntrospector`.\n\nStill, shouldn't any of `@JsonIgnoreProperties` or mixins do the trick?\n\nLeaving this issue open because this may be a bug with handling ignored properties; but may be closed at any time as far as I am concerned.\n"
            },
            {
                "content": "@mmazi Yes, it should; the problem here is because `Exception`s (or in general `Throwable`s) do not use standard POJO serializers/deserializers. The reason is that there are couple of problems in making things work with fully default handling. As a result, support for many annotations is missing or incomplete.\nThis is not intentional, but rather based on having to implement support explicitly, and some aspects are missing; it should be possible to add support for ignorals for example.\n\nThank you for reporting the problem; I hope we will be able to fix this in a future version.\n"
            },
            {
                "content": "I am not able to reproduce the issue: tests show that use of `@JsonIgnoreProperties` works for serialization as expected, but the exception comes from deserialization. That does not fail either, but perhaps it's due to GAE specific problem. At this point I would need a way to reproduce specific problem assuming it still exists (which I'm not sure).\n"
            },
            {
                "content": "I'm having the same problem.  The issue is that Google App Engine restricts reflection of java.lang.Throwable (and probably other classes in the JDK).  Could exception serialization use Throwable.initCause() and Throwable.setStacktrace() rather than setting private fields? \n"
            },
            {
                "content": "@davidkilliansc it is quite difficult to work on this without reproduction. But in theory it might be possible to suppress `setAccess()` call in cases where it can (if it can) be determined that accessor is not needed.\nIt's just that it's tricky to ensure call does not happen, from unit test, to guard against regression.\n"
            },
            {
                "content": "@cowtowncoder For testable reproduction, maybe a security policy for the tests could be configured to restrict reflection access to Throwable.  \n"
            },
            {
                "content": "@davidkilliansc that's a great idea. I wonder how easy that'd be... I haven't created those, but sounds doable. Would be useful for other tests too I bet.\n"
            },
            {
                "content": "Looking at `ThrowableDeserializer`, \"cause\" is indeed passed via `initCause()`, no attempt is made to try to set underlying field.\n\nHowever, it is entirely possible that attempt to call `setAccess()` is made at an earlier point when potential property is found. This should be deferred since that property is later on replaced so access change is not really needed that early.\n"
            },
            {
                "content": "Added an ugly fix for 2.7.8, in which specific check is made for `cause` field of `Throwable` and skip forced access change if so.\nAlso added test that sets specific `SecurityManager` to trigger fail on J2SE JDK; thanks again @davidkilliansc for the suggestion!\n\nWill try to improve the fix for 2.8 (to ideally just defer forced access override), but wanted to keep patch somewhat minimal for 2.7 to reduce chance of regression.\n"
            },
            {
                "content": "@cowtowncoder Fantastic, thanks!\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug was to add a custom JacksonAnnotationIntrospector and to make a check for the 'cause' field of Throwable in the ThrowableDeserializer, skipping forced access change if so. A SecurityManager was also set up to trigger a fail on J2SE JDK. Finally, a test was added to ensure the fix did not cause any regression."
    },
    "JacksonXml_2_src/main/java/com/fasterxml/jackson/dataformat/xml/deser/XmlTokenStream.java_309_356": {
        "src": "private final int _next() throws XMLStreamException\n    {\n        switch (_currentState) {\n        case XML_ATTRIBUTE_VALUE:\n            ++_nextAttributeIndex;\n            // fall through\n        case XML_START_ELEMENT: // attributes to return?\n            if (_nextAttributeIndex < _attributeCount) {\n                _localName = _xmlReader.getAttributeLocalName(_nextAttributeIndex);\n                _namespaceURI = _xmlReader.getAttributeNamespace(_nextAttributeIndex);\n                _textValue = _xmlReader.getAttributeValue(_nextAttributeIndex);\n                return (_currentState = XML_ATTRIBUTE_NAME);\n            }\n            // otherwise need to find START/END_ELEMENT or text\n            String text = _collectUntilTag();\n            // If we have no/all-whitespace text followed by START_ELEMENT, ignore text\n            if (_xmlReader.getEventType() == XMLStreamReader.START_ELEMENT) {\n                    return _initStartElement();\n            }\n            // For END_ELEMENT we will return text, if any\n            if (text != null) {\n                _textValue = text;\n                return (_currentState = XML_TEXT);\n            }\n            return _handleEndElement();\n\n        case XML_ATTRIBUTE_NAME:\n            // if we just returned name, will need to just send value next\n            return (_currentState = XML_ATTRIBUTE_VALUE);\n        case XML_TEXT:\n            // mixed text with other elements\n            // text followed by END_ELEMENT\n            return _handleEndElement();\n        case XML_END:\n            return XML_END;\n//            throw new IllegalStateException(\"No more XML tokens available (end of input)\");\n        }\n\n        // Ok: must be END_ELEMENT; see what tag we get (or end)\n        switch (_skipUntilTag()) {\n        case XMLStreamConstants.END_DOCUMENT:\n            return (_currentState = XML_END);\n        case XMLStreamConstants.END_ELEMENT:\n            return _handleEndElement();\n        }\n        // START_ELEMENT...\n        return _initStartElement();\n    }",
        "src_wo_comments": "private final int _next ( ) throws XMLStreamException { switch ( _currentState ) { case XML_ATTRIBUTE_VALUE : ++ _nextAttributeIndex ; case XML_START_ELEMENT : if ( _nextAttributeIndex < _attributeCount ) { _localName = _xmlReader . getAttributeLocalName ( _nextAttributeIndex ) ; _namespaceURI = _xmlReader . getAttributeNamespace ( _nextAttributeIndex ) ; _textValue = _xmlReader . getAttributeValue ( _nextAttributeIndex ) ; return ( _currentState = XML_ATTRIBUTE_NAME ) ; } String text = _collectUntilTag ( ) ; if ( _xmlReader . getEventType ( ) == XMLStreamReader . START_ELEMENT ) { return _initStartElement ( ) ; } if ( text != null ) { _textValue = text ; return ( _currentState = XML_TEXT ) ; } return _handleEndElement ( ) ; case XML_ATTRIBUTE_NAME : return ( _currentState = XML_ATTRIBUTE_VALUE ) ; case XML_TEXT : return _handleEndElement ( ) ; case XML_END : return XML_END ; } switch ( _skipUntilTag ( ) ) { case XMLStreamConstants . END_DOCUMENT : return ( _currentState = XML_END ) ; case XMLStreamConstants . END_ELEMENT : return _handleEndElement ( ) ; } return _initStartElement ( ) ; }",
        "fixed_src": "private final int _next() throws XMLStreamException\n    {\n        switch (_currentState) {\n        case XML_ATTRIBUTE_VALUE:\n            ++_nextAttributeIndex;\n            // fall through\n        case XML_START_ELEMENT: // attributes to return?\n            if (_nextAttributeIndex < _attributeCount) {\n                _localName = _xmlReader.getAttributeLocalName(_nextAttributeIndex);\n                _namespaceURI = _xmlReader.getAttributeNamespace(_nextAttributeIndex);\n                _textValue = _xmlReader.getAttributeValue(_nextAttributeIndex);\n                return (_currentState = XML_ATTRIBUTE_NAME);\n            }\n            // otherwise need to find START/END_ELEMENT or text\n            String text = _collectUntilTag();\n            final boolean startElementNext = _xmlReader.getEventType() == XMLStreamReader.START_ELEMENT;\n            // If we have no/all-whitespace text followed by START_ELEMENT, ignore text\n            if (startElementNext) {\n                if (text == null || _allWs(text)) {\n                    _mixedText = false;\n                    return _initStartElement();\n                }\n                _mixedText = true;\n                _textValue = text;\n                return (_currentState = XML_TEXT);\n            }\n            // For END_ELEMENT we will return text, if any\n            if (text != null) {\n                _mixedText = false;\n                _textValue = text;\n                return (_currentState = XML_TEXT);\n            }\n            _mixedText = false;\n            return _handleEndElement();\n\n        case XML_ATTRIBUTE_NAME:\n            // if we just returned name, will need to just send value next\n            return (_currentState = XML_ATTRIBUTE_VALUE);\n        case XML_TEXT:\n            // mixed text with other elements\n            if (_mixedText){\n                _mixedText = false;\n                return _initStartElement();\n            }\n            // text followed by END_ELEMENT\n            return _handleEndElement();\n        case XML_END:\n            return XML_END;\n//            throw new IllegalStateException(\"No more XML tokens available (end of input)\");\n        }\n\n        // Ok: must be END_ELEMENT; see what tag we get (or end)\n        switch (_skipUntilTag()) {\n        case XMLStreamConstants.END_DOCUMENT:\n            return (_currentState = XML_END);\n        case XMLStreamConstants.END_ELEMENT:\n            return _handleEndElement();\n        }\n        // START_ELEMENT...\n        return _initStartElement();\n    }",
        "fixed_src_wo_comments": "private final int _next ( ) throws XMLStreamException { switch ( _currentState ) { case XML_ATTRIBUTE_VALUE : ++ _nextAttributeIndex ; case XML_START_ELEMENT : if ( _nextAttributeIndex < _attributeCount ) { _localName = _xmlReader . getAttributeLocalName ( _nextAttributeIndex ) ; _namespaceURI = _xmlReader . getAttributeNamespace ( _nextAttributeIndex ) ; _textValue = _xmlReader . getAttributeValue ( _nextAttributeIndex ) ; return ( _currentState = XML_ATTRIBUTE_NAME ) ; } String text = _collectUntilTag ( ) ; final boolean startElementNext = _xmlReader . getEventType ( ) == XMLStreamReader . START_ELEMENT ; if ( startElementNext ) { if ( text == null || _allWs ( text ) ) { _mixedText = false ; return _initStartElement ( ) ; } _mixedText = true ; _textValue = text ; return ( _currentState = XML_TEXT ) ; } if ( text != null ) { _mixedText = false ; _textValue = text ; return ( _currentState = XML_TEXT ) ; } _mixedText = false ; return _handleEndElement ( ) ; case XML_ATTRIBUTE_NAME : return ( _currentState = XML_ATTRIBUTE_VALUE ) ; case XML_TEXT : if ( _mixedText ) { _mixedText = false ; return _initStartElement ( ) ; } return _handleEndElement ( ) ; case XML_END : return XML_END ; } switch ( _skipUntilTag ( ) ) { case XMLStreamConstants . END_DOCUMENT : return ( _currentState = XML_END ) ; case XMLStreamConstants . END_ELEMENT : return _handleEndElement ( ) ; } return _initStartElement ( ) ; }",
        "summary": "Mixed content not supported if there are child elements.",
        "Description": "@XmlText is only supported if there are no child elements, support could be improved with some changes in XmlTokenStream.\nI successfully made some changes in XmlTokenStream, it's working in my personal case, but it needs more tests.\nIf agreed, I could provide a patch.\n\nExample:\nInput string : `\"<windSpeed units=\\\"kt\\\">27<radius>20</radius></windSpeed>\"`\n \"CxmlWindSpeed\" class :\n\n```\npublic class WindSpeed {\n\n    public static class Radius {\n        @JacksonXmlProperty(isAttribute = true)\n        private String sector;\n        @JacksonXmlProperty(isAttribute = true)\n        private String units;\n        @JacksonXmlText\n        private int value;\n        ..../ Getters and Setters code/....\n    }\n    @JacksonXmlProperty(isAttribute = true)\n    private String units;\n    @JacksonXmlProperty(isAttribute = true)\n    private String source;\n    @JacksonXmlText\n    private int value;\n    @JacksonXmlElementWrapper(useWrapping = false)\n    private List<Radius> radius;\n    ..../ Getters and Setters code/....\n}\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "@hvdp31 Yes, a patch (via PR or stand-alone if small) would make sense, and would be very welcome!\n"
            },
            {
                "content": "Please find here a patch for XmlTockenStream. It does the trick in my own case and pass the tests provided with jackson-dataformat-xml.\nI'm still interested in improving this part of code for other xml samples.\n\n[XmlTokenStream.txt](https://github.com/FasterXML/jackson-dataformat-xml/files/301991/XmlTokenStream.txt)\n"
            },
            {
                "content": "@hvdp31 That certainly looks delightfully simple! I'll try to have a look at this soon; I just released 2.8.0.rc1, and this would make it in rc2 (in theory perhaps even backport for 2.7.5, but this is where I don't know how good our unit test coverage is).\n"
            },
            {
                "content": "Ok added implementation along suggested lines, and added a simple test. I assume handling would not work with certain other combinations (like between end- and start-tags), but supported case seems like most common one so it should be good start, or perhaps sufficient all around.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the source code bug is to use a patch provided by the user in the form of a PR or stand-alone if small, which is a fix for the XmlTockenStream. The patch has been tested and is expected to work in most cases. The patch has been added to the code along with a simple test."
    },
    "Compress_32_src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveInputStream.java_481_518": {
        "src": "private void applyPaxHeadersToCurrentEntry(Map<String, String> headers) {\n        /*\n         * The following headers are defined for Pax.\n         * atime, ctime, charset: cannot use these without changing TarArchiveEntry fields\n         * mtime\n         * comment\n         * gid, gname\n         * linkpath\n         * size\n         * uid,uname\n         * SCHILY.devminor, SCHILY.devmajor: don't have setters/getters for those\n         */\n        for (Entry<String, String> ent : headers.entrySet()){\n            String key = ent.getKey();\n            String val = ent.getValue();\n            if (\"path\".equals(key)){\n                currEntry.setName(val);\n            } else if (\"linkpath\".equals(key)){\n                currEntry.setLinkName(val);\n            } else if (\"gid\".equals(key)){\n                currEntry.setGroupId(Integer.parseInt(val));\n            } else if (\"gname\".equals(key)){\n                currEntry.setGroupName(val);\n            } else if (\"uid\".equals(key)){\n                currEntry.setUserId(Integer.parseInt(val));\n            } else if (\"uname\".equals(key)){\n                currEntry.setUserName(val);\n            } else if (\"size\".equals(key)){\n                currEntry.setSize(Long.parseLong(val));\n            } else if (\"mtime\".equals(key)){\n                currEntry.setModTime((long) (Double.parseDouble(val) * 1000));\n            } else if (\"SCHILY.devminor\".equals(key)){\n                currEntry.setDevMinor(Integer.parseInt(val));\n            } else if (\"SCHILY.devmajor\".equals(key)){\n                currEntry.setDevMajor(Integer.parseInt(val));\n            }\n        }\n    }",
        "src_wo_comments": "private void applyPaxHeadersToCurrentEntry ( Map < String , String > headers ) { for ( Entry < String , String > ent : headers . entrySet ( ) ) { String key = ent . getKey ( ) ; String val = ent . getValue ( ) ; if ( \"path\" . equals ( key ) ) { currEntry . setName ( val ) ; } else if ( \"linkpath\" . equals ( key ) ) { currEntry . setLinkName ( val ) ; } else if ( \"gid\" . equals ( key ) ) { currEntry . setGroupId ( Integer . parseInt ( val ) ) ; } else if ( \"gname\" . equals ( key ) ) { currEntry . setGroupName ( val ) ; } else if ( \"uid\" . equals ( key ) ) { currEntry . setUserId ( Integer . parseInt ( val ) ) ; } else if ( \"uname\" . equals ( key ) ) { currEntry . setUserName ( val ) ; } else if ( \"size\" . equals ( key ) ) { currEntry . setSize ( Long . parseLong ( val ) ) ; } else if ( \"mtime\" . equals ( key ) ) { currEntry . setModTime ( ( long ) ( Double . parseDouble ( val ) * 1000 ) ) ; } else if ( \"SCHILY.devminor\" . equals ( key ) ) { currEntry . setDevMinor ( Integer . parseInt ( val ) ) ; } else if ( \"SCHILY.devmajor\" . equals ( key ) ) { currEntry . setDevMajor ( Integer . parseInt ( val ) ) ; } } }",
        "fixed_src": "private void applyPaxHeadersToCurrentEntry(Map<String, String> headers) {\n        /*\n         * The following headers are defined for Pax.\n         * atime, ctime, charset: cannot use these without changing TarArchiveEntry fields\n         * mtime\n         * comment\n         * gid, gname\n         * linkpath\n         * size\n         * uid,uname\n         * SCHILY.devminor, SCHILY.devmajor: don't have setters/getters for those\n         */\n        for (Entry<String, String> ent : headers.entrySet()){\n            String key = ent.getKey();\n            String val = ent.getValue();\n            if (\"path\".equals(key)){\n                currEntry.setName(val);\n            } else if (\"linkpath\".equals(key)){\n                currEntry.setLinkName(val);\n            } else if (\"gid\".equals(key)){\n                currEntry.setGroupId(Long.parseLong(val));\n            } else if (\"gname\".equals(key)){\n                currEntry.setGroupName(val);\n            } else if (\"uid\".equals(key)){\n                currEntry.setUserId(Long.parseLong(val));\n            } else if (\"uname\".equals(key)){\n                currEntry.setUserName(val);\n            } else if (\"size\".equals(key)){\n                currEntry.setSize(Long.parseLong(val));\n            } else if (\"mtime\".equals(key)){\n                currEntry.setModTime((long) (Double.parseDouble(val) * 1000));\n            } else if (\"SCHILY.devminor\".equals(key)){\n                currEntry.setDevMinor(Integer.parseInt(val));\n            } else if (\"SCHILY.devmajor\".equals(key)){\n                currEntry.setDevMajor(Integer.parseInt(val));\n            }\n        }\n    }",
        "fixed_src_wo_comments": "private void applyPaxHeadersToCurrentEntry ( Map < String , String > headers ) { for ( Entry < String , String > ent : headers . entrySet ( ) ) { String key = ent . getKey ( ) ; String val = ent . getValue ( ) ; if ( \"path\" . equals ( key ) ) { currEntry . setName ( val ) ; } else if ( \"linkpath\" . equals ( key ) ) { currEntry . setLinkName ( val ) ; } else if ( \"gid\" . equals ( key ) ) { currEntry . setGroupId ( Long . parseLong ( val ) ) ; } else if ( \"gname\" . equals ( key ) ) { currEntry . setGroupName ( val ) ; } else if ( \"uid\" . equals ( key ) ) { currEntry . setUserId ( Long . parseLong ( val ) ) ; } else if ( \"uname\" . equals ( key ) ) { currEntry . setUserName ( val ) ; } else if ( \"size\" . equals ( key ) ) { currEntry . setSize ( Long . parseLong ( val ) ) ; } else if ( \"mtime\" . equals ( key ) ) { currEntry . setModTime ( ( long ) ( Double . parseDouble ( val ) * 1000 ) ) ; } else if ( \"SCHILY.devminor\" . equals ( key ) ) { currEntry . setDevMinor ( Integer . parseInt ( val ) ) ; } else if ( \"SCHILY.devmajor\" . equals ( key ) ) { currEntry . setDevMajor ( Integer . parseInt ( val ) ) ; } } }",
        "summary": "TarArchiveInputStream rejects uid or gid >= 0x80000000",
        "Description": "A POSIX-format archive that came from sysdiagnose produces NumberFormatException[1] when I try to read it with TarArchiveInputStream.\n\nThe relevant part of the .tar file looks like this:\n\n   18 uid=429496729\n\nThat's the uid of 'nobody' on Mac OS (on Mac OS, uid_t is 'unsigned int').\n\nPOSIX doesn't say anything about the width of the uid extended header[2], so I assume the tar file is okay. GNU tar doesn't have trouble with it.\n\nThe relevant code, in applyPaxHeadersToCurrentEntry:\n\n            } else if (\"gid\".equals(key)){\n                currEntry.setGroupId(Integer.parseInt(val));\n...\n            } else if (\"uid\".equals(key)){\n                currEntry.setUserId(Integer.parseInt(val));\n\nuid_t and gid_t are typically unsigned 32-bit integers, so these should presumably use Long.parseLong to handle integers with the top bit set (and TarArchiveEntry would need some modifications to handle large uid and gid, too).\n\n[1] java.lang.NumberFormatException: For input string: \"4294967294\"\n        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n        at java.lang.Integer.parseInt(Integer.java:495)\n        at java.lang.Integer.parseInt(Integer.java:527)\n        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.applyPaxHeadersToCurrentEntry(TarArchiveInputStream.java:488)\n        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.paxHeaders(TarArchiveInputStream.java:415)\n        at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:295)\n\n[2] http://pubs.opengroup.org/onlinepubs/9699919799/utilities/pax.html#tag_20_92_13_03\nuid\nThe user ID of the file owner, expressed as a decimal number using digits from the ISO/IEC 646:1991 standard. This record shall override the uid field in the following header block(s). When used in write or copy mode, pax shall include a uid extended header record for each file whose owner ID is greater than 2097151 (octal 7777777).",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-314",
        "comments": [
            "I'll create a separate issue for the ability to write entries with group ids > 07777777 and fix that first so I can create a unit test for this one :-)",
            "fixed with svn revision 1678430"
        ],
        "summarized_discussion": "\n\nThe bug was fixed with svn revision 1678430, and a separate issue was created to allow for writing entries with group ids greater than 07777777."
    },
    "Compress_24_src/main/java/org/apache/commons/compress/archivers/tar/TarUtils.java_102_153": {
        "src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        if (buffer[start] == 0) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Trim all trailing NULs and spaces.\n        // The ustar and POSIX tar specs require a trailing NUL or\n        // space but some implementations use the extra digit for big\n        // sizes/uids/gids ...\n        byte trailer = buffer[end - 1];\n        if (trailer == 0 || trailer == ' '){\n            end--;\n        } else {\n            throw new IllegalArgumentException(\n                    exceptionMessage(buffer, offset, length, end-1, trailer));\n        }\n        trailer = buffer[end - 1];\n        while (start < end - 1 && (trailer == 0 || trailer == ' ')) {\n            end--;\n            trailer = buffer[end - 1];\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } if ( buffer [ start ] == 0 ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer = buffer [ end - 1 ] ; if ( trailer == 0 || trailer == ' ' ) { end -- ; } else { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , end - 1 , trailer ) ) ; } trailer = buffer [ end - 1 ] ; while ( start < end - 1 && ( trailer == 0 || trailer == ' ' ) ) { end -- ; trailer = buffer [ end - 1 ] ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "fixed_src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        if (buffer[start] == 0) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Trim all trailing NULs and spaces.\n        // The ustar and POSIX tar specs require a trailing NUL or\n        // space but some implementations use the extra digit for big\n        // sizes/uids/gids ...\n        byte trailer = buffer[end - 1];\n        while (start < end && (trailer == 0 || trailer == ' ')) {\n            end--;\n            trailer = buffer[end - 1];\n        }\n        if (start == end) {\n            throw new IllegalArgumentException(\n                    exceptionMessage(buffer, offset, length, start, trailer));\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "fixed_src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } if ( buffer [ start ] == 0 ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer = buffer [ end - 1 ] ; while ( start < end && ( trailer == 0 || trailer == ' ' ) ) { end -- ; trailer = buffer [ end - 1 ] ; } if ( start == end ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , trailer ) ) ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "summary": "TarArchiveInputStream fails to read entry with big user-id value",
        "Description": "Caused by: java.lang.IllegalArgumentException: Invalid byte 52 at offset 7 in '62410554' len=8\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:130)\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctalOrBinary(TarUtils.java:175)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:953)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:940)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:324)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:247)\n\t... 5 more",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-262",
        "comments": [
            "I first thought this might be a duplicate of one of the many fixes we've made after 1.4.1 but I still get the same exception with current trunk - will update the affected versions accordingly.\n\nDoes the archive contain anything confidential or can we add it to our test suite?",
            "Test again. I got the same exception in 1.5 1.6 and 1.7 build.",
            "I tested, that's why I added the versions to \"affects version\".  We totally agree, the problem still exists.",
            "In the example tar that you've now removed the user-id of the second entry contains \"62410554\" - this is supposed a NULL-terminated string representing an octal number <= 07777777 - obviously your example is not NULL-terminated.  Can you tell us which tar implementation has created the archive?\n\nGNU tar is willing to read the archive so this likely is yet another tar dialect for big numbers that we need to support in some way.",
            "fixed with svn revision 1570452\n\nI'd still love to know which implementation of tar you have used to create the archive so we can expand the docs."
        ],
        "summarized_discussion": "\n\nThe bug was fixed with svn revision 1570452. The cause of the bug was that the user-id of the second entry contained an octal number that was not NULL-terminated. The tar implementation used to create the archive is unknown."
    },
    "JacksonDatabind_19_src/main/java/com/fasterxml/jackson/databind/type/TypeFactory.java_1018_1031": {
        "src": "private JavaType _mapType(Class<?> rawClass)\n    {\n        // 28-May-2015, tatu: Properties are special, as per [databind#810]\n        JavaType[] typeParams = findTypeParameters(rawClass, Map.class);\n        // ok to have no types (\"raw\")\n        if (typeParams == null) {\n            return MapType.construct(rawClass, _unknownType(), _unknownType());\n        }\n        // but exactly 2 types if any found\n        if (typeParams.length != 2) {\n            throw new IllegalArgumentException(\"Strange Map type \"+rawClass.getName()+\": can not determine type parameters\");\n        }\n        return MapType.construct(rawClass, typeParams[0], typeParams[1]);\n    }",
        "src_wo_comments": "private JavaType _mapType ( Class < ? > rawClass ) { JavaType [ ] typeParams = findTypeParameters ( rawClass , Map . class ) ; if ( typeParams == null ) { return MapType . construct ( rawClass , _unknownType ( ) , _unknownType ( ) ) ; } if ( typeParams . length != 2 ) { throw new IllegalArgumentException ( \"Strange Map type \" + rawClass . getName ( ) + \": can not determine type parameters\" ) ; } return MapType . construct ( rawClass , typeParams [ 0 ] , typeParams [ 1 ] ) ; }",
        "fixed_src": "private JavaType _mapType(Class<?> rawClass)\n    {\n        // 28-May-2015, tatu: Properties are special, as per [databind#810]\n        if (rawClass == Properties.class) {\n            return MapType.construct(rawClass, CORE_TYPE_STRING, CORE_TYPE_STRING);\n        }\n        JavaType[] typeParams = findTypeParameters(rawClass, Map.class);\n        // ok to have no types (\"raw\")\n        if (typeParams == null) {\n            return MapType.construct(rawClass, _unknownType(), _unknownType());\n        }\n        // but exactly 2 types if any found\n        if (typeParams.length != 2) {\n            throw new IllegalArgumentException(\"Strange Map type \"+rawClass.getName()+\": can not determine type parameters\");\n        }\n        return MapType.construct(rawClass, typeParams[0], typeParams[1]);\n    }",
        "fixed_src_wo_comments": "private JavaType _mapType ( Class < ? > rawClass ) { if ( rawClass == Properties . class ) { return MapType . construct ( rawClass , CORE_TYPE_STRING , CORE_TYPE_STRING ) ; } JavaType [ ] typeParams = findTypeParameters ( rawClass , Map . class ) ; if ( typeParams == null ) { return MapType . construct ( rawClass , _unknownType ( ) , _unknownType ( ) ) ; } if ( typeParams . length != 2 ) { throw new IllegalArgumentException ( \"Strange Map type \" + rawClass . getName ( ) + \": can not determine type parameters\" ) ; } return MapType . construct ( rawClass , typeParams [ 0 ] , typeParams [ 1 ] ) ; }",
        "summary": "Force value coercion for `java.util.Properties`, so that values are `String`s",
        "Description": "Currently there is no custom handling for `java.util.Properties`, and although it is possible to use it (since it really is a `Map` under the hood), results are only good if values are already `String`s.\nThe problem here is that `Properties` is actually declared as `Map<String,Object>`, probably due to backwards-compatibility constraints.\n\nBut Jackson should know better: perhaps by `TypeFactory` tweaking parameterizations a bit?\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe bug in the source code was caused by a missing break statement in a switch block. The solution to this bug is to add a break statement to the switch block so that the code will exit the switch block after the desired operation has been performed."
    },
    "Mockito_34_src/org/mockito/internal/invocation/InvocationMatcher.java_103_111": {
        "src": "public void captureArgumentsFrom(Invocation i) {\n        int k = 0;\n        for (Matcher m : matchers) {\n            if (m instanceof CapturesArguments) {\n                ((CapturesArguments) m).captureFrom(i.getArguments()[k]);\n            }\n            k++;\n        }\n    }",
        "src_wo_comments": "public void captureArgumentsFrom ( Invocation i ) { int k = 0 ; for ( Matcher m : matchers ) { if ( m instanceof CapturesArguments ) { ( ( CapturesArguments ) m ) . captureFrom ( i . getArguments ( ) [ k ] ) ; } k ++ ; } }",
        "fixed_src": "public void captureArgumentsFrom(Invocation i) {\n        int k = 0;\n        for (Matcher m : matchers) {\n            if (m instanceof CapturesArguments && i.getArguments().length > k) {\n                ((CapturesArguments) m).captureFrom(i.getArguments()[k]);\n            }\n            k++;\n        }\n    }",
        "fixed_src_wo_comments": "public void captureArgumentsFrom ( Invocation i ) { int k = 0 ; for ( Matcher m : matchers ) { if ( m instanceof CapturesArguments && i . getArguments ( ) . length > k ) { ( ( CapturesArguments ) m ) . captureFrom ( i . getArguments ( ) [ k ] ) ; } k ++ ; } }",
        "summary": "Source files should not be put in binary JAR ",
        "Description": "Source files (`*.java`) should not be put into binary `mockito-core.jar`. It stupefies Idea to show decompiled file even when source jar is available.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Duplicate of #28\nThough the team thought it was better for project that don't rely on maven like source repositories.\ncc @szczepiq \n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to use maven like source repositories, as suggested by the team."
    },
    "JacksonDatabind_9_src/main/java/com/fasterxml/jackson/databind/ser/std/StdKeySerializer.java_24_35": {
        "src": "@Override\n    public void serialize(Object value, JsonGenerator jgen, SerializerProvider provider) throws IOException {\n        String str;\n        \n        if (value instanceof Date) {\n            provider.defaultSerializeDateKey((Date) value, jgen);\n            return;\n        } else {\n            str = value.toString();\n        }\n        jgen.writeFieldName(str);\n    }",
        "src_wo_comments": "@ Override public void serialize ( Object value , JsonGenerator jgen , SerializerProvider provider ) throws IOException { String str ; if ( value instanceof Date ) { provider . defaultSerializeDateKey ( ( Date ) value , jgen ) ; return ; } else { str = value . toString ( ) ; } jgen . writeFieldName ( str ) ; }",
        "fixed_src": "@Override\n    public void serialize(Object value, JsonGenerator jgen, SerializerProvider provider) throws IOException {\n        String str;\n        Class<?> cls = value.getClass();\n        \n        if (cls == String.class) {\n            str = (String) value;\n        } else if (Date.class.isAssignableFrom(cls)) {\n            provider.defaultSerializeDateKey((Date) value, jgen);\n            return;\n        } else if (cls == Class.class) {\n            str = ((Class<?>) value).getName();\n        } else {\n            str = value.toString();\n        }\n        jgen.writeFieldName(str);\n    }",
        "fixed_src_wo_comments": "@ Override public void serialize ( Object value , JsonGenerator jgen , SerializerProvider provider ) throws IOException { String str ; Class < ? > cls = value . getClass ( ) ; if ( cls == String . class ) { str = ( String ) value ; } else if ( Date . class . isAssignableFrom ( cls ) ) { provider . defaultSerializeDateKey ( ( Date ) value , jgen ) ; return ; } else if ( cls == Class . class ) { str = ( ( Class < ? > ) value ) . getName ( ) ; } else { str = value . toString ( ) ; } jgen . writeFieldName ( str ) ; }",
        "summary": "Deserializing Map<Class<? extends Object>, String>",
        "Description": "I am having problems deserializing my `Map<Class<? extends Object>, String>`. Simple test case demonstrates it:\n\n``` java\n@Test\npublic void testMapWithClassAsKey() throws Exception {\n    Map<Class<? extends Object>, String> map = new HashMap<>();\n    map.put(ArrayList.class, \"ArrayList\");\n    map.put(HashMap.class, \"HashMap\");\n\n    ObjectMapper mapper = new ObjectMapper();\n\n    String json = mapper.writerWithDefaultPrettyPrinter().writeValueAsString(map);\n    System.out.println(json);\n    mapper.readValue(json, new TypeReference<Map<Class<? extends Object>, String>>(){});\n}\n```\n\nThis test serializes the map as:\n\n``` json\n{\n    \"class java.util.ArrayList\" : \"ArrayList\",\n    \"class java.util.HashMap\" : \"HashMap\"\n}\n```\n\n`mapper.readValue(json, new TypeReference<Map<Class<? extends Object>, String>>(){});` then throws a `Exception`:\n\n```\ncom.fasterxml.jackson.databind.exc.InvalidFormatException: Can not construct     Map key of type java.lang.Class from String \"class java.util.ArrayList\": not a valid representation: Can not construct Map key of type java.lang.Class from String \"class java.util.ArrayList\": unable to parse key as Class\n at [Source: ...\n```\n\nAs i understood from #630 the KeyDeserializer for Class should be part of Jackson. Am I missing something?\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Looks like problem is with serialization, not deserialization -- output is not correct, as it appears to use `Class.toString()`. So need to have a look at `MapSerializer`s handling of key types.\n"
            },
            {
                "content": "Added handling of `Class<?>` keys; unfortunately key serializers are separate from value serializers.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to add handling of Class<?> keys to the MapSerializer, as the issue appears to be with serialization rather than deserialization."
    },
    "JacksonDatabind_35_src/main/java/com/fasterxml/jackson/databind/jsontype/impl/AsWrapperTypeDeserializer.java_78_120": {
        "src": "@SuppressWarnings(\"resource\")\n    private final Object _deserialize(JsonParser p, DeserializationContext ctxt) throws IOException\n    {\n        // 02-Aug-2013, tatu: May need to use native type ids\n        if (p.canReadTypeId()) {\n            Object typeId = p.getTypeId();\n            if (typeId != null) {\n                return _deserializeWithNativeTypeId(p, ctxt, typeId);\n            }\n        }\n        // first, sanity checks\n        if (p.getCurrentToken() != JsonToken.START_OBJECT) {\n            throw ctxt.wrongTokenException(p, JsonToken.START_OBJECT,\n                    \"need JSON Object to contain As.WRAPPER_OBJECT type information for class \"+baseTypeName());\n        }\n            // should always get field name, but just in case...\n            if (p.nextToken() != JsonToken.FIELD_NAME) {\n                throw ctxt.wrongTokenException(p, JsonToken.FIELD_NAME,\n                        \"need JSON String that contains type id (for subtype of \"+baseTypeName()+\")\");\n            }\n        final String typeId = p.getText();\n        JsonDeserializer<Object> deser = _findDeserializer(ctxt, typeId);\n        p.nextToken();\n\n        // Minor complication: we may need to merge type id in?\n        if (_typeIdVisible && p.getCurrentToken() == JsonToken.START_OBJECT) {\n            // but what if there's nowhere to add it in? Error? Or skip? For now, skip.\n            TokenBuffer tb = new TokenBuffer(null, false);\n            tb.writeStartObject(); // recreate START_OBJECT\n            tb.writeFieldName(_typePropertyName);\n            tb.writeString(typeId);\n            p = JsonParserSequence.createFlattened(tb.asParser(p), p);\n            p.nextToken();\n        }\n        \n        Object value = deser.deserialize(p, ctxt);\n        // And then need the closing END_OBJECT\n        if (p.nextToken() != JsonToken.END_OBJECT) {\n            throw ctxt.wrongTokenException(p, JsonToken.END_OBJECT,\n                    \"expected closing END_OBJECT after type information and deserialized value\");\n        }\n        return value;\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"resource\" ) private final Object _deserialize ( JsonParser p , DeserializationContext ctxt ) throws IOException { if ( p . canReadTypeId ( ) ) { Object typeId = p . getTypeId ( ) ; if ( typeId != null ) { return _deserializeWithNativeTypeId ( p , ctxt , typeId ) ; } } if ( p . getCurrentToken ( ) != JsonToken . START_OBJECT ) { throw ctxt . wrongTokenException ( p , JsonToken . START_OBJECT , \"need JSON Object to contain As.WRAPPER_OBJECT type information for class \" + baseTypeName ( ) ) ; } if ( p . nextToken ( ) != JsonToken . FIELD_NAME ) { throw ctxt . wrongTokenException ( p , JsonToken . FIELD_NAME , \"need JSON String that contains type id (for subtype of \" + baseTypeName ( ) + \")\" ) ; } final String typeId = p . getText ( ) ; JsonDeserializer < Object > deser = _findDeserializer ( ctxt , typeId ) ; p . nextToken ( ) ; if ( _typeIdVisible && p . getCurrentToken ( ) == JsonToken . START_OBJECT ) { TokenBuffer tb = new TokenBuffer ( null , false ) ; tb . writeStartObject ( ) ; tb . writeFieldName ( _typePropertyName ) ; tb . writeString ( typeId ) ; p = JsonParserSequence . createFlattened ( tb . asParser ( p ) , p ) ; p . nextToken ( ) ; } Object value = deser . deserialize ( p , ctxt ) ; if ( p . nextToken ( ) != JsonToken . END_OBJECT ) { throw ctxt . wrongTokenException ( p , JsonToken . END_OBJECT , \"expected closing END_OBJECT after type information and deserialized value\" ) ; } return value ; }",
        "fixed_src": "@SuppressWarnings(\"resource\")\n    private final Object _deserialize(JsonParser p, DeserializationContext ctxt) throws IOException\n    {\n        // 02-Aug-2013, tatu: May need to use native type ids\n        if (p.canReadTypeId()) {\n            Object typeId = p.getTypeId();\n            if (typeId != null) {\n                return _deserializeWithNativeTypeId(p, ctxt, typeId);\n            }\n        }\n        // first, sanity checks\n        JsonToken t = p.getCurrentToken();\n        if (t == JsonToken.START_OBJECT) {\n            // should always get field name, but just in case...\n            if (p.nextToken() != JsonToken.FIELD_NAME) {\n                throw ctxt.wrongTokenException(p, JsonToken.FIELD_NAME,\n                        \"need JSON String that contains type id (for subtype of \"+baseTypeName()+\")\");\n            }\n        } else if (t != JsonToken.FIELD_NAME) {\n            throw ctxt.wrongTokenException(p, JsonToken.START_OBJECT,\n                    \"need JSON Object to contain As.WRAPPER_OBJECT type information for class \"+baseTypeName());\n        }\n        final String typeId = p.getText();\n        JsonDeserializer<Object> deser = _findDeserializer(ctxt, typeId);\n        p.nextToken();\n\n        // Minor complication: we may need to merge type id in?\n        if (_typeIdVisible && p.getCurrentToken() == JsonToken.START_OBJECT) {\n            // but what if there's nowhere to add it in? Error? Or skip? For now, skip.\n            TokenBuffer tb = new TokenBuffer(null, false);\n            tb.writeStartObject(); // recreate START_OBJECT\n            tb.writeFieldName(_typePropertyName);\n            tb.writeString(typeId);\n            p = JsonParserSequence.createFlattened(tb.asParser(p), p);\n            p.nextToken();\n        }\n        \n        Object value = deser.deserialize(p, ctxt);\n        // And then need the closing END_OBJECT\n        if (p.nextToken() != JsonToken.END_OBJECT) {\n            throw ctxt.wrongTokenException(p, JsonToken.END_OBJECT,\n                    \"expected closing END_OBJECT after type information and deserialized value\");\n        }\n        return value;\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"resource\" ) private final Object _deserialize ( JsonParser p , DeserializationContext ctxt ) throws IOException { if ( p . canReadTypeId ( ) ) { Object typeId = p . getTypeId ( ) ; if ( typeId != null ) { return _deserializeWithNativeTypeId ( p , ctxt , typeId ) ; } } JsonToken t = p . getCurrentToken ( ) ; if ( t == JsonToken . START_OBJECT ) { if ( p . nextToken ( ) != JsonToken . FIELD_NAME ) { throw ctxt . wrongTokenException ( p , JsonToken . FIELD_NAME , \"need JSON String that contains type id (for subtype of \" + baseTypeName ( ) + \")\" ) ; } } else if ( t != JsonToken . FIELD_NAME ) { throw ctxt . wrongTokenException ( p , JsonToken . START_OBJECT , \"need JSON Object to contain As.WRAPPER_OBJECT type information for class \" + baseTypeName ( ) ) ; } final String typeId = p . getText ( ) ; JsonDeserializer < Object > deser = _findDeserializer ( ctxt , typeId ) ; p . nextToken ( ) ; if ( _typeIdVisible && p . getCurrentToken ( ) == JsonToken . START_OBJECT ) { TokenBuffer tb = new TokenBuffer ( null , false ) ; tb . writeStartObject ( ) ; tb . writeFieldName ( _typePropertyName ) ; tb . writeString ( typeId ) ; p = JsonParserSequence . createFlattened ( tb . asParser ( p ) , p ) ; p . nextToken ( ) ; } Object value = deser . deserialize ( p , ctxt ) ; if ( p . nextToken ( ) != JsonToken . END_OBJECT ) { throw ctxt . wrongTokenException ( p , JsonToken . END_OBJECT , \"expected closing END_OBJECT after type information and deserialized value\" ) ; } return value ; }",
        "summary": "Problem with Object Id and Type Id as Wrapper Object (regression in 2.5.1)",
        "Description": "(note: originally from https://github.com/FasterXML/jackson-module-jaxb-annotations/issues/51)\n\nLooks like fix for #669 caused a regression for the special use case of combining type and object ids, with wrapper-object type id inclusion. The problem started with 2.5.1.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Fixed for 2.6.5 (and, if ever released, 2.5.5-1 micro-patch), as well as `master` for 2.7.0 (-rc3, if one released).\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been fixed for version 2.6.5 and 2.5.5-1 micro-patch, as well as the 'master' version for 2.7.0 and any potential 2.7.0 -rc3 release."
    },
    "Math_106_src/java/org/apache/commons/math/fraction/ProperFractionFormat.java_130_206": {
        "src": "public Fraction parse(String source, ParsePosition pos) {\n        // try to parse improper fraction\n        Fraction ret = super.parse(source, pos);\n        if (ret != null) {\n            return ret;\n        }\n        \n        int initialIndex = pos.getIndex();\n\n        // parse whitespace\n        parseAndIgnoreWhitespace(source, pos);\n\n        // parse whole\n        Number whole = getWholeFormat().parse(source, pos);\n        if (whole == null) {\n            // invalid integer number\n            // set index back to initial, error index should already be set\n            // character examined.\n            pos.setIndex(initialIndex);\n            return null;\n        }\n\n        // parse whitespace\n        parseAndIgnoreWhitespace(source, pos);\n        \n        // parse numerator\n        Number num = getNumeratorFormat().parse(source, pos);\n        if (num == null) {\n            // invalid integer number\n            // set index back to initial, error index should already be set\n            // character examined.\n            pos.setIndex(initialIndex);\n            return null;\n        }\n        \n            // minus signs should be leading, invalid expression\n\n        // parse '/'\n        int startIndex = pos.getIndex();\n        char c = parseNextCharacter(source, pos);\n        switch (c) {\n        case 0 :\n            // no '/'\n            // return num as a fraction\n            return new Fraction(num.intValue(), 1);\n        case '/' :\n            // found '/', continue parsing denominator\n            break;\n        default :\n            // invalid '/'\n            // set index back to initial, error index should be the last\n            // character examined.\n            pos.setIndex(initialIndex);\n            pos.setErrorIndex(startIndex);\n            return null;\n        }\n\n        // parse whitespace\n        parseAndIgnoreWhitespace(source, pos);\n\n        // parse denominator\n        Number den = getDenominatorFormat().parse(source, pos);\n        if (den == null) {\n            // invalid integer number\n            // set index back to initial, error index should already be set\n            // character examined.\n            pos.setIndex(initialIndex);\n            return null;\n        }\n        \n            // minus signs must be leading, invalid\n\n        int w = whole.intValue();\n        int n = num.intValue();\n        int d = den.intValue();\n        return new Fraction(((Math.abs(w) * d) + n) * MathUtils.sign(w), d);\n    }",
        "src_wo_comments": "public Fraction parse ( String source , ParsePosition pos ) { Fraction ret = super . parse ( source , pos ) ; if ( ret != null ) { return ret ; } int initialIndex = pos . getIndex ( ) ; parseAndIgnoreWhitespace ( source , pos ) ; Number whole = getWholeFormat ( ) . parse ( source , pos ) ; if ( whole == null ) { pos . setIndex ( initialIndex ) ; return null ; } parseAndIgnoreWhitespace ( source , pos ) ; Number num = getNumeratorFormat ( ) . parse ( source , pos ) ; if ( num == null ) { pos . setIndex ( initialIndex ) ; return null ; } int startIndex = pos . getIndex ( ) ; char c = parseNextCharacter ( source , pos ) ; switch ( c ) { case 0 : return new Fraction ( num . intValue ( ) , 1 ) ; case '/' : break ; default : pos . setIndex ( initialIndex ) ; pos . setErrorIndex ( startIndex ) ; return null ; } parseAndIgnoreWhitespace ( source , pos ) ; Number den = getDenominatorFormat ( ) . parse ( source , pos ) ; if ( den == null ) { pos . setIndex ( initialIndex ) ; return null ; } int w = whole . intValue ( ) ; int n = num . intValue ( ) ; int d = den . intValue ( ) ; return new Fraction ( ( ( Math . abs ( w ) * d ) + n ) * MathUtils . sign ( w ) , d ) ; }",
        "fixed_src": "public Fraction parse(String source, ParsePosition pos) {\n        // try to parse improper fraction\n        Fraction ret = super.parse(source, pos);\n        if (ret != null) {\n            return ret;\n        }\n        \n        int initialIndex = pos.getIndex();\n\n        // parse whitespace\n        parseAndIgnoreWhitespace(source, pos);\n\n        // parse whole\n        Number whole = getWholeFormat().parse(source, pos);\n        if (whole == null) {\n            // invalid integer number\n            // set index back to initial, error index should already be set\n            // character examined.\n            pos.setIndex(initialIndex);\n            return null;\n        }\n\n        // parse whitespace\n        parseAndIgnoreWhitespace(source, pos);\n        \n        // parse numerator\n        Number num = getNumeratorFormat().parse(source, pos);\n        if (num == null) {\n            // invalid integer number\n            // set index back to initial, error index should already be set\n            // character examined.\n            pos.setIndex(initialIndex);\n            return null;\n        }\n        \n        if (num.intValue() < 0) {\n            // minus signs should be leading, invalid expression\n            pos.setIndex(initialIndex);\n            return null;\n        }\n\n        // parse '/'\n        int startIndex = pos.getIndex();\n        char c = parseNextCharacter(source, pos);\n        switch (c) {\n        case 0 :\n            // no '/'\n            // return num as a fraction\n            return new Fraction(num.intValue(), 1);\n        case '/' :\n            // found '/', continue parsing denominator\n            break;\n        default :\n            // invalid '/'\n            // set index back to initial, error index should be the last\n            // character examined.\n            pos.setIndex(initialIndex);\n            pos.setErrorIndex(startIndex);\n            return null;\n        }\n\n        // parse whitespace\n        parseAndIgnoreWhitespace(source, pos);\n\n        // parse denominator\n        Number den = getDenominatorFormat().parse(source, pos);\n        if (den == null) {\n            // invalid integer number\n            // set index back to initial, error index should already be set\n            // character examined.\n            pos.setIndex(initialIndex);\n            return null;\n        }\n        \n        if (den.intValue() < 0) {\n            // minus signs must be leading, invalid\n            pos.setIndex(initialIndex);\n            return null;\n        }\n\n        int w = whole.intValue();\n        int n = num.intValue();\n        int d = den.intValue();\n        return new Fraction(((Math.abs(w) * d) + n) * MathUtils.sign(w), d);\n    }",
        "fixed_src_wo_comments": "public Fraction parse ( String source , ParsePosition pos ) { Fraction ret = super . parse ( source , pos ) ; if ( ret != null ) { return ret ; } int initialIndex = pos . getIndex ( ) ; parseAndIgnoreWhitespace ( source , pos ) ; Number whole = getWholeFormat ( ) . parse ( source , pos ) ; if ( whole == null ) { pos . setIndex ( initialIndex ) ; return null ; } parseAndIgnoreWhitespace ( source , pos ) ; Number num = getNumeratorFormat ( ) . parse ( source , pos ) ; if ( num == null ) { pos . setIndex ( initialIndex ) ; return null ; } if ( num . intValue ( ) < 0 ) { pos . setIndex ( initialIndex ) ; return null ; } int startIndex = pos . getIndex ( ) ; char c = parseNextCharacter ( source , pos ) ; switch ( c ) { case 0 : return new Fraction ( num . intValue ( ) , 1 ) ; case '/' : break ; default : pos . setIndex ( initialIndex ) ; pos . setErrorIndex ( startIndex ) ; return null ; } parseAndIgnoreWhitespace ( source , pos ) ; Number den = getDenominatorFormat ( ) . parse ( source , pos ) ; if ( den == null ) { pos . setIndex ( initialIndex ) ; return null ; } if ( den . intValue ( ) < 0 ) { pos . setIndex ( initialIndex ) ; return null ; } int w = whole . intValue ( ) ; int n = num . intValue ( ) ; int d = den . intValue ( ) ; return new Fraction ( ( ( Math . abs ( w ) * d ) + n ) * MathUtils . sign ( w ) , d ) ; }",
        "summary": "[math] Function math.fraction.ProperFractionFormat.parse(String, ParsePosition) return illogical result",
        "Description": "Hello,\n\nI find illogical returned result from function \"Fraction parse(String source, \nParsePostion pos)\" (in class ProperFractionFormat of the Fraction Package) of \nthe Commons Math library. Please see the following code segment for more \ndetails:\n\n\"\nProperFractionFormat properFormat = new ProperFractionFormat();\nresult = null;\nString source = \"1 -1 / 2\";\nParsePosition pos = new ParsePosition(0);\n\n//Test 1 : fail \npublic void testParseNegative(){\n \n   String source = \"-1 -2 / 3\";\n   ParsePosition pos = new ParsePosition(0);\n\n   Fraction actual = properFormat.parse(source, pos);\n   assertNull(actual);\n}\n\n// Test2: success\npublic void testParseNegative(){\n \n   String source = \"-1 -2 / 3\";\n   ParsePosition pos = new ParsePosition(0);\n\n   Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3\n   assertEquals(1, source.getNumerator());\n   assertEquals(3, source.getDenominator());\n}\n\n\"\n\nNote: Similarly, when I passed in the following inputs: \n  input 2: (source = \u00931 2 / -3\u0094, pos = 0)\n  input 3: ( source = \u0094 -1 -2 / 3\u0094, pos = 0)\n\nFunction \"Fraction parse(String, ParsePosition)\" returned Fraction 1/3 (means \nthe result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs \nabove.\n \nI think the function does not handle parsing the numberator/ denominator \nproperly incase input string provide invalid numerator/denominator. \n\nThank you!",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-60",
        "comments": [
            "Thank you for reporting this.  I agree that the treatment of minus signs in the fractional parts of mixed fractions is not correct.  I think, however, that the correct solution is to throw a ParseException when the fractional part is\nnegative.  Otherwise, we have to disambiguate expressions such as \"-1 -1/3\"  which could mean -4/3 or -2/3.  I think it is better to view these expressions as meaningless.  If there are no objections, I will make that change.",
            "Changed ProperFractionFormat to reject embedded minus, per last comment.",
            "\n   [[ Old comment, sent by email on Fri, 19 May 2006 12:29:16 -0500 ]]\n\nThank you to get back to me.\n\nHave a great weekend,\nNhung\n\n\n-----------------------------------------------------------------------------\n--------------------------\n",
            "doesn't the answers provided by Phil Steitz suit you ? This issue is marked as closed, do you want to reopen it ?",
            "Yes, it's great. I don't need it to be reopenned. Thank you very much!\n\nNhu\n\n\n"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to throw a ParseException when the fractional part is negative. This will prevent expressions such as \"-1 -1/3\" from being interpreted as either -4/3 or -2/3. The change has been implemented and the issue is marked as closed."
    },
    "Math_50_src/main/java/org/apache/commons/math/analysis/solvers/BaseSecantSolver.java_128_253": {
        "src": "protected final double doSolve() {\n        // Get initial solution\n        double x0 = getMin();\n        double x1 = getMax();\n        double f0 = computeObjectiveValue(x0);\n        double f1 = computeObjectiveValue(x1);\n\n        // If one of the bounds is the exact root, return it. Since these are\n        // not under-approximations or over-approximations, we can return them\n        // regardless of the allowed solutions.\n        if (f0 == 0.0) {\n            return x0;\n        }\n        if (f1 == 0.0) {\n            return x1;\n        }\n\n        // Verify bracketing of initial solution.\n        verifyBracketing(x0, x1);\n\n        // Get accuracies.\n        final double ftol = getFunctionValueAccuracy();\n        final double atol = getAbsoluteAccuracy();\n        final double rtol = getRelativeAccuracy();\n\n        // Keep track of inverted intervals, meaning that the left bound is\n        // larger than the right bound.\n        boolean inverted = false;\n\n        // Keep finding better approximations.\n        while (true) {\n            // Calculate the next approximation.\n            final double x = x1 - ((f1 * (x1 - x0)) / (f1 - f0));\n            final double fx = computeObjectiveValue(x);\n\n            // If the new approximation is the exact root, return it. Since\n            // this is not an under-approximation or an over-approximation,\n            // we can return it regardless of the allowed solutions.\n            if (fx == 0.0) {\n                return x;\n            }\n\n            // Update the bounds with the new approximation.\n            if (f1 * fx < 0) {\n                // The value of x1 has switched to the other bound, thus inverting\n                // the interval.\n                x0 = x1;\n                f0 = f1;\n                inverted = !inverted;\n            } else {\n                switch (method) {\n                case ILLINOIS:\n                    f0 *= 0.5;\n                    break;\n                case PEGASUS:\n                    f0 *= f1 / (f1 + fx);\n                    break;\n                case REGULA_FALSI:\n                    // Nothing.\n                    if (x == x1) {\n                        x0 = 0.5 * (x0 + x1 - FastMath.max(rtol * FastMath.abs(x1), atol));\n                        f0 = computeObjectiveValue(x0);\n                    }\n                    break;\n                default:\n                    // Should never happen.\n                    throw new MathInternalError();\n                }\n            }\n            // Update from [x0, x1] to [x0, x].\n            x1 = x;\n            f1 = fx;\n\n            // If the function value of the last approximation is too small,\n            // given the function value accuracy, then we can't get closer to\n            // the root than we already are.\n            if (FastMath.abs(f1) <= ftol) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    if (inverted) {\n                        return x1;\n                    }\n                    break;\n                case RIGHT_SIDE:\n                    if (!inverted) {\n                        return x1;\n                    }\n                    break;\n                case BELOW_SIDE:\n                    if (f1 <= 0) {\n                        return x1;\n                    }\n                    break;\n                case ABOVE_SIDE:\n                    if (f1 >= 0) {\n                        return x1;\n                    }\n                    break;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n\n            // If the current interval is within the given accuracies, we\n            // are satisfied with the current approximation.\n            if (FastMath.abs(x1 - x0) < FastMath.max(rtol * FastMath.abs(x1),\n                                                     atol)) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    return inverted ? x1 : x0;\n                case RIGHT_SIDE:\n                    return inverted ? x0 : x1;\n                case BELOW_SIDE:\n                    return (f1 <= 0) ? x1 : x0;\n                case ABOVE_SIDE:\n                    return (f1 >= 0) ? x1 : x0;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n        }\n    }",
        "src_wo_comments": "protected final double doSolve ( ) { double x0 = getMin ( ) ; double x1 = getMax ( ) ; double f0 = computeObjectiveValue ( x0 ) ; double f1 = computeObjectiveValue ( x1 ) ; if ( f0 == 0.0 ) { return x0 ; } if ( f1 == 0.0 ) { return x1 ; } verifyBracketing ( x0 , x1 ) ; final double ftol = getFunctionValueAccuracy ( ) ; final double atol = getAbsoluteAccuracy ( ) ; final double rtol = getRelativeAccuracy ( ) ; boolean inverted = false ; while ( true ) { final double x = x1 - ( ( f1 * ( x1 - x0 ) ) / ( f1 - f0 ) ) ; final double fx = computeObjectiveValue ( x ) ; if ( fx == 0.0 ) { return x ; } if ( f1 * fx < 0 ) { x0 = x1 ; f0 = f1 ; inverted = ! inverted ; } else { switch ( method ) { case ILLINOIS : f0 *= 0.5 ; break ; case PEGASUS : f0 *= f1 / ( f1 + fx ) ; break ; case REGULA_FALSI : if ( x == x1 ) { x0 = 0.5 * ( x0 + x1 - FastMath . max ( rtol * FastMath . abs ( x1 ) , atol ) ) ; f0 = computeObjectiveValue ( x0 ) ; } break ; default : throw new MathInternalError ( ) ; } } x1 = x ; f1 = fx ; if ( FastMath . abs ( f1 ) <= ftol ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : if ( inverted ) { return x1 ; } break ; case RIGHT_SIDE : if ( ! inverted ) { return x1 ; } break ; case BELOW_SIDE : if ( f1 <= 0 ) { return x1 ; } break ; case ABOVE_SIDE : if ( f1 >= 0 ) { return x1 ; } break ; default : throw new MathInternalError ( ) ; } } if ( FastMath . abs ( x1 - x0 ) < FastMath . max ( rtol * FastMath . abs ( x1 ) , atol ) ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : return inverted ? x1 : x0 ; case RIGHT_SIDE : return inverted ? x0 : x1 ; case BELOW_SIDE : return ( f1 <= 0 ) ? x1 : x0 ; case ABOVE_SIDE : return ( f1 >= 0 ) ? x1 : x0 ; default : throw new MathInternalError ( ) ; } } } }",
        "fixed_src": "protected final double doSolve() {\n        // Get initial solution\n        double x0 = getMin();\n        double x1 = getMax();\n        double f0 = computeObjectiveValue(x0);\n        double f1 = computeObjectiveValue(x1);\n\n        // If one of the bounds is the exact root, return it. Since these are\n        // not under-approximations or over-approximations, we can return them\n        // regardless of the allowed solutions.\n        if (f0 == 0.0) {\n            return x0;\n        }\n        if (f1 == 0.0) {\n            return x1;\n        }\n\n        // Verify bracketing of initial solution.\n        verifyBracketing(x0, x1);\n\n        // Get accuracies.\n        final double ftol = getFunctionValueAccuracy();\n        final double atol = getAbsoluteAccuracy();\n        final double rtol = getRelativeAccuracy();\n\n        // Keep track of inverted intervals, meaning that the left bound is\n        // larger than the right bound.\n        boolean inverted = false;\n\n        // Keep finding better approximations.\n        while (true) {\n            // Calculate the next approximation.\n            final double x = x1 - ((f1 * (x1 - x0)) / (f1 - f0));\n            final double fx = computeObjectiveValue(x);\n\n            // If the new approximation is the exact root, return it. Since\n            // this is not an under-approximation or an over-approximation,\n            // we can return it regardless of the allowed solutions.\n            if (fx == 0.0) {\n                return x;\n            }\n\n            // Update the bounds with the new approximation.\n            if (f1 * fx < 0) {\n                // The value of x1 has switched to the other bound, thus inverting\n                // the interval.\n                x0 = x1;\n                f0 = f1;\n                inverted = !inverted;\n            } else {\n                switch (method) {\n                case ILLINOIS:\n                    f0 *= 0.5;\n                    break;\n                case PEGASUS:\n                    f0 *= f1 / (f1 + fx);\n                    break;\n                case REGULA_FALSI:\n                    // Nothing.\n                    break;\n                default:\n                    // Should never happen.\n                    throw new MathInternalError();\n                }\n            }\n            // Update from [x0, x1] to [x0, x].\n            x1 = x;\n            f1 = fx;\n\n            // If the function value of the last approximation is too small,\n            // given the function value accuracy, then we can't get closer to\n            // the root than we already are.\n            if (FastMath.abs(f1) <= ftol) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    if (inverted) {\n                        return x1;\n                    }\n                    break;\n                case RIGHT_SIDE:\n                    if (!inverted) {\n                        return x1;\n                    }\n                    break;\n                case BELOW_SIDE:\n                    if (f1 <= 0) {\n                        return x1;\n                    }\n                    break;\n                case ABOVE_SIDE:\n                    if (f1 >= 0) {\n                        return x1;\n                    }\n                    break;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n\n            // If the current interval is within the given accuracies, we\n            // are satisfied with the current approximation.\n            if (FastMath.abs(x1 - x0) < FastMath.max(rtol * FastMath.abs(x1),\n                                                     atol)) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    return inverted ? x1 : x0;\n                case RIGHT_SIDE:\n                    return inverted ? x0 : x1;\n                case BELOW_SIDE:\n                    return (f1 <= 0) ? x1 : x0;\n                case ABOVE_SIDE:\n                    return (f1 >= 0) ? x1 : x0;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n        }\n    }",
        "fixed_src_wo_comments": "protected final double doSolve ( ) { double x0 = getMin ( ) ; double x1 = getMax ( ) ; double f0 = computeObjectiveValue ( x0 ) ; double f1 = computeObjectiveValue ( x1 ) ; if ( f0 == 0.0 ) { return x0 ; } if ( f1 == 0.0 ) { return x1 ; } verifyBracketing ( x0 , x1 ) ; final double ftol = getFunctionValueAccuracy ( ) ; final double atol = getAbsoluteAccuracy ( ) ; final double rtol = getRelativeAccuracy ( ) ; boolean inverted = false ; while ( true ) { final double x = x1 - ( ( f1 * ( x1 - x0 ) ) / ( f1 - f0 ) ) ; final double fx = computeObjectiveValue ( x ) ; if ( fx == 0.0 ) { return x ; } if ( f1 * fx < 0 ) { x0 = x1 ; f0 = f1 ; inverted = ! inverted ; } else { switch ( method ) { case ILLINOIS : f0 *= 0.5 ; break ; case PEGASUS : f0 *= f1 / ( f1 + fx ) ; break ; case REGULA_FALSI : break ; default : throw new MathInternalError ( ) ; } } x1 = x ; f1 = fx ; if ( FastMath . abs ( f1 ) <= ftol ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : if ( inverted ) { return x1 ; } break ; case RIGHT_SIDE : if ( ! inverted ) { return x1 ; } break ; case BELOW_SIDE : if ( f1 <= 0 ) { return x1 ; } break ; case ABOVE_SIDE : if ( f1 >= 0 ) { return x1 ; } break ; default : throw new MathInternalError ( ) ; } } if ( FastMath . abs ( x1 - x0 ) < FastMath . max ( rtol * FastMath . abs ( x1 ) , atol ) ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : return inverted ? x1 : x0 ; case RIGHT_SIDE : return inverted ? x0 : x1 ; case BELOW_SIDE : return ( f1 <= 0 ) ? x1 : x0 ; case ABOVE_SIDE : return ( f1 >= 0 ) ? x1 : x0 ; default : throw new MathInternalError ( ) ; } } } }",
        "summary": "\"RegulaFalsiSolver\" failure",
        "Description": "The following unit test:\n{code}\n@Test\npublic void testBug() {\n    final UnivariateRealFunction f = new UnivariateRealFunction() {\n            @Override\n            public double value(double x) {\n                return Math.exp(x) - Math.pow(Math.PI, 3.0);\n            }\n        };\n\n    UnivariateRealSolver solver = new RegulaFalsiSolver();\n    double root = solver.solve(100, f, 1, 10);\n}\n{code}\nfails with\n{noformat}\nillegal state: maximal count (100) exceeded: evaluations\n{noformat}\n\nUsing \"PegasusSolver\", the answer is found after 17 evaluations.\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-631",
        "comments": [
            "Reported by Axel Kramer in MATH-599.",
            "The problem was due to the fact that at some point, the update formula always gave the same value: Nothing was being updated and the loop went on until the number of evaluations was exhausted.\n\nI've committed a tentative solution in revision 1154614.\nHowever:\n# I'm not sure that it doesn't have any adverse side-effects on the bracketing property.\n# It is quite probably not a pristine \"regula falsi\" algorithm anymore.\n\nPlease review.\n\nAnyways, for the function that triggered the problem (see \"testIssue631\" in \"RegulaFalsiSolverTest.java\"), the (modified) {{RegulaFalsiSolver}} takes 3624 evaluations (versus 17 for {{PegasusSolver}}). We should probably add a word of warning in the class Javadoc.\n",
            "I just got back from a 3 week vacation, so I couldn't reply earlier.\n\nThe documentation for the RegulaFalsiSolver states: \"Unlike the Secant method, convergence is guaranteed by maintaining a bracketed solution.\" While this is theoretically true, in this case it is not so, because (if I understand correctly) only a single bound is updated repeatedly, and the update is too small to matter (has no effect), due to the double representation.\n\nThe change you propose (which is difficult to see as you also change other things in the same commit) is to modify x0 and f0 if the new value of x and x1 are equal. As I see it, this changes the algorithm, and it is no longer the Regula Falsi method as known from literature. I'm therefore against this change.\n\nThe problem that is identified in this issue is very similar to the well-known problem of the Regula Falsi method: it converges very slowly for certain problems, due to one side being updated all the time, while the other one stays the same. The Illinois and Pegasus algorithms solve exactly this problem, and are well-documented in literature.\n\nI therefore think it would be better if the RegulaFalsiSolver kept it's original implementation, and for this problem the Illinois or Pegasus method should then be used instead.\n\nThe other changes (if statements to switch with default, extracting bound switch statements, etc) can be kept, if you wish.\n\nThe suggestion to add a warning to the Secant and Regula Falsi solvers that this is a possible problem, and the solution (use Illinois or Pegasus), would indeed be a good idea. In general, adding a note that the Illinois and Pegasus algorithms perform better, would be a good idea regardless of this issue.\n\nOnce more, to be clear, I don't think this issue is a bug. It is a result of the limited convergence of the Regula Falsi method combined with the implications of limited double precision. The limited convergence of the algorithm is a property of the algorithm, and should in my opinion not be changed. I also don't think that trying to work around the limited double precision would be a good idea.",
            "> (which is difficult to see as you also change other things in the same commit)\n\nSorry, but I didn't hit the solution right away, i.e. before changing those two additional little things to make the code clearer (for me)...\n\nThe only actual change is that the {{REGULA_FALSI}} enum was not used (i.e. with the {{switch}} little change, the corresponding {{case}} would have been empty) whereas now it contains the update of x0 to avoid an infinite loop.\n\nThe other (cosmetic) change was to take these two statements\n{code}\nx1 = x;\nf1 = fx;\n{code}\nout of the previous {{if}} and {{else}} blocks, as they were duplicated there (which made me wonder whether it was a bug that they were _not_ different).\n\nYou say\n> [...] convergence is guaranteed [...]\n> [...] it converges very slowly for certain problems, [...]\n> [...] The limited convergence of the algorithm is a property of the algorithm, [...]\n\nAll the above imply that one expects that the algorithm _can_ find the solution.\nHowever, in this implementation, it _can't_.\nTherefore there is a bug, somewhere.\n\nI agree that it is a limitation of double precision. But, IMHO, leaving the code as-is is not a good idea because because it leads to the impression that the \"Regula Falsi\" mathematical algorithm can fail to converge, which is not correct (IIUC).\nTherefore, we could add a comment stating that the _implementation_ with limited precision can fail to converge but that would be akin to saying to users: \"Here is a code, but don't use it.\"\nPersonally, I would prefer to say: \"Because of limited precision, the implementation can fail to converge. In those cases, we slightly modified the original algorithm in order to avoid failure.\"\n",
            "{quote}\nAll the above imply that one expects that the algorithm can find the solution.\nHowever, in this implementation, it can't.\nTherefore there is a bug, somewhere.\n{quote}\n\nHere, the bug is in the algorithm itself, not in the implementation.\n\n{quote}\nit leads to the impression that the \"Regula Falsi\" mathematical algorithm can fail to converge, which is not correct (IIUC).\n{quote}\n\nIt is correct. Regula Falsi fails to converge, or rather it can take a too large number of iteration to converge. This is exactly this behavior that has lead to the construction of other algorithms like Illinois or Pegasus. These two algorithms try to detect the case when the same end of the interval is always updated, and the other end remains unchanged. Once they have detected this, they slightly change the value at one end to trick the linear evaluation into choosing a value that is very likely to have the required sign to update this other end. In fact, in many cases depending of the sign of the curvature near the root, as soon as one end is very close to the root the linear interpolation will always remain on the same side of the root and hence will update this end.\n\nI agree with Dennis here, the change needed to ensure convergence is not tool long is to choose a better algorithm, such as Illinois, Pegasus ... or the nth order bracketing solver I recently added. Regula Falsi should remain the reference Regula Falsi, just as secant and Brent should remain the reference ones.\n",
            "\"fails to converge\" and \"large number of iteration to converge\" are completely different things.\n\nThe documentation says: \"convergence is guaranteed\". Is _that_ false?\n\nMoreover, for the function reported in this issue, the problem is not that it takes a large number iterations, it is that the loop is _literally_ infinite because at some point, nothing changes anymore.\n\nStated otherwise: If implemented with larger/infinite precision, would it converge?\nIn the affirmative, then in my opinion it means that the plain \"Regula Falsi\" method cannot be implemented with double precision (or that its convergence properties are not as stated in the docs) or that there is a bug in the implementation.\n\nIn the former case, why keep something that will never be used (as we'll warn users that they should use \"Pegasus\" or \"Illinois\" but certainly not \"RegulaFalsi\")? IMHO, we could just state in the docs that \"RegulaFalsi\" was not implemented because it is demonstrably less efficient and sometimes fails to work.\n\nA less radical alternative would be to keep the test I've inserted in the code (at line 186) and throw a {{MathIllegalStateException}} if it passes. The previous behaviour (infinite loop) is a bug in CM.\n",
            "{quote}\nThe documentation says: \"convergence is guaranteed\". Is that false?\n{quote}\n\nIt depends on what is called convergence.\nIf convergence is evaluated only as the best of the two endpoints (measured along y axis), yes convergence is guaranteed and in this case it is very slow. This is what appears in many analysis books.\nIf convergence is evaluated by ensuring the bracketing interval (measured along x axis) reduces to zero (i.e. both endpoints converge to the root), convergence is not guaranteed.\n\nThe first case is achieved with our implementation by using the function accuracy setting. The second case is achieved with our implementation by using relative accuracy and absolute accuracy settings, which both are computed along x axis.\n\nI fear that there are several different references about convergence for this method (just as for Brent). So we already are able to implement both views.\n\nWithout any change to our implementation, we reach convergence for this example by setting the function accuracy to 7.4e-13 or above, and it is slow (about 3500 evaluations). The default setting for function accuracy is very low (1.0e-15) and in this case, given the variation rate of the function near the root, it is equivalent to completely ignore convergence on y on only check the convergence on the interval length along x. \n",
            "I think we should either stick with the standard implementation of Regula Falsi or drop the class altogether.  Different rootfinders are going to perform better / worse for different functions and parameter values and I don't think it is a good idea to try to modify our implementations of the algorithms to try to work around their shortcomings for problem instances for which they are not well-suited.  It is much better to stick with standard algorithms, document them, and leave it to users to choose among implementations.  \n\nRegula Falsi is not a good general-purpose rootfinder, but it does perform well for some problems (or parts of problems) and the original submission was a working implementation, so I would say revert the changes and keep it.",
            "I understand what you say. But however you put it, there is a bug; if not in the implementation, then in the API. It is not expected behaviour that something which must be changed (function accuracy threshold) to ensure correct behaviour (avoid an undetected infinite loop) is not a mandatory parameter.\nTo debug this, I started by raising the absolute accuracy threshold (the first default parameter, thus the first obvious thing to do) to 1e-2 and was stunned that I couldn't get anything after 1000000 iterations!\n\nTherefore I maintain that, at a minimum, we put a line that will detect the infinite loop and raise an exception identifying _that_ problem and not let the user wait for \"TooManyEvaluationsException\" to be raised, as that will induce the unaware (me) to just allow more evaluations and try again.\n\nThis solution does not corrupt the algorithm; it just adds protection.\n",
            "I disagree with your statement about setting accuracy.  All of this is configurable and if not set, you get the (documented) defaults.  This is all documented.  If the documentation is not clear, then we can improve it.  A user who applies Regula Falsi to the problem instance being examined here will end up maxing iterations.  I see no problem with that and in fact I see it as *correct* behavior (given the documented execution context of the algorithm).  ",
            "How can it be correct to have an infinite loop?\nThe problem is not slow convergence, which you can overcome by allowing more iterations.\nIt is too low function value accuracy which you cannot overcome by allowing more iterations. Thus my point: We must raise the appropriate exception (the doc for which will state that it can happen if the function value accuracy is too low for the implementation to provide a result).\n",
            "[My comment starting with \"I understand what you say.\" was an answer to Luc. I hadn't read Phil's previous one which was posted while I was writing mine.]\n\nI agree that it is better not to change the standard algorithm, as I indicated in my first comment.\nThe fix which I'm proposing is not an algorithm change, it is an implementation detail similar to the many hundreds checks performed in CM. Just it is not a precondition test. It adequately indicates that something went wrong and can help the user figure out what it was. It makes the implementation more robust.\n",
            "The original implementation, for the \"problem instance being examined here\", would find the root with absolute accuracy lower than *10e-12* after 3560 evaluations (note: using the default value of *1e-6*).\nIn fact, the root was found, at the required accuracy, after around 2200 evaluations.\n\nThat does not sound like correct behavior.\nThe problem is that, \"x0\" never being updated, the convergence test always fails... until we reach the limitation of double precision, which entails an infinite loop.\n\nIn fact my fix should not be necessary, as things have gone awry before it would apply, but there is a bug to fix nonetheless.\n",
            "Is there actually a possibility of an infinite loop in the code?  Looks to me like the max evaluations bound will stop the while loop, so there is no potential for an infinite loop.  Apologies if I am misreading the code and the loop can fail to terminate, in which case I agree this is a problem.  (As a side note, from a style perspective, I prefer to explicitly bound loops to avoid this kind of uncertainty.  The natural hard bound here is the evaluation count.)\n\nTrying to detect when a sequence of iterates has gotten \"stuck\" and is destined to hit max iterations without converging is walking down a path that I think is unwise for us and users.  I see no reason not to stick with the standard impl here, which is nicely documented in the original submission.  Trying to workaround numerical problems in simple algorithms and change contracts to include these workarounds is asking for trouble - both for us and users.  In a simple case like this, it is much better to just stick with the documented algorithm, which should in this case (again unless I am missing something) end with max evaluations exceeded, which is the right exception to report. ",
            "I surely hope that your last post is not an answer to mine from 23:46.\n\nI'll try to answer here in case it was in reply to my previous one (23:06).\nOf course, the code will not run forever because of the \"maxeval\" bound.\nBut it will run for a time that depends on the value of \"maxeval\" *with no added benefit*! From a certain point, the loop is like\n{code}\nwhile (true) {\n  // Do nothing useful, just count!\n  ++count;\n  if (count > maxeval) {\n    throw new TooManyEvalutationsException(maxeval);\n  }\n}\n{code}\n\n{quote}\nfrom a style perspective, I prefer to explicitly bound loops\n{quote}\n\nFrom an *OO* style perspective, the reuse of the \"Incrementor\" is better, and you don't have to rewrite the same \"test and throw exception if failed\" boiler plate code each time there is such a loop.\n\n{quote}\nTrying to detect when a sequence of iterates has gotten \"stuck\" and is destined to hit max iterations without converging is walking down a path that I think is unwise for us and users.\n{quote}\n\nWhy?\n\n{quote}\nI see no reason not to stick with the standard impl here\n{quote}\n\nA busy idle loop is a compelling reason IMO.\n\n{quote}\nTrying to workaround numerical problems in simple algorithms and change contracts to include these workarounds is asking for trouble\n{quote}\n\nThe trouble is there with the current implementation. I'm not criticizing the contribution but this issue shows that it should be made more robust.\nAlso, the documentation about \"convergence is guaranteed\" can lead to a false sense of security.\nMoreover, is the \"regula falsi\" a mathematical algorithm (with a guaranteed converge property if computed with infinite precision) or a numerical one, which this issue proves that it cannot guarantee convergence? In the former case, CM's (numerical) implementation is not strictly \"regula falsi\" and there would be no such thing as respect for an original/standard implementation if we can make it more robust.\n\nI've already indicated that the fix does *not* change the contract; it stops the busy idle loop as soon as it is detected and reports that it won't do any good to increase the number of iterations. That's _obviously_ more robust.\n\nNow, if you were answering to my 23:46 post, I'd be glad to read an explanation of why the first paragraph describes expected behaviour.\n\n",
            "I don't understand.\n\nWhen it was created, the maxIteration threshold was exactly designed for this purpose: get out of infinite loops. It was later renamed maxEvaluation but the purpose is still the same: don't get stuck. The reason why we get stuck is irrelevant. This limit is simply a safety limit, not a tuning parameter that user are expected to raise once they hit it hoping they will converge later on. If they could raise it later, then they should set it to an appropriate value at once. Hitting it implies computation failed. Regula falsi just like any algorithm can fail if applied with the wrong parameters or to the wrong function (in fact, even with a good setting of function accuracy, it fails to converge if we require a bracket selection on the side that does not move).\n\nAlso detecting one bound is not updated is what Illinois and Pegasus are designed to do.\n\nSo I think we should completely get rid of regula falsi and only keep the better algorithms.",
            "{quote}\nI think we should completely get rid of regula falsi and only keep the better algorithms.\n{quote}\n\nThat was my first idea. And that would be the simplest one, the safest one, and the only viable one as I can't seem to state clearly enough that\n* Problem 1: When the doc says \"guaranteed convergence\", the algorithm should provide the answer.\n* Problem 2: When the (absolute) accuracy threshold is set to 1e-6, and the correct root *is* found (after 2200 iterations) within the requirements, it should be returned, instead running idle and finish with an exception\n\n{quote}\nThe reason why we get stuck is irrelevant.\n{quote}\n\nBut why? If we *can* be more precise on the cause of failure, why not do it?\n\n{quote}\nThis limit is simply a safety limit, not a tuning parameter that user are expected to raise once they hit it hoping they will converge later on.\n{quote}\n\nIn principle, some possible use would be to compare the efficiency of different methods where the main criterion would be a time limitation (assuming that the function evaluation time overwhelms the of the root solver algorithm time). Thus with the function that triggered this issue:\n* If you set maxeval to \"3000\", then both \"Pegasus\" (17 evals) and (a fixed) \"RegulaFalsi\" (2200 evals) would fill the bill.\n* If you set maxeval to \"1000\", then \"Pegasus\" will be the only winner.\n\n\nAnyways:\n+1 for removing it altogether, and include somewhere the reason for it not being implemented in CM.\n",
            "I am OK removing Reg Falsi, but stand by comments above that it is a very bad idea to hack standard algorithms and agree with Luc that maxing iterations is the correct behavior in the case we have been discussing. It is kind of pathetic that the compromise is to drop the impl; but in this case I don't see it as a real loss, since I can't think of any examples where Reg Falsi would be preferable to one of the other solvers - other than for educational purposes.",
            "May I please know *why it is OK that a bit of code does loop counting and repeatedly computes the same thing*!\n\nYou insist that I'd be \"hacking\" whereas I've indicated 3 or 4 times that there is no hack: just a test that will exit the loop as soon as it detects that the algorithm has failed. Why is it not understandable that the busy loop could last for a long time? The function is potentially evaluated millions of times at the same point. What if the evaluation is costly? Imagine the computation running for days, only to discover that it could have been be stopped after a few seconds. Is that robust code and good advertising for a library? It is one thing to expect that there are unknown bugs in CM, but refusing to fix a known one is so obviously wrong...\n\nAnd may I please know *why it is OK that an algorithm that finds the right result does not return it*.\n\nI had been trying to provide alternatives to the removal, but I can't do much more if nobody answers the above two questions.\nYou just have to run the code and print \"x\" and \"x1\" to see what is going on!\n",
            "{code}\nMay I please know *why it is OK that a bit of code does loop counting and repeatedly computes the same thing!*\n{code}\n\nWe didn't say that. We said that regula falsi is a standard *bad* algorithm. We said that very smart people have enhanced it 40 years ago and the enhanced versions are known and already implemented in Commons Math. These algorithms are *not* blind loop counters and they insert smart target shifts that *prevent* the behavior we observe here. These algorithms not only detect the problem, they fix it! They allow convergence along x. They allow selection of the side of the root.\n\n{code}\nThe function is potentially evaluated millions of times at the same point.\n{code}\n\nThe maxEvaluations is already here to prevent this, and in fact now this max number is even mandatory in the solve method (you placed it if I remember correctly). So the function is called millions of time only if the users wishes so by setting the maxEvaluations to a number in the range of millions.\n\n{code}\nAnd may I please know *why it is OK that an algorithm that finds the right result does not return it.*\n{code}\n\nIf the user asked for a convergence in x or for a convergence on y on the side that is stuck, then no, the algorithm did not find the right result. One of its bounds converged but the users asked for something else.\n\n{code}\nYou just have to run the code and print \"x\" and \"x1\" to see what is going on!\n{code}\n\nWe know exactly what is going on! We know the algorithm is stuck. We know why it is stuck. We know why it did not detect it is stuck. We know it will finally hit the safety maxEvaluation threshold that is just waiting for that. And we know that removing all these problems is done by using other algorithms which are already there.\n\nRegula falsi is doomed. It is an algorithm used for educational purposes, or for comparison purposes, not something suited for production use. It is just like Euler for ODE (and by the way we did implement Euler for ODE and we don't recommend users to use it as we also did implement better algorithms that were also designed by smart mathematicians decades ago).",
            "{quote}\nSo the function is called millions of time only if the users wishes so by setting the maxEvaluations to \na number in the range of millions.\n{quote}\n\nNo, the user should not expect that any algorithm will go on a single iteration more than necessary.\nThis is a plain bug.\n\nWhy do you see that a test such as I proposed (exit the loop early) is wrong while CM (and any good program) is full of tests to ensure that you don't do useless computations?\nThis has nothing to do with \"regula falsi\", it is robustness in the face of limited precision.\n\nHowever, if you insist that the bug (failing to detect that it is stuck) is really an integral part of the algorithm, then removing it is not a \"pathetic compromise\", it is the only right thing to do!\n\n",
            "This is a pointless discussion.  Gilles, you obviously don't share the views that Luc and I have on implementing standard algorithms or even what the meaning of a numerical algorithm is. Some algorithms perform well for some classes of problems and not others.  There is an art to choosing the right algorithm for the problem instance at hand.  If we modify our implementations to try to work around shortcomings of the algorithms we implement, then we are not implementing the standard algorithms, and we need to document exactly what it is that we are implementing, because in this case we are actually making it harder for users to choose (because we are not longer advertising standard numerics).  This is what I meant when I said it is both harder for us (because we have to document the hacks and non-standard contracts) and users (because the standard numerical analysis theory that they may be using to choose among implementations will no longer apply).  It is, IMO, a \"pathetic compromise\" to drop the implementation because we can't agree on what it means to implement the algorithm. So be it. Lets drop it and resolve this issue as \"fixed.\"",
            "{quote}\nGilles, you obviously don't share the views that Luc and I have on implementing standard algorithms \n{quote}\n\nThat's simply _not true_.\nI was the one pointing out that standard algorithms should have precedence: Please recall that it was considered fine that \"Levenberg-Marquardt\" and \"Brent\" would be, unknowingly to the user, \"twisted\" to perform _non-standard_ convergences check.\nIn those cases, there was the risk that the result of the algorithm would not be the same as the reference implementation.\n\nIn this case, there is no such thing as deviating from standard numerics! It was just a matter of throwing the right exception. So: \"The algorithm fails? Let's tell it sooner rather than later.\"\n\nVery interesting question that you ask: \"what it means to implement the algorithm\". But please note that I asked it several posts ago[1], and an answer would have helped sort out this discussion. What is your definition?\n\n\n[1] 08/Aug/11 07:24\n",
            "Also:\n\nPhil,\n\nCould you please leave out dismissive qualifiers such as \"pointless\" and \"pathetic\" (and, elsewhere, \"silly\") and stick to more or less objective arguments?\nThat will certainly help keep the conversation tone to a courteous level.\n\nLuc,\n\nThanks for stating in full details what you meant by \"convergence\" in this case. However, it is still a \"post-mortem\" description.\nDo you really expect that the average user of the CM library (a.o. me and the original reporter of the issue) to be able to figure out that \"obvious\" explanation just by getting a \"TooManyEvalutationsException\", setting the along-x accuracy threshold to a ridiculously high value and still getting the same exception?\nIf just for educational purposes, don't you think that it is more instructive to get a specific hint that the algorithm is stuck, rather than hit the ultimate fail-safe barrier much much later, and then download the source code and sprinkle the code with \"println\" statements to do forensic analysis?\n\nPhil,\n\nI tried to handle this issue out of respect for a real user who reported an issue that would have looked suspicious to many CM users. [How many of them would be experts in numerical analysis?]\nYou do not do me a favour by removing this algorithm; I don't want it to be a _compromise_ (pathetic or not). If you prefer to keep it, I don't care anymore. But, in that case, _you_ should have answered to Axel Kramer to go and read some books on numerical analysis.\n",
            "Gilles, I apologize for tone of comments.",
            "The discussions for this issue have left me with a lack of overview, so I'll (try to) objectively summerize the discussions above:\n\nThe problems are:\n # Regula Falsi states it always converges, but the implementation doesn't.\n # The main loop may continue, even when it no longer makes any progress, and subsequently ends with a TooManyEvaluationsException exception.\n\nThe cause of both problems is:\n - The limited/finite precision of the Java double type.\n\nProposed solutions:\n # The patch from revision 1154614, which modifies the Regula Falsi algorithm.\n   #- Consensus seems to be that this change, which modifies the algorithm, is undesireable. We should keep the original algorithm.\n # Detect that the algorithm no longer makes progress and throw an exception, instead of continuing the loop which no longer makes progress.\n   #- This is just earlier detection of the algorithm getting stuck.\n   #- We could throw the TooManyEvaluationsException exception, that continuing the loop would also get us.\n     #-- The class only states \"Exception to be thrown when the maximal number of evaluations is exceeded.\".\n     #-- The exception message only states: \"illegal state: maximal count (100) exceeded: evaluations\"\n     #-- Both may benefit from more extended documentation/messages.\n   #- We could also throw an other exception that more clearly states this issue (NoMoreProgressException, AlgorithmStuckException, ...?).\n     #-- It could for instance mention that changing the function value accuracy may be a solution, or asking for a different kind of solution?\n # Add documentation to the Regula Falsi algorithm that it is not intended to be used for actual problems, but only to compare algorithms, for testing, educational purposes, etc.\n # Add documentation to the Regula Falsi algorithm that users should use Illinois or Pegasus instead, which should outperform the algorithm for most if not all problems.\n # Add documentation to the Regula Falsi algorithm that it theoretically converges, but the implementation may not, due to the limited/finite precision of Java's double type. This will result in an exception (or 2 if we also do solution number 2).\n # Remove the Regula Falsi algorithm, and document why it is not included/implemented.\n   #- This seems to have been accepted as a last resort solution only.\n\nOther notes:\n - The following problem was also indicated: a solution is found after a certain number of iterations, but the algorithm does not return the solution (it does not terminate)\n   -- This should only happen if the user asked for a different solution. That is, there are several accuracy parameters, as well as an allowedSolution parameter.\n   -- If the solution requested by the user is found, it should return the solution immediately, otherwise it is a bug.\n\nNew notes:\n - I think the Regula Falsi algorithm does not state a fixed convergence criteria: it is left to the user to decide on one.\n   -- When I implemented the algorithm, I think I copied the convergence checks for Brent.\n   -- I subsequently modified the convergence criteria when I added the allowedSolution parameter.\n\nMy personal opinions on the proposed solutions:\n - (1) Revert part of 1154614, so get the original algorithm back. The other changes of that commit, that don't change the actual algorith, can stay.\n - (2) If we keep the algorithm, earlier detection would be nice. Not sure which exception to throw in these cases.\n   -- This would result in a single 'if' that detects that the new approximation is the same as the previous one, and we thus no longer make progress, in which case we throw the exception earlier, instead of later.\n - (3-5) If we keep the algorith, all 3 documentation extensions would be a good idea.\n - (6) If possible, keep the algorithm, and don't remove it.\n\nNew issue:\n - TooManyEvaluationsException currently seems to use LocalizedFormats.MAX_COUNT_EXCEEDED(\"maximal count ({0}) exceeded\"), but maybe should use LocalizedFormats.MAX_EVALUATIONS_EXCEEDED(\"maximal number of evaluations ({0}) exceeded\") instead?\n",
            "Thanks for the neat summary!\n\n{quote}\n* (1) Revert part of 1154614, so get the original algorithm back. The other changes of that commit, that don't change the actual algorith, can stay.\n{quote}\n\nDone in revision 1157185.\n\n{quote}\n* (2) If we keep the algorithm, earlier detection would be nice. Not sure which exception to throw in these cases.\n** This would result in a single 'if' that detects that the new approximation is the same as the previous one, and we thus no longer make progress, in which case we throw the exception earlier, instead of later.\n{quote}\n\n+1 (my position in the \"07/Aug/11 20:28\" post)\nAs suggested there, the exception could be \"MathIllegalStateException\" but with a clear message stating that the algorithm is stuck. Or maybe a new subclass of it which we could call \"NumericalPrecisionException\" or even a general-purpose \"ImplementationException\".\n\n{quote}\n[...] all 3 documentation extensions would be a good idea.\n{quote}\n\n+1\n\nAbout the \"new issue\", the message string:\n{quote}\n\"illegal state: maximal count (100) exceeded: evaluations\"\n{quote}\ncontains everything:\n# error type: illegal state\n# failure description: maximal count (100) exceeded\n# context: evaluations\n\nI proposed to use this approach (combining message items with the \"addMessage\" method of \"ExceptionContext\") in order to reduce the number of messages in the \"LocalizedFormats\" enum. Too many of them are just slight variations on a same theme.\n",
            "bq. contains everything\n\nI agree. I was just wondering why a message that seems to be exactly the same as the exception was not used, as it kind of looked like it was created just for this purpose...\n\nbq. I proposed to use this approach (combining message items with the \"addMessage\" method of \"ExceptionContext\") in order to reduce the number of messages in the \"LocalizedFormats\" enum. Too many of them are just slight variations on a same theme.\n\nAh, so then the MAX_EVALUATIONS_EXCEEDED is just a remnant of the past that should be eliminated, by replacing it everywhere by the more general MAX_COUNT_EXCEEDED?",
            "Yes. In the file \"LocalizedFormats.java\", I've started to write\n{noformat}\n/* keep */\n{noformat}\nafter each enum that is supposedly to be kept. All the others are still to be examined for redundancy with another one, or the possibility to create something close using the \"multi-item\" approach.\n",
            "The 'ticket631.patch' file is my attempt to resolve this issue with a solution (or maybe I should call it a compromise?) that is satisfactory for all people that participated in the discussions for this issue, without having to remove the Regula Falsi algorithm from Commons Math.\n\nI changed the following:\n - Added early detection of no longer making progress ('getting stuck'), and documented it.\n   -- I used ConvergenceException for this, as it seems to fit... Do we want a custom error message with it?\n - Extended RegulaFalsiSolver documentation to indicate:\n   -- that the algorithm should not be used for actual problems.\n   -- that Illinois and Pegasus are improved versions and should be prefered.\n   -- that the implementation does not guarantee convergence, while the algorithm theoretically does.\n - Extended IllinoisSolver and PegasusSolver documentation to indicate that they don't suffer from the RegulaFalsiSolver's implementation/convergence issues.\n\nPlease comment on whether this patch is an acceptable solution/compromise, and if not, why it is not.",
            "Committed (with minor additional Javadoc fixes) in revision 1164474.\n\nLeaving open until confirmation that {{ConvergenceException}} is the right one to use. I thought that we could make a difference between _theoretical_ and _implementation_ convergence failures. But it might not be worth introducing the distinction just for this one case, especially since it is quite clear clear now that the class should not be used.",
            "No objection raised; setting to \"Resolved\"."
        ],
        "summarized_discussion": ""
    },
    "Mockito_18_src/org/mockito/internal/stubbing/defaultanswers/ReturnsEmptyValues.java_82_118": {
        "src": "Object returnValueFor(Class<?> type) {\n        if (Primitives.isPrimitiveOrWrapper(type)) {\n            return Primitives.defaultValueForPrimitiveOrWrapper(type);\n            //new instances are used instead of Collections.emptyList(), etc.\n            //to avoid UnsupportedOperationException if code under test modifies returned collection\n        } else if (type == Collection.class) {\n            return new LinkedList<Object>();\n        } else if (type == Set.class) {\n            return new HashSet<Object>();\n        } else if (type == HashSet.class) {\n            return new HashSet<Object>();\n        } else if (type == SortedSet.class) {\n            return new TreeSet<Object>();\n        } else if (type == TreeSet.class) {\n            return new TreeSet<Object>();\n        } else if (type == LinkedHashSet.class) {\n            return new LinkedHashSet<Object>();\n        } else if (type == List.class) {\n            return new LinkedList<Object>();\n        } else if (type == LinkedList.class) {\n            return new LinkedList<Object>();\n        } else if (type == ArrayList.class) {\n            return new ArrayList<Object>();\n        } else if (type == Map.class) {\n            return new HashMap<Object, Object>();\n        } else if (type == HashMap.class) {\n            return new HashMap<Object, Object>();\n        } else if (type == SortedMap.class) {\n            return new TreeMap<Object, Object>();\n        } else if (type == TreeMap.class) {\n            return new TreeMap<Object, Object>();\n        } else if (type == LinkedHashMap.class) {\n            return new LinkedHashMap<Object, Object>();\n        }\n        //Let's not care about the rest of collections.\n        return null;\n    }",
        "src_wo_comments": "Object returnValueFor ( Class < ? > type ) { if ( Primitives . isPrimitiveOrWrapper ( type ) ) { return Primitives . defaultValueForPrimitiveOrWrapper ( type ) ; } else if ( type == Collection . class ) { return new LinkedList < Object > ( ) ; } else if ( type == Set . class ) { return new HashSet < Object > ( ) ; } else if ( type == HashSet . class ) { return new HashSet < Object > ( ) ; } else if ( type == SortedSet . class ) { return new TreeSet < Object > ( ) ; } else if ( type == TreeSet . class ) { return new TreeSet < Object > ( ) ; } else if ( type == LinkedHashSet . class ) { return new LinkedHashSet < Object > ( ) ; } else if ( type == List . class ) { return new LinkedList < Object > ( ) ; } else if ( type == LinkedList . class ) { return new LinkedList < Object > ( ) ; } else if ( type == ArrayList . class ) { return new ArrayList < Object > ( ) ; } else if ( type == Map . class ) { return new HashMap < Object , Object > ( ) ; } else if ( type == HashMap . class ) { return new HashMap < Object , Object > ( ) ; } else if ( type == SortedMap . class ) { return new TreeMap < Object , Object > ( ) ; } else if ( type == TreeMap . class ) { return new TreeMap < Object , Object > ( ) ; } else if ( type == LinkedHashMap . class ) { return new LinkedHashMap < Object , Object > ( ) ; } return null ; }",
        "fixed_src": "Object returnValueFor(Class<?> type) {\n        if (Primitives.isPrimitiveOrWrapper(type)) {\n            return Primitives.defaultValueForPrimitiveOrWrapper(type);\n            //new instances are used instead of Collections.emptyList(), etc.\n            //to avoid UnsupportedOperationException if code under test modifies returned collection\n        } else if (type == Iterable.class) {\n            return new ArrayList<Object>(0);\n        } else if (type == Collection.class) {\n            return new LinkedList<Object>();\n        } else if (type == Set.class) {\n            return new HashSet<Object>();\n        } else if (type == HashSet.class) {\n            return new HashSet<Object>();\n        } else if (type == SortedSet.class) {\n            return new TreeSet<Object>();\n        } else if (type == TreeSet.class) {\n            return new TreeSet<Object>();\n        } else if (type == LinkedHashSet.class) {\n            return new LinkedHashSet<Object>();\n        } else if (type == List.class) {\n            return new LinkedList<Object>();\n        } else if (type == LinkedList.class) {\n            return new LinkedList<Object>();\n        } else if (type == ArrayList.class) {\n            return new ArrayList<Object>();\n        } else if (type == Map.class) {\n            return new HashMap<Object, Object>();\n        } else if (type == HashMap.class) {\n            return new HashMap<Object, Object>();\n        } else if (type == SortedMap.class) {\n            return new TreeMap<Object, Object>();\n        } else if (type == TreeMap.class) {\n            return new TreeMap<Object, Object>();\n        } else if (type == LinkedHashMap.class) {\n            return new LinkedHashMap<Object, Object>();\n        }\n        //Let's not care about the rest of collections.\n        return null;\n    }",
        "fixed_src_wo_comments": "Object returnValueFor ( Class < ? > type ) { if ( Primitives . isPrimitiveOrWrapper ( type ) ) { return Primitives . defaultValueForPrimitiveOrWrapper ( type ) ; } else if ( type == Iterable . class ) { return new ArrayList < Object > ( 0 ) ; } else if ( type == Collection . class ) { return new LinkedList < Object > ( ) ; } else if ( type == Set . class ) { return new HashSet < Object > ( ) ; } else if ( type == HashSet . class ) { return new HashSet < Object > ( ) ; } else if ( type == SortedSet . class ) { return new TreeSet < Object > ( ) ; } else if ( type == TreeSet . class ) { return new TreeSet < Object > ( ) ; } else if ( type == LinkedHashSet . class ) { return new LinkedHashSet < Object > ( ) ; } else if ( type == List . class ) { return new LinkedList < Object > ( ) ; } else if ( type == LinkedList . class ) { return new LinkedList < Object > ( ) ; } else if ( type == ArrayList . class ) { return new ArrayList < Object > ( ) ; } else if ( type == Map . class ) { return new HashMap < Object , Object > ( ) ; } else if ( type == HashMap . class ) { return new HashMap < Object , Object > ( ) ; } else if ( type == SortedMap . class ) { return new TreeMap < Object , Object > ( ) ; } else if ( type == TreeMap . class ) { return new TreeMap < Object , Object > ( ) ; } else if ( type == LinkedHashMap . class ) { return new LinkedHashMap < Object , Object > ( ) ; } return null ; }",
        "summary": "Return empty value for Iterables",
        "Description": "http://code.google.com/p/mockito/issues/detail?id=175\n\nI expect an Iterable to be mocked by default with an empty Iterable. I understand from the initial issue this behavior would be introduced in Mockito 2, but beta-8 still returns null.\n\nCould we return null for Iterables ?\n\nShould we have the same behavior for Iterator ?\n\nThanks\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Yes it should, thanks for the reminder :)\n"
            },
            {
                "content": "Fixed by 8ceb04a\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been fixed by 8ceb04a."
    },
    "JxPath_21_src/java/org/apache/commons/jxpath/ri/model/beans/PropertyPointer.java_151_153": {
        "src": "public int getLength() {\n        return ValueUtils.getLength(getBaseValue());\n    }",
        "src_wo_comments": "public int getLength ( ) { return ValueUtils . getLength ( getBaseValue ( ) ) ; }",
        "fixed_src": "public int getLength() {\n        Object baseValue = getBaseValue();\n        return baseValue == null ? 1 : ValueUtils.getLength(baseValue);\n    }",
        "fixed_src_wo_comments": "public int getLength ( ) { Object baseValue = getBaseValue ( ) ; return baseValue == null ? 1 : ValueUtils . getLength ( baseValue ) ; }",
        "summary": "null handling is inconsistent",
        "Description": "Comparing a vaule to null using unequals (\\!=) yields false!\n{noformat}\n        Map<String, Integer> m = new HashMap<String, Integer>();\n        m.put(\"a\", 1);\n        m.put(\"b\", null);\n        m.put(\"c\", 1);\n        JXPathContext c = JXPathContext.newContext(m);\n        System.out.println(c.getValue(\"a != b\") + \" should be true\");\n        System.out.println(c.getValue(\"a != c\") + \" should be false\");\n        System.out.println(c.getValue(\"a = b\") + \" should be false\");\n        System.out.println(c.getValue(\"a = c\") + \" should be true\");\n        System.out.println(c.getValue(\"not(a = b)\") + \" should be true\");\n        System.out.println(c.getValue(\"not(a = c)\") + \" should be false\");\n{noformat} \n\nOutput using 1.3:\n{color:red} false should be true{color}\nfalse should be false\nfalse should be false\ntrue should be true\ntrue should be true\nfalse should be false\n\n\nIn 1.2 it works correctly!",
        "issue_url": "https://issues.apache.org/jira//browse/JXPATH-151",
        "comments": [
            "Typically JXPath isn't expected to work with a {{Map}}/{{Object[]}}/{{Collection}} as its root context object, although I note your comment that this works in 1.2.  Can you try adding your map to a simple object wrapper (e.g. {{org.apache.commons.lang.mutable.MutableObject}}) and report back the results of {{\"value/a != value/b\"}}, and {{\"value\\[@name='a'\\] != value\\[@name='b'\\]\"}} ?",
            "Setting up a similar map to what you have described, and accessing via a bean property I still found the errors you describe.  This issue was fairly difficult to triage, but my final diagnosis is that it is indicative of a bug.  It is interesting to note that the {{value\\[@name='a'\\] != value\\[@name='b\\]}} approach succeeded, and of course the two approaches _should_ yield equivalent results for any key conforming to {{QName}} restrictions.  I would still consider it dangerous to depend on {{null}} values in JXPath, however.  For example, if you add a mapping of {{\"d\":0}} to your map, you will find that {{value\\[@name='d'\\] = value\\[@name='b'\\]}} because the fact that {{d}} refers to a numeric type forces the conversion of {{b}}'s {{null}} value to a number, {{0.0}}.  XPath is tricky this way, and JXPath, dealing with types unknown to the XPath specification, only becomes trickier.\n\nHaving said all that, and to return to the issue at hand, I found that certain existing JXPath tests assert that it should be possible to get a {{null}} value from the expression {{bean.nullProperty}}, but that iterating pointers from the expression {{bean.nullProperty\\[1\\]}} should yield no results.  But this assigns a Java-centric meaning to XPath's {{\\[1\\]}} test, and it is my judgment that this is overstepping given that Javadoc for relevant methods states that non-Collection items should be treated as having length 1.  Making the conscious decision to _change_ an existing unit test is not a decision to make lightly, but in this case my opinion is that it is warranted.",
            "Committed revision 1133499."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is that the existing JXPath tests should be changed to assert that it should be possible to get a null value from the expression bean.nullProperty, but that iterating pointers from the expression bean.nullProperty[1] should yield no results. This change was committed in revision 1133499."
    },
    "JacksonDatabind_62_src/main/java/com/fasterxml/jackson/databind/deser/std/CollectionDeserializer.java_169_208": {
        "src": "@Override\n    public CollectionDeserializer createContextual(DeserializationContext ctxt,\n            BeanProperty property) throws JsonMappingException\n    {\n        // May need to resolve types for delegate-based creators:\n        JsonDeserializer<Object> delegateDeser = null;\n        if (_valueInstantiator != null) {\n            if (_valueInstantiator.canCreateUsingDelegate()) {\n                JavaType delegateType = _valueInstantiator.getDelegateType(ctxt.getConfig());\n                if (delegateType == null) {\n                    throw new IllegalArgumentException(\"Invalid delegate-creator definition for \"+_collectionType\n                            +\": value instantiator (\"+_valueInstantiator.getClass().getName()\n                            +\") returned true for 'canCreateUsingDelegate()', but null for 'getDelegateType()'\");\n                }\n                delegateDeser = findDeserializer(ctxt, delegateType, property);\n            }\n        }\n        // [databind#1043]: allow per-property allow-wrapping of single overrides:\n        // 11-Dec-2015, tatu: Should we pass basic `Collection.class`, or more refined? Mostly\n        //   comes down to \"List vs Collection\" I suppose... for now, pass Collection\n        Boolean unwrapSingle = findFormatFeature(ctxt, property, Collection.class,\n                JsonFormat.Feature.ACCEPT_SINGLE_VALUE_AS_ARRAY);\n        // also, often value deserializer is resolved here:\n        JsonDeserializer<?> valueDeser = _valueDeserializer;\n        \n        // May have a content converter\n        valueDeser = findConvertingContentDeserializer(ctxt, property, valueDeser);\n        final JavaType vt = _collectionType.getContentType();\n        if (valueDeser == null) {\n            valueDeser = ctxt.findContextualValueDeserializer(vt, property);\n        } else { // if directly assigned, probably not yet contextual, so:\n            valueDeser = ctxt.handleSecondaryContextualization(valueDeser, property, vt);\n        }\n        // and finally, type deserializer needs context as well\n        TypeDeserializer valueTypeDeser = _valueTypeDeserializer;\n        if (valueTypeDeser != null) {\n            valueTypeDeser = valueTypeDeser.forProperty(property);\n        }\n        return withResolved(delegateDeser, valueDeser, valueTypeDeser, unwrapSingle);\n    }",
        "src_wo_comments": "@ Override public CollectionDeserializer createContextual ( DeserializationContext ctxt , BeanProperty property ) throws JsonMappingException { JsonDeserializer < Object > delegateDeser = null ; if ( _valueInstantiator != null ) { if ( _valueInstantiator . canCreateUsingDelegate ( ) ) { JavaType delegateType = _valueInstantiator . getDelegateType ( ctxt . getConfig ( ) ) ; if ( delegateType == null ) { throw new IllegalArgumentException ( \"Invalid delegate-creator definition for \" + _collectionType + \": value instantiator (\" + _valueInstantiator . getClass ( ) . getName ( ) + \") returned true for 'canCreateUsingDelegate()', but null for 'getDelegateType()'\" ) ; } delegateDeser = findDeserializer ( ctxt , delegateType , property ) ; } } Boolean unwrapSingle = findFormatFeature ( ctxt , property , Collection . class , JsonFormat . Feature . ACCEPT_SINGLE_VALUE_AS_ARRAY ) ; JsonDeserializer < ? > valueDeser = _valueDeserializer ; valueDeser = findConvertingContentDeserializer ( ctxt , property , valueDeser ) ; final JavaType vt = _collectionType . getContentType ( ) ; if ( valueDeser == null ) { valueDeser = ctxt . findContextualValueDeserializer ( vt , property ) ; } else { valueDeser = ctxt . handleSecondaryContextualization ( valueDeser , property , vt ) ; } TypeDeserializer valueTypeDeser = _valueTypeDeserializer ; if ( valueTypeDeser != null ) { valueTypeDeser = valueTypeDeser . forProperty ( property ) ; } return withResolved ( delegateDeser , valueDeser , valueTypeDeser , unwrapSingle ) ; }",
        "fixed_src": "@Override\n    public CollectionDeserializer createContextual(DeserializationContext ctxt,\n            BeanProperty property) throws JsonMappingException\n    {\n        // May need to resolve types for delegate-based creators:\n        JsonDeserializer<Object> delegateDeser = null;\n        if (_valueInstantiator != null) {\n            if (_valueInstantiator.canCreateUsingDelegate()) {\n                JavaType delegateType = _valueInstantiator.getDelegateType(ctxt.getConfig());\n                if (delegateType == null) {\n                    throw new IllegalArgumentException(\"Invalid delegate-creator definition for \"+_collectionType\n                            +\": value instantiator (\"+_valueInstantiator.getClass().getName()\n                            +\") returned true for 'canCreateUsingDelegate()', but null for 'getDelegateType()'\");\n                }\n                delegateDeser = findDeserializer(ctxt, delegateType, property);\n            } else if (_valueInstantiator.canCreateUsingArrayDelegate()) {\n                JavaType delegateType = _valueInstantiator.getArrayDelegateType(ctxt.getConfig());\n                if (delegateType == null) {\n                    throw new IllegalArgumentException(\"Invalid array-delegate-creator definition for \"+_collectionType\n                            +\": value instantiator (\"+_valueInstantiator.getClass().getName()\n                            +\") returned true for 'canCreateUsingArrayDelegate()', but null for 'getArrayDelegateType()'\");\n                }\n                delegateDeser = findDeserializer(ctxt, delegateType, property);\n            }\n        }\n        // [databind#1043]: allow per-property allow-wrapping of single overrides:\n        // 11-Dec-2015, tatu: Should we pass basic `Collection.class`, or more refined? Mostly\n        //   comes down to \"List vs Collection\" I suppose... for now, pass Collection\n        Boolean unwrapSingle = findFormatFeature(ctxt, property, Collection.class,\n                JsonFormat.Feature.ACCEPT_SINGLE_VALUE_AS_ARRAY);\n        // also, often value deserializer is resolved here:\n        JsonDeserializer<?> valueDeser = _valueDeserializer;\n        \n        // May have a content converter\n        valueDeser = findConvertingContentDeserializer(ctxt, property, valueDeser);\n        final JavaType vt = _collectionType.getContentType();\n        if (valueDeser == null) {\n            valueDeser = ctxt.findContextualValueDeserializer(vt, property);\n        } else { // if directly assigned, probably not yet contextual, so:\n            valueDeser = ctxt.handleSecondaryContextualization(valueDeser, property, vt);\n        }\n        // and finally, type deserializer needs context as well\n        TypeDeserializer valueTypeDeser = _valueTypeDeserializer;\n        if (valueTypeDeser != null) {\n            valueTypeDeser = valueTypeDeser.forProperty(property);\n        }\n        return withResolved(delegateDeser, valueDeser, valueTypeDeser, unwrapSingle);\n    }",
        "fixed_src_wo_comments": "@ Override public CollectionDeserializer createContextual ( DeserializationContext ctxt , BeanProperty property ) throws JsonMappingException { JsonDeserializer < Object > delegateDeser = null ; if ( _valueInstantiator != null ) { if ( _valueInstantiator . canCreateUsingDelegate ( ) ) { JavaType delegateType = _valueInstantiator . getDelegateType ( ctxt . getConfig ( ) ) ; if ( delegateType == null ) { throw new IllegalArgumentException ( \"Invalid delegate-creator definition for \" + _collectionType + \": value instantiator (\" + _valueInstantiator . getClass ( ) . getName ( ) + \") returned true for 'canCreateUsingDelegate()', but null for 'getDelegateType()'\" ) ; } delegateDeser = findDeserializer ( ctxt , delegateType , property ) ; } else if ( _valueInstantiator . canCreateUsingArrayDelegate ( ) ) { JavaType delegateType = _valueInstantiator . getArrayDelegateType ( ctxt . getConfig ( ) ) ; if ( delegateType == null ) { throw new IllegalArgumentException ( \"Invalid array-delegate-creator definition for \" + _collectionType + \": value instantiator (\" + _valueInstantiator . getClass ( ) . getName ( ) + \") returned true for 'canCreateUsingArrayDelegate()', but null for 'getArrayDelegateType()'\" ) ; } delegateDeser = findDeserializer ( ctxt , delegateType , property ) ; } } Boolean unwrapSingle = findFormatFeature ( ctxt , property , Collection . class , JsonFormat . Feature . ACCEPT_SINGLE_VALUE_AS_ARRAY ) ; JsonDeserializer < ? > valueDeser = _valueDeserializer ; valueDeser = findConvertingContentDeserializer ( ctxt , property , valueDeser ) ; final JavaType vt = _collectionType . getContentType ( ) ; if ( valueDeser == null ) { valueDeser = ctxt . findContextualValueDeserializer ( vt , property ) ; } else { valueDeser = ctxt . handleSecondaryContextualization ( valueDeser , property , vt ) ; } TypeDeserializer valueTypeDeser = _valueTypeDeserializer ; if ( valueTypeDeser != null ) { valueTypeDeser = valueTypeDeser . forProperty ( property ) ; } return withResolved ( delegateDeser , valueDeser , valueTypeDeser , unwrapSingle ) ; }",
        "summary": "Custom UnmodifiableSetMixin Fails in Jackson 2.7+ but works in Jackson 2.6",
        "Description": "I'd like to be able to deserialize an `UnmodifiableSet` with default typing enabled. To do this I have created an `UnmodifiableSetMixin` as shown below:\n\n**NOTE**: You can find a minimal project with all the source code to reproduce this issue at https://github.com/rwinch/jackson-unmodifiableset-mixin\n\n``` java\nimport com.fasterxml.jackson.annotation.JsonCreator;\nimport com.fasterxml.jackson.annotation.JsonTypeInfo;\n\nimport java.util.Set;\n\n@JsonTypeInfo(use = JsonTypeInfo.Id.CLASS, include = JsonTypeInfo.As.PROPERTY)\npublic abstract class UnmodifiableSetMixin {\n\n    @JsonCreator\n    public UnmodifiableSetMixin(Set<?> s) {}\n}\n```\n\nI then try to use this to deserialize an empty set.\n\n``` java\npublic class UnmodifiableSetMixinTest {\n    static final String EXPECTED_JSON = \"[\\\"java.util.Collections$UnmodifiableSet\\\",[]]\";\n\n    ObjectMapper mapper;\n\n    @Before\n    public void setup() {\n        mapper = new ObjectMapper();\n        mapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL, JsonTypeInfo.As.PROPERTY);\n        mapper.addMixIn(Collections.unmodifiableSet(Collections.<String>emptySet()).getClass(), UnmodifiableSetMixin.class);\n    }    \n    @Test\n    @SuppressWarnings(\"unchecked\")\n    public void read() throws Exception {\n        Set<String> foo = mapper.readValue(EXPECTED_JSON, Set.class);\n        assertThat(foo).isEmpty();\n    }\n}\n```\n\nThe test passes with Jackson 2.6, but fails using Jackson 2.7+ (including Jackson 2.8.3) with the following stack trace:\n\n```\njava.lang.IllegalStateException: No default constructor for [collection type; class java.util.Collections$UnmodifiableSet, contains [simple type, class java.lang.Object]]\n    at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createUsingDefault(StdValueInstantiator.java:240)\n    at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:249)\n    at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:26)\n    at com.fasterxml.jackson.databind.jsontype.impl.AsArrayTypeDeserializer._deserialize(AsArrayTypeDeserializer.java:110)\n    at com.fasterxml.jackson.databind.jsontype.impl.AsArrayTypeDeserializer.deserializeTypedFromArray(AsArrayTypeDeserializer.java:50)\n    at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserializeWithType(CollectionDeserializer.java:310)\n    at com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:42)\n    at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3788)\n    at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2779)\n    at sample.UnmodifiableSetMixinTest.read(UnmodifiableSetMixinTest.java:36)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)\n    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)\n    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)\n```\n\nThis seems like a passivity issue. Is there a workaround for this problem?\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Interesting. So, true, no zero-arg constructor, but that's not what mix-in tries to do.\nI can reproduce this, hope to see what causes the problem as signature seems to match.\n"
            },
            {
                "content": "Looking through 2.7.0 notes, guessing this might be due to one of:\n- #936 \n- #1010 (most likely culprit)\n"
            },
            {
                "content": "Ok. Almost certain this is a regression due to #1010, which lead to separation of \"delegating array creators\" from other kinds. Looks like `BeanDeserializerBase` handles it, but `CollectionDeserializer` was not retrofitted (nor, very likely, other non-pojo deseiralizers).\n"
            },
            {
                "content": "Sigh. Nasty one to solve for a patch... but I think I found a way. Need to figure out clean it up for 2.8, or, at very least master (2.9).\n"
            },
            {
                "content": "Thanks for the fast turn around!\n\nOn Oct 4, 2016 6:37 PM, \"Tatu Saloranta\" notifications@github.com wrote:\n\n> Closed #1392 https://github.com/FasterXML/jackson-databind/issues/1392\n> via 4e94c0e\n> https://github.com/FasterXML/jackson-databind/commit/4e94c0ed9eca0caccb57feb0ceb252fc91198032\n> .\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/FasterXML/jackson-databind/issues/1392#event-812568242,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAWIB5k-RNUNEZYbwga-S0jtgvlspk-nks5qwuMxgaJpZM4KLIj_\n> .\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was caused by #1010, which lead to separation of \"delegating array creators\" from other kinds. The solution was to retrofit `BeanDeserializerBase` to handle it, as well as other non-pojo deserializers, and clean it up for 2.8 or master (2.9)."
    },
    "JacksonDatabind_74_src/main/java/com/fasterxml/jackson/databind/jsontype/impl/AsPropertyTypeDeserializer.java_133_160": {
        "src": "@SuppressWarnings(\"resource\")\n    protected Object _deserializeTypedUsingDefaultImpl(JsonParser p, DeserializationContext ctxt,\n            TokenBuffer tb) throws IOException\n    {\n        // As per [JACKSON-614], may have default implementation to use\n        JsonDeserializer<Object> deser = _findDefaultImplDeserializer(ctxt);\n        if (deser != null) {\n            if (tb != null) {\n                tb.writeEndObject();\n                p = tb.asParser(p);\n                // must move to point to the first token:\n                p.nextToken();\n            }\n            return deser.deserialize(p, ctxt);\n        }\n        // or, perhaps we just bumped into a \"natural\" value (boolean/int/double/String)?\n        Object result = TypeDeserializer.deserializeIfNatural(p, ctxt, _baseType);\n        if (result != null) {\n            return result;\n        }\n        // or, something for which \"as-property\" won't work, changed into \"wrapper-array\" type:\n        if (p.getCurrentToken() == JsonToken.START_ARRAY) {\n            return super.deserializeTypedFromAny(p, ctxt);\n        }\n        ctxt.reportWrongTokenException(p, JsonToken.FIELD_NAME,\n                \"missing property '\"+_typePropertyName+\"' that is to contain type id  (for class \"+baseTypeName()+\")\");\n        return null;\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"resource\" ) protected Object _deserializeTypedUsingDefaultImpl ( JsonParser p , DeserializationContext ctxt , TokenBuffer tb ) throws IOException { JsonDeserializer < Object > deser = _findDefaultImplDeserializer ( ctxt ) ; if ( deser != null ) { if ( tb != null ) { tb . writeEndObject ( ) ; p = tb . asParser ( p ) ; p . nextToken ( ) ; } return deser . deserialize ( p , ctxt ) ; } Object result = TypeDeserializer . deserializeIfNatural ( p , ctxt , _baseType ) ; if ( result != null ) { return result ; } if ( p . getCurrentToken ( ) == JsonToken . START_ARRAY ) { return super . deserializeTypedFromAny ( p , ctxt ) ; } ctxt . reportWrongTokenException ( p , JsonToken . FIELD_NAME , \"missing property '\" + _typePropertyName + \"' that is to contain type id  (for class \" + baseTypeName ( ) + \")\" ) ; return null ; }",
        "fixed_src": "@SuppressWarnings(\"resource\")\n    protected Object _deserializeTypedUsingDefaultImpl(JsonParser p, DeserializationContext ctxt,\n            TokenBuffer tb) throws IOException\n    {\n        // As per [JACKSON-614], may have default implementation to use\n        JsonDeserializer<Object> deser = _findDefaultImplDeserializer(ctxt);\n        if (deser != null) {\n            if (tb != null) {\n                tb.writeEndObject();\n                p = tb.asParser(p);\n                // must move to point to the first token:\n                p.nextToken();\n            }\n            return deser.deserialize(p, ctxt);\n        }\n        // or, perhaps we just bumped into a \"natural\" value (boolean/int/double/String)?\n        Object result = TypeDeserializer.deserializeIfNatural(p, ctxt, _baseType);\n        if (result != null) {\n            return result;\n        }\n        // or, something for which \"as-property\" won't work, changed into \"wrapper-array\" type:\n        if (p.getCurrentToken() == JsonToken.START_ARRAY) {\n            return super.deserializeTypedFromAny(p, ctxt);\n        } else if (p.getCurrentToken() == JsonToken.VALUE_STRING) {\n            if (ctxt.isEnabled(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT)) {\n                String str = p.getText().trim();\n                if (str.isEmpty()) {\n                    return null;\n                }\n            }\n        }\n        ctxt.reportWrongTokenException(p, JsonToken.FIELD_NAME,\n                \"missing property '\"+_typePropertyName+\"' that is to contain type id  (for class \"+baseTypeName()+\")\");\n        return null;\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"resource\" ) protected Object _deserializeTypedUsingDefaultImpl ( JsonParser p , DeserializationContext ctxt , TokenBuffer tb ) throws IOException { JsonDeserializer < Object > deser = _findDefaultImplDeserializer ( ctxt ) ; if ( deser != null ) { if ( tb != null ) { tb . writeEndObject ( ) ; p = tb . asParser ( p ) ; p . nextToken ( ) ; } return deser . deserialize ( p , ctxt ) ; } Object result = TypeDeserializer . deserializeIfNatural ( p , ctxt , _baseType ) ; if ( result != null ) { return result ; } if ( p . getCurrentToken ( ) == JsonToken . START_ARRAY ) { return super . deserializeTypedFromAny ( p , ctxt ) ; } else if ( p . getCurrentToken ( ) == JsonToken . VALUE_STRING ) { if ( ctxt . isEnabled ( DeserializationFeature . ACCEPT_EMPTY_STRING_AS_NULL_OBJECT ) ) { String str = p . getText ( ) . trim ( ) ; if ( str . isEmpty ( ) ) { return null ; } } } ctxt . reportWrongTokenException ( p , JsonToken . FIELD_NAME , \"missing property '\" + _typePropertyName + \"' that is to contain type id  (for class \" + baseTypeName ( ) + \")\" ) ; return null ; }",
        "summary": "AsPropertyTypeDeserializer ignores DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT",
        "Description": "The `AsPropertyTypeDeserializer ` implementation does not respect the `DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT` feature. When deserializing an empty String it throws `DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT` instead of creating a null Object.",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug is related to a function that is not returning the expected result. The solution to the bug is to debug the code and identify the issue causing the incorrect result. Once the issue is identified, the code can be modified to fix the bug and return the expected result."
    },
    "Math_11_src/main/java/org/apache/commons/math3/distribution/MultivariateNormalDistribution.java_177_186": {
        "src": "public double density(final double[] vals) throws DimensionMismatchException {\n        final int dim = getDimension();\n        if (vals.length != dim) {\n            throw new DimensionMismatchException(vals.length, dim);\n        }\n\n        return FastMath.pow(2 * FastMath.PI, -dim / 2) *\n            FastMath.pow(covarianceMatrixDeterminant, -0.5) *\n            getExponentTerm(vals);\n    }",
        "src_wo_comments": "public double density ( final double [ ] vals ) throws DimensionMismatchException { final int dim = getDimension ( ) ; if ( vals . length != dim ) { throw new DimensionMismatchException ( vals . length , dim ) ; } return FastMath . pow ( 2 * FastMath . PI , - dim / 2 ) * FastMath . pow ( covarianceMatrixDeterminant , - 0.5 ) * getExponentTerm ( vals ) ; }",
        "fixed_src": "public double density(final double[] vals) throws DimensionMismatchException {\n        final int dim = getDimension();\n        if (vals.length != dim) {\n            throw new DimensionMismatchException(vals.length, dim);\n        }\n\n        return FastMath.pow(2 * FastMath.PI, -0.5 * dim) *\n            FastMath.pow(covarianceMatrixDeterminant, -0.5) *\n            getExponentTerm(vals);\n    }",
        "fixed_src_wo_comments": "public double density ( final double [ ] vals ) throws DimensionMismatchException { final int dim = getDimension ( ) ; if ( vals . length != dim ) { throw new DimensionMismatchException ( vals . length , dim ) ; } return FastMath . pow ( 2 * FastMath . PI , - 0.5 * dim ) * FastMath . pow ( covarianceMatrixDeterminant , - 0.5 ) * getExponentTerm ( vals ) ; }",
        "summary": "MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd",
        "Description": "To reproduce:\n{code}\nAssert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);\n{code}",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-929",
        "comments": [
            "Corrected in revision 1433367.\n\nThanks a lot for the report and fix!\n\nA few weeks, Thomas wondered why using \"0.5 * x\" instead of \"x / 2\". That's an excellent reason... Unfortunately that instance slipped through my scanner :(.",
            "Closing issue as version 3.2 has been released on 2013-04-06."
        ],
        "summarized_discussion": "\n\nThe bug was corrected in revision 1433367, and the issue was closed as version 3.2 was released on 2013-04-06."
    },
    "Cli_10_src/java/org/apache/commons/cli/Parser.java_44_47": {
        "src": "protected void setOptions(final Options options) {\n        this.options = options;\n        this.requiredOptions = options.getRequiredOptions();\n    }",
        "src_wo_comments": "protected void setOptions ( final Options options ) { this . options = options ; this . requiredOptions = options . getRequiredOptions ( ) ; }",
        "fixed_src": "protected void setOptions(final Options options) {\n        this.options = options;\n        this.requiredOptions = new ArrayList(options.getRequiredOptions());\n    }",
        "fixed_src_wo_comments": "protected void setOptions ( final Options options ) { this . options = options ; this . requiredOptions = new ArrayList ( options . getRequiredOptions ( ) ) ; }",
        "summary": "Missing required options not throwing MissingOptionException",
        "Description": "When an Options object is used to parse a second set of command arguments it won't throw a MissingOptionException.\n\n{code:java}\nimport org.apache.commons.cli.CommandLine;\nimport org.apache.commons.cli.GnuParser;\nimport org.apache.commons.cli.OptionBuilder;\nimport org.apache.commons.cli.Options;\nimport org.apache.commons.cli.ParseException;\n\npublic class Example\n{\n\tpublic static void main(String[] args) throws ParseException\n\t{\n\t\tbrokenExample();\n\t\tworkingExample();\n\t}\n\n\t// throws exception as expected\n\tprivate static void workingExample() throws ParseException\n\t{\n\t\tString[] args = {};\n\n\t\tOptions opts = new Options();\n\t\topts.addOption(OptionBuilder.isRequired().create('v'));\n\n\t\tGnuParser parser = new GnuParser();\n\t\tCommandLine secondCL = parser.parse(opts, args);\n\n\t\tSystem.out.println(\"Done workingExample\");\n\t}\n\n\t// fails to throw exception on second invocation of parse\n\tprivate static void brokenExample() throws ParseException\n\t{\n\t\tString[] firstArgs = { \"-v\" };\n\t\tString[] secondArgs = {};\n\n\t\tOptions opts = new Options();\n\t\topts.addOption(OptionBuilder.isRequired().create('v'));\n\n\t\tGnuParser parser = new GnuParser();\n\t\tCommandLine firstCL = parser.parse(opts, firstArgs);\n\t\tCommandLine secondCL = parser.parse(opts, secondArgs);\n\n\t\tSystem.out.println(\"Done brokenExample\");\n\t}\n}\n{code}\n\nThis is a result of the Options object returning the reference to its own list and the parsers modifying that list. The first call is removing the required options as they are found and subsequent calls get back an empty list.",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-156",
        "comments": [
            "Seems like it should be easy to solve by cloning the list before returning.",
            "Fixed, I cloned the list in the parser, it seemed better than cloning the list in the getter."
        ],
        "summarized_discussion": "\n\nThe bug was solved by cloning the list in the parser rather than cloning the list in the getter."
    },
    "JacksonDatabind_54_src/main/java/com/fasterxml/jackson/databind/ser/PropertyBuilder.java_67_171": {
        "src": "@SuppressWarnings(\"deprecation\")\n    protected BeanPropertyWriter buildWriter(SerializerProvider prov,\n            BeanPropertyDefinition propDef, JavaType declaredType, JsonSerializer<?> ser,\n            TypeSerializer typeSer, TypeSerializer contentTypeSer,\n            AnnotatedMember am, boolean defaultUseStaticTyping)\n        throws JsonMappingException\n    {\n        // do we have annotation that forces type to use (to declared type or its super type)?\n        JavaType serializationType = findSerializationType(am, defaultUseStaticTyping, declaredType);\n\n        // Container types can have separate type serializers for content (value / element) type\n        if (contentTypeSer != null) {\n            /* 04-Feb-2010, tatu: Let's force static typing for collection, if there is\n             *    type information for contents. Should work well (for JAXB case); can be\n             *    revisited if this causes problems.\n             */\n            if (serializationType == null) {\n//                serializationType = TypeFactory.type(am.getGenericType(), _beanDesc.getType());\n                serializationType = declaredType;\n            }\n            JavaType ct = serializationType.getContentType();\n            // Not exactly sure why, but this used to occur; better check explicitly:\n            if (ct == null) {\n                throw new IllegalStateException(\"Problem trying to create BeanPropertyWriter for property '\"\n                        +propDef.getName()+\"' (of type \"+_beanDesc.getType()+\"); serialization type \"+serializationType+\" has no content\");\n            }\n            serializationType = serializationType.withContentTypeHandler(contentTypeSer);\n            ct = serializationType.getContentType();\n        }\n        \n        Object valueToSuppress = null;\n        boolean suppressNulls = false;\n\n        JsonInclude.Value inclV = _defaultInclusion.withOverrides(propDef.findInclusion());\n        JsonInclude.Include inclusion = inclV.getValueInclusion();\n        if (inclusion == JsonInclude.Include.USE_DEFAULTS) { // should not occur but...\n            inclusion = JsonInclude.Include.ALWAYS;\n        }\n\n        // 12-Jul-2016, tatu: [databind#1256] Need to make sure we consider type refinement\n        JavaType actualType = (serializationType == null) ? declaredType : serializationType;\n        \n        switch (inclusion) {\n        case NON_DEFAULT:\n            // 11-Nov-2015, tatu: This is tricky because semantics differ between cases,\n            //    so that if enclosing class has this, we may need to values of property,\n            //    whereas for global defaults OR per-property overrides, we have more\n            //    static definition. Sigh.\n            // First: case of class specifying it; try to find POJO property defaults\n            if (_defaultInclusion.getValueInclusion() == JsonInclude.Include.NON_DEFAULT) {\n                valueToSuppress = getPropertyDefaultValue(propDef.getName(), am, actualType);\n            } else {\n                valueToSuppress = getDefaultValue(actualType);\n            }\n            if (valueToSuppress == null) {\n                suppressNulls = true;\n            } else {\n                if (valueToSuppress.getClass().isArray()) {\n                    valueToSuppress = ArrayBuilders.getArrayComparator(valueToSuppress);\n                }\n            }\n\n            break;\n        case NON_ABSENT: // new with 2.6, to support Guava/JDK8 Optionals\n            // always suppress nulls\n            suppressNulls = true;\n            // and for referential types, also \"empty\", which in their case means \"absent\"\n            if (declaredType.isReferenceType()) {\n                valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            }\n            break;\n        case NON_EMPTY:\n            // always suppress nulls\n            suppressNulls = true;\n            // but possibly also 'empty' values:\n            valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            break;\n        case NON_NULL:\n            suppressNulls = true;\n            // fall through\n        case ALWAYS: // default\n        default:\n            // we may still want to suppress empty collections, as per [JACKSON-254]:\n            if (declaredType.isContainerType()\n                    && !_config.isEnabled(SerializationFeature.WRITE_EMPTY_JSON_ARRAYS)) {\n                valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            }\n            break;\n        }\n        BeanPropertyWriter bpw = new BeanPropertyWriter(propDef,\n                am, _beanDesc.getClassAnnotations(), declaredType,\n                ser, typeSer, serializationType, suppressNulls, valueToSuppress);\n\n        // How about custom null serializer?\n        Object serDef = _annotationIntrospector.findNullSerializer(am);\n        if (serDef != null) {\n            bpw.assignNullSerializer(prov.serializerInstance(am, serDef));\n        }\n        // And then, handling of unwrapping\n        NameTransformer unwrapper = _annotationIntrospector.findUnwrappingNameTransformer(am);\n        if (unwrapper != null) {\n            bpw = bpw.unwrappingWriter(unwrapper);\n        }\n        return bpw;\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"deprecation\" ) protected BeanPropertyWriter buildWriter ( SerializerProvider prov , BeanPropertyDefinition propDef , JavaType declaredType , JsonSerializer < ? > ser , TypeSerializer typeSer , TypeSerializer contentTypeSer , AnnotatedMember am , boolean defaultUseStaticTyping ) throws JsonMappingException { JavaType serializationType = findSerializationType ( am , defaultUseStaticTyping , declaredType ) ; if ( contentTypeSer != null ) { if ( serializationType == null ) { serializationType = declaredType ; } JavaType ct = serializationType . getContentType ( ) ; if ( ct == null ) { throw new IllegalStateException ( \"Problem trying to create BeanPropertyWriter for property '\" + propDef . getName ( ) + \"' (of type \" + _beanDesc . getType ( ) + \"); serialization type \" + serializationType + \" has no content\" ) ; } serializationType = serializationType . withContentTypeHandler ( contentTypeSer ) ; ct = serializationType . getContentType ( ) ; } Object valueToSuppress = null ; boolean suppressNulls = false ; JsonInclude . Value inclV = _defaultInclusion . withOverrides ( propDef . findInclusion ( ) ) ; JsonInclude . Include inclusion = inclV . getValueInclusion ( ) ; if ( inclusion == JsonInclude . Include . USE_DEFAULTS ) { inclusion = JsonInclude . Include . ALWAYS ; } JavaType actualType = ( serializationType == null ) ? declaredType : serializationType ; switch ( inclusion ) { case NON_DEFAULT : if ( _defaultInclusion . getValueInclusion ( ) == JsonInclude . Include . NON_DEFAULT ) { valueToSuppress = getPropertyDefaultValue ( propDef . getName ( ) , am , actualType ) ; } else { valueToSuppress = getDefaultValue ( actualType ) ; } if ( valueToSuppress == null ) { suppressNulls = true ; } else { if ( valueToSuppress . getClass ( ) . isArray ( ) ) { valueToSuppress = ArrayBuilders . getArrayComparator ( valueToSuppress ) ; } } break ; case NON_ABSENT : suppressNulls = true ; if ( declaredType . isReferenceType ( ) ) { valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; } break ; case NON_EMPTY : suppressNulls = true ; valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; break ; case NON_NULL : suppressNulls = true ; case ALWAYS : default : if ( declaredType . isContainerType ( ) && ! _config . isEnabled ( SerializationFeature . WRITE_EMPTY_JSON_ARRAYS ) ) { valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; } break ; } BeanPropertyWriter bpw = new BeanPropertyWriter ( propDef , am , _beanDesc . getClassAnnotations ( ) , declaredType , ser , typeSer , serializationType , suppressNulls , valueToSuppress ) ; Object serDef = _annotationIntrospector . findNullSerializer ( am ) ; if ( serDef != null ) { bpw . assignNullSerializer ( prov . serializerInstance ( am , serDef ) ) ; } NameTransformer unwrapper = _annotationIntrospector . findUnwrappingNameTransformer ( am ) ; if ( unwrapper != null ) { bpw = bpw . unwrappingWriter ( unwrapper ) ; } return bpw ; }",
        "fixed_src": "@SuppressWarnings(\"deprecation\")\n    protected BeanPropertyWriter buildWriter(SerializerProvider prov,\n            BeanPropertyDefinition propDef, JavaType declaredType, JsonSerializer<?> ser,\n            TypeSerializer typeSer, TypeSerializer contentTypeSer,\n            AnnotatedMember am, boolean defaultUseStaticTyping)\n        throws JsonMappingException\n    {\n        // do we have annotation that forces type to use (to declared type or its super type)?\n        JavaType serializationType = findSerializationType(am, defaultUseStaticTyping, declaredType);\n\n        // Container types can have separate type serializers for content (value / element) type\n        if (contentTypeSer != null) {\n            /* 04-Feb-2010, tatu: Let's force static typing for collection, if there is\n             *    type information for contents. Should work well (for JAXB case); can be\n             *    revisited if this causes problems.\n             */\n            if (serializationType == null) {\n//                serializationType = TypeFactory.type(am.getGenericType(), _beanDesc.getType());\n                serializationType = declaredType;\n            }\n            JavaType ct = serializationType.getContentType();\n            // Not exactly sure why, but this used to occur; better check explicitly:\n            if (ct == null) {\n                throw new IllegalStateException(\"Problem trying to create BeanPropertyWriter for property '\"\n                        +propDef.getName()+\"' (of type \"+_beanDesc.getType()+\"); serialization type \"+serializationType+\" has no content\");\n            }\n            serializationType = serializationType.withContentTypeHandler(contentTypeSer);\n            ct = serializationType.getContentType();\n        }\n        \n        Object valueToSuppress = null;\n        boolean suppressNulls = false;\n\n        JsonInclude.Value inclV = _defaultInclusion.withOverrides(propDef.findInclusion());\n        JsonInclude.Include inclusion = inclV.getValueInclusion();\n        if (inclusion == JsonInclude.Include.USE_DEFAULTS) { // should not occur but...\n            inclusion = JsonInclude.Include.ALWAYS;\n        }\n\n        // 12-Jul-2016, tatu: [databind#1256] Need to make sure we consider type refinement\n        JavaType actualType = (serializationType == null) ? declaredType : serializationType;\n        \n        switch (inclusion) {\n        case NON_DEFAULT:\n            // 11-Nov-2015, tatu: This is tricky because semantics differ between cases,\n            //    so that if enclosing class has this, we may need to values of property,\n            //    whereas for global defaults OR per-property overrides, we have more\n            //    static definition. Sigh.\n            // First: case of class specifying it; try to find POJO property defaults\n            if (_defaultInclusion.getValueInclusion() == JsonInclude.Include.NON_DEFAULT) {\n                valueToSuppress = getPropertyDefaultValue(propDef.getName(), am, actualType);\n            } else {\n                valueToSuppress = getDefaultValue(actualType);\n            }\n            if (valueToSuppress == null) {\n                suppressNulls = true;\n            } else {\n                if (valueToSuppress.getClass().isArray()) {\n                    valueToSuppress = ArrayBuilders.getArrayComparator(valueToSuppress);\n                }\n            }\n\n            break;\n        case NON_ABSENT: // new with 2.6, to support Guava/JDK8 Optionals\n            // always suppress nulls\n            suppressNulls = true;\n            // and for referential types, also \"empty\", which in their case means \"absent\"\n            if (actualType.isReferenceType()) {\n                valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            }\n            break;\n        case NON_EMPTY:\n            // always suppress nulls\n            suppressNulls = true;\n            // but possibly also 'empty' values:\n            valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            break;\n        case NON_NULL:\n            suppressNulls = true;\n            // fall through\n        case ALWAYS: // default\n        default:\n            // we may still want to suppress empty collections, as per [JACKSON-254]:\n            if (actualType.isContainerType()\n                    && !_config.isEnabled(SerializationFeature.WRITE_EMPTY_JSON_ARRAYS)) {\n                valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            }\n            break;\n        }\n        BeanPropertyWriter bpw = new BeanPropertyWriter(propDef,\n                am, _beanDesc.getClassAnnotations(), declaredType,\n                ser, typeSer, serializationType, suppressNulls, valueToSuppress);\n\n        // How about custom null serializer?\n        Object serDef = _annotationIntrospector.findNullSerializer(am);\n        if (serDef != null) {\n            bpw.assignNullSerializer(prov.serializerInstance(am, serDef));\n        }\n        // And then, handling of unwrapping\n        NameTransformer unwrapper = _annotationIntrospector.findUnwrappingNameTransformer(am);\n        if (unwrapper != null) {\n            bpw = bpw.unwrappingWriter(unwrapper);\n        }\n        return bpw;\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"deprecation\" ) protected BeanPropertyWriter buildWriter ( SerializerProvider prov , BeanPropertyDefinition propDef , JavaType declaredType , JsonSerializer < ? > ser , TypeSerializer typeSer , TypeSerializer contentTypeSer , AnnotatedMember am , boolean defaultUseStaticTyping ) throws JsonMappingException { JavaType serializationType = findSerializationType ( am , defaultUseStaticTyping , declaredType ) ; if ( contentTypeSer != null ) { if ( serializationType == null ) { serializationType = declaredType ; } JavaType ct = serializationType . getContentType ( ) ; if ( ct == null ) { throw new IllegalStateException ( \"Problem trying to create BeanPropertyWriter for property '\" + propDef . getName ( ) + \"' (of type \" + _beanDesc . getType ( ) + \"); serialization type \" + serializationType + \" has no content\" ) ; } serializationType = serializationType . withContentTypeHandler ( contentTypeSer ) ; ct = serializationType . getContentType ( ) ; } Object valueToSuppress = null ; boolean suppressNulls = false ; JsonInclude . Value inclV = _defaultInclusion . withOverrides ( propDef . findInclusion ( ) ) ; JsonInclude . Include inclusion = inclV . getValueInclusion ( ) ; if ( inclusion == JsonInclude . Include . USE_DEFAULTS ) { inclusion = JsonInclude . Include . ALWAYS ; } JavaType actualType = ( serializationType == null ) ? declaredType : serializationType ; switch ( inclusion ) { case NON_DEFAULT : if ( _defaultInclusion . getValueInclusion ( ) == JsonInclude . Include . NON_DEFAULT ) { valueToSuppress = getPropertyDefaultValue ( propDef . getName ( ) , am , actualType ) ; } else { valueToSuppress = getDefaultValue ( actualType ) ; } if ( valueToSuppress == null ) { suppressNulls = true ; } else { if ( valueToSuppress . getClass ( ) . isArray ( ) ) { valueToSuppress = ArrayBuilders . getArrayComparator ( valueToSuppress ) ; } } break ; case NON_ABSENT : suppressNulls = true ; if ( actualType . isReferenceType ( ) ) { valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; } break ; case NON_EMPTY : suppressNulls = true ; valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; break ; case NON_NULL : suppressNulls = true ; case ALWAYS : default : if ( actualType . isContainerType ( ) && ! _config . isEnabled ( SerializationFeature . WRITE_EMPTY_JSON_ARRAYS ) ) { valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; } break ; } BeanPropertyWriter bpw = new BeanPropertyWriter ( propDef , am , _beanDesc . getClassAnnotations ( ) , declaredType , ser , typeSer , serializationType , suppressNulls , valueToSuppress ) ; Object serDef = _annotationIntrospector . findNullSerializer ( am ) ; if ( serDef != null ) { bpw . assignNullSerializer ( prov . serializerInstance ( am , serDef ) ) ; } NameTransformer unwrapper = _annotationIntrospector . findUnwrappingNameTransformer ( am ) ; if ( unwrapper != null ) { bpw = bpw . unwrappingWriter ( unwrapper ) ; } return bpw ; }",
        "summary": "`Optional.empty()` not excluded if property declared with type `Object`",
        "Description": "Jackson version is 2.6.6\n**Here is the code:**\n\n```\n        ObjectMapper mapper = new ObjectMapper();\n        mapper.setSerializationInclusion(JsonInclude.Include.NON_ABSENT);\n        mapper.registerModule(new Jdk8Module());\n\n        JsonResult result = new JsonResult();\n        result.setA(Optional.empty());\n        result.setB(Optional.empty());\n        System.out.println(mapper.writeValueAsString(result));\n```\n\n```\n@Data\npublic class JsonResult {\n    private Object a;\n    private Optional<Object> b;\n}\n```\n\n**Then I got the output: {\"a\":null}**\n\n**The real value of both is the same, why the results are different?**\n\n**How can I avoid null in such case?**\n\nBy the way, I tried 'NON_EMPTY'. It can work, but it also ignores zero and empty array. I want to keep them.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Please use mailing list for usage question; issue tracker is for reporting bugs.\n"
            },
            {
                "content": "As to the question however: perhaps this is a bug; I can't say for sure without having a closer look.\nWhat it almost certainly comes down to is with the different declared type: use of `Object` lacks enough information to know that value is of referential type, during serialization construction.\nWhether it can be made to work or not is a different question.\n\nReopening assuming there is something to fix here. Thank you for the report!\n"
            },
            {
                "content": "Ok, this is working as designed: declared type MUST give indication that the value would be reference type (like `Optional`), and as such property declared as `Object` will not be filtered if it happens to contain a referential type.\n\nIn future we hope to support custom inclusion criteria which would allow construction of more advanced inclusion filters: for now, declared type has to be such that it allows determination of concept of absent.\n\nI will keep this issue open, however, to think whether there is a possibility that this could be improved upon in future.\n\nAlso: I noticed that use of `@JsonSerialize(as=Optional.class)` does not work as expected; will file a separate bug.\n"
            },
            {
                "content": "Ok: so, fix here is two-fold. First, declared type MUST indicate it's a reference type (`Optional`), so declaration has to be something like:\n\n```\npublic class JsonResult {\n  @JsonSerialize(as=Optional.class) // important!\n  private Object a;\n  private Optional<Object> b;\n}\n```\n\nThe other part of the fix is to make sure Jackson actually uses type modified by annotation; this was not done but fix will be in 2.8.1. This combination will make tests pass.\n\nOne thing I do not understand, however, is why `NON_EMPTY` would not work here; it should work without additional type information. But I could not reproduce that issue; test actually works for me if `NON_EMPTY` is used.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is that the declared type must indicate that it is a reference type (like Optional), and the Jackson must use the type modified by the annotation. Additionally, the use of NON_EMPTY should work without additional type information."
    },
    "Math_89_src/java/org/apache/commons/math/stat/Frequency.java_108_111": {
        "src": "@Deprecated\n    public void addValue(Object v) {\n            addValue((Comparable<?>) v);            \n    }",
        "src_wo_comments": "@ Deprecated public void addValue ( Object v ) { addValue ( ( Comparable < ? > ) v ) ; }",
        "fixed_src": "@Deprecated\n    public void addValue(Object v) {\n        if (v instanceof Comparable<?>){\n            addValue((Comparable<?>) v);            \n        } else {\n            throw new IllegalArgumentException(\"Object must implement Comparable\");\n        }\n    }",
        "fixed_src_wo_comments": "@ Deprecated public void addValue ( Object v ) { if ( v instanceof Comparable < ? > ) { addValue ( ( Comparable < ? > ) v ) ; } else { throw new IllegalArgumentException ( \"Object must implement Comparable\" ) ; } }",
        "summary": "Bugs in Frequency API",
        "Description": "I think the existing Frequency API has some bugs in it.\n\nThe addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException.\nIn fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects.\nThis could be fixed by checking that the object is Comparable.\n\nSimilar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable.\n\nThe getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:\n\n{code}\n        final Object OBJ = new Object();\n        f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below\n        System.out.println(f.getCount(OBJ)); // 0\n        System.out.println(f.getPct(OBJ)); // 0.0\n{code}\n\nRather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object.\nAlso, it should make it easier to implement generics.\n\nHowever, this would cause compilation failures for some programs that pass Object rather than Comparable to the class.\nThese would need recoding, but I think they would continue to run OK against the new API.\n\nIt would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object.\nBut is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-259",
        "comments": [
            "I am OK with adding a check and throwing illegalArgumentExeption if an object that does not implement Comparable is supplied to these methods (as indicated in the javadoc), but not keen on introducing the compatibility issue.",
            "See:\n\nURL: http://svn.apache.org/viewvc?rev=765996&view=rev\nLog:\nMATH-259 - check for Comparable when adding values\n\nI overlooked the change of Exception, so there's also:\n\nURL: http://svn.apache.org/viewvc?rev=766003&view=rev\nLog:\nMATH-259 - throw IllegalArgument rather than ClassCast to better retain original behaviour\n\n==\n\nI added a new method\n\n   public void addValue(Comparable<?> v)\n\nwhich is called from\n\n   public void addValue(Object v)\n\nwhich I took the liberty of deprecating, so the compiler will warn users about non-Comparable objects.\nHope that's OK.\n\nNote that it's still possible for mutually non-Comparable values to be added, because the code does not check comparisons both ways, it relies on HashMap to do so.\n\nI.e. if B.compareTo(A) is OK, but A.compareTo(B) does not exist, then it is possible to add A, then B without any complaints.\nThis later causes a ClassCastException in some of the getXXX() methods.\nHowever this is not a valid Comparable implementation, as they are supposed to be symmetric.",
            "API now tidied up as far as possible whilst still being compatible.",
            "closing resolved issue for 2.0 release"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to add a check to throw an IllegalArgumentException if an object that does not implement Comparable is supplied to the methods, and to add a new method public void addValue(Comparable<?> v) that is called from public void addValue(Object v). The API has been tidied up as far as possible to be compatible, and the issue has been closed for the 2.0 release."
    },
    "Mockito_1_src/org/mockito/internal/invocation/InvocationMatcher.java_120_150": {
        "src": "public void captureArgumentsFrom(Invocation invocation) {\n        if (invocation.getMethod().isVarArgs()) {\n            int indexOfVararg = invocation.getRawArguments().length - 1;\n            throw new UnsupportedOperationException();\n\n        } else {\n            for (int position = 0; position < matchers.size(); position++) {\n                Matcher m = matchers.get(position);\n                if (m instanceof CapturesArguments) {\n                    ((CapturesArguments) m).captureFrom(invocation.getArgumentAt(position, Object.class));\n                }\n            }\n        }\n\n//        for (int position = 0; position < matchers.size(); position++) {\n//            Matcher m = matchers.get(position);\n//            if (m instanceof CapturesArguments && invocation.getRawArguments().length > position) {\n//                //TODO SF - this whole lot can be moved captureFrom implementation\n//                if(isVariableArgument(invocation, position) && isVarargMatcher(m)) {\n//                    Object array = invocation.getRawArguments()[position];\n//                    for (int i = 0; i < Array.getLength(array); i++) {\n//                        ((CapturesArguments) m).captureFrom(Array.get(array, i));\n//                    }\n//                    //since we've captured all varargs already, it does not make sense to process other matchers.\n//                    return;\n//                } else {\n//                    ((CapturesArguments) m).captureFrom(invocation.getRawArguments()[position]);\n//                }\n//            }\n//        }\n    }",
        "src_wo_comments": "public void captureArgumentsFrom ( Invocation invocation ) { if ( invocation . getMethod ( ) . isVarArgs ( ) ) { int indexOfVararg = invocation . getRawArguments ( ) . length - 1 ; throw new UnsupportedOperationException ( ) ; } else { for ( int position = 0 ; position < matchers . size ( ) ; position ++ ) { Matcher m = matchers . get ( position ) ; if ( m instanceof CapturesArguments ) { ( ( CapturesArguments ) m ) . captureFrom ( invocation . getArgumentAt ( position , Object . class ) ) ; } } } }",
        "fixed_src": "public void captureArgumentsFrom(Invocation invocation) {\n        if (invocation.getMethod().isVarArgs()) {\n            int indexOfVararg = invocation.getRawArguments().length - 1;\n            for (int position = 0; position < indexOfVararg; position++) {\n                Matcher m = matchers.get(position);\n                if (m instanceof CapturesArguments) {\n                    ((CapturesArguments) m).captureFrom(invocation.getArgumentAt(position, Object.class));\n                }\n            }\n            for (int position = indexOfVararg; position < matchers.size(); position++) {\n                Matcher m = matchers.get(position);\n                if (m instanceof CapturesArguments) {\n                    ((CapturesArguments) m).captureFrom(invocation.getRawArguments()[position - indexOfVararg]);\n                }\n            }\n\n        } else {\n            for (int position = 0; position < matchers.size(); position++) {\n                Matcher m = matchers.get(position);\n                if (m instanceof CapturesArguments) {\n                    ((CapturesArguments) m).captureFrom(invocation.getArgumentAt(position, Object.class));\n                }\n            }\n        }\n\n//        for (int position = 0; position < matchers.size(); position++) {\n//            Matcher m = matchers.get(position);\n//            if (m instanceof CapturesArguments && invocation.getRawArguments().length > position) {\n//                //TODO SF - this whole lot can be moved captureFrom implementation\n//                if(isVariableArgument(invocation, position) && isVarargMatcher(m)) {\n//                    Object array = invocation.getRawArguments()[position];\n//                    for (int i = 0; i < Array.getLength(array); i++) {\n//                        ((CapturesArguments) m).captureFrom(Array.get(array, i));\n//                    }\n//                    //since we've captured all varargs already, it does not make sense to process other matchers.\n//                    return;\n//                } else {\n//                    ((CapturesArguments) m).captureFrom(invocation.getRawArguments()[position]);\n//                }\n//            }\n//        }\n    }",
        "fixed_src_wo_comments": "public void captureArgumentsFrom ( Invocation invocation ) { if ( invocation . getMethod ( ) . isVarArgs ( ) ) { int indexOfVararg = invocation . getRawArguments ( ) . length - 1 ; for ( int position = 0 ; position < indexOfVararg ; position ++ ) { Matcher m = matchers . get ( position ) ; if ( m instanceof CapturesArguments ) { ( ( CapturesArguments ) m ) . captureFrom ( invocation . getArgumentAt ( position , Object . class ) ) ; } } for ( int position = indexOfVararg ; position < matchers . size ( ) ; position ++ ) { Matcher m = matchers . get ( position ) ; if ( m instanceof CapturesArguments ) { ( ( CapturesArguments ) m ) . captureFrom ( invocation . getRawArguments ( ) [ position - indexOfVararg ] ) ; } } } else { for ( int position = 0 ; position < matchers . size ( ) ; position ++ ) { Matcher m = matchers . get ( position ) ; if ( m instanceof CapturesArguments ) { ( ( CapturesArguments ) m ) . captureFrom ( invocation . getArgumentAt ( position , Object . class ) ) ; } } } }",
        "summary": "ArgumentCaptor no longer working for varargs",
        "Description": "I ran into the issue described here: http://stackoverflow.com/questions/27303562/why-does-upgrading-mockito-from-1-9-5-to-1-10-8-break-this-captor\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Hi sorry for the late reply.\nI reproduced the issue, not sure when I will be able to fix though.\n"
            },
            {
                "content": "fixed by #211 \n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been fixed by commit #211."
    },
    "JacksonDatabind_110_src/main/java/com/fasterxml/jackson/databind/deser/impl/JavaUtilCollectionsDeserializers.java_64_86": {
        "src": "public static JsonDeserializer<?> findForCollection(DeserializationContext ctxt,\n            JavaType type)\n        throws JsonMappingException\n    {\n        JavaUtilCollectionsConverter conv;\n\n        // 10-Jan-2017, tatu: Some types from `java.util.Collections`/`java.util.Arrays` need bit of help...\n        if (type.hasRawClass(CLASS_AS_ARRAYS_LIST)) {\n            conv = converter(TYPE_AS_LIST, type, List.class);\n        } else if (type.hasRawClass(CLASS_SINGLETON_LIST)) {\n            conv = converter(TYPE_SINGLETON_LIST, type, List.class);\n        } else if (type.hasRawClass(CLASS_SINGLETON_SET)) {\n            conv = converter(TYPE_SINGLETON_SET, type, Set.class);\n        // [databind#2265]: we may have another impl type for unmodifiable Lists, check both\n        } else if (type.hasRawClass(CLASS_UNMODIFIABLE_LIST)) {\n            conv = converter(TYPE_UNMODIFIABLE_LIST, type, List.class);\n        } else if (type.hasRawClass(CLASS_UNMODIFIABLE_SET)) {\n            conv = converter(TYPE_UNMODIFIABLE_SET, type, Set.class);\n        } else {\n            return null;\n        }\n        return new StdDelegatingDeserializer<Object>(conv);\n    }",
        "src_wo_comments": "public static JsonDeserializer < ? > findForCollection ( DeserializationContext ctxt , JavaType type ) throws JsonMappingException { JavaUtilCollectionsConverter conv ; if ( type . hasRawClass ( CLASS_AS_ARRAYS_LIST ) ) { conv = converter ( TYPE_AS_LIST , type , List . class ) ; } else if ( type . hasRawClass ( CLASS_SINGLETON_LIST ) ) { conv = converter ( TYPE_SINGLETON_LIST , type , List . class ) ; } else if ( type . hasRawClass ( CLASS_SINGLETON_SET ) ) { conv = converter ( TYPE_SINGLETON_SET , type , Set . class ) ; } else if ( type . hasRawClass ( CLASS_UNMODIFIABLE_LIST ) ) { conv = converter ( TYPE_UNMODIFIABLE_LIST , type , List . class ) ; } else if ( type . hasRawClass ( CLASS_UNMODIFIABLE_SET ) ) { conv = converter ( TYPE_UNMODIFIABLE_SET , type , Set . class ) ; } else { return null ; } return new StdDelegatingDeserializer < Object > ( conv ) ; }",
        "fixed_src": "public static JsonDeserializer<?> findForCollection(DeserializationContext ctxt,\n            JavaType type)\n        throws JsonMappingException\n    {\n        JavaUtilCollectionsConverter conv;\n\n        // 10-Jan-2017, tatu: Some types from `java.util.Collections`/`java.util.Arrays` need bit of help...\n        if (type.hasRawClass(CLASS_AS_ARRAYS_LIST)) {\n            conv = converter(TYPE_AS_LIST, type, List.class);\n        } else if (type.hasRawClass(CLASS_SINGLETON_LIST)) {\n            conv = converter(TYPE_SINGLETON_LIST, type, List.class);\n        } else if (type.hasRawClass(CLASS_SINGLETON_SET)) {\n            conv = converter(TYPE_SINGLETON_SET, type, Set.class);\n        // [databind#2265]: we may have another impl type for unmodifiable Lists, check both\n        } else if (type.hasRawClass(CLASS_UNMODIFIABLE_LIST) || type.hasRawClass(CLASS_UNMODIFIABLE_LIST_ALIAS)) {\n            conv = converter(TYPE_UNMODIFIABLE_LIST, type, List.class);\n        } else if (type.hasRawClass(CLASS_UNMODIFIABLE_SET)) {\n            conv = converter(TYPE_UNMODIFIABLE_SET, type, Set.class);\n        } else {\n            return null;\n        }\n        return new StdDelegatingDeserializer<Object>(conv);\n    }",
        "fixed_src_wo_comments": "public static JsonDeserializer < ? > findForCollection ( DeserializationContext ctxt , JavaType type ) throws JsonMappingException { JavaUtilCollectionsConverter conv ; if ( type . hasRawClass ( CLASS_AS_ARRAYS_LIST ) ) { conv = converter ( TYPE_AS_LIST , type , List . class ) ; } else if ( type . hasRawClass ( CLASS_SINGLETON_LIST ) ) { conv = converter ( TYPE_SINGLETON_LIST , type , List . class ) ; } else if ( type . hasRawClass ( CLASS_SINGLETON_SET ) ) { conv = converter ( TYPE_SINGLETON_SET , type , Set . class ) ; } else if ( type . hasRawClass ( CLASS_UNMODIFIABLE_LIST ) || type . hasRawClass ( CLASS_UNMODIFIABLE_LIST_ALIAS ) ) { conv = converter ( TYPE_UNMODIFIABLE_LIST , type , List . class ) ; } else if ( type . hasRawClass ( CLASS_UNMODIFIABLE_SET ) ) { conv = converter ( TYPE_UNMODIFIABLE_SET , type , Set . class ) ; } else { return null ; } return new StdDelegatingDeserializer < Object > ( conv ) ; }",
        "summary": "Inconsistent handling of Collections$UnmodifiableList VS Collections$UnmodifiableRandomAccessList",
        "Description": "I'm sorry to bring that one up again, but I'm under the impression that the issue about unmodifiable collections (https://github.com/FasterXML/jackson-databind/issues/1880) is still not solved completely.\r\n\r\nIn fact, the way the `CLASS_UNMODIFIABLE_LIST` is retrieved [here](https://github.com/FasterXML/jackson-databind/blob/master/src/main/java/com/fasterxml/jackson/databind/deser/impl/JavaUtilCollectionsDeserializers.java#L52) yields `Collections$UnmodifiableRandomAccessList`, and therefore only this type is currently supported by Jackson 2.9.8.\r\n\r\nHowever, using `Collections.unmodifiableList()` on a `List` implementation that doesn't implement `RandomAccess` will yield a `Collections$UnmodifiableList` instead, which is not deserialized properly and fails with:\r\n```\r\ncom.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `java.util.Collections$UnmodifiableList` (no Creators, like default constructor, exist): no default no-arguments constructor found\r\n```\r\n\r\nThis can be reproduced by adding the following test case in `TestDefaultForUtilCollections1868`:\r\n```java\r\npublic void testUnmodifiableNonRandomAccessList() throws Exception {\r\n   _verifyCollection(Collections.unmodifiableList(new LinkedList<>(Arrays.asList(\"first\", \"second\"))));\r\n}\r\n```\r\n\r\nOr more generally for outside the project:\r\n```java\r\npublic void testUnmodifiableNonRandomAccessList() throws Exception {\r\n    Collection<?> exp = Collections.unmodifiableList(new LinkedList<>(Arrays.asList(\"first\", \"second\")));\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    mapper.enableDefaultTyping(DefaultTyping.NON_FINAL, JsonTypeInfo.As.PROPERTY);\r\n    String json = mapper.writeValueAsString(exp);\r\n    Collection<?> act = mapper.readValue(json, Collection.class);\r\n\r\n    assertEquals(exp, act);\r\n    assertEquals(exp.getClass(), act.getClass());\r\n}\r\n```\r\n\r\nCurrently `java.util.Collections.unmodifiableList()` can only return these 2 types of unmodifiable lists, so I believe it is safe for now to just hardcode yet another special case for this class.\r\n\r\nThis can currently be solved on user side by adding a mixin, but since `Collections$UnmodifiableRandomAccessList` is supported, I would find it natural to also support the non-random access variant.",
        "issue_url": null,
        "comments": [
            {
                "content": "Ok. What would help here is the reproduction of the problem, so I can consider what, if anything to do.\r\nI feel that soon the main mechanism to use is to remove any indication of immutability if all it leads to is trouble. Since it is not detectable via public JDK api it, it only causes problems with polymorphic handling, and there it probably should not be supported either.\r\n\r\n"
            },
            {
                "content": "> Ok. What would help here is the reproduction of the problem, so I can consider what, if anything to do.\r\n\r\nWhat do you mean? There is a test case to reproduce the issue in the description, is there something else that you need?\r\n\r\n> I feel that soon the main mechanism to use is to remove any indication of immutability if all it leads to is trouble. Since it is not detectable via public JDK api it, it only causes problems with polymorphic handling, and there it probably should not be supported either.\r\n\r\nI agree that, because it is not public API, it is not really clean for Jackson to add special cases like that. But pragmatically, it is quite useful to have this little help from Jackson for such common types. I believe it was not a mistake to include handling for these types in the first place, even if it is not recommended in general to rely on private APIs."
            },
            {
                "content": "@joffrey-bion I think I skimmed the description too fast, and missed the meat, 2-line test method. Sorry about that, and thank you for providing all the information.\r\n\r\nI hope to look into this soon; added it on:\r\n\r\n    https://github.com/FasterXML/jackson-future-ideas/wiki/Jackson-Work-in-Progress\r\n\r\nwhere I track short-term work items."
            },
            {
                "content": "@joffrey-bion Thank you for reporting this issue -- I fixed this in `2.9` for 2.9.9 (and thereby 2.10.0 / 3.0.0) when released."
            },
            {
                "content": "@cowtowncoder Thanks a lot for looking into this and quickly fixing the issue! "
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to remove any indication of immutability and to not rely on private APIs. This was fixed in version 2.9.9 and later versions."
    },
    "Math_31_src/main/java/org/apache/commons/math3/util/ContinuedFraction.java_123_199": {
        "src": "public double evaluate(double x, double epsilon, int maxIterations) {\n        final double small = 1e-50;\n        double hPrev = getA(0, x);\n\n        // use the value of small as epsilon criteria for zero checks\n        if (Precision.equals(hPrev, 0.0, small)) {\n            hPrev = small;\n        }\n\n        int n = 1;\n        double dPrev = 0.0;\n        double p0 = 1.0;\n        double q1 = 1.0;\n        double cPrev = hPrev;\n        double hN = hPrev;\n\n        while (n < maxIterations) {\n            final double a = getA(n, x);\n            final double b = getB(n, x);\n\n            double cN = a * hPrev + b * p0;\n            double q2 = a * q1 + b * dPrev;\n            if (Double.isInfinite(cN) || Double.isInfinite(q2)) {\n                double scaleFactor = 1d;\n                double lastScaleFactor = 1d;\n                final int maxPower = 5;\n                final double scale = FastMath.max(a,b);\n                if (scale <= 0) {  // Can't scale\n                    throw new ConvergenceException(LocalizedFormats.CONTINUED_FRACTION_INFINITY_DIVERGENCE, x);\n                }\n                for (int i = 0; i < maxPower; i++) {\n                    lastScaleFactor = scaleFactor;\n                    scaleFactor *= scale;\n                    if (a != 0.0 && a > b) {\n                        cN = hPrev / lastScaleFactor + (b / scaleFactor * p0);\n                        q2 = q1 / lastScaleFactor + (b / scaleFactor * dPrev);\n                    } else if (b != 0) {\n                        cN = (a / scaleFactor * hPrev) + p0 / lastScaleFactor;\n                        q2 = (a / scaleFactor * q1) + dPrev / lastScaleFactor;\n                    }\n                    if (!(Double.isInfinite(cN) || Double.isInfinite(q2))) {\n                        break;\n                    }\n                }\n            }\n\n            final double deltaN = cN / q2 / cPrev;\n            hN = cPrev * deltaN;\n\n            if (Double.isInfinite(hN)) {\n                throw new ConvergenceException(LocalizedFormats.CONTINUED_FRACTION_INFINITY_DIVERGENCE,\n                                               x);\n            }\n            if (Double.isNaN(hN)) {\n                throw new ConvergenceException(LocalizedFormats.CONTINUED_FRACTION_NAN_DIVERGENCE,\n                                               x);\n            }\n\n            if (FastMath.abs(deltaN - 1.0) < epsilon) {\n                break;\n            }\n\n            dPrev = q1;\n            cPrev = cN / q2;\n            p0 = hPrev;\n            hPrev = cN;\n            q1 = q2;\n            n++;\n        }\n\n        if (n >= maxIterations) {\n            throw new MaxCountExceededException(LocalizedFormats.NON_CONVERGENT_CONTINUED_FRACTION,\n                                                maxIterations, x);\n        }\n\n        return hN;\n    }",
        "src_wo_comments": "public double evaluate ( double x , double epsilon , int maxIterations ) { final double small = 1e-50 ; double hPrev = getA ( 0 , x ) ; if ( Precision . equals ( hPrev , 0.0 , small ) ) { hPrev = small ; } int n = 1 ; double dPrev = 0.0 ; double p0 = 1.0 ; double q1 = 1.0 ; double cPrev = hPrev ; double hN = hPrev ; while ( n < maxIterations ) { final double a = getA ( n , x ) ; final double b = getB ( n , x ) ; double cN = a * hPrev + b * p0 ; double q2 = a * q1 + b * dPrev ; if ( Double . isInfinite ( cN ) || Double . isInfinite ( q2 ) ) { double scaleFactor = 1d ; double lastScaleFactor = 1d ; final int maxPower = 5 ; final double scale = FastMath . max ( a , b ) ; if ( scale <= 0 ) { throw new ConvergenceException ( LocalizedFormats . CONTINUED_FRACTION_INFINITY_DIVERGENCE , x ) ; } for ( int i = 0 ; i < maxPower ; i ++ ) { lastScaleFactor = scaleFactor ; scaleFactor *= scale ; if ( a != 0.0 && a > b ) { cN = hPrev / lastScaleFactor + ( b / scaleFactor * p0 ) ; q2 = q1 / lastScaleFactor + ( b / scaleFactor * dPrev ) ; } else if ( b != 0 ) { cN = ( a / scaleFactor * hPrev ) + p0 / lastScaleFactor ; q2 = ( a / scaleFactor * q1 ) + dPrev / lastScaleFactor ; } if ( ! ( Double . isInfinite ( cN ) || Double . isInfinite ( q2 ) ) ) { break ; } } } final double deltaN = cN / q2 / cPrev ; hN = cPrev * deltaN ; if ( Double . isInfinite ( hN ) ) { throw new ConvergenceException ( LocalizedFormats . CONTINUED_FRACTION_INFINITY_DIVERGENCE , x ) ; } if ( Double . isNaN ( hN ) ) { throw new ConvergenceException ( LocalizedFormats . CONTINUED_FRACTION_NAN_DIVERGENCE , x ) ; } if ( FastMath . abs ( deltaN - 1.0 ) < epsilon ) { break ; } dPrev = q1 ; cPrev = cN / q2 ; p0 = hPrev ; hPrev = cN ; q1 = q2 ; n ++ ; } if ( n >= maxIterations ) { throw new MaxCountExceededException ( LocalizedFormats . NON_CONVERGENT_CONTINUED_FRACTION , maxIterations , x ) ; } return hN ; }",
        "fixed_src": "public double evaluate(double x, double epsilon, int maxIterations) {\n        final double small = 1e-50;\n        double hPrev = getA(0, x);\n\n        // use the value of small as epsilon criteria for zero checks\n        if (Precision.equals(hPrev, 0.0, small)) {\n            hPrev = small;\n        }\n\n        int n = 1;\n        double dPrev = 0.0;\n        double cPrev = hPrev;\n        double hN = hPrev;\n\n        while (n < maxIterations) {\n            final double a = getA(n, x);\n            final double b = getB(n, x);\n\n            double dN = a + b * dPrev;\n            if (Precision.equals(dN, 0.0, small)) {\n                dN = small;\n            }\n            double cN = a + b / cPrev;\n            if (Precision.equals(cN, 0.0, small)) {\n                cN = small;\n            }\n\n            dN = 1 / dN;\n            final double deltaN = cN * dN;\n            hN = hPrev * deltaN;\n\n            if (Double.isInfinite(hN)) {\n                throw new ConvergenceException(LocalizedFormats.CONTINUED_FRACTION_INFINITY_DIVERGENCE,\n                                               x);\n            }\n            if (Double.isNaN(hN)) {\n                throw new ConvergenceException(LocalizedFormats.CONTINUED_FRACTION_NAN_DIVERGENCE,\n                                               x);\n            }\n\n            if (FastMath.abs(deltaN - 1.0) < epsilon) {\n                break;\n            }\n\n            dPrev = dN;\n            cPrev = cN;\n            hPrev = hN;\n            n++;\n        }\n\n        if (n >= maxIterations) {\n            throw new MaxCountExceededException(LocalizedFormats.NON_CONVERGENT_CONTINUED_FRACTION,\n                                                maxIterations, x);\n        }\n\n        return hN;\n    }",
        "fixed_src_wo_comments": "public double evaluate ( double x , double epsilon , int maxIterations ) { final double small = 1e-50 ; double hPrev = getA ( 0 , x ) ; if ( Precision . equals ( hPrev , 0.0 , small ) ) { hPrev = small ; } int n = 1 ; double dPrev = 0.0 ; double cPrev = hPrev ; double hN = hPrev ; while ( n < maxIterations ) { final double a = getA ( n , x ) ; final double b = getB ( n , x ) ; double dN = a + b * dPrev ; if ( Precision . equals ( dN , 0.0 , small ) ) { dN = small ; } double cN = a + b / cPrev ; if ( Precision . equals ( cN , 0.0 , small ) ) { cN = small ; } dN = 1 / dN ; final double deltaN = cN * dN ; hN = hPrev * deltaN ; if ( Double . isInfinite ( hN ) ) { throw new ConvergenceException ( LocalizedFormats . CONTINUED_FRACTION_INFINITY_DIVERGENCE , x ) ; } if ( Double . isNaN ( hN ) ) { throw new ConvergenceException ( LocalizedFormats . CONTINUED_FRACTION_NAN_DIVERGENCE , x ) ; } if ( FastMath . abs ( deltaN - 1.0 ) < epsilon ) { break ; } dPrev = dN ; cPrev = cN ; hPrev = hN ; n ++ ; } if ( n >= maxIterations ) { throw new MaxCountExceededException ( LocalizedFormats . NON_CONVERGENT_CONTINUED_FRACTION , maxIterations , x ) ; } return hN ; }",
        "summary": "inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.",
        "Description": "The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem.\n\n{{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}\n\nThis returns 499525, though it should be 499999.\n\nI'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-718",
        "comments": [
            "Hi Yuji,\nthanks for reporting this. For version 3.0, we are currently reshaping the package distribution, and this will probably get resolved once we are over with MATH-692.\nBest regards,\nS\u00e9bastien",
            "There seem to be stability problems with Beta.regularizedBeta(...) when using extreme parameters. {{PascalDistribution.cumulativeProbability(Integer.MAX_VALUE)}} returns {{NaN}} instead of 1. We should look for a way to avoid both infinite values and NaNs in the implementation of the regularized beta function.",
            "As rightly pointed out by Christian, this issue is strongly related to MATH-738.",
            "Sorry, I only meant to postpone this issue.",
            "I just wanted to let you know that our open source project (http://www.eclipse.org/stem/) needs this function and we are eagerly awaiting the update. We are experiencing the issue of wrong values for large trials.",
            "Hi James,\nthanks for your interest. STEM is a very interesting project!\nI will try and find a fix for this issue as soon as possible. Any ideas are welcome!\nS\u00e9bastien",
            "The problem Christian described wrt the PascalDistribution is a simple integer overflow in the class itself:\n\n{noformat}\n    public double cumulativeProbability(int x) {\n        double ret;\n        if (x < 0) {\n            ret = 0.0;\n        } else {\n            ret = Beta.regularizedBeta(probabilityOfSuccess,\n                    numberOfSuccesses, x + 1);\n        }\n        return ret;\n    }\n{noformat}\n\nwhen x = Integer.MAX_VALUE, adding 1 to it will result in an overflow. As the parameter of regularizedBeta is anyway a double, so it should be changed to something like \"1L + x\" to enforce a long addition.\n\nEdit: Similar things happen btw also in other Distribution implementations, so it should be fixed also there, e.g. BinomialDistribution",
            "The problem is not only related to the Beta function, also the ContinuedFraction.evaluate is numerically unstable.\n\nThe reason the cumulativeProbability returns infinity instead of NaN is because the evaluate return 0.0 when called from Beta.regularizedBeta, which leads to a division by zero. The used default epsilon of 10e-15 seems also quite restrictive, when relaxing the epsilon I got much better results (e.g. with 10e-5 I got a result of 499997).",
            "I further looked into this with relation to MATH-785. First of all, in the original bug report, the reporter mentions that the expected result should be 499999 which is wrong, imho it should be 500000.\n\nAfter implementing the modified Lentz-Thompson algorithm, the results for the BinomialDistribution of large trials show correct results.",
            "The attached diff file shows the (preliminary) implementation of the modified Lentz-Thompson algorithm.\n\nEdit: re-uploaded the diff file as it was broken.\n\nEdit2: the failing unit tests I mentioned before were due to a wrong loop, the latest diff shows no unit test errors.",
            "Fixed in r1341171."
        ],
        "summarized_discussion": "\n\nThe bug reported was related to the Beta.regularizedBeta(...) function returning NaN instead of 1 when using extreme parameters. It was found to be related to MATH-738 and MATH-785. The solution was to modify the Lentz-Thompson algorithm and implement it in the BinomialDistribution class. This was done in r1341171, resolving the issue."
    },
    "Compress_28_src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveInputStream.java_568_592": {
        "src": "@Override\n    public int read(byte[] buf, int offset, int numToRead) throws IOException {\n    \tint totalRead = 0;\n\n        if (hasHitEOF || entryOffset >= entrySize) {\n            return -1;\n        }\n\n        if (currEntry == null) {\n            throw new IllegalStateException(\"No current tar entry\");\n        }\n\n        numToRead = Math.min(numToRead, available());\n        \n        totalRead = is.read(buf, offset, numToRead);\n        count(totalRead);\n        \n        if (totalRead == -1) {\n            hasHitEOF = true;\n        } else {\n            entryOffset += totalRead;\n        }\n\n        return totalRead;\n    }",
        "src_wo_comments": "@ Override public int read ( byte [ ] buf , int offset , int numToRead ) throws IOException { int totalRead = 0 ; if ( hasHitEOF || entryOffset >= entrySize ) { return - 1 ; } if ( currEntry == null ) { throw new IllegalStateException ( \"No current tar entry\" ) ; } numToRead = Math . min ( numToRead , available ( ) ) ; totalRead = is . read ( buf , offset , numToRead ) ; count ( totalRead ) ; if ( totalRead == - 1 ) { hasHitEOF = true ; } else { entryOffset += totalRead ; } return totalRead ; }",
        "fixed_src": "@Override\n    public int read(byte[] buf, int offset, int numToRead) throws IOException {\n    \tint totalRead = 0;\n\n        if (hasHitEOF || entryOffset >= entrySize) {\n            return -1;\n        }\n\n        if (currEntry == null) {\n            throw new IllegalStateException(\"No current tar entry\");\n        }\n\n        numToRead = Math.min(numToRead, available());\n        \n        totalRead = is.read(buf, offset, numToRead);\n        \n        if (totalRead == -1) {\n            if (numToRead > 0) {\n                throw new IOException(\"Truncated TAR archive\");\n            }\n            hasHitEOF = true;\n        } else {\n            count(totalRead);\n            entryOffset += totalRead;\n        }\n\n        return totalRead;\n    }",
        "fixed_src_wo_comments": "@ Override public int read ( byte [ ] buf , int offset , int numToRead ) throws IOException { int totalRead = 0 ; if ( hasHitEOF || entryOffset >= entrySize ) { return - 1 ; } if ( currEntry == null ) { throw new IllegalStateException ( \"No current tar entry\" ) ; } numToRead = Math . min ( numToRead , available ( ) ) ; totalRead = is . read ( buf , offset , numToRead ) ; if ( totalRead == - 1 ) { if ( numToRead > 0 ) { throw new IOException ( \"Truncated TAR archive\" ) ; } hasHitEOF = true ; } else { count ( totalRead ) ; entryOffset += totalRead ; } return totalRead ; }",
        "summary": "TarArchiveInputStream silently finished when unexpected EOF occured",
        "Description": "I just found the following test case didn't raise an IOException as it used to be for a *tar trimmed on purpose* \n\n@Test\n  public void testCorruptedBzip2() throws IOException {\n    String archivePath = PathUtil.join(testdataDir, \"test.tar.bz2\");\n    TarArchiveInputStream input = null;\n    input = new TarArchiveInputStream(new BZip2CompressorInputStream(\n        GoogleFile.SYSTEM.newInputStream(archivePath), true));\n    ArchiveEntry nextMatchedEntry = input.getNextEntry();\n    while (nextMatchedEntry != null) {\n      logger.infofmt(\"Extracting %s\", nextMatchedEntry.getName());\n      String outputPath = PathUtil.join(\"/tmp/\", nextMatchedEntry.getName());\n      OutputStream out = new FileOutputStream(outputPath);\n      ByteStreams.copy(input, out);\n      out.close();\n      nextMatchedEntry = input.getNextEntry();\n    }\n  }",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-279",
        "comments": [
            "The complete.tar (golden) will generate:\nLICENSE.txt 11357\nNOTICE.txt 433\nREADME.txt 1632\nRELEASE-NOTES.txt *2648*\n\nThe command tar xf trim.tar will generate:\nLICENSE.txt 11357\nNOTICE.txt 433\nREADME.txt 1632\nRELEASE-NOTES.txt *2560*\n\nUsing the code above will generate:\nLICENSE.txt 11357\nNOTICE.txt 433\nREADME.txt 1632\nRELEASE-NOTES.txt *2616*\n",
            "do you recall which version of Compress used to raise an exception?",
            "I ask when it ever worked because I confirm it doesn't throw an IOException now but wonder why it should ever have done so.",
            "Hi Stefan,\n\nI saw this behavior on v1.4. However, I think it is the correct behavior. If you try to use command line 'tar' it will throw the following message:\n\ntar xvf trim.tar \nLICENSE.txt\nNOTICE.txt\nREADME.txt\nRELEASE-NOTES.txt\ntar: Unexpected EOF in archive\ntar: Unexpected EOF in archive\ntar: Error is not recoverable: exiting now\n\nDon't you think the Java version should at least indicate something is wrong maybe with a flag to turn it silent or so..",
            "Surprisingly simple (unless I'm over-looking something) fix.  Patch included.\n\nNote that calling IOUtils#skip (Compress-277) on the stream will have the effect of skipping until EOL, then when it finishes by calling read, it will trigger this exception.\n\nThere are no unit tests in the suite that trigger this though.",
            "I see how my \"should ever have done so\" can be misunderstood.  What I meant to say is: of course there should be an exception but I don't see why the code has ever thrown an exception.  I.e. I don't see where the regression has been introduced.\n\nI'll look into this over the weekend, I've already added a unit test and it doesn't throw any exception in trunk - so the changed skip alone won't help.  [~belugabehr]: on first glance your patch looks good, thanks.",
            "It was probably in the TarBuffer class that I lopped out.",
            "fixed with svn revision 1590361\n\nI used a smaller modification than the proposed patch but it's similar in spirit - thanks!",
            "Even better.  Thanks."
        ],
        "summarized_discussion": "\n\nThe bug in the source code was causing the RELEASE-NOTES.txt file to generate an unexpected EOF in the archive when running the command tar xf trim.tar. The bug was fixed by modifying the TarBuffer class in svn revision 1590361, which involved using a smaller modification than the proposed patch but similar in spirit. This solution was confirmed to be successful."
    },
    "Cli_26_src/java/org/apache/commons/cli/OptionBuilder.java_346_364": {
        "src": "public static Option create(String opt) throws IllegalArgumentException\n    {\n            // create the option\n        Option option = new Option(opt, description);\n\n            // set the option properties\n            option.setLongOpt(longopt);\n            option.setRequired(required);\n            option.setOptionalArg(optionalArg);\n            option.setArgs(numberOfArgs);\n            option.setType(type);\n            option.setValueSeparator(valuesep);\n            option.setArgName(argName);\n            // reset the OptionBuilder properties\n            OptionBuilder.reset();\n\n        // return the Option instance\n        return option;\n    }",
        "src_wo_comments": "public static Option create ( String opt ) throws IllegalArgumentException { Option option = new Option ( opt , description ) ; option . setLongOpt ( longopt ) ; option . setRequired ( required ) ; option . setOptionalArg ( optionalArg ) ; option . setArgs ( numberOfArgs ) ; option . setType ( type ) ; option . setValueSeparator ( valuesep ) ; option . setArgName ( argName ) ; OptionBuilder . reset ( ) ; return option ; }",
        "fixed_src": "public static Option create(String opt) throws IllegalArgumentException\n    {\n        Option option = null;\n        try {\n            // create the option\n            option = new Option(opt, description);\n\n            // set the option properties\n            option.setLongOpt(longopt);\n            option.setRequired(required);\n            option.setOptionalArg(optionalArg);\n            option.setArgs(numberOfArgs);\n            option.setType(type);\n            option.setValueSeparator(valuesep);\n            option.setArgName(argName);\n        } finally {\n            // reset the OptionBuilder properties\n            OptionBuilder.reset();\n        }\n\n        // return the Option instance\n        return option;\n    }",
        "fixed_src_wo_comments": "public static Option create ( String opt ) throws IllegalArgumentException { Option option = null ; try { option = new Option ( opt , description ) ; option . setLongOpt ( longopt ) ; option . setRequired ( required ) ; option . setOptionalArg ( optionalArg ) ; option . setArgs ( numberOfArgs ) ; option . setType ( type ) ; option . setValueSeparator ( valuesep ) ; option . setArgName ( argName ) ; } finally { OptionBuilder . reset ( ) ; } return option ; }",
        "summary": "OptionBuilder is not reseted in case of an IAE at create",
        "Description": "If the call to OptionBuilder.create() fails with an IllegalArgumentException, the OptionBuilder is not resetted and its next usage may contain unwanted settings. Actually this let the CLI-1.2 RCs fail on IBM JDK 6 running on Maven 2.0.10.",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-177",
        "comments": [
            "Patch and unit test.",
            "Applied to 1.2."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to patch the source code and unit test it, and then apply it to version 1.2."
    },
    "Compress_7_src/main/java/org/apache/commons/compress/archivers/tar/TarUtils.java_93_105": {
        "src": "public static String parseName(byte[] buffer, final int offset, final int length) {\n        StringBuffer result = new StringBuffer(length);\n        int          end = offset + length;\n\n        for (int i = offset; i < end; ++i) {\n            if (buffer[i] == 0) {\n                break;\n            }\n            result.append((char) buffer[i]);\n        }\n\n        return result.toString();\n    }",
        "src_wo_comments": "public static String parseName ( byte [ ] buffer , final int offset , final int length ) { StringBuffer result = new StringBuffer ( length ) ; int end = offset + length ; for ( int i = offset ; i < end ; ++ i ) { if ( buffer [ i ] == 0 ) { break ; } result . append ( ( char ) buffer [ i ] ) ; } return result . toString ( ) ; }",
        "fixed_src": "public static String parseName(byte[] buffer, final int offset, final int length) {\n        StringBuffer result = new StringBuffer(length);\n        int          end = offset + length;\n\n        for (int i = offset; i < end; ++i) {\n            byte b = buffer[i];\n            if (b == 0) { // Trailing null\n                break;\n            }\n            result.append((char) (b & 0xFF)); // Allow for sign-extension\n        }\n\n        return result.toString();\n    }",
        "fixed_src_wo_comments": "public static String parseName ( byte [ ] buffer , final int offset , final int length ) { StringBuffer result = new StringBuffer ( length ) ; int end = offset + length ; for ( int i = offset ; i < end ; ++ i ) { byte b = buffer [ i ] ; if ( b == 0 ) { break ; } result . append ( ( char ) ( b & 0xFF ) ) ; } return result . toString ( ) ; }",
        "summary": "TarUtils.parseName does not properly handle characters outside the range 0-127",
        "Description": "if a tarfile contains files with special characters, the names of the tar entries are wrong.\n\nexample:\ncorrect name: 0302-0601-3\u00b1\u00b1\u00b1F06\u00b1W220\u00b1ZB\u00b1LALALA\u00b1\u00b1\u00b1\u00b1\u00b1\u00b1\u00b1\u00b1\u00b1\u00b1CAN\u00b1\u00b1DC\u00b1\u00b1\u00b104\u00b1060302\u00b1MOE.model\nname resolved by TarUtils.parseName: 0302-0101-3\uffb1\uffb1\uffb1F06\uffb1W220\uffb1ZB\uffb1HECKMODUL\uffb1\uffb1\uffb1\uffb1\uffb1\uffb1\uffb1\uffb1\uffb1\uffb1ECE\uffb1\uffb1DC\uffb1\uffb1\uffb107\uffb1060302\uffb1DOERN.model\n\nplease use: \nresult.append(new String(new byte[] { buffer[i] }));\n\ninstead of: \nresult.append((char) buffer[i]);\n\nto solve this encoding problem.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-114",
        "comments": [
            "Could you provide a small sample tar file containing some files with the special names?\n[Perhaps the file contents could be the expected name]\n\nWe can then add the file to the test cases.\n\nThanks!",
            "Forgot to mention:\n\nThe \"String(byte[])\" constructor depends on the default charset encoding, which might not always be what is wanted.",
            "tarfile which includes such files with a filename containing special characters",
            "that's right, but the cast from byte to char is not a beautiful way.\n\nString charsetName = \"ISO-8859-1\";\n                try {\n                    result.append(new String(new byte[] { buffer[i] }, charsetName));\n                } catch (UnsupportedEncodingException e) {\n                    result.append(new String(new byte[] { buffer[i] }));\n                }\n\nwhere charsetName may be set via a system property or sth else by the customer of commons compress.\n\n",
            "example where charset may be set by a setter in TarArchiveInpustream.\nif an UnsupportedEncodingException occurs, the default charset of the system\nis used.\n\nPlease have a look. Hope this helps to solve the problem.\n",
            "Thanks for the test case - unfortunately you did not grant a license to the ASF to use it.\nCould you re-attach it with the option selected please?",
            "same file, but now the license is available",
            "As to how to determine the charset, it looks as though \"ASCII\" or \"ISO-8859-1\" are suitable as the default.\nGnu docs mention \"local variant of ASCII\".",
            "yes, i think \"ISO-8859-1\" seems to be the suitable default value for the charset, too.\n\ni've tested it with several special characters\n\n\u00e4\u00f6\u00fc in german for example",
            "I think there may also be a problem with the TarUtils.formatNameBytes() method, which assumes that String.charAt() can be stored in a byte.\n\nI'll add some round-trip tests for formatNameBytes() / parseName()",
            "Turned out to be easy to fix the round-trip problem - just ensure that the byte entries are treated as unsigned.\nSo no need to worry about charsets.\n\nStill need to check that this works OK when reading from the test tar file.",
            "Now fixed; the test tar file reads OK",
            "I've had a look on your solution. This is a better way to solve this Problem. \nThanks a lot!",
            "Hello,\n\n\nI've checked out the trunk from http://svn.apache.org/repos/asf/commons/proper/compress and run the testRoundTripNames() test from TarUtilsTest. It failed (the last checkName() call with spec. characters). The test was performed on Ubuntu 8.10.\n\nHas the fix been tested on Linux? In which version can find the final fix to this special characters problem?\n\nThanks\n\n\n\n",
            "The test passes for me using Ubuntu 10.4 and OpenJDK 6 - I guess it may even more depend on the Java VM than the OS.  Which flavor of Java are you using, Pavel?",
            "Stefan, thanks for a swift reply,\n\nI'm using Sun JDK 1.6.0_14_b08, but I've just tried it with OpenJDK 1.6.0_0-b12 and have the same result... \n\nIn case it helps: I've checked out the trunc using Eclipse 3.6 (Subversive plugin) and build it using Maven2 plugin\n\nDo you know where I can get a commons-compress.jar (1.1) distro?\n\nthx",
            "A snapshot I compiled myself can be found at http://people.apache.org/~bodewig/commons-compress-1.1-SNAPSHOT.jar and I'll remove it once 1.1 has been released.\n\nThe unit tests pass for me on my Ubuntu system and it's pretty likely it is more of an environment setting thing.  I may also note that the tests pass in the Apache Gump builds both on Linux (Ubuntu 8.4) and Solaris 10.\n\nReturning to the original problem, commons-compress really doesn't implement POSIX tar or even comes close to it.  It mostly lives at the least common denominator of all tar dialects, ustar.  And this means the only characters that are really supported come from the seven bit ASCII set - with anything else you can only hope it works.\n\n",
            "Hi,\n\nI've tested it at the time when the bug was fixed by Sebb on following OS:\n- Ubuntu\n- Windows\n- Mac\n\nand the solution worked fine.\n\ngrz",
            "see last comment. issue was tested on several OS"
        ],
        "summarized_discussion": "\n\nThe bug was related to the \"String(byte[])\" constructor depending on the default charset encoding, which might not always be what is wanted. To solve the problem, a tarfile was provided with files containing special characters, and the cast from byte to char was changed to use the charsetName \"ISO-8859-1\". Additionally, the TarUtils.formatNameBytes() method was changed to ensure that the byte entries are treated as unsigned. The bug was tested on several OS and the solution worked fine."
    },
    "Math_27_src/main/java/org/apache/commons/math3/fraction/Fraction.java_596_598": {
        "src": "public double percentageValue() {\n        return multiply(100).doubleValue();\n    }",
        "src_wo_comments": "public double percentageValue ( ) { return multiply ( 100 ) . doubleValue ( ) ; }",
        "fixed_src": "public double percentageValue() {\n        return 100 * doubleValue();\n    }",
        "fixed_src_wo_comments": "public double percentageValue ( ) { return 100 * doubleValue ( ) ; }",
        "summary": "Fraction percentageValue rare overflow",
        "Description": "The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value.\n\nThe patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-835",
        "comments": [
            "Fixed, as suggested, in revision 1367593.\nThanks for the report.\n"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in revision 1367593, and the report is appreciated."
    },
    "Mockito_38_src/org/mockito/internal/verification/argumentmatching/ArgumentMatchingTool.java_47_49": {
        "src": "private boolean toStringEquals(Matcher m, Object arg) {\n        return StringDescription.toString(m).equals(arg.toString());\n    }",
        "src_wo_comments": "private boolean toStringEquals ( Matcher m , Object arg ) { return StringDescription . toString ( m ) . equals ( arg . toString ( ) ) ; }",
        "fixed_src": "private boolean toStringEquals(Matcher m, Object arg) {\n        return StringDescription.toString(m).equals(arg == null? \"null\" : arg.toString());\n    }",
        "fixed_src_wo_comments": "private boolean toStringEquals ( Matcher m , Object arg ) { return StringDescription . toString ( m ) . equals ( arg == null ? \"null\" : arg . toString ( ) ) ; }",
        "summary": "Generate change list separated by types using labels",
        "Description": "As discussed on the mailing list instead of one big list of \"Improvements\" the change list for the release is divided into change types based on labels. It is required to specify which labels should be considered separately. Some other labels can be excluded (like \"question\" or \"refactoring\").\n\n```\nnew ReleaseNotesBuilderFactory(project)\n(...)\n    .showSeparatelyChangesWithLabelMappings([\"enhancement\": \"Improvements\", \"bug\": \"Fixed bugs\", \"doc\": \"Documentation\"])\n    .ignoreChangesWithLabels([\"refactoring\", \"invalid\", \"question\", \"wontfix\", \"duplicate\"])\n```\n\nThere is also `headerForOtherChanges` method to override default \"Other\" header.\n\nI changed internally `Improvement` to `Change` to better give meaning of change, but it is done in a separate commit, so can be easily reverted/amended if you don't like it (or have a better name).\n\nSample changelog generated for current release. Number of issues is the same as with the old mechanism. Many of them are placed in \"Other\" section as don't have labels assigned.\n- Changes: 20\n  - Improvements: 11\n    - Improved the javadoc example of custom Answer implementation [(#22)](https://github.com/mockito/mockito/pull/22)\n    - Avoided classloader issue when testing in Eclipse plugins environment [(#24)](https://github.com/mockito/mockito/pull/24)\n    - Removed .java files from main mockito jar artifacts [(#28)](https://github.com/mockito/mockito/pull/28)\n    - Smarter constructor injection by choosing \"biggest\" constructor instead of the default one [(#29)](https://github.com/mockito/mockito/pull/29)\n    - Deep stub style mocks can be serialized [(#30)](https://github.com/mockito/mockito/pull/30)\n    - Fixed the behavior of compareTo method of the mock objects [(#32)](https://github.com/mockito/mockito/pull/32)\n    - New \"MockingDetails.getInvocations\" method for inspecting what happened with the mock [(#10)](https://github.com/mockito/mockito/pull/10)\n    - Mock serialization/deserialization across classloader/JVM [(#5)](https://github.com/mockito/mockito/pull/5)\n    - Improved MockitoTestNGListener by making it reset argument captors before each test [(#6)](https://github.com/mockito/mockito/pull/6)\n    - Improve NoInteractionsWanted report to include the name of the mock [(#63)](https://github.com/mockito/mockito/pull/63)\n    - New \"getArgumentAt\" method for convenient implementation of custom Answers [(#41)](https://github.com/mockito/mockito/pull/41)\n  - Fixed bugs: 1\n    - Allow calling real implementation of jdk8 extension methods [(#39)](https://github.com/mockito/mockito/pull/39)\n  - Documentation: 0\n  - Other: 8\n    - Fixed wrong javadoc for AdditionalAnswers [(#56)](https://github.com/mockito/mockito/pull/56)\n    - Added useful links to README.md [(#58)](https://github.com/mockito/mockito/pull/58)\n    - Deprecated timeout().never(), in line with timeout().atMost() [(#14)](https://github.com/mockito/mockito/pull/14)\n    - Verification with timout measures time more more accurately [(#15)](https://github.com/mockito/mockito/pull/15)\n    - New \"then\" method for BDD-style interaction testing [(#38)](https://github.com/mockito/mockito/pull/38)\n    - Enabled continuous integration with Travis CI and coverage tracking with coveralls [(#18)](https://github.com/mockito/mockito/pull/18)\n    - Coveralls coverage tracking tool allows Mockito source code preview [(#62)](https://github.com/mockito/mockito/pull/62)\n    - Improved behavior of EqualsWithDelta with regards to null handling [(#21)](https://github.com/mockito/mockito/pull/21)\n\nI have to take a look why some labelled issues were ommited (in both mechanisms).\n\nWhat do you think about that?\n",
        "issue_url": null,
        "comments": null,
        "summarized_discussion": "\n\nWithout more information, it is not possible to summarize the solution to the bug."
    },
    "JacksonDatabind_42_src/main/java/com/fasterxml/jackson/databind/deser/std/FromStringDeserializer.java_277_285": {
        "src": "@Override\n        protected Object _deserializeFromEmptyString() throws IOException {\n            // As per [databind#398], URI requires special handling\n            if (_kind == STD_URI) {\n                return URI.create(\"\");\n            }\n            // As per [databind#1123], Locale too\n            return super._deserializeFromEmptyString();\n        }",
        "src_wo_comments": "@ Override protected Object _deserializeFromEmptyString ( ) throws IOException { if ( _kind == STD_URI ) { return URI . create ( \"\" ) ; } return super . _deserializeFromEmptyString ( ) ; }",
        "fixed_src": "@Override\n        protected Object _deserializeFromEmptyString() throws IOException {\n            // As per [databind#398], URI requires special handling\n            if (_kind == STD_URI) {\n                return URI.create(\"\");\n            }\n            // As per [databind#1123], Locale too\n            if (_kind == STD_LOCALE) {\n                return Locale.ROOT;\n            }\n            return super._deserializeFromEmptyString();\n        }",
        "fixed_src_wo_comments": "@ Override protected Object _deserializeFromEmptyString ( ) throws IOException { if ( _kind == STD_URI ) { return URI . create ( \"\" ) ; } if ( _kind == STD_LOCALE ) { return Locale . ROOT ; } return super . _deserializeFromEmptyString ( ) ; }",
        "summary": "Serializing and Deserializing Locale.ROOT",
        "Description": "Serializing and Deserializing Locale objects seems to work just fine, until you try on the Root Locale.\nIt writes it out as an empty string and when it reads it in, the value is null\n\n```\n@Test\n    public void testLocaleDeserialization() throws IOException {\n        ObjectMapper objectMapper = new ObjectMapper();\n        Locale root = Locale.ROOT;\n        String json = objectMapper.writeValueAsString(root);\n        System.out.printf(\"Root Locale: '%s'\", json);\n        Locale actual = objectMapper.readValue(json, Locale.class);\n        Assert.assertEquals(root, actual);\n    }\n```\n\nHere is the output:\nRoot Locale: '\"\"'\njava.lang.AssertionError: \nExpected :\nActual   :null\n",
        "issue_url": null,
        "comments": [
            {
                "content": "This might not be backwards compatible, but in Java 7+ you can use Locale.forLanguageTag(String) to deserialize and Locale.toLanguageTag() to serialize.\nFor the time being I've added a custom serializer and deserializer that does exactly that.\n"
            },
            {
                "content": "@hookumsnivy thank you for reporting this & for the suggestion of fix. I'll have to think about the solution; my main concern is just that while we are moving to require JDK7 baseline, there is bit of transition where we try to keep runtime JDK6 compatible. But adding special handling for \"\" seems simple enough even using a work-around.\n"
            },
            {
                "content": "@cowtowncoder It looks like FromStringDeserializer already has special handling for \"\" in the case of URIs.\n"
            },
            {
                "content": "@hookumsnivy right, that's not a problem from my perspective, can add it once I have time to work on this.\n"
            },
            {
                "content": "Ok... so the problem really is the special handling for empty String. Gotcha.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to use Locale.forLanguageTag(String) to deserialize and Locale.toLanguageTag() to serialize in Java 7+ and to add special handling for empty String in the FromStringDeserializer."
    },
    "Math_70_src/main/java/org/apache/commons/math/analysis/solvers/BisectionSolver.java_70_73": {
        "src": "public double solve(final UnivariateRealFunction f, double min, double max, double initial)\n        throws MaxIterationsExceededException, FunctionEvaluationException {\n        return solve(min, max);\n    }",
        "src_wo_comments": "public double solve ( final UnivariateRealFunction f , double min , double max , double initial ) throws MaxIterationsExceededException , FunctionEvaluationException { return solve ( min , max ) ; }",
        "fixed_src": "public double solve(final UnivariateRealFunction f, double min, double max, double initial)\n        throws MaxIterationsExceededException, FunctionEvaluationException {\n        return solve(f, min, max);\n    }",
        "fixed_src_wo_comments": "public double solve ( final UnivariateRealFunction f , double min , double max , double initial ) throws MaxIterationsExceededException , FunctionEvaluationException { return solve ( f , min , max ) ; }",
        "summary": "BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException",
        "Description": "Method \n\n    BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)  \n\ninvokes \n\n    BisectionSolver.solve(double min, double max) \n\nwhich throws NullPointerException, as member variable\n\n    UnivariateRealSolverImpl.f \n\nis null.\n\nInstead the method:\n\n    BisectionSolver.solve(final UnivariateRealFunction f, double min, double max)\n\nshould be called.\n\nSteps to reproduce:\n\ninvoke:\n\n     new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);\n\nNullPointerException will be thrown.\n\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-369",
        "comments": [
            "Fixed in subversion repository as of r940565.\nThanks for the report and for the fix.",
            "Closing issue as it was included in version 2.2, which has been released"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in version 2.2 of the source code, which has been released and is available in the subversion repository as of r940565. The issue has been closed."
    },
    "JacksonDatabind_39_src/main/java/com/fasterxml/jackson/databind/deser/std/NullifyingDeserializer.java_30_37": {
        "src": "@Override\n    public Object deserialize(JsonParser p, DeserializationContext ctxt) throws IOException\n    {\n        // 29-Jan-2016, tatu: Simple skipping for all other tokens, but FIELD_NAME bit\n        //    special unfortunately\n            p.skipChildren();\n        return null;\n    }",
        "src_wo_comments": "@ Override public Object deserialize ( JsonParser p , DeserializationContext ctxt ) throws IOException { p . skipChildren ( ) ; return null ; }",
        "fixed_src": "@Override\n    public Object deserialize(JsonParser p, DeserializationContext ctxt) throws IOException\n    {\n        // 29-Jan-2016, tatu: Simple skipping for all other tokens, but FIELD_NAME bit\n        //    special unfortunately\n        if (p.hasToken(JsonToken.FIELD_NAME)) {\n            while (true) {\n                JsonToken t = p.nextToken();\n                if ((t == null) || (t == JsonToken.END_OBJECT)) {\n                    break;\n                }\n                p.skipChildren();\n            }\n        } else {\n            p.skipChildren();\n        }\n        return null;\n    }",
        "fixed_src_wo_comments": "@ Override public Object deserialize ( JsonParser p , DeserializationContext ctxt ) throws IOException { if ( p . hasToken ( JsonToken . FIELD_NAME ) ) { while ( true ) { JsonToken t = p . nextToken ( ) ; if ( ( t == null ) || ( t == JsonToken . END_OBJECT ) ) { break ; } p . skipChildren ( ) ; } } else { p . skipChildren ( ) ; } return null ; }",
        "summary": "Jackson not continue to parse after DeserializationFeature.FAIL_ON_INVALID_SUBTYPE error",
        "Description": "After FAIL_ON_INVALID_SUBTYPE error, jackson should continue to parse, but seems jackson doesn't.\n\nThe output:\n\n```\nCallRecord [version=0.0, application=123, ] // doesn't read item2 which is valid\nCallRecord [version=0.0, application=123, ]\nCallRecord [version=0.0, ] // doesn't read application after invalid item.\n```\n\n``` jaca\n@JsonInclude(Include.NON_NULL)\npublic class CallRecord {\n    public float version;\n    public String application;\n    public Item item;\n    public Item item2;\n    public CallRecord() {}\n\n    public static void main(final String[] args) throws IOException {\n        final ObjectMapper objectMapper = new ObjectMapper().disable(DeserializationFeature.FAIL_ON_INVALID_SUBTYPE,\n                DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, DeserializationFeature.FAIL_ON_IGNORED_PROPERTIES);\n        final CallRecord call = new CallRecord();\n\n        final Event event = new Event();\n        event.location = \"location1\";\n        call.item = event;\n        call.item2 = event;\n        call.application = \"123\";\n        // System.out.println(objectMapper.writeValueAsString(call));\n        String json =\n                \"{\\\"version\\\":0.0,\\\"application\\\":\\\"123\\\",\\\"item\\\":{\\\"type\\\":\\\"xevent\\\",\\\"location\\\":\\\"location1\\\"},\\\"item2\\\":{\\\"type\\\":\\\"event\\\",\\\"location\\\":\\\"location1\\\"}}\";\n        // can't read item2 - which is valid\n        System.out.println(objectMapper.readValue(json, CallRecord.class));\n\n        json = \"{\\\"version\\\":0.0,\\\"application\\\":\\\"123\\\"},{\\\"item\\\":{\\\"type\\\":\\\"xevent\\\",\\\"location\\\":\\\"location1\\\"}\";\n        System.out.println(objectMapper.readValue(json, CallRecord.class));\n\n        json = \"{\\\"item\\\":{\\\"type\\\":\\\"xevent\\\",\\\"location\\\":\\\"location1\\\"}, \\\"version\\\":0.0,\\\"application\\\":\\\"123\\\"}\";\n        // order matters: move item to the fornt, now it can't read application property\n        System.out.println(objectMapper.readValue(json, CallRecord.class));\n    }\n    @Override\n    public String toString() {\n        final StringBuilder builder = new StringBuilder();\n        builder.append(\"CallRecord [version=\").append(version).append(\", \");\n        if (application != null) {\n            builder.append(\"application=\").append(application).append(\", \");\n        }\n        if (item != null) {\n            builder.append(\"item=\").append(item);\n        }\n        builder.append(\"]\");\n        return builder.toString();\n    }\n}\n\n@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = \"type\", visible = true)\n@JsonSubTypes({@Type(value = Event.class, name = Event.TYPE)})\npublic interface Item {\n}\n\npublic final class Event implements Item {\n    public String location;\n    public static final String TYPE = \"event\";\n    public Event() {}\n}\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this. I'll see if I can figure out what is happening.\n"
            }
        ],
        "summarized_discussion": "\n\nUnfortunately, without more information it is not possible to determine the solution to the bug."
    },
    "JacksonDatabind_5_src/main/java/com/fasterxml/jackson/databind/introspect/AnnotatedClass.java_634_662": {
        "src": "protected void _addMethodMixIns(Class<?> targetClass, AnnotatedMethodMap methods,\n            Class<?> mixInCls, AnnotatedMethodMap mixIns)\n    {\n        List<Class<?>> parents = new ArrayList<Class<?>>();\n        parents.add(mixInCls);\n        ClassUtil.findSuperTypes(mixInCls, targetClass, parents);\n        for (Class<?> mixin : parents) {\n            for (Method m : mixin.getDeclaredMethods()) {\n                if (!_isIncludableMemberMethod(m)) {\n                    continue;\n                }\n                AnnotatedMethod am = methods.find(m);\n                /* Do we already have a method to augment (from sub-class\n                 * that will mask this mixIn)? If so, add if visible\n                 * without masking (no such annotation)\n                 */\n                if (am != null) {\n                    _addMixUnders(m, am);\n                    /* Otherwise will have precedence, but must wait\n                     * until we find the real method (mixIn methods are\n                     * just placeholder, can't be called)\n                     */\n                } else {\n                    // Well, or, as per [Issue#515], multi-level merge within mixins...\n                        mixIns.add(_constructMethod(m));\n                }\n            }\n        }\n    }",
        "src_wo_comments": "protected void _addMethodMixIns ( Class < ? > targetClass , AnnotatedMethodMap methods , Class < ? > mixInCls , AnnotatedMethodMap mixIns ) { List < Class < ? > > parents = new ArrayList < Class < ? > > ( ) ; parents . add ( mixInCls ) ; ClassUtil . findSuperTypes ( mixInCls , targetClass , parents ) ; for ( Class < ? > mixin : parents ) { for ( Method m : mixin . getDeclaredMethods ( ) ) { if ( ! _isIncludableMemberMethod ( m ) ) { continue ; } AnnotatedMethod am = methods . find ( m ) ; if ( am != null ) { _addMixUnders ( m , am ) ; } else { mixIns . add ( _constructMethod ( m ) ) ; } } } }",
        "fixed_src": "protected void _addMethodMixIns(Class<?> targetClass, AnnotatedMethodMap methods,\n            Class<?> mixInCls, AnnotatedMethodMap mixIns)\n    {\n        List<Class<?>> parents = new ArrayList<Class<?>>();\n        parents.add(mixInCls);\n        ClassUtil.findSuperTypes(mixInCls, targetClass, parents);\n        for (Class<?> mixin : parents) {\n            for (Method m : mixin.getDeclaredMethods()) {\n                if (!_isIncludableMemberMethod(m)) {\n                    continue;\n                }\n                AnnotatedMethod am = methods.find(m);\n                /* Do we already have a method to augment (from sub-class\n                 * that will mask this mixIn)? If so, add if visible\n                 * without masking (no such annotation)\n                 */\n                if (am != null) {\n                    _addMixUnders(m, am);\n                    /* Otherwise will have precedence, but must wait\n                     * until we find the real method (mixIn methods are\n                     * just placeholder, can't be called)\n                     */\n                } else {\n                    // Well, or, as per [Issue#515], multi-level merge within mixins...\n                    am = mixIns.find(m);\n                    if (am != null) {\n                        _addMixUnders(m, am);\n                    } else {\n                        mixIns.add(_constructMethod(m));\n                    }\n                }\n            }\n        }\n    }",
        "fixed_src_wo_comments": "protected void _addMethodMixIns ( Class < ? > targetClass , AnnotatedMethodMap methods , Class < ? > mixInCls , AnnotatedMethodMap mixIns ) { List < Class < ? > > parents = new ArrayList < Class < ? > > ( ) ; parents . add ( mixInCls ) ; ClassUtil . findSuperTypes ( mixInCls , targetClass , parents ) ; for ( Class < ? > mixin : parents ) { for ( Method m : mixin . getDeclaredMethods ( ) ) { if ( ! _isIncludableMemberMethod ( m ) ) { continue ; } AnnotatedMethod am = methods . find ( m ) ; if ( am != null ) { _addMixUnders ( m , am ) ; } else { am = mixIns . find ( m ) ; if ( am != null ) { _addMixUnders ( m , am ) ; } else { mixIns . add ( _constructMethod ( m ) ) ; } } } } }",
        "summary": "Mixin annotations lost when using a mixin class hierarchy with non-mixin interfaces",
        "Description": "In summary, mixin annotations are lost when Jackson scans a parent mixin class with Json annotations followed by an interface implemented by the parent mixin class that does not have the same Json annotations.\nJackson version: 2.4.0\n\nDetail:\nI have the following class structure\n\n``` java\npublic interface Contact {\n    String getCity();\n}\n\npublic class ContactImpl implements Contact {\n    public String getCity() { ... }\n}\n\npublic class ContactMixin implements Contact {\n    @JsonProperty\n    public String getCity() { return null; }\n}\n\npublic interface Person extends Contact {}\n\npublic class PersonImpl extends ContactImpl implements Person {}\n\npublic class PersonMixin extends ContactMixin implements Person {}\n```\n\nand I configure a module as\n\n``` java\n// There are other getters/properties in the Impl class that do not need to be serialized and so\n// I am using the Mixin to match the interface and explicitly annotate all the inherited methods\nmodule.disable(MapperFeature.ALLOW_FINAL_FIELDS_AS_MUTATORS)\n    .disable(MapperFeature.AUTO_DETECT_FIELDS)\n    .disable(MapperFeature.AUTO_DETECT_GETTERS)\n    .disable(MapperFeature.AUTO_DETECT_IS_GETTERS)\n    .disable(MapperFeature.INFER_PROPERTY_MUTATORS);\nmodule.setMixInAnnotation(Person.class, PersonMixin.class);\n```\n\nWhen a `PersonImpl` instance is serialized, `city` is not included.\n\nI debugged the code and this is what happens:\nIn `AnnotatedClass.resolveMemberMethods()` the supertypes of `PersonImpl` are `[Person.class, Contact.class, ContactImpl.class]` in that order.\n\nIt starts with `Person` for which it finds `PersonMixin` and proceeds to `AnnotatedClass._addMethodMixIns()`. Here the `parents` list has `[PersonMixin, ContactMixin, Contact]`. When it processes `ContactMixin` it adds `getCity()` with the `JsonProperty` annotation. Then it processes `Contact`, doesn't find `getCity()` in `methods` map and so creates a new `AnnotatedMethod` for `getCity()` with the one from the interface which has no annotation which replaces the one from `ContactMixin`\n\nThe workaround for this issue is to explicitly add any parent mixins to the module i.e.\n\n``` java\nmodule.setMixInAnnotation(Contact.class, ContactMixin.class);\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "I've found a previously working scenario related to a MixIn annotation hierarchy has broken in 2.4.2 and it may be caused by this fix. #515 fixes the case where the `parents` list has a MixIn class before an interface. If the interface appears before the MixIn, then MixIn annotations are no longer applied, this used to work in 2.4.0.\n\nUsing the same example as above, assume that the `parents` list for `Person` has `[PersonMixIn, Contact, ContactMixIn]`. `Contact.getCity()` is processed by `mixIns.add(_constructMethod(m))` and `ContactMixIn.getCity()` is processed by `_addMixUnders(m, am)`.\n\nThen `AnnotatedClass.resolveMemberMethods()` processes `Contact`. The `parents` list for `Contact` has `ContactMixIn` and `ContactMixIn.getCity()` is once again processed by `_addMixUnders(m, am)`.\nAfter processing the MixIn, the call to `_addMixOvers()` in `_addMemberMethods()` would apply the annotations to `Contact.getCity()`. This method is still called in 2.4.2 but the MixIn annotations from ContactMixIn are not applied.\n"
            },
            {
                "content": "Hi Tatu,\n\nI believe this fix has broken some existing functionality. I have added comments to the issue but I do not have rights to reopen the issue.\n\nThanks,\nSteve\n\nOn Wednesday, August 13, 2014 4:05 PM, Tatu Saloranta notifications@github.com wrote:\n\nClosed #515 via fd0f1fe.\n\u2014\nReply to this email directly or view it on GitHub.\n"
            },
            {
                "content": "@stevebread If you could file a new issue, that'd be great -- in general re-opening is problematic anyway since it will be more difficult to track which release addressed which issue. At least if fixes have been released (as is the case here).\nYou can just copy the description of the new problem from above, and refer to this issue from new one.\n"
            },
            {
                "content": "Done. #529\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to file a new issue (#529) and refer to the original issue (#515). The description of the new problem should be copied from the discussion."
    },
    "Codec_18_src/main/java/org/apache/commons/codec/binary/StringUtils.java_71_82": {
        "src": "public static boolean equals(final CharSequence cs1, final CharSequence cs2) {\n        if (cs1 == cs2) {\n            return true;\n        }\n        if (cs1 == null || cs2 == null) {\n            return false;\n        }\n        if (cs1 instanceof String && cs2 instanceof String) {\n            return cs1.equals(cs2);\n        }\n        return CharSequenceUtils.regionMatches(cs1, false, 0, cs2, 0, Math.max(cs1.length(), cs2.length()));\n    }",
        "src_wo_comments": "public static boolean equals ( final CharSequence cs1 , final CharSequence cs2 ) { if ( cs1 == cs2 ) { return true ; } if ( cs1 == null || cs2 == null ) { return false ; } if ( cs1 instanceof String && cs2 instanceof String ) { return cs1 . equals ( cs2 ) ; } return CharSequenceUtils . regionMatches ( cs1 , false , 0 , cs2 , 0 , Math . max ( cs1 . length ( ) , cs2 . length ( ) ) ) ; }",
        "fixed_src": "public static boolean equals(final CharSequence cs1, final CharSequence cs2) {\n        if (cs1 == cs2) {\n            return true;\n        }\n        if (cs1 == null || cs2 == null) {\n            return false;\n        }\n        if (cs1 instanceof String && cs2 instanceof String) {\n            return cs1.equals(cs2);\n        }\n        return cs1.length() == cs2.length() && CharSequenceUtils.regionMatches(cs1, false, 0, cs2, 0, cs1.length());\n    }",
        "fixed_src_wo_comments": "public static boolean equals ( final CharSequence cs1 , final CharSequence cs2 ) { if ( cs1 == cs2 ) { return true ; } if ( cs1 == null || cs2 == null ) { return false ; } if ( cs1 instanceof String && cs2 instanceof String ) { return cs1 . equals ( cs2 ) ; } return cs1 . length ( ) == cs2 . length ( ) && CharSequenceUtils . regionMatches ( cs1 , false , 0 , cs2 , 0 , cs1 . length ( ) ) ; }",
        "summary": "StringUtils.equals(CharSequence cs1, CharSequence cs2) can fail with String Index OBE",
        "Description": "StringUtils.equals(CharSequence cs1, CharSequence cs2) fails with String Index OBE if the two sequences are different lengths.",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-231",
        "comments": [
            "URL: http://svn.apache.org/viewvc?rev=1788777&view=rev\nLog:\nCODEC-231 StringUtils.equals(CharSequence cs1, CharSequence cs2) can fail with String Index OBE\n\nModified:\n    commons/proper/codec/trunk/src/changes/changes.xml\n    commons/proper/codec/trunk/src/main/java/org/apache/commons/codec/binary/StringUtils.java\n    commons/proper/codec/trunk/src/test/java/org/apache/commons/codec/binary/StringUtilsTest.java\n"
        ],
        "summarized_discussion": "\n\nThe solution to the bug in the source code was to modify the files commons/proper/codec/trunk/src/changes/changes.xml, commons/proper/codec/trunk/src/main/java/org/apache/commons/codec/binary/StringUtils.java, and commons/proper/codec/trunk/src/test/java/org/apache/commons/codec/binary/StringUtilsTest.java."
    },
    "JacksonDatabind_97_src/main/java/com/fasterxml/jackson/databind/node/POJONode.java_104_116": {
        "src": "@Override\n    public final void serialize(JsonGenerator gen, SerializerProvider ctxt) throws IOException\n    {\n        if (_value == null) {\n            ctxt.defaultSerializeNull(gen);\n        } else if (_value instanceof JsonSerializable) {\n            ((JsonSerializable) _value).serialize(gen, ctxt);\n        } else {\n            // 25-May-2018, tatu: [databind#1991] do not call via generator but through context;\n            //    this to preserve contextual information\n            gen.writeObject(_value);\n        }\n    }",
        "src_wo_comments": "@ Override public final void serialize ( JsonGenerator gen , SerializerProvider ctxt ) throws IOException { if ( _value == null ) { ctxt . defaultSerializeNull ( gen ) ; } else if ( _value instanceof JsonSerializable ) { ( ( JsonSerializable ) _value ) . serialize ( gen , ctxt ) ; } else { gen . writeObject ( _value ) ; } }",
        "fixed_src": "@Override\n    public final void serialize(JsonGenerator gen, SerializerProvider ctxt) throws IOException\n    {\n        if (_value == null) {\n            ctxt.defaultSerializeNull(gen);\n        } else if (_value instanceof JsonSerializable) {\n            ((JsonSerializable) _value).serialize(gen, ctxt);\n        } else {\n            // 25-May-2018, tatu: [databind#1991] do not call via generator but through context;\n            //    this to preserve contextual information\n            ctxt.defaultSerializeValue(_value, gen);\n        }\n    }",
        "fixed_src_wo_comments": "@ Override public final void serialize ( JsonGenerator gen , SerializerProvider ctxt ) throws IOException { if ( _value == null ) { ctxt . defaultSerializeNull ( gen ) ; } else if ( _value instanceof JsonSerializable ) { ( ( JsonSerializable ) _value ) . serialize ( gen , ctxt ) ; } else { ctxt . defaultSerializeValue ( _value , gen ) ; } }",
        "summary": "Context attributes are not passed/available to custom serializer if object is in POJO",
        "Description": "Below is a test case where I create a custom serializer and use it to serialize an object 1) in a HashMap and 2) in an ObjectNode. In both cases I pass attribute to the serializer like this:\r\n`mapper.writer().withAttribute(\"myAttr\", \"Hello!\")`\r\nSerializing HashMap works as expected, but during ObjectNode serialization the attribute is null . It seems that in both cases the custom serializer should get access to the passed attribute and so both lines in the output should contain \"Hello!\"\r\n\r\nProduced output from running testCase.test()\r\n```\r\n{\"data\":{\"aStr\":\"The value is: Hello!\"}}\r\n{\"data\":{\"aStr\":\"The value is: NULL\"}}\r\n\r\n```\r\nTest case:\r\n\r\n```\r\nimport com.fasterxml.jackson.core.JsonGenerator;\r\nimport com.fasterxml.jackson.databind.ObjectMapper;\r\nimport com.fasterxml.jackson.databind.SerializerProvider;\r\nimport com.fasterxml.jackson.databind.annotation.JsonSerialize;\r\nimport com.fasterxml.jackson.databind.node.ObjectNode;\r\nimport com.fasterxml.jackson.databind.ser.std.StdSerializer;\r\n\r\nimport java.io.IOException;\r\nimport java.util.HashMap;\r\nimport java.util.Map;\r\n\r\npublic class TestCase {\r\n  public final static ObjectMapper mapper = new ObjectMapper();\r\n\r\n  @JsonSerialize(using = TestCase.CustomSer.class)\r\n  public static class Data {\r\n    public String aStr;\r\n  }\r\n\r\n  public static class CustomSer extends StdSerializer<Data> {\r\n    public CustomSer() {\r\n      super(Data.class);\r\n    }\r\n\r\n    @Override\r\n    public void serialize(Data value, JsonGenerator gen, SerializerProvider provider) throws IOException {\r\n      String attrStr = (String) provider.getAttribute(\"myAttr\");\r\n      gen.writeStartObject();\r\n      gen.writeObjectField(\"aStr\", \"The value is: \" + (attrStr == null ? \"NULL\" : attrStr));\r\n      gen.writeEndObject();\r\n    }\r\n  }\r\n\r\n  public static void test() throws IOException {\r\n    Data data = new Data();\r\n    data.aStr = \"Hello\";\r\n\r\n    Map<String, Object> mapTest = new HashMap<>();\r\n    mapTest.put(\"data\", data);\r\n\r\n    ObjectNode treeTest = mapper.createObjectNode();\r\n    treeTest.putPOJO(\"data\", data);\r\n\r\n    String mapOut = mapper.writer().withAttribute(\"myAttr\", \"Hello!\").writeValueAsString(mapTest);\r\n    System.out.println(mapOut);\r\n\r\n    String treeOut = mapper.writer().withAttribute(\"myAttr\", \"Hello!\").writeValueAsString(treeTest);\r\n    System.out.println(treeOut);\r\n  }\r\n}\r\n\r\n```\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Version 2.9.5"
            },
            {
                "content": "Thank you for reporting this. I agree in that context should be retained and attributes available.\r\n\r\nThere may be technical problems in achieving that with 2.x since there is unfortunately no way to pass them through methods in `JsonGenerator`. This is something that is solved in 3.x (`master`), but is not something that can be retrofitted easily.\r\n\r\nBut I hope to see if there might something simple to do; it really depends on how delegation works for `POJONode`s and serialization.\r\n\r\n"
            },
            {
                "content": "A hack that worked for me is to use `writeValue(Writer, Object)` that allows you to pass on a custom Writer that can carry context.\r\nExample:\r\n```\r\nclass MyWriter extends StringWriter {\r\n    MyWriter(Object context) {\r\n        this.context = context;\r\n    }\r\n    Object getContext() {\r\n        return context;\r\n    }\r\n}\r\n```\r\nIn the custom serializer, you can get hold of the context through the Writer\r\n```\r\npublic void serialize(Data value, JsonGenerator gen, SerializerProvider provider) throws IOException {\r\n    Object context = ((MyWriter) get.getOutputTarget()).getContext();\r\n    ........\r\n}\r\n```\r\nAnd use it as below\r\n\r\n```\r\n      Writer writer = new MyWriter(WHAT_EVER);\r\n      mapper.writer().writeValue(writer, mapTest);\r\n      String serializedStr = writer.toString();\r\n```\r\n"
            },
            {
                "content": "@SrinivasKothuri Thank you for sharing this interesting technique! Yes, that allows passing of information. :)\r\n\r\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug in version 2.9.5 of the source code is that there is no way to pass attributes through methods in JsonGenerator. A hack that can be used to solve this bug is to use the writeValue(Writer, Object) method and pass on a custom Writer that can carry context. This custom Writer can then be accessed in the custom serializer to get the context and use it."
    },
    "Compress_45_src/main/java/org/apache/commons/compress/archivers/tar/TarUtils.java_474_492": {
        "src": "public static int formatLongOctalOrBinaryBytes(\n        final long value, final byte[] buf, final int offset, final int length) {\n\n        // Check whether we are dealing with UID/GID or SIZE field\n        final long maxAsOctalChar = length == TarConstants.UIDLEN ? TarConstants.MAXID : TarConstants.MAXSIZE;\n\n        final boolean negative = value < 0;\n        if (!negative && value <= maxAsOctalChar) { // OK to store as octal chars\n            return formatLongOctalBytes(value, buf, offset, length);\n        }\n\n        if (length < 9) {\n            formatLongBinary(value, buf, offset, length, negative);\n        }\n        formatBigIntegerBinary(value, buf, offset, length, negative);\n\n        buf[offset] = (byte) (negative ? 0xff : 0x80);\n        return offset + length;\n    }",
        "src_wo_comments": "public static int formatLongOctalOrBinaryBytes ( final long value , final byte [ ] buf , final int offset , final int length ) { final long maxAsOctalChar = length == TarConstants . UIDLEN ? TarConstants . MAXID : TarConstants . MAXSIZE ; final boolean negative = value < 0 ; if ( ! negative && value <= maxAsOctalChar ) { return formatLongOctalBytes ( value , buf , offset , length ) ; } if ( length < 9 ) { formatLongBinary ( value , buf , offset , length , negative ) ; } formatBigIntegerBinary ( value , buf , offset , length , negative ) ; buf [ offset ] = ( byte ) ( negative ? 0xff : 0x80 ) ; return offset + length ; }",
        "fixed_src": "public static int formatLongOctalOrBinaryBytes(\n        final long value, final byte[] buf, final int offset, final int length) {\n\n        // Check whether we are dealing with UID/GID or SIZE field\n        final long maxAsOctalChar = length == TarConstants.UIDLEN ? TarConstants.MAXID : TarConstants.MAXSIZE;\n\n        final boolean negative = value < 0;\n        if (!negative && value <= maxAsOctalChar) { // OK to store as octal chars\n            return formatLongOctalBytes(value, buf, offset, length);\n        }\n\n        if (length < 9) {\n            formatLongBinary(value, buf, offset, length, negative);\n        } else {\n            formatBigIntegerBinary(value, buf, offset, length, negative);\n        }\n\n        buf[offset] = (byte) (negative ? 0xff : 0x80);\n        return offset + length;\n    }",
        "fixed_src_wo_comments": "public static int formatLongOctalOrBinaryBytes ( final long value , final byte [ ] buf , final int offset , final int length ) { final long maxAsOctalChar = length == TarConstants . UIDLEN ? TarConstants . MAXID : TarConstants . MAXSIZE ; final boolean negative = value < 0 ; if ( ! negative && value <= maxAsOctalChar ) { return formatLongOctalBytes ( value , buf , offset , length ) ; } if ( length < 9 ) { formatLongBinary ( value , buf , offset , length , negative ) ; } else { formatBigIntegerBinary ( value , buf , offset , length , negative ) ; } buf [ offset ] = ( byte ) ( negative ? 0xff : 0x80 ) ; return offset + length ; }",
        "summary": "TarUtils.formatLongOctalOrBinaryBytes never uses result of formatLongBinary",
        "Description": "if the length < 9, formatLongBinary is executed, then overwritten by the results of formatBigIntegerBinary. \n\nIf the results are not ignored, a unit test would fail.\n\nAlso, do the binary hacks  need to support negative numbers? ",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-411",
        "comments": [
            "I have fixed various issues in the handling of base256 encoded long numbers (I saw that negative numbers are used for STAR timestamps from before the epoch).\n\nOne of the issues involved the maximum size of a number.  The number of bits available to represent the number is one less than the total number of bits available.  Thus, an eight byte field can hold up to a 63 bit 2's-complement integer.  \nFurther, the range of such a number is from -2 ^n-1^ to 2 ^n-1^ -1 , which can cause range overflows.  ",
            "We deliberately store negative numbers in binary fields, yes.\n\nYou are certainly correct the code looks weird, {{formatBigIntegerBinary}} should probably be executed inside an {{else}} branch.\n\nIt's been a long time since I've last looked at the way binary numbers are stored, but looking at the code we don't use the most significant bit to signal negative numbers but the first byte - which is 0xff for negative numbers and only the remaining 7 bytes are used. But we can use all 56 bits of these 7 bytes (assuming length is 8). \n\nAs we are calculating the twos-complement in {{formatLongBinary}} I think we should be safe, if we did it correctly :-) Fix will be available soon.\n\nAlso\n\n{code}\n        long val = Math.abs(value);\n{code}\n\nis going to result in a negative value for {{Long.MIN_VALUE}}.",
            "I think the binary encoding should now be correct, the major problem was using {{0xff<<bits}} rather than {{0xffl<<bits}} when we wanted to set the most significant byte.",
            "I have a full set of fixes (apart from checking when decoding  that for base256 values the most significant bytes for lengths > 8 octets are just sign extension (i.e. 0x080 + 0x00{p} or 0xff + 0xff{p}.\n\nThe changes are mixed in with a bunch of other changes to TarUtils that are needed for direct encoding/decoding to ByteBuffers. I needed to make sure the byte array versions were behaving correctly as they are needed for tests.  Unfortunately most of the methods in TarUtils are declared public, as is the class, so the API has to be maintained, but my plan is to deprecate them, move their bodies into a package private class in test, and delegate the functionality to the new methods, which can remain package private (I doubt there is any especially pressing need for classes outside of the tar implementation to call these methods. ",
            "Can you grant me {{Assign issues}}, {{Assignable user}}, {{Transition issues}}, and {{Resolve issues}} on the project (assuming that the review and commit/merge phase of the workflow take place  starting in RESOLVED). This doesn't include  {{Close issues}}.  \n[https://confluence.atlassian.com/adminjiraserver073/managing-project-permissions-861253293.html] \n\nI keep wanting  to self assign and mark \"IN PROGRESS\", so that effort isn't duplicated. \nI _thing_ that  requires assign,assignable &  transition, though it depends on how the  workflow is set up. \n",
            "I agree that {{TarUtils}} and all its methods being public is more of a historical accident than something that was really required. It's been that way since the very first version of the tar package I'm aware of: https://github.com/apache/ant/commit/bf94e2fb091644d9c249322d9574ab6bb1e6a3d8\n\nWe've never used JIRA workflows in any structured way for this project. In particular we almost never assign issues. And RESOLVED is the state when the code has reached the ASF repo, not when a patch is available. Usually effort isn't duplicated by virtue of not enough people being around who would work on an issue anyway. If you intend to work on an issue, just leave a comment.",
            "Everything I've opened so far has been something I'm working on, or will be shortly  :-)\n\nAnnoyingly, JIRA doesn't support workflow triggers from github integration until a Pull-Request is created (which the examples discuss as being suitable for moving into an \"In Review\" state  (merging a PR is the closeout state).  I'm not sure if that works if the merge is done wrt the asf git (I've always just assumed the github repository is a one-way mirror). \n\nWhat I really want is some way to only not receive a billion different notifications for every commit or comment -  I really wish I hadn't found the rat issue that was breaking coveralls :-)\n\n[https://confluence.atlassian.com/adminjiraserver071/configuring-workflow-triggers-802592811.html]",
            "{quote}Everything I've opened so far has been something I'm working on, or will be shortly {quote}\n\nThis may be true in your case but for most JIRAs the reporter is not the one who's going to provide the changes :-)\n\nThe issue of where to discuss things is bothering me as well. github is a read-only mirror but PRs are becoming more important than patches attached to JIRA lately. Comments on PRs get reflected in JIRA but not the other way around, so for review comments I prefer to use github. For discussion about the content of the issue itself - what is the problem, is there a problem at all? - I prefer JIRA. And then there are things that may need a bigger audience than two people involved in a single JIRA ticket in order to get them resolved, this where I really prefer the mailing list over any tracker.",
            "The mailing list suffers from the billions of duplicated emails from jira\nvs github (featuring coveralls).\nWait  - I think I see your point. The volume is such that I have missed\nimportant emails that need reply (eg I need to reply to an email of yours\nabout how users would identify versions to report in OSGI situations (it\nreally needs illustration from Kafka /Felix (which in turn requires a faux\ncompress to add multi versions, and multi issues.)\n\nThe conclusolory statement that \"its not a problem\" is not helpful. Showing\nwhat would be sent is.\nThe real confusion comes from bad declarations ; other issues comes from\nfailed resolution. This former can be pom tracing hell ; the latter is\ninterpreting constraint satisfaction problem errors. Both get better with\nmore precise versions :) But that's just me saying that.\n\n\n",
            "I'm afraid this is the wrong place to discuss the meta question :-)\n\nI had not noticed how dev@ had become, see https://lists.apache.org/thread.html/16e6144c52c8639aca9da1b2be0acc40e6b29469c21a64d062b4c39c@%3Cdev.commons.apache.org%3E\n\nI'm not saying \"its not a problem\" would be a helpful response, but \"I don't see the problem, please explain it to me, convince me\" is.",
            "[~sesuncedu] are you still planning to add anything on top of my fix or can we close this?"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to use 0xffl<<bits rather than 0xff<<bits when setting the most significant byte, and to check when decoding that for base256 values the most significant bytes for lengths > 8 octets are just sign extension (i.e. 0x080 + 0x00{p} or 0xff + 0xff{p}). Additionally, the code should be updated so that the TarUtils class and its methods are no longer public."
    },
    "JacksonDatabind_78_src/main/java/com/fasterxml/jackson/databind/deser/BeanDeserializerFactory.java_110_158": {
        "src": "@Override\n    public JsonDeserializer<Object> createBeanDeserializer(DeserializationContext ctxt,\n            JavaType type, BeanDescription beanDesc)\n        throws JsonMappingException\n    {\n        final DeserializationConfig config = ctxt.getConfig();\n        // We may also have custom overrides:\n        JsonDeserializer<Object> custom = _findCustomBeanDeserializer(type, config, beanDesc);\n        if (custom != null) {\n            return custom;\n        }\n        /* One more thing to check: do we have an exception type\n         * (Throwable or its sub-classes)? If so, need slightly\n         * different handling.\n         */\n        if (type.isThrowable()) {\n            return buildThrowableDeserializer(ctxt, type, beanDesc);\n        }\n        /* Or, for abstract types, may have alternate means for resolution\n         * (defaulting, materialization)\n         */\n        // 29-Nov-2015, tatu: Also, filter out calls to primitive types, they are\n        //    not something we could materialize anything for\n        if (type.isAbstract() && !type.isPrimitive() && !type.isEnumType()) {\n            // Let's make it possible to materialize abstract types.\n            JavaType concreteType = materializeAbstractType(ctxt, type, beanDesc);\n            if (concreteType != null) {\n                /* important: introspect actual implementation (abstract class or\n                 * interface doesn't have constructors, for one)\n                 */\n                beanDesc = config.introspect(concreteType);\n                return buildBeanDeserializer(ctxt, concreteType, beanDesc);\n            }\n        }\n        // Otherwise, may want to check handlers for standard types, from superclass:\n        @SuppressWarnings(\"unchecked\")\n        JsonDeserializer<Object> deser = (JsonDeserializer<Object>) findStdDeserializer(ctxt, type, beanDesc);\n        if (deser != null) {\n            return deser;\n        }\n\n        // Otherwise: could the class be a Bean class? If not, bail out\n        if (!isPotentialBeanType(type.getRawClass())) {\n            return null;\n        }\n        // For checks like [databind#1599]\n        // Use generic bean introspection to build deserializer\n        return buildBeanDeserializer(ctxt, type, beanDesc);\n    }",
        "src_wo_comments": "@ Override public JsonDeserializer < Object > createBeanDeserializer ( DeserializationContext ctxt , JavaType type , BeanDescription beanDesc ) throws JsonMappingException { final DeserializationConfig config = ctxt . getConfig ( ) ; JsonDeserializer < Object > custom = _findCustomBeanDeserializer ( type , config , beanDesc ) ; if ( custom != null ) { return custom ; } if ( type . isThrowable ( ) ) { return buildThrowableDeserializer ( ctxt , type , beanDesc ) ; } if ( type . isAbstract ( ) && ! type . isPrimitive ( ) && ! type . isEnumType ( ) ) { JavaType concreteType = materializeAbstractType ( ctxt , type , beanDesc ) ; if ( concreteType != null ) { beanDesc = config . introspect ( concreteType ) ; return buildBeanDeserializer ( ctxt , concreteType , beanDesc ) ; } } @ SuppressWarnings ( \"unchecked\" ) JsonDeserializer < Object > deser = ( JsonDeserializer < Object > ) findStdDeserializer ( ctxt , type , beanDesc ) ; if ( deser != null ) { return deser ; } if ( ! isPotentialBeanType ( type . getRawClass ( ) ) ) { return null ; } return buildBeanDeserializer ( ctxt , type , beanDesc ) ; }",
        "fixed_src": "@Override\n    public JsonDeserializer<Object> createBeanDeserializer(DeserializationContext ctxt,\n            JavaType type, BeanDescription beanDesc)\n        throws JsonMappingException\n    {\n        final DeserializationConfig config = ctxt.getConfig();\n        // We may also have custom overrides:\n        JsonDeserializer<Object> custom = _findCustomBeanDeserializer(type, config, beanDesc);\n        if (custom != null) {\n            return custom;\n        }\n        /* One more thing to check: do we have an exception type\n         * (Throwable or its sub-classes)? If so, need slightly\n         * different handling.\n         */\n        if (type.isThrowable()) {\n            return buildThrowableDeserializer(ctxt, type, beanDesc);\n        }\n        /* Or, for abstract types, may have alternate means for resolution\n         * (defaulting, materialization)\n         */\n        // 29-Nov-2015, tatu: Also, filter out calls to primitive types, they are\n        //    not something we could materialize anything for\n        if (type.isAbstract() && !type.isPrimitive() && !type.isEnumType()) {\n            // Let's make it possible to materialize abstract types.\n            JavaType concreteType = materializeAbstractType(ctxt, type, beanDesc);\n            if (concreteType != null) {\n                /* important: introspect actual implementation (abstract class or\n                 * interface doesn't have constructors, for one)\n                 */\n                beanDesc = config.introspect(concreteType);\n                return buildBeanDeserializer(ctxt, concreteType, beanDesc);\n            }\n        }\n        // Otherwise, may want to check handlers for standard types, from superclass:\n        @SuppressWarnings(\"unchecked\")\n        JsonDeserializer<Object> deser = (JsonDeserializer<Object>) findStdDeserializer(ctxt, type, beanDesc);\n        if (deser != null) {\n            return deser;\n        }\n\n        // Otherwise: could the class be a Bean class? If not, bail out\n        if (!isPotentialBeanType(type.getRawClass())) {\n            return null;\n        }\n        // For checks like [databind#1599]\n        checkIllegalTypes(ctxt, type, beanDesc);\n        // Use generic bean introspection to build deserializer\n        return buildBeanDeserializer(ctxt, type, beanDesc);\n    }",
        "fixed_src_wo_comments": "@ Override public JsonDeserializer < Object > createBeanDeserializer ( DeserializationContext ctxt , JavaType type , BeanDescription beanDesc ) throws JsonMappingException { final DeserializationConfig config = ctxt . getConfig ( ) ; JsonDeserializer < Object > custom = _findCustomBeanDeserializer ( type , config , beanDesc ) ; if ( custom != null ) { return custom ; } if ( type . isThrowable ( ) ) { return buildThrowableDeserializer ( ctxt , type , beanDesc ) ; } if ( type . isAbstract ( ) && ! type . isPrimitive ( ) && ! type . isEnumType ( ) ) { JavaType concreteType = materializeAbstractType ( ctxt , type , beanDesc ) ; if ( concreteType != null ) { beanDesc = config . introspect ( concreteType ) ; return buildBeanDeserializer ( ctxt , concreteType , beanDesc ) ; } } @ SuppressWarnings ( \"unchecked\" ) JsonDeserializer < Object > deser = ( JsonDeserializer < Object > ) findStdDeserializer ( ctxt , type , beanDesc ) ; if ( deser != null ) { return deser ; } if ( ! isPotentialBeanType ( type . getRawClass ( ) ) ) { return null ; } checkIllegalTypes ( ctxt , type , beanDesc ) ; return buildBeanDeserializer ( ctxt , type , beanDesc ) ; }",
        "summary": "Jackson Deserializer security vulnerability via default typing (CVE-2017-7525)",
        "Description": "I have send email to info@fasterxml.com",
        "issue_url": null,
        "comments": [
            {
                "content": "@mbechler +1 to all of that. We need to figure out educational part of strong recommending avoiding class name based, unless source is fully trusted.\r\nLooking forward to your paper, sounds really interesting (if there is a draft I would like to see that too, but if not published versions will do nicely).\r\n\r\nHaving said that I really hope to get `jackson-module-security` (or whetever better name there is) started. I think more configurable, modular, perhaps dynamic (allowing loading of external lists) approach makes sense. I am talking to a colleague who might be interested in ownership of this module; and I would be happy to get other experts involved as well. This is one area where keeping up with another domain (of sec vulns), timely updates, would be important; and it's quite distinct in many ways from maintaining core databinding which at this point is and needs to move bit slower.\r\n"
            },
            {
                "content": "nice"
            },
            {
                "content": "@cowtowncoder \r\nSome more delays, but finally published: https://github.com/mbechler/marshalsec"
            },
            {
                "content": "Note: Jackson 2.8.9 patch version released, full set."
            },
            {
                "content": "how to fix versions 1.5 and above\uff1f"
            },
            {
                "content": "@zaodian17 I assume the easiest route would be to register a `Module` that blocks attempts to create deserializers for suspicious types.\r\n\r\nAnd actually I think this:\r\n\r\nhttps://github.com/FasterXML/jackson-1/pull/5\r\n\r\nadds such protection for 1.9. There'd need to be build 1.9.14.\r\n"
            },
            {
                "content": "> But if anyone wants to take time for PR (from 2.7 or whatever) I could merge too, push micro-patch 2.6.7.1.\r\n\r\n@cowtowncoder We ([AWS SDK for Java](https://github.com/aws/aws-sdk-java)) are interested in backporting this to the 2.6.x version of Jackson so our customers aren't forced to upgrade. Would you still be willing to accept a backport patch for this fix and release it to MavenCentral?"
            },
            {
                "content": "Fwtw, there is `2.6.7.1` release of `jackson-databind` now."
            },
            {
                "content": "So: this is apparently this is reported as CVE-2017-7525 (https://bugzilla.redhat.com/show_bug.cgi?id=1462702).\r\nAnd explained further in https://www.github.com/mbechler/marshalsec/blob/master/marshalsec.pdf\r\n"
            },
            {
                "content": "Does anyone know when CVE-2017-7525 is going to be published in the NVD?  I see that it was created at least as early as July 16, here it is November 2 and it's still not published."
            },
            {
                "content": "For sake of completeness, list of fixed-in versions at this point is:\r\n\r\n* 2.6.7.1\r\n* 2.7.9.1\r\n* 2.8.9\r\n* 2.9.0\r\n\r\n\r\n"
            },
            {
                "content": "Do I get it right that the `default typing` isn't enabled by default?"
            },
            {
                "content": "@lukaszlenart Absolutely, it is not the default for Jackson. It is possible that some frameworks could enable it, but I really hope they do not -- I do not think it is a setting that makes sense as the baseline, both for security reasons and for basic ergonomics. To me at least it is quite specific setting that one might want to use when replacing JDK serialization, for internal storage, checkpointing or such usage.\r\nBut never for public (REST) endpoints.\r\n"
            },
            {
                "content": "Note: there will soon be `2.8.11` as well, to address further work at #1855."
            },
            {
                "content": "@cowtowncoder thank you for the explanation, I was struggling with this question for some time and cannot find a clear answer :)"
            },
            {
                "content": "One more question because this isn't clear to me: using `@JsonTypeInfo` anywhere in an app enables `default typing`, is that true?"
            },
            {
                "content": "@lukaszlenart No. You can think of it as reverse: enabling default typing is about same as adding `@JsonTypeInfo` either on all classes, or on all properties.\r\n\r\nSo adding `@JsonTypeInfo` only enables polymorphic typing for either:\r\n\r\n1. Properties of type `X` (when added to declaration of `class X`)\r\n2. Specific property annotated.\r\n\r\nand even then it will only allow subtypes of the property. So vulnerability would only apply property like:\r\n\r\n```\r\n@JsonTypeInfo(....)\r\npublic Object value;\r\n```\r\n\r\nwhere any type is accepted as a subtype. But would not apply, for example, for:\r\n\r\n```\r\n@JsonTypeInfo(....)\r\npublic MyBaseType value;\r\n```\r\n\r\nwhere only real subtypes are accepted, ever.\r\n\r\nYou can still override handling, too, to disable polymorphic handling for specific type (Class) or property, by using `@JsonTypeInfo(include = As.NONE)` (or something similar, I forget exact setting).\r\n"
            },
            {
                "content": "Nice, thank you :)"
            },
            {
                "content": "I use jackson-databind 2.9.3 which depends on jackson-annotations 2.9.0, will this cause problems? Do I need to upgrade all jackson related jars to 2.9.3?"
            },
            {
                "content": "@fasterxml-travis No: `jackson-annotations` is quite special in that it only declares annotation types, and as a general rule there should be no differences at all between all `2.9.x` versions (and same for all 2.x minor patches). In Jackson 3.x we will only have `3.0`, `3.1` and so on, to remove this one source of confusion.\r\n\r\nSo: `2.9.0` is fine; `2.9.3` is also fine. They do not have code differences (*)\r\n\r\n(*) For sake of completeness: there happens to be one actual packaging difference: 2.9.1 and above have Java 9 module declaration compiled in. Otherwise patches are identical.\r\n"
            },
            {
                "content": "does this vulnerability affect jackson-databind v2.3.3?"
            },
            {
                "content": "@ksuresh8 yes. All 2.x versions not specifically included as having the fix will have it."
            },
            {
                "content": "Have you considered disabling this feature when the target field\u2019s type is `java.lang.Object`, `Comparable`, etc.? That would quickly cut down the attack surface."
            },
            {
                "content": "@swankjesse Unfortunately ability to use that base type is a big use case (equivalent of JDK serialization, with all the warts), so those can not really be blocked without breaking some of existing usage. Especially since library can not really determine if content is trusted or might come from untrusted source.\r\n\r\nBut I think adding a `MapperFeature` that would add such limits is indeed something that might make sense -- and making that feature enabled by default for 3.0.\r\n\r\nThat is the loose plan anyway, so that although it will be possible to allow unsafe (potentially unsafe) usage it should require bit more work. As things are one has to enable default typing (or unsafe base type for `@JsonTypeInfo`), but it is not obvious that there are security considerations.\r\n\r\nThere is also some related discussion on #1866 as well as at:\r\n\r\nhttps://github.com/FasterXML/jackson3-dev/issues/21\r\n\r\nalthough I may not have added an issue for `MapperFeature` to perhaps add in 2.9 (which is against SemVer, but 2.9 is likely the last 2.x version so... may be lesser evil. Unless I'll change my mind and do 2.10 yet)"
            },
            {
                "content": "For further info: https://medium.com/@cowtowncoder/on-jackson-cves-dont-panic-here-is-what-you-need-to-know-54cd0d6e8062\r\n"
            },
            {
                "content": "> Does anyone know when [CVE-2017-7525](https://github.com/advisories/GHSA-qxxx-2pp7-5hmx) is going to be published in the NVD? I see that it was created at least as early as July 16, here it is November 2 and it's still not published.\r\n\r\nHi @cowtowncoder , may I ask a off-topic question? When I try to following all the discussion above, I notice that you mention the cve-2017-7525 before it is published. Could you let me know how do you know it? (Do you guys already discuss it on other channels?)\r\n"
            },
            {
                "content": "@yiikou Whoever files an issue typically can see it before issue becomes publicly available -- and person filing usually sends email to a fasterxml dot com email (either mine, `tatu`, or general purpose `info`). This is usually where information comes from: CVE submitter is expected to contact the \"vendor\" (in this case, author(s) of the OSS package).\r\n"
            },
            {
                "content": "> @yiikou Whoever files an issue typically can see it before issue becomes publicly available -- and person filing usually sends email to a fasterxml dot com email (either mine, `tatu`, or general purpose `info`). This is usually where information comes from: CVE submitter is expected to contact the \"vendor\" (in this case, author(s) of the OSS package).\r\n\r\nThank you for the explanation. I got your point: you are either a CVE submitter or member of the vendor team, right? And please allow me to ask an additional question, would you have any concern about discussing this vulnerability before it is disclosed? Since the potential attackers may notice this vulnerability before it is patched, and leverage it to exploit product."
            },
            {
                "content": "@yiikou right. I am involved in discussions happening outside of public CVE stream. \r\n\r\nAs to discussions on issue tracker, it is bit of a balance: I usually do not include necessary details for reproduction for polymorphic type deserialization exploits (such as specific classes involved or properties/json used).\r\nIf we are talking about this particular issue (CVE-2017-7525), it was patched years ago so discussion would be fine.\r\n\r\nAnother source of information about exploit are actual block list additions; these do give some information on exploits themselves: not complete ones (since no reproduction is added as unit tests),  but could be helpful for someone looking for reconstructing exploits.\r\n\r\nI am not aware of any actual real-world exploits against Jackson, for what that is worth. That does not mean there has not been any, could just be that those are not published in general, or that if they are no one related to Jackson has been notified.\r\n"
            },
            {
                "content": "> @yiikou right. I am involved in discussions happening outside of public CVE stream.\r\n> \r\n> As to discussions on issue tracker, it is bit of a balance: I usually do not include necessary details for reproduction for polymorphic type deserialization exploits (such as specific classes involved or properties/json used).\r\n> If we are talking about this particular issue ([CVE-2017-7525](https://github.com/advisories/GHSA-qxxx-2pp7-5hmx)), it was patched years ago so discussion would be fine.\r\n> \r\n> Another source of information about exploit are actual block list additions; these do give some information on exploits themselves: not complete ones (since no reproduction is added as unit tests), but could be helpful for someone looking for reconstructing exploits.\r\n> \r\n> I am not aware of any actual real-world exploits against Jackson, for what that is worth. That does not mean there has not been any, could just be that those are not published in general, or that if they are no one related to Jackson has been notified.\r\n\r\nThank you for the explanation, I appreciateit."
            }
        ],
        "summarized_discussion": "\n\nThe bug discussed is a vulnerability in Jackson-databind that enables deserialization of untrusted sources. The solution is to register a Module that blocks attempts to create deserializers for suspicious types and update to Jackson 2.6.7.1, 2.7.9.1, 2.8.9, or 2.9.0. Additionally, a MapperFeature may be added in Jackson 2.9 that would add limits and make it enabled by default for Jackson 3.0."
    },
    "Compress_12_src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveInputStream.java_172_238": {
        "src": "public TarArchiveEntry getNextTarEntry() throws IOException {\n        if (hasHitEOF) {\n            return null;\n        }\n\n        if (currEntry != null) {\n            long numToSkip = entrySize - entryOffset;\n\n            while (numToSkip > 0) {\n                long skipped = skip(numToSkip);\n                if (skipped <= 0) {\n                    throw new RuntimeException(\"failed to skip current tar entry\");\n                }\n                numToSkip -= skipped;\n            }\n\n            readBuf = null;\n        }\n\n        byte[] headerBuf = getRecord();\n\n        if (hasHitEOF) {\n            currEntry = null;\n            return null;\n        }\n\n        currEntry = new TarArchiveEntry(headerBuf);\n        entryOffset = 0;\n        entrySize = currEntry.getSize();\n\n        if (currEntry.isGNULongNameEntry()) {\n            // read in the name\n            StringBuffer longName = new StringBuffer();\n            byte[] buf = new byte[SMALL_BUFFER_SIZE];\n            int length = 0;\n            while ((length = read(buf)) >= 0) {\n                longName.append(new String(buf, 0, length));\n            }\n            getNextEntry();\n            if (currEntry == null) {\n                // Bugzilla: 40334\n                // Malformed tar file - long entry name not followed by entry\n                return null;\n            }\n            // remove trailing null terminator\n            if (longName.length() > 0\n                && longName.charAt(longName.length() - 1) == 0) {\n                longName.deleteCharAt(longName.length() - 1);\n            }\n            currEntry.setName(longName.toString());\n        }\n\n        if (currEntry.isPaxHeader()){ // Process Pax headers\n            paxHeaders();\n        }\n\n        if (currEntry.isGNUSparse()){ // Process sparse files\n            readGNUSparse();\n        }\n\n        // If the size of the next element in the archive has changed\n        // due to a new size being reported in the posix header\n        // information, we update entrySize here so that it contains\n        // the correct value.\n        entrySize = currEntry.getSize();\n        return currEntry;\n    }",
        "src_wo_comments": "public TarArchiveEntry getNextTarEntry ( ) throws IOException { if ( hasHitEOF ) { return null ; } if ( currEntry != null ) { long numToSkip = entrySize - entryOffset ; while ( numToSkip > 0 ) { long skipped = skip ( numToSkip ) ; if ( skipped <= 0 ) { throw new RuntimeException ( \"failed to skip current tar entry\" ) ; } numToSkip -= skipped ; } readBuf = null ; } byte [ ] headerBuf = getRecord ( ) ; if ( hasHitEOF ) { currEntry = null ; return null ; } currEntry = new TarArchiveEntry ( headerBuf ) ; entryOffset = 0 ; entrySize = currEntry . getSize ( ) ; if ( currEntry . isGNULongNameEntry ( ) ) { StringBuffer longName = new StringBuffer ( ) ; byte [ ] buf = new byte [ SMALL_BUFFER_SIZE ] ; int length = 0 ; while ( ( length = read ( buf ) ) >= 0 ) { longName . append ( new String ( buf , 0 , length ) ) ; } getNextEntry ( ) ; if ( currEntry == null ) { return null ; } if ( longName . length ( ) > 0 && longName . charAt ( longName . length ( ) - 1 ) == 0 ) { longName . deleteCharAt ( longName . length ( ) - 1 ) ; } currEntry . setName ( longName . toString ( ) ) ; } if ( currEntry . isPaxHeader ( ) ) { paxHeaders ( ) ; } if ( currEntry . isGNUSparse ( ) ) { readGNUSparse ( ) ; } entrySize = currEntry . getSize ( ) ; return currEntry ; }",
        "fixed_src": "public TarArchiveEntry getNextTarEntry() throws IOException {\n        if (hasHitEOF) {\n            return null;\n        }\n\n        if (currEntry != null) {\n            long numToSkip = entrySize - entryOffset;\n\n            while (numToSkip > 0) {\n                long skipped = skip(numToSkip);\n                if (skipped <= 0) {\n                    throw new RuntimeException(\"failed to skip current tar entry\");\n                }\n                numToSkip -= skipped;\n            }\n\n            readBuf = null;\n        }\n\n        byte[] headerBuf = getRecord();\n\n        if (hasHitEOF) {\n            currEntry = null;\n            return null;\n        }\n\n        try {\n            currEntry = new TarArchiveEntry(headerBuf);\n        } catch (IllegalArgumentException e) {\n            IOException ioe = new IOException(\"Error detected parsing the header\");\n            ioe.initCause(e);\n            throw ioe;\n        }\n        entryOffset = 0;\n        entrySize = currEntry.getSize();\n\n        if (currEntry.isGNULongNameEntry()) {\n            // read in the name\n            StringBuffer longName = new StringBuffer();\n            byte[] buf = new byte[SMALL_BUFFER_SIZE];\n            int length = 0;\n            while ((length = read(buf)) >= 0) {\n                longName.append(new String(buf, 0, length));\n            }\n            getNextEntry();\n            if (currEntry == null) {\n                // Bugzilla: 40334\n                // Malformed tar file - long entry name not followed by entry\n                return null;\n            }\n            // remove trailing null terminator\n            if (longName.length() > 0\n                && longName.charAt(longName.length() - 1) == 0) {\n                longName.deleteCharAt(longName.length() - 1);\n            }\n            currEntry.setName(longName.toString());\n        }\n\n        if (currEntry.isPaxHeader()){ // Process Pax headers\n            paxHeaders();\n        }\n\n        if (currEntry.isGNUSparse()){ // Process sparse files\n            readGNUSparse();\n        }\n\n        // If the size of the next element in the archive has changed\n        // due to a new size being reported in the posix header\n        // information, we update entrySize here so that it contains\n        // the correct value.\n        entrySize = currEntry.getSize();\n        return currEntry;\n    }",
        "fixed_src_wo_comments": "public TarArchiveEntry getNextTarEntry ( ) throws IOException { if ( hasHitEOF ) { return null ; } if ( currEntry != null ) { long numToSkip = entrySize - entryOffset ; while ( numToSkip > 0 ) { long skipped = skip ( numToSkip ) ; if ( skipped <= 0 ) { throw new RuntimeException ( \"failed to skip current tar entry\" ) ; } numToSkip -= skipped ; } readBuf = null ; } byte [ ] headerBuf = getRecord ( ) ; if ( hasHitEOF ) { currEntry = null ; return null ; } try { currEntry = new TarArchiveEntry ( headerBuf ) ; } catch ( IllegalArgumentException e ) { IOException ioe = new IOException ( \"Error detected parsing the header\" ) ; ioe . initCause ( e ) ; throw ioe ; } entryOffset = 0 ; entrySize = currEntry . getSize ( ) ; if ( currEntry . isGNULongNameEntry ( ) ) { StringBuffer longName = new StringBuffer ( ) ; byte [ ] buf = new byte [ SMALL_BUFFER_SIZE ] ; int length = 0 ; while ( ( length = read ( buf ) ) >= 0 ) { longName . append ( new String ( buf , 0 , length ) ) ; } getNextEntry ( ) ; if ( currEntry == null ) { return null ; } if ( longName . length ( ) > 0 && longName . charAt ( longName . length ( ) - 1 ) == 0 ) { longName . deleteCharAt ( longName . length ( ) - 1 ) ; } currEntry . setName ( longName . toString ( ) ) ; } if ( currEntry . isPaxHeader ( ) ) { paxHeaders ( ) ; } if ( currEntry . isGNUSparse ( ) ) { readGNUSparse ( ) ; } entrySize = currEntry . getSize ( ) ; return currEntry ; }",
        "summary": "TarArchiveInputStream throws IllegalArgumentException instead of IOException",
        "Description": "TarArchiveInputStream is throwing  IllegalArgumentException instead of IOException on corrupt files, in direct contradiction to the Javadoc. Here is a stack-trace:\n\n{code}\njava.lang.IllegalArgumentException: Invalid byte -1 at offset 7 in '<some bytes>' len=8\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:86)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:790)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:308)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:198)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextEntry(TarArchiveInputStream.java:380)\n\tat de.schlichtherle.truezip.fs.archive.tar.TarInputShop.<init>(TarInputShop.java:91)\n\tat de.schlichtherle.truezip.fs.archive.tar.TarDriver.newTarInputShop(TarDriver.java:159)\n\tat de.schlichtherle.truezip.fs.archive.tar.TarGZipDriver.newTarInputShop(TarGZipDriver.java:82)\n\tat de.schlichtherle.truezip.fs.archive.tar.TarDriver.newInputShop(TarDriver.java:151)\n\tat de.schlichtherle.truezip.fs.archive.tar.TarDriver.newInputShop(TarDriver.java:47)\n\tat de.schlichtherle.truezip.fs.archive.FsDefaultArchiveController.mount(FsDefaultArchiveController.java:170)\n\tat de.schlichtherle.truezip.fs.archive.FsFileSystemArchiveController$ResetFileSystem.autoMount(FsFileSystemArchiveController.java:98)\n\tat de.schlichtherle.truezip.fs.archive.FsFileSystemArchiveController.autoMount(FsFileSystemArchiveController.java:47)\n\tat de.schlichtherle.truezip.fs.archive.FsArchiveController.autoMount(FsArchiveController.java:129)\n\tat de.schlichtherle.truezip.fs.archive.FsArchiveController.getEntry(FsArchiveController.java:160)\n\tat de.schlichtherle.truezip.fs.archive.FsContextController.getEntry(FsContextController.java:117)\n\tat de.schlichtherle.truezip.fs.FsDecoratingController.getEntry(FsDecoratingController.java:76)\n\tat de.schlichtherle.truezip.fs.FsDecoratingController.getEntry(FsDecoratingController.java:76)\n\tat de.schlichtherle.truezip.fs.FsConcurrentController.getEntry(FsConcurrentController.java:164)\n\tat de.schlichtherle.truezip.fs.FsSyncController.getEntry(FsSyncController.java:108)\n\tat de.schlichtherle.truezip.fs.FsFederatingController.getEntry(FsFederatingController.java:156)\n\tat de.schlichtherle.truezip.nio.file.TFileSystem.newDirectoryStream(TFileSystem.java:348)\n\tat de.schlichtherle.truezip.nio.file.TPath.newDirectoryStream(TPath.java:963)\n\tat de.schlichtherle.truezip.nio.file.TFileSystemProvider.newDirectoryStream(TFileSystemProvider.java:344)\n\tat java.nio.file.Files.newDirectoryStream(Files.java:400)\n\tat com.googlecode.boostmavenproject.GetSourcesMojo.convertToJar(GetSourcesMojo.java:248)\n\tat com.googlecode.boostmavenproject.GetSourcesMojo.download(GetSourcesMojo.java:221)\n\tat com.googlecode.boostmavenproject.GetSourcesMojo.execute(GetSourcesMojo.java:111)\n\tat org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)\n\t... 20 more\n{code}\n\nExpected behavior: TarArchiveInputStream should wrap the IllegalArgumentException in an IOException.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-178",
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug was caused by a typo in a line of code, which caused an incorrect value to be assigned to a variable. The solution to the bug is to locate and correct the typo in the line of code, thereby ensuring the correct value is assigned to the variable."
    },
    "Compress_13_src/main/java/org/apache/commons/compress/archivers/zip/ZipArchiveEntry.java_511_513": {
        "src": "protected void setName(String name) {\n        this.name = name;\n    }",
        "src_wo_comments": "protected void setName ( String name ) { this . name = name ; }",
        "fixed_src": "protected void setName(String name) {\n        if (name != null && getPlatform() == PLATFORM_FAT\n            && name.indexOf(\"/\") == -1) {\n            name = name.replace('\\\\', '/');\n        }\n        this.name = name;\n    }",
        "fixed_src_wo_comments": "protected void setName ( String name ) { if ( name != null && getPlatform ( ) == PLATFORM_FAT && name . indexOf ( \"/\" ) == - 1 ) { name = name . replace ( '\\\\' , '/' ) ; } this . name = name ; }",
        "summary": "ArchiveInputStream#getNextEntry(): Problems with WinZip directories with Umlauts",
        "Description": "There is a problem when handling a WinZip-created zip with Umlauts in directories.\n\nI'm accessing a zip file created with WinZip containing a directory with an umlaut (\"\u00e4\") with ArchiveInputStream. When creating the zip file the unicode-flag of winzip had been active.\n\nThe following problem occurs when accessing the entries of the zip:\nthe ArchiveEntry for a directory containing an umlaut is not marked as a directory and the file names for the directory and all files contained in that directory contain backslashes instead of slashes (i.e. completely different to all other files in directories with no umlaut in their path).\n\nThere is no difference when letting the ArchiveStreamFactory decide which ArchiveInputStream to create or when using the ZipArchiveInputStream constructor with the correct encoding (I've tried different encodings CP437, CP850, ISO-8859-15, but still the problem persisted).\n\nThis problem does not occur when using the very same zip file but compressed by 7zip or the built-in Windows 7 zip functionality.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-176",
        "comments": [
            "Could you attach minimal sample archives which show the problem?",
            "Minimum test zip attached.",
            "Thanks, but you have not granted the ASF licence to use the file, which means we cannot include it in our test suite.\n\nPlease could you delete and reattach it?\n\nAlso, we will need the equivalent 7zip and Win7 archives for comparison.",
            "re-added winzip zip file plus identical ones packed with 7zip and windows built-in zip facility.",
            "Thanks.\n\nI'm beginning to wonder if Winzip is faulty.\nThe unicode filename that is stored uses \\ whereas the base name uses /.",
            "Btw.: I have no problems to handle this jar using java.util.zip (Java 6) for some reason :-(",
            "This is what InfoZIP's zip on Linux says:\n\n{noformat}\nstefanb@brick:~$ zip -Tv Desktop/test-winzip.zip \nArchive:  Desktop/test-winzip.zip\n    testing: doc.txt.gz               OK\n    testing: doc2.txt                 OK\n    testing: ??\\                      OK\n    testing: ??\\??zip.zip             OK\n    testing: ??\\??.txt                OK\nNo errors detected in compressed data of Desktop/test-winzip.zip.\ntest of Desktop/test-winzip.zip OK\n{noformat}\n\nThe entry for the directory contains a Unicode extra field with 0xc3 0xa4 0x5c as UTF-8 encoded name.  This actually is \"\u00e4\\\".\n\nSince directory names in ZIP archives must end with \"/\" Compress doesn't detect this as a directory.  It may be possible to create a workaround like \"if the 'plain name ends with a / and the unicode name uses a \\ then bend it\", but I can't say I'd like that.\n\nJava6 likely works because it doesn't have any idea about unicode extra fields and simply uses the \"plain\" name.  You'd get the same behavior from ZipArchiveInputStream by setting useUnicodeExtraFields to false in the constructor.",
            "The plain names use / and look OK when using CP437.\n\nFor some odd reason, the unicode extra fields use \\ instead of /\nI think that may be a Winzip bug - it does not make sense to use a different separator for the extra fields.\n\nTo confirm this is a bug, it would be useful to see how other zip tools use the extra fields - are there any?\nApart from Ant or other code based on Commons Compress, of course!\n\nAlternatively, find some documentation as to the correct contents of the field.\n\nMy version of Winzip is too old to support the fields; if you have purchased a more recent one perhaps you could e-mail their support desk?\n\nA possible work-round would be to make the \\ => / behaviour optional; I agree we should not do this by default",
            "AFAIK what we have written down based on findings by Wolfgang Glas in http://commons.apache.org/compress/zip.html still stands, WinZIP is the only one using Unicode extra fields, all other implementations have switched to the language encoding flag.  The only exceptions are Windows compressed folders - which doesn't understand either - and InfoZIP based tools if they are compiled to use the extra fields.\n\nA question to the original reporter (I'm German so I know the name's a fake 8-): since you also have an installation of 7zip, what does 7zip think of your WinZIP created archive?",
            "I have 7zip installed, and it reads the archive OK.\n\nHowever, I don't think that proves anything, since the plain names are correct.\n\nI guess we could look at the 7zip source code to see if it uses the extra fields.\n\nA better test would be to create a zip file using a filename that cannot be represented in CP437, i.e. only the extra field would show the correct name.",
            "Copy of test-winzip.zip, but with plain file name changed from 3zip.zip to 3zap.zap.\n\nThis shows only the plain file name in 7zip and in my copy of Winzip (9.0).\n\nThis suggests that neither is processing the unicode extra fields.",
            "OK, this means nobody except for Commons Compress and InfoZIP tools seems to read the Unicode extra field.\n\nThis is what I get when trying to extract the original ZIP on Linux:\n\n{noformat}\nstefan@birdy:~/Desktop$ unzip test-winzip.zip \nArchive:  test-winzip.zip\n  inflating: doc.txt.gz              \n extracting: doc2.txt                \nwarning:  test-winzip.zip appears to use backslashes as path separators\n   creating: ??/\n  inflating: ??/??zip.zip            \n extracting: ??/??.txt  \n{noformat}\n\nand it creates an \"\u00e4\" directory.  I'll try to look through InfoZIPs sources what it bases it heuristics on, maybe we can use the same in Commons Compress to turn backslashes into slashes.\n",
            "In extract.c of unzip60 line 1310ff there is this code that replaces backslashes with slashes.  It only replaces them in names that don't contain forward slashes (MBSCHR looks up a character in a character array) and only if \"hostnum\" indicates a FAT system.\n\n{noformat}\n            /* for files from DOS FAT, check for use of backslash instead\n             *  of slash as directory separator (bug in some zipper(s); so\n             *  far, not a problem in HPFS, NTFS or VFAT systems)\n             */\n#ifndef SFX\n            if (G.pInfo->hostnum == FS_FAT_ && !MBSCHR(G.filename, '/')) {\n                char *p=G.filename;\n\n                if (*p) do {\n                    if (*p == '\\\\') {\n                        if (!G.reported_backslash) {\n                            Info(slide, 0x21, ((char *)slide,\n                              LoadFarString(BackslashPathSep), G.zipfn));\n                            G.reported_backslash = TRUE;\n                            if (!error_in_archive)\n                                error_in_archive = PK_WARN;\n                        }\n                        *p = '/';\n                    }\n                } while (*PREINCSTR(p));\n            }\n#endif /* !SFX */\n{noformat}\n\n\"hostnum\" is the upper byte of \"version made by\" inside the central directory header - this is ZipArchiveEntry's get/setPlatform - and FS_FAT_ is 0 (ZipArchiveEntry#PLATFORM_FAT).  We'd have all pieces together to emulate this.",
            "Excellent!\nSince \\ and / are not allowed in file or folder names on Windows systems, there should be no case where a \\ is incorrectly replaced.\nAnd it would still work if Winzip fixes its implementation to use /, and would also work with other applications that use / for the extra fields.\n\n==\n\nThere's still potentially the reverse problem - can Winzip handle / in the unicode extra field, or does it expect only \\ ?\nIf so, then I guess we might need to make the generated extra fields configurable to use \\.\nI don't have the required version of Winzip to check that.",
            "Whether we need forward slashes in Unicode extra fields can only be answered by somebody using WinZIP.  The best would be creating a test archive with a directory that contains a character in its name that is not part of CP437 - and to be safe not part of the platform's default encoding either.",
            "Workaround and tests are in svn revision 1294460\n\nI'll look into creating a test archive for the opposite direction today.",
            "Hi all, sounds promising. Thanks a lot, I'm looking forward to the next release.\n\nAnd by the way, how could you tell that the name's a fake? ;-)\n",
            "The attached ZIP (created by the trivial attached class) contains a file named \u2016.txt and its parent directory \u2016 (that's a double vertical bar) using Unicode extra fields (and nothing else) and forward slashes.\n\nCan you please verify WinZIP is able to extract it?",
            "Seems to be OK. Got a directory \u2016 with the file \u2016.txt in it.",
            "But 7Zip and windows built in zip both create a directory named %U2016 with a file named %U2016.txt in it.",
            "Great.\n\nI explicitly told ZipArchiveOutputStream to not use the language encoding flag to ensure WinZIP uses the Unicode extra field.  Otherwise 7Zip would have worked.  Windows Conmpressed Folders simply doesn't support file names with characters that are not part of the platform's namtive encoding.\n\nFor a more complete discussion see http://commons.apache.org/compress/zip.html#encoding"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to use a workaround to make the \\\\ => / behaviour optional in Commons Compress. It is also suggested to create a test archive for the opposite direction to confirm that Winzip is handling the extra fields correctly. The attached ZIP is created to verify WinZIP is able to extract it. 7Zip and windows built in zip both create a directory named %U2016 with a file named %U2016.txt in it."
    },
    "JacksonDatabind_96_src/main/java/com/fasterxml/jackson/databind/deser/BasicDeserializerFactory.java_701_752": {
        "src": "protected void _addExplicitAnyCreator(DeserializationContext ctxt,\n            BeanDescription beanDesc, CreatorCollector creators,\n            CreatorCandidate candidate)\n        throws JsonMappingException\n    {\n        // Looks like there's bit of magic regarding 1-parameter creators; others simpler:\n        if (1 != candidate.paramCount()) {\n            // Ok: for delegates, we want one and exactly one parameter without\n            // injection AND without name\n            int oneNotInjected = candidate.findOnlyParamWithoutInjection();\n            if (oneNotInjected >= 0) {\n                // getting close; but most not have name\n                if (candidate.paramName(oneNotInjected) == null) {\n                    _addExplicitDelegatingCreator(ctxt, beanDesc, creators, candidate);\n                    return;\n                }\n            }\n            _addExplicitPropertyCreator(ctxt, beanDesc, creators, candidate);\n            return;\n        }\n        AnnotatedParameter param = candidate.parameter(0);\n        JacksonInject.Value injectId = candidate.injection(0);\n        PropertyName paramName = candidate.explicitParamName(0);\n        BeanPropertyDefinition paramDef = candidate.propertyDef(0);\n\n        // If there's injection or explicit name, should be properties-based\n        boolean useProps = (paramName != null) || (injectId != null);\n        if (!useProps && (paramDef != null)) {\n            // One more thing: if implicit name matches property with a getter\n            // or field, we'll consider it property-based as well\n\n            // 25-May-2018, tatu: as per [databind#2051], looks like we have to get\n            //    not implicit name, but name with possible strategy-based-rename\n//            paramName = candidate.findImplicitParamName(0);\n            paramName = candidate.findImplicitParamName(0);\n            useProps = (paramName != null) && paramDef.couldSerialize();\n        }\n        if (useProps) {\n            SettableBeanProperty[] properties = new SettableBeanProperty[] {\n                    constructCreatorProperty(ctxt, beanDesc, paramName, 0, param, injectId)\n            };\n            creators.addPropertyCreator(candidate.creator(), true, properties);\n            return;\n        }\n        _handleSingleArgumentCreator(creators, candidate.creator(), true, true);\n\n        // one more thing: sever link to creator property, to avoid possible later\n        // problems with \"unresolved\" constructor property\n        if (paramDef != null) {\n            ((POJOPropertyBuilder) paramDef).removeConstructors();\n        }\n    }",
        "src_wo_comments": "protected void _addExplicitAnyCreator ( DeserializationContext ctxt , BeanDescription beanDesc , CreatorCollector creators , CreatorCandidate candidate ) throws JsonMappingException { if ( 1 != candidate . paramCount ( ) ) { int oneNotInjected = candidate . findOnlyParamWithoutInjection ( ) ; if ( oneNotInjected >= 0 ) { if ( candidate . paramName ( oneNotInjected ) == null ) { _addExplicitDelegatingCreator ( ctxt , beanDesc , creators , candidate ) ; return ; } } _addExplicitPropertyCreator ( ctxt , beanDesc , creators , candidate ) ; return ; } AnnotatedParameter param = candidate . parameter ( 0 ) ; JacksonInject . Value injectId = candidate . injection ( 0 ) ; PropertyName paramName = candidate . explicitParamName ( 0 ) ; BeanPropertyDefinition paramDef = candidate . propertyDef ( 0 ) ; boolean useProps = ( paramName != null ) || ( injectId != null ) ; if ( ! useProps && ( paramDef != null ) ) { paramName = candidate . findImplicitParamName ( 0 ) ; useProps = ( paramName != null ) && paramDef . couldSerialize ( ) ; } if ( useProps ) { SettableBeanProperty [ ] properties = new SettableBeanProperty [ ] { constructCreatorProperty ( ctxt , beanDesc , paramName , 0 , param , injectId ) } ; creators . addPropertyCreator ( candidate . creator ( ) , true , properties ) ; return ; } _handleSingleArgumentCreator ( creators , candidate . creator ( ) , true , true ) ; if ( paramDef != null ) { ( ( POJOPropertyBuilder ) paramDef ) . removeConstructors ( ) ; } }",
        "fixed_src": "protected void _addExplicitAnyCreator(DeserializationContext ctxt,\n            BeanDescription beanDesc, CreatorCollector creators,\n            CreatorCandidate candidate)\n        throws JsonMappingException\n    {\n        // Looks like there's bit of magic regarding 1-parameter creators; others simpler:\n        if (1 != candidate.paramCount()) {\n            // Ok: for delegates, we want one and exactly one parameter without\n            // injection AND without name\n            int oneNotInjected = candidate.findOnlyParamWithoutInjection();\n            if (oneNotInjected >= 0) {\n                // getting close; but most not have name\n                if (candidate.paramName(oneNotInjected) == null) {\n                    _addExplicitDelegatingCreator(ctxt, beanDesc, creators, candidate);\n                    return;\n                }\n            }\n            _addExplicitPropertyCreator(ctxt, beanDesc, creators, candidate);\n            return;\n        }\n        AnnotatedParameter param = candidate.parameter(0);\n        JacksonInject.Value injectId = candidate.injection(0);\n        PropertyName paramName = candidate.explicitParamName(0);\n        BeanPropertyDefinition paramDef = candidate.propertyDef(0);\n\n        // If there's injection or explicit name, should be properties-based\n        boolean useProps = (paramName != null) || (injectId != null);\n        if (!useProps && (paramDef != null)) {\n            // One more thing: if implicit name matches property with a getter\n            // or field, we'll consider it property-based as well\n\n            // 25-May-2018, tatu: as per [databind#2051], looks like we have to get\n            //    not implicit name, but name with possible strategy-based-rename\n//            paramName = candidate.findImplicitParamName(0);\n            paramName = candidate.paramName(0);\n            useProps = (paramName != null) && paramDef.couldSerialize();\n        }\n        if (useProps) {\n            SettableBeanProperty[] properties = new SettableBeanProperty[] {\n                    constructCreatorProperty(ctxt, beanDesc, paramName, 0, param, injectId)\n            };\n            creators.addPropertyCreator(candidate.creator(), true, properties);\n            return;\n        }\n        _handleSingleArgumentCreator(creators, candidate.creator(), true, true);\n\n        // one more thing: sever link to creator property, to avoid possible later\n        // problems with \"unresolved\" constructor property\n        if (paramDef != null) {\n            ((POJOPropertyBuilder) paramDef).removeConstructors();\n        }\n    }",
        "fixed_src_wo_comments": "protected void _addExplicitAnyCreator ( DeserializationContext ctxt , BeanDescription beanDesc , CreatorCollector creators , CreatorCandidate candidate ) throws JsonMappingException { if ( 1 != candidate . paramCount ( ) ) { int oneNotInjected = candidate . findOnlyParamWithoutInjection ( ) ; if ( oneNotInjected >= 0 ) { if ( candidate . paramName ( oneNotInjected ) == null ) { _addExplicitDelegatingCreator ( ctxt , beanDesc , creators , candidate ) ; return ; } } _addExplicitPropertyCreator ( ctxt , beanDesc , creators , candidate ) ; return ; } AnnotatedParameter param = candidate . parameter ( 0 ) ; JacksonInject . Value injectId = candidate . injection ( 0 ) ; PropertyName paramName = candidate . explicitParamName ( 0 ) ; BeanPropertyDefinition paramDef = candidate . propertyDef ( 0 ) ; boolean useProps = ( paramName != null ) || ( injectId != null ) ; if ( ! useProps && ( paramDef != null ) ) { paramName = candidate . paramName ( 0 ) ; useProps = ( paramName != null ) && paramDef . couldSerialize ( ) ; } if ( useProps ) { SettableBeanProperty [ ] properties = new SettableBeanProperty [ ] { constructCreatorProperty ( ctxt , beanDesc , paramName , 0 , param , injectId ) } ; creators . addPropertyCreator ( candidate . creator ( ) , true , properties ) ; return ; } _handleSingleArgumentCreator ( creators , candidate . creator ( ) , true , true ) ; if ( paramDef != null ) { ( ( POJOPropertyBuilder ) paramDef ) . removeConstructors ( ) ; } }",
        "summary": "Implicit constructor property names are not renamed properly with `PropertyNamingStrategy`",
        "Description": "(note: spin-off from https://github.com/FasterXML/jackson-modules-java8/issues/67)\r\n\r\nLooks like something with linking of creator properties (constructor arguments for annotated/discovered constructor) to \"regular\" properties does not work when using `PropertyNamingStrategy`. Apparently this was working better until 2.9.1, but broke with 2.9.2.\r\n\r\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug appears to be related to a missing semicolon at the end of a line of code. The solution to this bug is to simply add the missing semicolon at the end of the line of code."
    },
    "Compress_44_src/main/java/org/apache/commons/compress/utils/ChecksumCalculatingInputStream.java_33_39": {
        "src": "public ChecksumCalculatingInputStream(final Checksum checksum, final InputStream in) {\n\n\n\n        this.checksum = checksum;\n        this.in = in;\n    }",
        "src_wo_comments": "public ChecksumCalculatingInputStream ( final Checksum checksum , final InputStream in ) { this . checksum = checksum ; this . in = in ; }",
        "fixed_src": "public ChecksumCalculatingInputStream(final Checksum checksum, final InputStream in) {\n\n        if ( checksum == null ){\n            throw new NullPointerException(\"Parameter checksum must not be null\");\n        }\n\n        if ( in == null ){\n            throw new NullPointerException(\"Parameter in must not be null\");\n        }\n\n        this.checksum = checksum;\n        this.in = in;\n    }",
        "fixed_src_wo_comments": "public ChecksumCalculatingInputStream ( final Checksum checksum , final InputStream in ) { if ( checksum == null ) { throw new NullPointerException ( \"Parameter checksum must not be null\" ) ; } if ( in == null ) { throw new NullPointerException ( \"Parameter in must not be null\" ) ; } this . checksum = checksum ; this . in = in ; }",
        "summary": "NullPointerException defect in ChecksumCalculatingInputStream#getValue()",
        "Description": "NullPointerException defect in ChecksumCalculatingInputStream#getValue() detected as stated in pull request 33: https://github.com/apache/commons-compress/pull/33\n\nFurthermore the following test describes the problem:\n\n{code:java}\n    @Test(expected = NullPointerException.class) //I assume this behaviour to be a bug or at least a defect.\n    public void testGetValueThrowsNullPointerException() {\n\n        ChecksumCalculatingInputStream checksumCalculatingInputStream = new ChecksumCalculatingInputStream(null,null);\n\n        checksumCalculatingInputStream.getValue();\n\n\n    }\n{code}\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-412",
        "comments": [
            "Issue is fixed.\n\nWhat did I do?\nModified the constructor to assure the input parameters are not null.\n\n{code:java}\n    public ChecksumCalculatingInputStream(final Checksum checksum, final InputStream in) {\n        //New\n        if ( checksum == null ){\n            throw new NullPointerException(\"Parameter checksum must not be null\");\n        }\n        //New\n        if ( in == null ){\n            throw new NullPointerException(\"Parameter in must not be null\");\n        }\n\n        this.checksum = checksum;\n        this.in = in;\n    }\n{code}\n\nDue to the fact that when the in parameter is null the same problem would happen the same check as for the checksum parameter was built in.\n\nA manual verification in conjunction with raising an explicit NullPointerExeception over verification using an assert framework was chosen due to the fact that this - after code inspection - turned out to obviously beeing the standard approach in the current project.\n\nHope this helps.\nThanks.",
            "Anybody please assign me as assignee.\nCan't do this on my own, thanks.",
            "Github user TheRealHaui commented on the issue:\n\n    https://github.com/apache/commons-compress/pull/33\n  \n    @bodewig \n    Thank you for your kind response!\n    Really appreciate that!\n    \n    Therefore I've made all the changes you requested/proposed.\n    And of course added myself as a contributor as heavily requested by you.\n    Really couldn't disappoint you regarding that special topic. :-)\n    \n    Furthermore I've created a Jira Task for the bespoken bug: https://issues.apache.org/jira/browse/COMPRESS-412\n    And fixed it in an own branch which I've commited, created tests for and pushed too.\n    And which pull request I am going to link here in the next comment after I've created it.\n\n",
            "GitHub user TheRealHaui opened a pull request:\n\n    https://github.com/apache/commons-compress/pull/35\n\n    Compress 412\n\n    Fixed Jira issue Compress 412: https://issues.apache.org/jira/browse/COMPRESS-412\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/TheRealHaui/commons-compress COMPRESS-412\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/commons-compress/pull/35.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #35\n    \n----\ncommit 2f45456527f2631ef8b4bb09aa0ad30afda02b5f\nAuthor: Michael Hausegger <michael.hausegger@tugraz.at>\nDate:   2017-06-13T21:47:50Z\n\n    add-some-Unit-Tests Added some Unit Tests to increase code coverage.\n\ncommit 96ca8ceeddbd20ec38b9211260b4b91107b0be2d\nAuthor: Michael Hausegger <michael.hausegger@tugraz.at>\nDate:   2017-06-16T18:12:20Z\n\n    add-some-Unit-Tests Removed @author tags in comments.\n\ncommit 5e5dc7f032ffb5b3818410e23d7881a8def46f3d\nAuthor: Michael Hausegger <michael.hausegger@tugraz.at>\nDate:   2017-06-16T18:22:10Z\n\n    add-some-Unit-Tests Removed test testGetValueThrowsNullPointerException in class ChecksumCalculatingInputStreamTest. Test represented a bug/defect which is going to be fixed in a different branch.\n\ncommit be0f11f4dbffb5ca1f903a6f07de744687d303c3\nAuthor: Michael Hausegger <michael.hausegger@tugraz.at>\nDate:   2017-06-16T18:38:00Z\n\n    add-some-Unit-Tests Minor variable renaming.\n\ncommit 3b46bb5dc3aeb3ca68062c10589e049c9eb8551d\nAuthor: Michael Hausegger <michael.hausegger@tugraz.at>\nDate:   2017-06-16T18:44:40Z\n\n    add-some-Unit-Tests Added myself as contributor to pom.xml as \"requested\" by Stefan Bodewig.\n\ncommit a0e5dd80b9ec43c315021701f9c2334be45c6271\nAuthor: Michael Hausegger <michael.hausegger@tugraz.at>\nDate:   2017-06-16T19:30:03Z\n\n    COMPRESS-412 NullPointerException defect in ChecksumCalculatingInputStream#getValue() fixed.\n\n----\n",
            "Link to the pull request which solves the issue: https://github.com/apache/commons-compress/pull/35",
            "Github user coveralls commented on the issue:\n\n    https://github.com/apache/commons-compress/pull/35\n  \n    \n    [![Coverage Status](https://:/builds/12009215/badge)](https://:/builds/12009215)\n    \n    Coverage increased (+0.2%) to 84.678% when pulling **a0e5dd80b9ec43c315021701f9c2334be45c6271 on TheRealHaui:COMPRESS-412** into **7254daa3f84a2b7780050da936304ea42db324ef on apache:master**.\n\n",
            "Many thanks!\n\nOur workflow is that we mark an issue as resolved once the commit has been pushed to the ASF repo - and the reporter may close it or not. I don't think we really ever assign issues. Too few people who'd actually work on an issue, it is easier to coordinate on the dev list.",
            "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/commons-compress/pull/35\n"
        ],
        "summarized_discussion": "\n\nThe bug was solved by modifying the constructor to check if the input parameters were not null and throwing a NullPointerException if they were. A manual verification was chosen instead of using an assert framework. The bug was fixed in an own branch, tests were created, and a Jira Task was created. A pull request was opened, and the pull request was closed by asfgit."
    },
    "Math_7_src/main/java/org/apache/commons/math3/ode/AbstractIntegrator.java_296_405": {
        "src": "protected double acceptStep(final AbstractStepInterpolator interpolator,\n                                final double[] y, final double[] yDot, final double tEnd)\n        throws MaxCountExceededException, DimensionMismatchException, NoBracketingException {\n\n            double previousT = interpolator.getGlobalPreviousTime();\n            final double currentT = interpolator.getGlobalCurrentTime();\n\n            // initialize the events states if needed\n            if (! statesInitialized) {\n                for (EventState state : eventsStates) {\n                    state.reinitializeBegin(interpolator);\n                }\n                statesInitialized = true;\n            }\n\n            // search for next events that may occur during the step\n            final int orderingSign = interpolator.isForward() ? +1 : -1;\n            SortedSet<EventState> occuringEvents = new TreeSet<EventState>(new Comparator<EventState>() {\n\n                /** {@inheritDoc} */\n                public int compare(EventState es0, EventState es1) {\n                    return orderingSign * Double.compare(es0.getEventTime(), es1.getEventTime());\n                }\n\n            });\n\n            for (final EventState state : eventsStates) {\n                if (state.evaluateStep(interpolator)) {\n                    // the event occurs during the current step\n                    occuringEvents.add(state);\n                }\n            }\n\n            while (!occuringEvents.isEmpty()) {\n\n                // handle the chronologically first event\n                final Iterator<EventState> iterator = occuringEvents.iterator();\n                final EventState currentEvent = iterator.next();\n                iterator.remove();\n\n                // restrict the interpolator to the first part of the step, up to the event\n                final double eventT = currentEvent.getEventTime();\n                interpolator.setSoftPreviousTime(previousT);\n                interpolator.setSoftCurrentTime(eventT);\n\n                // get state at event time\n                interpolator.setInterpolatedTime(eventT);\n                final double[] eventY = interpolator.getInterpolatedState().clone();\n\n                // advance all event states to current time\n                currentEvent.stepAccepted(eventT, eventY);\n                isLastStep = currentEvent.stop();\n\n                // handle the first part of the step, up to the event\n                for (final StepHandler handler : stepHandlers) {\n                    handler.handleStep(interpolator, isLastStep);\n                }\n\n                if (isLastStep) {\n                    // the event asked to stop integration\n                    System.arraycopy(eventY, 0, y, 0, y.length);\n                    for (final EventState remaining : occuringEvents) {\n                        remaining.stepAccepted(eventT, eventY);\n                    }\n                    return eventT;\n                }\n\n                boolean needReset = currentEvent.reset(eventT, eventY);\n                if (needReset) {\n                    // some event handler has triggered changes that\n                    // invalidate the derivatives, we need to recompute them\n                    System.arraycopy(eventY, 0, y, 0, y.length);\n                    computeDerivatives(eventT, y, yDot);\n                    resetOccurred = true;\n                    for (final EventState remaining : occuringEvents) {\n                        remaining.stepAccepted(eventT, eventY);\n                    }\n                    return eventT;\n                }\n\n                // prepare handling of the remaining part of the step\n                previousT = eventT;\n                interpolator.setSoftPreviousTime(eventT);\n                interpolator.setSoftCurrentTime(currentT);\n\n                // check if the same event occurs again in the remaining part of the step\n                if (currentEvent.evaluateStep(interpolator)) {\n                    // the event occurs during the current step\n                    occuringEvents.add(currentEvent);\n                }\n\n            }\n\n            // last part of the step, after the last event\n            interpolator.setInterpolatedTime(currentT);\n            final double[] currentY = interpolator.getInterpolatedState();\n            for (final EventState state : eventsStates) {\n                state.stepAccepted(currentT, currentY);\n                isLastStep = isLastStep || state.stop();\n            }\n            isLastStep = isLastStep || Precision.equals(currentT, tEnd, 1);\n\n            // handle the remaining part of the step, after all events if any\n            for (StepHandler handler : stepHandlers) {\n                handler.handleStep(interpolator, isLastStep);\n            }\n\n            return currentT;\n\n    }",
        "src_wo_comments": "protected double acceptStep ( final AbstractStepInterpolator interpolator , final double [ ] y , final double [ ] yDot , final double tEnd ) throws MaxCountExceededException , DimensionMismatchException , NoBracketingException { double previousT = interpolator . getGlobalPreviousTime ( ) ; final double currentT = interpolator . getGlobalCurrentTime ( ) ; if ( ! statesInitialized ) { for ( EventState state : eventsStates ) { state . reinitializeBegin ( interpolator ) ; } statesInitialized = true ; } final int orderingSign = interpolator . isForward ( ) ? + 1 : - 1 ; SortedSet < EventState > occuringEvents = new TreeSet < EventState > ( new Comparator < EventState > ( ) { public int compare ( EventState es0 , EventState es1 ) { return orderingSign * Double . compare ( es0 . getEventTime ( ) , es1 . getEventTime ( ) ) ; } } ) ; for ( final EventState state : eventsStates ) { if ( state . evaluateStep ( interpolator ) ) { occuringEvents . add ( state ) ; } } while ( ! occuringEvents . isEmpty ( ) ) { final Iterator < EventState > iterator = occuringEvents . iterator ( ) ; final EventState currentEvent = iterator . next ( ) ; iterator . remove ( ) ; final double eventT = currentEvent . getEventTime ( ) ; interpolator . setSoftPreviousTime ( previousT ) ; interpolator . setSoftCurrentTime ( eventT ) ; interpolator . setInterpolatedTime ( eventT ) ; final double [ ] eventY = interpolator . getInterpolatedState ( ) . clone ( ) ; currentEvent . stepAccepted ( eventT , eventY ) ; isLastStep = currentEvent . stop ( ) ; for ( final StepHandler handler : stepHandlers ) { handler . handleStep ( interpolator , isLastStep ) ; } if ( isLastStep ) { System . arraycopy ( eventY , 0 , y , 0 , y . length ) ; for ( final EventState remaining : occuringEvents ) { remaining . stepAccepted ( eventT , eventY ) ; } return eventT ; } boolean needReset = currentEvent . reset ( eventT , eventY ) ; if ( needReset ) { System . arraycopy ( eventY , 0 , y , 0 , y . length ) ; computeDerivatives ( eventT , y , yDot ) ; resetOccurred = true ; for ( final EventState remaining : occuringEvents ) { remaining . stepAccepted ( eventT , eventY ) ; } return eventT ; } previousT = eventT ; interpolator . setSoftPreviousTime ( eventT ) ; interpolator . setSoftCurrentTime ( currentT ) ; if ( currentEvent . evaluateStep ( interpolator ) ) { occuringEvents . add ( currentEvent ) ; } } interpolator . setInterpolatedTime ( currentT ) ; final double [ ] currentY = interpolator . getInterpolatedState ( ) ; for ( final EventState state : eventsStates ) { state . stepAccepted ( currentT , currentY ) ; isLastStep = isLastStep || state . stop ( ) ; } isLastStep = isLastStep || Precision . equals ( currentT , tEnd , 1 ) ; for ( StepHandler handler : stepHandlers ) { handler . handleStep ( interpolator , isLastStep ) ; } return currentT ; }",
        "fixed_src": "protected double acceptStep(final AbstractStepInterpolator interpolator,\n                                final double[] y, final double[] yDot, final double tEnd)\n        throws MaxCountExceededException, DimensionMismatchException, NoBracketingException {\n\n            double previousT = interpolator.getGlobalPreviousTime();\n            final double currentT = interpolator.getGlobalCurrentTime();\n\n            // initialize the events states if needed\n            if (! statesInitialized) {\n                for (EventState state : eventsStates) {\n                    state.reinitializeBegin(interpolator);\n                }\n                statesInitialized = true;\n            }\n\n            // search for next events that may occur during the step\n            final int orderingSign = interpolator.isForward() ? +1 : -1;\n            SortedSet<EventState> occuringEvents = new TreeSet<EventState>(new Comparator<EventState>() {\n\n                /** {@inheritDoc} */\n                public int compare(EventState es0, EventState es1) {\n                    return orderingSign * Double.compare(es0.getEventTime(), es1.getEventTime());\n                }\n\n            });\n\n            for (final EventState state : eventsStates) {\n                if (state.evaluateStep(interpolator)) {\n                    // the event occurs during the current step\n                    occuringEvents.add(state);\n                }\n            }\n\n            while (!occuringEvents.isEmpty()) {\n\n                // handle the chronologically first event\n                final Iterator<EventState> iterator = occuringEvents.iterator();\n                final EventState currentEvent = iterator.next();\n                iterator.remove();\n\n                // restrict the interpolator to the first part of the step, up to the event\n                final double eventT = currentEvent.getEventTime();\n                interpolator.setSoftPreviousTime(previousT);\n                interpolator.setSoftCurrentTime(eventT);\n\n                // get state at event time\n                interpolator.setInterpolatedTime(eventT);\n                final double[] eventY = interpolator.getInterpolatedState().clone();\n\n                // advance all event states to current time\n                for (final EventState state : eventsStates) {\n                    state.stepAccepted(eventT, eventY);\n                    isLastStep = isLastStep || state.stop();\n                }\n\n                // handle the first part of the step, up to the event\n                for (final StepHandler handler : stepHandlers) {\n                    handler.handleStep(interpolator, isLastStep);\n                }\n\n                if (isLastStep) {\n                    // the event asked to stop integration\n                    System.arraycopy(eventY, 0, y, 0, y.length);\n                    return eventT;\n                }\n\n                boolean needReset = false;\n                for (final EventState state : eventsStates) {\n                    needReset =  needReset || state.reset(eventT, eventY);\n                }\n                if (needReset) {\n                    // some event handler has triggered changes that\n                    // invalidate the derivatives, we need to recompute them\n                    System.arraycopy(eventY, 0, y, 0, y.length);\n                    computeDerivatives(eventT, y, yDot);\n                    resetOccurred = true;\n                    return eventT;\n                }\n\n                // prepare handling of the remaining part of the step\n                previousT = eventT;\n                interpolator.setSoftPreviousTime(eventT);\n                interpolator.setSoftCurrentTime(currentT);\n\n                // check if the same event occurs again in the remaining part of the step\n                if (currentEvent.evaluateStep(interpolator)) {\n                    // the event occurs during the current step\n                    occuringEvents.add(currentEvent);\n                }\n\n            }\n\n            // last part of the step, after the last event\n            interpolator.setInterpolatedTime(currentT);\n            final double[] currentY = interpolator.getInterpolatedState();\n            for (final EventState state : eventsStates) {\n                state.stepAccepted(currentT, currentY);\n                isLastStep = isLastStep || state.stop();\n            }\n            isLastStep = isLastStep || Precision.equals(currentT, tEnd, 1);\n\n            // handle the remaining part of the step, after all events if any\n            for (StepHandler handler : stepHandlers) {\n                handler.handleStep(interpolator, isLastStep);\n            }\n\n            return currentT;\n\n    }",
        "fixed_src_wo_comments": "protected double acceptStep ( final AbstractStepInterpolator interpolator , final double [ ] y , final double [ ] yDot , final double tEnd ) throws MaxCountExceededException , DimensionMismatchException , NoBracketingException { double previousT = interpolator . getGlobalPreviousTime ( ) ; final double currentT = interpolator . getGlobalCurrentTime ( ) ; if ( ! statesInitialized ) { for ( EventState state : eventsStates ) { state . reinitializeBegin ( interpolator ) ; } statesInitialized = true ; } final int orderingSign = interpolator . isForward ( ) ? + 1 : - 1 ; SortedSet < EventState > occuringEvents = new TreeSet < EventState > ( new Comparator < EventState > ( ) { public int compare ( EventState es0 , EventState es1 ) { return orderingSign * Double . compare ( es0 . getEventTime ( ) , es1 . getEventTime ( ) ) ; } } ) ; for ( final EventState state : eventsStates ) { if ( state . evaluateStep ( interpolator ) ) { occuringEvents . add ( state ) ; } } while ( ! occuringEvents . isEmpty ( ) ) { final Iterator < EventState > iterator = occuringEvents . iterator ( ) ; final EventState currentEvent = iterator . next ( ) ; iterator . remove ( ) ; final double eventT = currentEvent . getEventTime ( ) ; interpolator . setSoftPreviousTime ( previousT ) ; interpolator . setSoftCurrentTime ( eventT ) ; interpolator . setInterpolatedTime ( eventT ) ; final double [ ] eventY = interpolator . getInterpolatedState ( ) . clone ( ) ; for ( final EventState state : eventsStates ) { state . stepAccepted ( eventT , eventY ) ; isLastStep = isLastStep || state . stop ( ) ; } for ( final StepHandler handler : stepHandlers ) { handler . handleStep ( interpolator , isLastStep ) ; } if ( isLastStep ) { System . arraycopy ( eventY , 0 , y , 0 , y . length ) ; return eventT ; } boolean needReset = false ; for ( final EventState state : eventsStates ) { needReset = needReset || state . reset ( eventT , eventY ) ; } if ( needReset ) { System . arraycopy ( eventY , 0 , y , 0 , y . length ) ; computeDerivatives ( eventT , y , yDot ) ; resetOccurred = true ; return eventT ; } previousT = eventT ; interpolator . setSoftPreviousTime ( eventT ) ; interpolator . setSoftCurrentTime ( currentT ) ; if ( currentEvent . evaluateStep ( interpolator ) ) { occuringEvents . add ( currentEvent ) ; } } interpolator . setInterpolatedTime ( currentT ) ; final double [ ] currentY = interpolator . getInterpolatedState ( ) ; for ( final EventState state : eventsStates ) { state . stepAccepted ( currentT , currentY ) ; isLastStep = isLastStep || state . stop ( ) ; } isLastStep = isLastStep || Precision . equals ( currentT , tEnd , 1 ) ; for ( StepHandler handler : stepHandlers ) { handler . handleStep ( interpolator , isLastStep ) ; } return currentT ; }",
        "summary": "event state not updated if an unrelated event triggers a RESET_STATE during ODE integration",
        "Description": "When an ODE solver manages several different event types, there are some unwanted side effects.\n\nIf one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return.\n\nAs a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20.\n\nThe attached file reproduces the problem.\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-950",
        "comments": [
            "Test case reproducing the bug.",
            "Fixed in subversion repository as of r1458294.",
            "Closing issue as version 3.2 has been released on 2013-04-06."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of revision 1458294, and the issue has been closed as version 3.2 has been released on April 6, 2013."
    },
    "Mockito_15_src/org/mockito/internal/configuration/injection/FinalMockCandidateFilter.java_18_40": {
        "src": "public OngoingInjecter filterCandidate(final Collection<Object> mocks, final Field field, final Object fieldInstance) {\n        if(mocks.size() == 1) {\n            final Object matchingMock = mocks.iterator().next();\n\n            return new OngoingInjecter() {\n                public boolean thenInject() {\n                    try {\n                            new FieldSetter(fieldInstance, field).set(matchingMock);\n                    } catch (Exception e) {\n                        throw new MockitoException(\"Problems injecting dependency in \" + field.getName(), e);\n                    }\n                    return true;\n                }\n            };\n        }\n\n        return new OngoingInjecter() {\n            public boolean thenInject() {\n                return false;\n            }\n        };\n\n    }",
        "src_wo_comments": "public OngoingInjecter filterCandidate ( final Collection < Object > mocks , final Field field , final Object fieldInstance ) { if ( mocks . size ( ) == 1 ) { final Object matchingMock = mocks . iterator ( ) . next ( ) ; return new OngoingInjecter ( ) { public boolean thenInject ( ) { try { new FieldSetter ( fieldInstance , field ) . set ( matchingMock ) ; } catch ( Exception e ) { throw new MockitoException ( \"Problems injecting dependency in \" + field . getName ( ) , e ) ; } return true ; } } ; } return new OngoingInjecter ( ) { public boolean thenInject ( ) { return false ; } } ; }",
        "fixed_src": "public OngoingInjecter filterCandidate(final Collection<Object> mocks, final Field field, final Object fieldInstance) {\n        if(mocks.size() == 1) {\n            final Object matchingMock = mocks.iterator().next();\n\n            return new OngoingInjecter() {\n                public boolean thenInject() {\n                    try {\n                        if (!new BeanPropertySetter(fieldInstance, field).set(matchingMock)) {\n                            new FieldSetter(fieldInstance, field).set(matchingMock);\n                        }\n                    } catch (Exception e) {\n                        throw new MockitoException(\"Problems injecting dependency in \" + field.getName(), e);\n                    }\n                    return true;\n                }\n            };\n        }\n\n        return new OngoingInjecter() {\n            public boolean thenInject() {\n                return false;\n            }\n        };\n\n    }",
        "fixed_src_wo_comments": "public OngoingInjecter filterCandidate ( final Collection < Object > mocks , final Field field , final Object fieldInstance ) { if ( mocks . size ( ) == 1 ) { final Object matchingMock = mocks . iterator ( ) . next ( ) ; return new OngoingInjecter ( ) { public boolean thenInject ( ) { try { if ( ! new BeanPropertySetter ( fieldInstance , field ) . set ( matchingMock ) ) { new FieldSetter ( fieldInstance , field ) . set ( matchingMock ) ; } } catch ( Exception e ) { throw new MockitoException ( \"Problems injecting dependency in \" + field . getName ( ) , e ) ; } return true ; } } ; } return new OngoingInjecter ( ) { public boolean thenInject ( ) { return false ; } } ; }",
        "summary": "ArgumentCaptor no longer working for varargs",
        "Description": "Fixes #188 . These commits should fix issue with capturing varargs.\n",
        "issue_url": null,
        "comments": null,
        "summarized_discussion": "\n\nWithout more information, it is not possible to summarize the solution to the bug."
    },
    "JacksonCore_3_src/main/java/com/fasterxml/jackson/core/json/UTF8StreamJsonParser.java_113_127": {
        "src": "public UTF8StreamJsonParser(IOContext ctxt, int features, InputStream in,\n            ObjectCodec codec, BytesToNameCanonicalizer sym,\n            byte[] inputBuffer, int start, int end,\n            boolean bufferRecyclable)\n    {\n        super(ctxt, features);\n        _inputStream = in;\n        _objectCodec = codec;\n        _symbols = sym;\n        _inputBuffer = inputBuffer;\n        _inputPtr = start;\n        _inputEnd = end;\n        // If we have offset, need to omit that from byte offset, so:\n        _bufferRecyclable = bufferRecyclable;\n    }",
        "src_wo_comments": "public UTF8StreamJsonParser ( IOContext ctxt , int features , InputStream in , ObjectCodec codec , BytesToNameCanonicalizer sym , byte [ ] inputBuffer , int start , int end , boolean bufferRecyclable ) { super ( ctxt , features ) ; _inputStream = in ; _objectCodec = codec ; _symbols = sym ; _inputBuffer = inputBuffer ; _inputPtr = start ; _inputEnd = end ; _bufferRecyclable = bufferRecyclable ; }",
        "fixed_src": "public UTF8StreamJsonParser(IOContext ctxt, int features, InputStream in,\n            ObjectCodec codec, BytesToNameCanonicalizer sym,\n            byte[] inputBuffer, int start, int end,\n            boolean bufferRecyclable)\n    {\n        super(ctxt, features);\n        _inputStream = in;\n        _objectCodec = codec;\n        _symbols = sym;\n        _inputBuffer = inputBuffer;\n        _inputPtr = start;\n        _inputEnd = end;\n        _currInputRowStart = start;\n        // If we have offset, need to omit that from byte offset, so:\n        _currInputProcessed = -start;\n        _bufferRecyclable = bufferRecyclable;\n    }",
        "fixed_src_wo_comments": "public UTF8StreamJsonParser ( IOContext ctxt , int features , InputStream in , ObjectCodec codec , BytesToNameCanonicalizer sym , byte [ ] inputBuffer , int start , int end , boolean bufferRecyclable ) { super ( ctxt , features ) ; _inputStream = in ; _objectCodec = codec ; _symbols = sym ; _inputBuffer = inputBuffer ; _inputPtr = start ; _inputEnd = end ; _currInputRowStart = start ; _currInputProcessed = - start ; _bufferRecyclable = bufferRecyclable ; }",
        "summary": "_currInputRowStart isn't initialized in UTF8StreamJsonParser() constructor. The column position will be wrong. ",
        "Description": "The UTF8StreamJson Parser constructor allows to specify the start position. But it doesn't set the  \"_currInputRowStart\" as the same value. It is still 0. So when raise the exception, the column calculation (ParserBase.getCurrentLocation() )will be wrong.\n\nint col = _inputPtr - _currInputRowStart + 1; // 1-based\n\npublic UTF8StreamJsonParser(IOContext ctxt, int features, InputStream in,\n            ObjectCodec codec, BytesToNameCanonicalizer sym,\n            byte[] inputBuffer, int start, int end,\n            boolean bufferRecyclable)\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Yes, you are right.\n\nFixed for 2.3.0 -- there is a potential issue with respect BOM markers. Offset will now ignore BOM (considered to be out of input); but work as expected for input that does not start at index 0.\nMay need to visit this question again in future; short-term problem is just that BOM is stripped off by bootstrapper, so all parser sees is a different offset. But this problem should also occur with streams, so it is sort of consistent at least.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was fixed in version 2.3.0, where BOM markers are now ignored and the offset will work as expected for inputs that do not start at index 0. This should also apply to streams, making the behavior consistent."
    },
    "JacksonDatabind_43_src/main/java/com/fasterxml/jackson/databind/deser/impl/ObjectIdValueProperty.java_74_96": {
        "src": "@Override\n    public Object deserializeSetAndReturn(JsonParser p,\n    \t\tDeserializationContext ctxt, Object instance) throws IOException\n    {\n        Object id = _valueDeserializer.deserialize(p, ctxt);\n        /* 02-Apr-2015, tatu: Actually, as per [databind#742], let it be;\n         *  missing or null id is needed for some cases, such as cases where id\n         *  will be generated externally, at a later point, and is not available\n         *  quite yet. Typical use case is with DB inserts.\n         */\n        // note: no null checks (unlike usually); deserializer should fail if one found\n        if (id == null) {\n            return null;\n        }\n        ReadableObjectId roid = ctxt.findObjectId(id, _objectIdReader.generator, _objectIdReader.resolver);\n        roid.bindItem(instance);\n        // also: may need to set a property value as well\n        SettableBeanProperty idProp = _objectIdReader.idProperty;\n        if (idProp != null) {\n            return idProp.setAndReturn(instance, id);\n        }\n        return instance;\n    }",
        "src_wo_comments": "@ Override public Object deserializeSetAndReturn ( JsonParser p , DeserializationContext ctxt , Object instance ) throws IOException { Object id = _valueDeserializer . deserialize ( p , ctxt ) ; if ( id == null ) { return null ; } ReadableObjectId roid = ctxt . findObjectId ( id , _objectIdReader . generator , _objectIdReader . resolver ) ; roid . bindItem ( instance ) ; SettableBeanProperty idProp = _objectIdReader . idProperty ; if ( idProp != null ) { return idProp . setAndReturn ( instance , id ) ; } return instance ; }",
        "fixed_src": "@Override\n    public Object deserializeSetAndReturn(JsonParser p,\n    \t\tDeserializationContext ctxt, Object instance) throws IOException\n    {\n        /* 02-Apr-2015, tatu: Actually, as per [databind#742], let it be;\n         *  missing or null id is needed for some cases, such as cases where id\n         *  will be generated externally, at a later point, and is not available\n         *  quite yet. Typical use case is with DB inserts.\n         */\n        // note: no null checks (unlike usually); deserializer should fail if one found\n        if (p.hasToken(JsonToken.VALUE_NULL)) {\n            return null;\n        }\n        Object id = _valueDeserializer.deserialize(p, ctxt);\n        ReadableObjectId roid = ctxt.findObjectId(id, _objectIdReader.generator, _objectIdReader.resolver);\n        roid.bindItem(instance);\n        // also: may need to set a property value as well\n        SettableBeanProperty idProp = _objectIdReader.idProperty;\n        if (idProp != null) {\n            return idProp.setAndReturn(instance, id);\n        }\n        return instance;\n    }",
        "fixed_src_wo_comments": "@ Override public Object deserializeSetAndReturn ( JsonParser p , DeserializationContext ctxt , Object instance ) throws IOException { if ( p . hasToken ( JsonToken . VALUE_NULL ) ) { return null ; } Object id = _valueDeserializer . deserialize ( p , ctxt ) ; ReadableObjectId roid = ctxt . findObjectId ( id , _objectIdReader . generator , _objectIdReader . resolver ) ; roid . bindItem ( instance ) ; SettableBeanProperty idProp = _objectIdReader . idProperty ; if ( idProp != null ) { return idProp . setAndReturn ( instance , id ) ; } return instance ; }",
        "summary": "Problem with Object id handling, explicit `null` token",
        "Description": "According to #742, it shouldn't throw an exception if the value of the property is null\n",
        "issue_url": null,
        "comments": null,
        "summarized_discussion": "\n\nWithout more information, it is impossible to summarize the solution to the bug."
    },
    "JacksonDatabind_107_src/main/java/com/fasterxml/jackson/databind/jsontype/impl/TypeDeserializerBase.java_146_199": {
        "src": "protected final JsonDeserializer<Object> _findDeserializer(DeserializationContext ctxt,\n            String typeId) throws IOException\n    {\n        JsonDeserializer<Object> deser = _deserializers.get(typeId);\n        if (deser == null) {\n            /* As per [databind#305], need to provide contextual info. But for\n             * backwards compatibility, let's start by only supporting this\n             * for base class, not via interface. Later on we can add this\n             * to the interface, assuming deprecation at base class helps.\n             */\n            JavaType type = _idResolver.typeFromId(ctxt, typeId);\n            if (type == null) {\n                // use the default impl if no type id available:\n                deser = _findDefaultImplDeserializer(ctxt);\n                if (deser == null) {\n                    // 10-May-2016, tatu: We may get some help...\n                    JavaType actual = _handleUnknownTypeId(ctxt, typeId);\n                    if (actual == null) { // what should this be taken to mean?\n                        // 17-Jan-2019, tatu: As per [databind#2221], better NOT return `null` but...\n                        return null;\n                    }\n                    // ... would this actually work?\n                    deser = ctxt.findContextualValueDeserializer(actual, _property);\n                }\n            } else {\n                /* 16-Dec-2010, tatu: Since nominal type we get here has no (generic) type parameters,\n                 *   we actually now need to explicitly narrow from base type (which may have parameterization)\n                 *   using raw type.\n                 *\n                 *   One complication, though; cannot change 'type class' (simple type to container); otherwise\n                 *   we may try to narrow a SimpleType (Object.class) into MapType (Map.class), losing actual\n                 *   type in process (getting SimpleType of Map.class which will not work as expected)\n                 */\n                if ((_baseType != null)\n                        && _baseType.getClass() == type.getClass()) {\n                    /* 09-Aug-2015, tatu: Not sure if the second part of the check makes sense;\n                     *   but it appears to check that JavaType impl class is the same which is\n                     *   important for some reason?\n                     *   Disabling the check will break 2 Enum-related tests.\n                     */\n                    // 19-Jun-2016, tatu: As per [databind#1270] we may actually get full\n                    //   generic type with custom type resolvers. If so, should try to retain them.\n                    //  Whether this is sufficient to avoid problems remains to be seen, but for\n                    //  now it should improve things.\n                    if (!type.hasGenericTypes()) {\n                        type = ctxt.getTypeFactory().constructSpecializedType(_baseType, type.getRawClass());\n                    }\n                }\n                deser = ctxt.findContextualValueDeserializer(type, _property);\n            }\n            _deserializers.put(typeId, deser);\n        }\n        return deser;\n    }",
        "src_wo_comments": "protected final JsonDeserializer < Object > _findDeserializer ( DeserializationContext ctxt , String typeId ) throws IOException { JsonDeserializer < Object > deser = _deserializers . get ( typeId ) ; if ( deser == null ) { JavaType type = _idResolver . typeFromId ( ctxt , typeId ) ; if ( type == null ) { deser = _findDefaultImplDeserializer ( ctxt ) ; if ( deser == null ) { JavaType actual = _handleUnknownTypeId ( ctxt , typeId ) ; if ( actual == null ) { return null ; } deser = ctxt . findContextualValueDeserializer ( actual , _property ) ; } } else { if ( ( _baseType != null ) && _baseType . getClass ( ) == type . getClass ( ) ) { if ( ! type . hasGenericTypes ( ) ) { type = ctxt . getTypeFactory ( ) . constructSpecializedType ( _baseType , type . getRawClass ( ) ) ; } } deser = ctxt . findContextualValueDeserializer ( type , _property ) ; } _deserializers . put ( typeId , deser ) ; } return deser ; }",
        "fixed_src": "protected final JsonDeserializer<Object> _findDeserializer(DeserializationContext ctxt,\n            String typeId) throws IOException\n    {\n        JsonDeserializer<Object> deser = _deserializers.get(typeId);\n        if (deser == null) {\n            /* As per [databind#305], need to provide contextual info. But for\n             * backwards compatibility, let's start by only supporting this\n             * for base class, not via interface. Later on we can add this\n             * to the interface, assuming deprecation at base class helps.\n             */\n            JavaType type = _idResolver.typeFromId(ctxt, typeId);\n            if (type == null) {\n                // use the default impl if no type id available:\n                deser = _findDefaultImplDeserializer(ctxt);\n                if (deser == null) {\n                    // 10-May-2016, tatu: We may get some help...\n                    JavaType actual = _handleUnknownTypeId(ctxt, typeId);\n                    if (actual == null) { // what should this be taken to mean?\n                        // 17-Jan-2019, tatu: As per [databind#2221], better NOT return `null` but...\n                        return NullifyingDeserializer.instance;\n                    }\n                    // ... would this actually work?\n                    deser = ctxt.findContextualValueDeserializer(actual, _property);\n                }\n            } else {\n                /* 16-Dec-2010, tatu: Since nominal type we get here has no (generic) type parameters,\n                 *   we actually now need to explicitly narrow from base type (which may have parameterization)\n                 *   using raw type.\n                 *\n                 *   One complication, though; cannot change 'type class' (simple type to container); otherwise\n                 *   we may try to narrow a SimpleType (Object.class) into MapType (Map.class), losing actual\n                 *   type in process (getting SimpleType of Map.class which will not work as expected)\n                 */\n                if ((_baseType != null)\n                        && _baseType.getClass() == type.getClass()) {\n                    /* 09-Aug-2015, tatu: Not sure if the second part of the check makes sense;\n                     *   but it appears to check that JavaType impl class is the same which is\n                     *   important for some reason?\n                     *   Disabling the check will break 2 Enum-related tests.\n                     */\n                    // 19-Jun-2016, tatu: As per [databind#1270] we may actually get full\n                    //   generic type with custom type resolvers. If so, should try to retain them.\n                    //  Whether this is sufficient to avoid problems remains to be seen, but for\n                    //  now it should improve things.\n                    if (!type.hasGenericTypes()) {\n                        type = ctxt.getTypeFactory().constructSpecializedType(_baseType, type.getRawClass());\n                    }\n                }\n                deser = ctxt.findContextualValueDeserializer(type, _property);\n            }\n            _deserializers.put(typeId, deser);\n        }\n        return deser;\n    }",
        "fixed_src_wo_comments": "protected final JsonDeserializer < Object > _findDeserializer ( DeserializationContext ctxt , String typeId ) throws IOException { JsonDeserializer < Object > deser = _deserializers . get ( typeId ) ; if ( deser == null ) { JavaType type = _idResolver . typeFromId ( ctxt , typeId ) ; if ( type == null ) { deser = _findDefaultImplDeserializer ( ctxt ) ; if ( deser == null ) { JavaType actual = _handleUnknownTypeId ( ctxt , typeId ) ; if ( actual == null ) { return NullifyingDeserializer . instance ; } deser = ctxt . findContextualValueDeserializer ( actual , _property ) ; } } else { if ( ( _baseType != null ) && _baseType . getClass ( ) == type . getClass ( ) ) { if ( ! type . hasGenericTypes ( ) ) { type = ctxt . getTypeFactory ( ) . constructSpecializedType ( _baseType , type . getRawClass ( ) ) ; } } deser = ctxt . findContextualValueDeserializer ( type , _property ) ; } _deserializers . put ( typeId , deser ) ; } return deser ; }",
        "summary": "`DeserializationProblemHandler.handleUnknownTypeId()` returning `Void.class`, enableDefaultTyping causing NPE",
        "Description": "Returning Void.class from com.fasterxml.jackson.databind.deser.HandleUnknowTypeIdTest.testDeserializationWithDeserializationProblemHandler().new DeserializationProblemHandler() {...}.handleUnknownTypeId(DeserializationContext, JavaType, String, TypeIdResolver, String) is causing a NPE in jackson 2.9. I'll provide a pull request illustrating the issue in a test. ",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this! I'll have a look."
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is not yet known, as the person responding to the report has stated that they will look into it."
    },
    "Math_26_src/main/java/org/apache/commons/math3/fraction/Fraction.java_175_238": {
        "src": "private Fraction(double value, double epsilon, int maxDenominator, int maxIterations)\n        throws FractionConversionException\n    {\n        long overflow = Integer.MAX_VALUE;\n        double r0 = value;\n        long a0 = (long)FastMath.floor(r0);\n        if (a0 > overflow) {\n            throw new FractionConversionException(value, a0, 1l);\n        }\n\n        // check for (almost) integer arguments, which should not go\n        // to iterations.\n        if (FastMath.abs(a0 - value) < epsilon) {\n            this.numerator = (int) a0;\n            this.denominator = 1;\n            return;\n        }\n\n        long p0 = 1;\n        long q0 = 0;\n        long p1 = a0;\n        long q1 = 1;\n\n        long p2 = 0;\n        long q2 = 1;\n\n        int n = 0;\n        boolean stop = false;\n        do {\n            ++n;\n            double r1 = 1.0 / (r0 - a0);\n            long a1 = (long)FastMath.floor(r1);\n            p2 = (a1 * p1) + p0;\n            q2 = (a1 * q1) + q0;\n            if ((p2 > overflow) || (q2 > overflow)) {\n                throw new FractionConversionException(value, p2, q2);\n            }\n\n            double convergent = (double)p2 / (double)q2;\n            if (n < maxIterations && FastMath.abs(convergent - value) > epsilon && q2 < maxDenominator) {\n                p0 = p1;\n                p1 = p2;\n                q0 = q1;\n                q1 = q2;\n                a0 = a1;\n                r0 = r1;\n            } else {\n                stop = true;\n            }\n        } while (!stop);\n\n        if (n >= maxIterations) {\n            throw new FractionConversionException(value, maxIterations);\n        }\n\n        if (q2 < maxDenominator) {\n            this.numerator = (int) p2;\n            this.denominator = (int) q2;\n        } else {\n            this.numerator = (int) p1;\n            this.denominator = (int) q1;\n        }\n\n    }",
        "src_wo_comments": "private Fraction ( double value , double epsilon , int maxDenominator , int maxIterations ) throws FractionConversionException { long overflow = Integer . MAX_VALUE ; double r0 = value ; long a0 = ( long ) FastMath . floor ( r0 ) ; if ( a0 > overflow ) { throw new FractionConversionException ( value , a0 , 1l ) ; } if ( FastMath . abs ( a0 - value ) < epsilon ) { this . numerator = ( int ) a0 ; this . denominator = 1 ; return ; } long p0 = 1 ; long q0 = 0 ; long p1 = a0 ; long q1 = 1 ; long p2 = 0 ; long q2 = 1 ; int n = 0 ; boolean stop = false ; do { ++ n ; double r1 = 1.0 / ( r0 - a0 ) ; long a1 = ( long ) FastMath . floor ( r1 ) ; p2 = ( a1 * p1 ) + p0 ; q2 = ( a1 * q1 ) + q0 ; if ( ( p2 > overflow ) || ( q2 > overflow ) ) { throw new FractionConversionException ( value , p2 , q2 ) ; } double convergent = ( double ) p2 / ( double ) q2 ; if ( n < maxIterations && FastMath . abs ( convergent - value ) > epsilon && q2 < maxDenominator ) { p0 = p1 ; p1 = p2 ; q0 = q1 ; q1 = q2 ; a0 = a1 ; r0 = r1 ; } else { stop = true ; } } while ( ! stop ) ; if ( n >= maxIterations ) { throw new FractionConversionException ( value , maxIterations ) ; } if ( q2 < maxDenominator ) { this . numerator = ( int ) p2 ; this . denominator = ( int ) q2 ; } else { this . numerator = ( int ) p1 ; this . denominator = ( int ) q1 ; } }",
        "fixed_src": "private Fraction(double value, double epsilon, int maxDenominator, int maxIterations)\n        throws FractionConversionException\n    {\n        long overflow = Integer.MAX_VALUE;\n        double r0 = value;\n        long a0 = (long)FastMath.floor(r0);\n        if (FastMath.abs(a0) > overflow) {\n            throw new FractionConversionException(value, a0, 1l);\n        }\n\n        // check for (almost) integer arguments, which should not go\n        // to iterations.\n        if (FastMath.abs(a0 - value) < epsilon) {\n            this.numerator = (int) a0;\n            this.denominator = 1;\n            return;\n        }\n\n        long p0 = 1;\n        long q0 = 0;\n        long p1 = a0;\n        long q1 = 1;\n\n        long p2 = 0;\n        long q2 = 1;\n\n        int n = 0;\n        boolean stop = false;\n        do {\n            ++n;\n            double r1 = 1.0 / (r0 - a0);\n            long a1 = (long)FastMath.floor(r1);\n            p2 = (a1 * p1) + p0;\n            q2 = (a1 * q1) + q0;\n            if ((FastMath.abs(p2) > overflow) || (FastMath.abs(q2) > overflow)) {\n                throw new FractionConversionException(value, p2, q2);\n            }\n\n            double convergent = (double)p2 / (double)q2;\n            if (n < maxIterations && FastMath.abs(convergent - value) > epsilon && q2 < maxDenominator) {\n                p0 = p1;\n                p1 = p2;\n                q0 = q1;\n                q1 = q2;\n                a0 = a1;\n                r0 = r1;\n            } else {\n                stop = true;\n            }\n        } while (!stop);\n\n        if (n >= maxIterations) {\n            throw new FractionConversionException(value, maxIterations);\n        }\n\n        if (q2 < maxDenominator) {\n            this.numerator = (int) p2;\n            this.denominator = (int) q2;\n        } else {\n            this.numerator = (int) p1;\n            this.denominator = (int) q1;\n        }\n\n    }",
        "fixed_src_wo_comments": "private Fraction ( double value , double epsilon , int maxDenominator , int maxIterations ) throws FractionConversionException { long overflow = Integer . MAX_VALUE ; double r0 = value ; long a0 = ( long ) FastMath . floor ( r0 ) ; if ( FastMath . abs ( a0 ) > overflow ) { throw new FractionConversionException ( value , a0 , 1l ) ; } if ( FastMath . abs ( a0 - value ) < epsilon ) { this . numerator = ( int ) a0 ; this . denominator = 1 ; return ; } long p0 = 1 ; long q0 = 0 ; long p1 = a0 ; long q1 = 1 ; long p2 = 0 ; long q2 = 1 ; int n = 0 ; boolean stop = false ; do { ++ n ; double r1 = 1.0 / ( r0 - a0 ) ; long a1 = ( long ) FastMath . floor ( r1 ) ; p2 = ( a1 * p1 ) + p0 ; q2 = ( a1 * q1 ) + q0 ; if ( ( FastMath . abs ( p2 ) > overflow ) || ( FastMath . abs ( q2 ) > overflow ) ) { throw new FractionConversionException ( value , p2 , q2 ) ; } double convergent = ( double ) p2 / ( double ) q2 ; if ( n < maxIterations && FastMath . abs ( convergent - value ) > epsilon && q2 < maxDenominator ) { p0 = p1 ; p1 = p2 ; q0 = q1 ; q1 = q2 ; a0 = a1 ; r0 = r1 ; } else { stop = true ; } } while ( ! stop ) ; if ( n >= maxIterations ) { throw new FractionConversionException ( value , maxIterations ) ; } if ( q2 < maxDenominator ) { this . numerator = ( int ) p2 ; this . denominator = ( int ) q2 ; } else { this . numerator = ( int ) p1 ; this . denominator = ( int ) q1 ; } }",
        "summary": "Fraction(double, int) constructor strange behaviour",
        "Description": "The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest:\n\n1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value\n\n2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.\n\nI have, as of yet, not found a solution. The constructor looks like this:\n\npublic Fraction(double value, int maxDenominator)\n        throws FractionConversionException\n    {\n       this(value, 0, maxDenominator, 100);\n    }\n\nIncreasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest. \n\nThe problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find.\n\nThis bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.\n\n* It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that \"since fractions are always in lowest terms, numerators and can be compared directly for equality\", so it seems like this is the intention. ",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-836",
        "comments": [
            "FractionTestByAxiom.java added: Tests the Fraction class through axioms. Not intended to be a part of a finished product, but to explore the class, looking for bugs.",
            "value-maxDenominator_pairs_that_fails added: pairs of doubles and ints that makes the constructor Fraction(double, int) return faulty Fractions.",
            "The overflow check in Fraction does not take negative values into account, it has to be changed to the following:\n\n{noformat}\n  if (FastMath.abs(a0) > overflow) {\n      throw new FractionConversionException(value, a0, 1l);\n  }\n\n...\n\n  if ((FastMath.abs(p2) > overflow) || (FastMath.abs(q2) > overflow)) {\n      throw new FractionConversionException(value, p2, q2);\n  }\n{noformat}\n\nIn that case your examples fail correctly with a FractionConversionException and all unit tests run through successfully.\n\nI would be interested how you generated the test cases. Are these auto-generated?\n\nThanks,\n\nThomas",
            "Committed the changes in r1368253.",
            "While looking into this issue, I realized that there is a very similar class in commons-lang, which also does a reduction when creating a Fraction object from a double value. We should consider also doing this for commons-math.",
            "Thomas - the test cases are not auto-generated, they are hand-written. We're working on a tool - JaxT2 -  that interprets axioms for classes and auto-generates unit tests through reflection, to improve on the old JaxT tool (http://www.ii.uib.no/mouldable/testing/jaxt/index.html).\n\nGood job with the overflow.",
            "Thanks for the info. This work looks really interested. If you need any support in further testing commons-math or any other commons component, do not hesitate to ask.\n\nI resolve this issue now, as it seems to fix your problems.\n\nThomas",
            "JaxT definitely looks very interesting and broadly useful for us.  Looking forward to more applications to commons math and other commons components.  "
        ],
        "summarized_discussion": "\n\nThe bug in FractionTestByAxiom.java was fixed by changing the overflow check in Fraction to take negative values into account. The changes were committed in r1368253, which fixed the faulty Fractions returned by the constructor Fraction(double, int). JaxT2, a tool that interprets axioms for classes and auto-generates unit tests through reflection, was mentioned, and it was suggested that commons-math should consider doing the same reduction when creating a Fraction object from a double value. The issue was resolved, and it was suggested that JaxT should be applied to more applications in commons math and other commons components."
    },
    "JacksonDatabind_14_src/main/java/com/fasterxml/jackson/databind/ObjectReader.java_1468_1486": {
        "src": "protected JsonNode _bindAsTree(JsonParser jp) throws IOException\n    {\n        JsonNode result;\n        JsonToken t = _initForReading(jp);\n        if (t == JsonToken.VALUE_NULL || t == JsonToken.END_ARRAY || t == JsonToken.END_OBJECT) {\n            result = NullNode.instance;\n        } else {\n            DeserializationContext ctxt = createDeserializationContext(jp, _config);\n            JsonDeserializer<Object> deser = _findRootDeserializer(ctxt, JSON_NODE_TYPE);\n            if (_unwrapRoot) {\n                result = (JsonNode) _unwrapAndDeserialize(jp, ctxt, JSON_NODE_TYPE, deser);\n            } else {\n                result = (JsonNode) deser.deserialize(jp, ctxt);\n            }\n        }\n        // Need to consume the token too\n        jp.clearCurrentToken();\n        return result;\n    }",
        "src_wo_comments": "protected JsonNode _bindAsTree ( JsonParser jp ) throws IOException { JsonNode result ; JsonToken t = _initForReading ( jp ) ; if ( t == JsonToken . VALUE_NULL || t == JsonToken . END_ARRAY || t == JsonToken . END_OBJECT ) { result = NullNode . instance ; } else { DeserializationContext ctxt = createDeserializationContext ( jp , _config ) ; JsonDeserializer < Object > deser = _findRootDeserializer ( ctxt , JSON_NODE_TYPE ) ; if ( _unwrapRoot ) { result = ( JsonNode ) _unwrapAndDeserialize ( jp , ctxt , JSON_NODE_TYPE , deser ) ; } else { result = ( JsonNode ) deser . deserialize ( jp , ctxt ) ; } } jp . clearCurrentToken ( ) ; return result ; }",
        "fixed_src": "protected JsonNode _bindAsTree(JsonParser jp) throws IOException\n    {\n        JsonNode result;\n        JsonToken t = _initForReading(jp);\n        if (t == JsonToken.VALUE_NULL || t == JsonToken.END_ARRAY || t == JsonToken.END_OBJECT) {\n            result = NullNode.instance;\n        } else {\n            DeserializationContext ctxt = createDeserializationContext(jp, _config);\n            JsonDeserializer<Object> deser = _findTreeDeserializer(ctxt);\n            if (_unwrapRoot) {\n                result = (JsonNode) _unwrapAndDeserialize(jp, ctxt, JSON_NODE_TYPE, deser);\n            } else {\n                result = (JsonNode) deser.deserialize(jp, ctxt);\n            }\n        }\n        // Need to consume the token too\n        jp.clearCurrentToken();\n        return result;\n    }",
        "fixed_src_wo_comments": "protected JsonNode _bindAsTree ( JsonParser jp ) throws IOException { JsonNode result ; JsonToken t = _initForReading ( jp ) ; if ( t == JsonToken . VALUE_NULL || t == JsonToken . END_ARRAY || t == JsonToken . END_OBJECT ) { result = NullNode . instance ; } else { DeserializationContext ctxt = createDeserializationContext ( jp , _config ) ; JsonDeserializer < Object > deser = _findTreeDeserializer ( ctxt ) ; if ( _unwrapRoot ) { result = ( JsonNode ) _unwrapAndDeserialize ( jp , ctxt , JSON_NODE_TYPE , deser ) ; } else { result = ( JsonNode ) deser . deserialize ( jp , ctxt ) ; } } jp . clearCurrentToken ( ) ; return result ; }",
        "summary": "Custom deserializer with parent object update",
        "Description": "Hi, I have custom deserializer for `DataA`. An instance of `DataA` is contained in `DataB`, when updating an existing instance of `DataB` (as opposed to creating a new one) I get an exception when deserializing via a `JsonNode` object (deserializing via a `String` object works). \n\n``` java\nimport java.io.IOException;\n\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport com.fasterxml.jackson.core.JsonParser;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.core.JsonToken;\nimport com.fasterxml.jackson.databind.DeserializationContext;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.deser.std.StdDeserializer;\nimport com.fasterxml.jackson.databind.module.SimpleModule;\n\npublic class TestDeserTest {\n    static class DataA {\n        public int i = 1;\n        public int j = 2;\n\n    }\n\n    static class DataB {\n        public DataA da = new DataA();\n        public int k = 3;\n    }\n\n    static class DataADeserializer extends StdDeserializer<DataA> {\n        private static final long serialVersionUID = 1L;\n\n        DataADeserializer() {\n            super(DataA.class);\n        }\n\n        public DataA deserialize(JsonParser jp, DeserializationContext ctxt)\n                throws JsonProcessingException, IOException {\n            assert (jp.getCurrentToken() == JsonToken.START_OBJECT);\n            JsonNode node = jp.getCodec().readTree(jp);\n\n            DataA da = new DataA();\n            da.i = 5;\n            return da;\n        }\n    }\n\n    @Test\n    public void test() throws IOException {\n        ObjectMapper mapper = new ObjectMapper();\n        SimpleModule module = new SimpleModule();\n        module.addDeserializer(DataA.class, new DataADeserializer());\n        mapper.registerModule(module);\n\n        DataB db = new DataB();\n        db.da.i = 11;\n        db.k = 13;\n        String jsonBString = mapper.writeValueAsString(db);\n        JsonNode jsonBNode = mapper.valueToTree(db);\n\n        // create parent\n        DataB dbNewViaString = mapper.readValue(jsonBString, DataB.class);\n        Assert.assertEquals(5, dbNewViaString.da.i);\n        Assert.assertEquals(13, dbNewViaString.k);\n\n        DataB dbNewViaNode = mapper.treeToValue(jsonBNode, DataB.class);\n        Assert.assertEquals(5, dbNewViaNode.da.i);\n        Assert.assertEquals(13, dbNewViaNode.k);\n\n        // update parent\n        DataB dbUpdViaString = new DataB();\n        DataB dbUpdViaNode = new DataB();\n\n        Assert.assertEquals(1, dbUpdViaString.da.i);\n        Assert.assertEquals(3, dbUpdViaString.k);\n        mapper.readerForUpdating(dbUpdViaString).readValue(jsonBString);\n        Assert.assertEquals(5, dbUpdViaString.da.i);\n        Assert.assertEquals(13, dbUpdViaString.k);\n\n        Assert.assertEquals(1, dbUpdViaNode.da.i);\n        Assert.assertEquals(3, dbUpdViaNode.k);\n        // FAILS HERE:\n        mapper.readerForUpdating(dbUpdViaNode).readValue(jsonBNode);\n        Assert.assertEquals(5, dbUpdViaNode.da.i);\n        Assert.assertEquals(13, dbUpdViaNode.k);\n    }\n}\n```\n\nThe trace:\n\n``` java\ncom.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field \"i\" (class myorg.TestDeserTest$DataB), not marked as ignorable (2 known properties: \"da\", \"k\"])\n at [Source: N/A; line: -1, column: -1] (through reference chain: myorg.DataB[\"da\"]->myorg.DataB[\"i\"])\n    at com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:51)\n    at com.fasterxml.jackson.databind.DeserializationContext.reportUnknownProperty(DeserializationContext.java:817)\n    at com.fasterxml.jackson.databind.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:954)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownProperty(BeanDeserializerBase.java:1324)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownVanilla(BeanDeserializerBase.java:1302)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:249)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:136)\n    at com.fasterxml.jackson.databind.ObjectReader._bindAsTree(ObjectReader.java:1478)\n    at com.fasterxml.jackson.databind.ObjectReader.readTree(ObjectReader.java:1020)\n    at myorg.TestDeserTest$DataADeserializer.deserialize(TestDeserTest.java:39)\n    at myorg.TestDeserTest$DataADeserializer.deserialize(TestDeserTest.java:1)\n    at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:523)\n    at com.fasterxml.jackson.databind.deser.impl.FieldProperty.deserializeAndSet(FieldProperty.java:101)\n    at com.fasterxml.jackson.databind.deser.impl.BeanPropertyMap.findDeserializeAndSet(BeanPropertyMap.java:285)\n    at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:220)\n    at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:1443)\n    at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1154)\n    at myorg.TestDeserTest.test(TestDeserTest.java:81)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)\n    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)\n    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)\n\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Interesting. I can reproduce it with given test case.\n\nOne thing I notice is that replacing `jp.getCodec().readTree(jp)` (or simpler equivalent of `jp.readValueAsTree()`) give the exception; but simple `jp.skipChildren()` does not.\nSo something in reading-as-tree is not working as expected...\n"
            },
            {
                "content": "Whoa. Internally it looks like `JsonNode` deserializer is resolved to be deserializer for `DataA`... wires getting crossed badly somewhere.\n"
            },
            {
                "content": "Actually, not problem with caching, but rather with `_rootDeserializer` that `ObjectReader` caches locally. Need to see how this can be by-passed.\n"
            },
            {
                "content": "Fixed in 2.5.3, although I did also backport it in 2.4.6 just in case.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was caused by the `JsonNode` deserializer being resolved to be deserializer for `DataA`. The solution was to fix this in version 2.5.3 of the source code, and backport it to version 2.4.6."
    },
    "Cli_27_src/java/org/apache/commons/cli/OptionGroup.java_86_106": {
        "src": "public void setSelected(Option option) throws AlreadySelectedException\n    {\n        if (option == null)\n        {\n            // reset the option previously selected\n            selected = null;\n            return;\n        }\n        \n        // if no option has already been selected or the \n        // same option is being reselected then set the\n        // selected member variable\n        if (selected == null || selected.equals(option.getOpt()))\n        {\n            selected = option.getOpt();\n        }\n        else\n        {\n            throw new AlreadySelectedException(this, option);\n        }\n    }",
        "src_wo_comments": "public void setSelected ( Option option ) throws AlreadySelectedException { if ( option == null ) { selected = null ; return ; } if ( selected == null || selected . equals ( option . getOpt ( ) ) ) { selected = option . getOpt ( ) ; } else { throw new AlreadySelectedException ( this , option ) ; } }",
        "fixed_src": "public void setSelected(Option option) throws AlreadySelectedException\n    {\n        if (option == null)\n        {\n            // reset the option previously selected\n            selected = null;\n            return;\n        }\n        \n        // if no option has already been selected or the \n        // same option is being reselected then set the\n        // selected member variable\n        if (selected == null || selected.equals(option.getKey()))\n        {\n            selected = option.getKey();\n        }\n        else\n        {\n            throw new AlreadySelectedException(this, option);\n        }\n    }",
        "fixed_src_wo_comments": "public void setSelected ( Option option ) throws AlreadySelectedException { if ( option == null ) { selected = null ; return ; } if ( selected == null || selected . equals ( option . getKey ( ) ) ) { selected = option . getKey ( ) ; } else { throw new AlreadySelectedException ( this , option ) ; } }",
        "summary": "Unable to select a pure long option in a group",
        "Description": "OptionGroup doesn't play nice with options with a long name and no short name. If the selected option hasn't a short name, group.setSelected(option) has no effect.\n",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-182",
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug is related to a function that is not working as expected. The bug is caused by an incorrect parameter being passed to the function. The solution to the bug is to ensure that the correct parameter is passed to the function."
    },
    "Math_30_src/main/java/org/apache/commons/math3/stat/inference/MannWhitneyUTest.java_168_184": {
        "src": "private double calculateAsymptoticPValue(final double Umin,\n                                             final int n1,\n                                             final int n2)\n        throws ConvergenceException, MaxCountExceededException {\n\n        final int n1n2prod = n1 * n2;\n\n        // http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation\n        final double EU = n1n2prod / 2.0;\n        final double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;\n\n        final double z = (Umin - EU) / FastMath.sqrt(VarU);\n\n        final NormalDistribution standardNormal = new NormalDistribution(0, 1);\n\n        return 2 * standardNormal.cumulativeProbability(z);\n    }",
        "src_wo_comments": "private double calculateAsymptoticPValue ( final double Umin , final int n1 , final int n2 ) throws ConvergenceException , MaxCountExceededException { final int n1n2prod = n1 * n2 ; final double EU = n1n2prod / 2.0 ; final double VarU = n1n2prod * ( n1 + n2 + 1 ) / 12.0 ; final double z = ( Umin - EU ) / FastMath . sqrt ( VarU ) ; final NormalDistribution standardNormal = new NormalDistribution ( 0 , 1 ) ; return 2 * standardNormal . cumulativeProbability ( z ) ; }",
        "fixed_src": "private double calculateAsymptoticPValue(final double Umin,\n                                             final int n1,\n                                             final int n2)\n        throws ConvergenceException, MaxCountExceededException {\n\n        final double n1n2prod = n1 * n2;\n\n        // http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation\n        final double EU = n1n2prod / 2.0;\n        final double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;\n\n        final double z = (Umin - EU) / FastMath.sqrt(VarU);\n\n        final NormalDistribution standardNormal = new NormalDistribution(0, 1);\n\n        return 2 * standardNormal.cumulativeProbability(z);\n    }",
        "fixed_src_wo_comments": "private double calculateAsymptoticPValue ( final double Umin , final int n1 , final int n2 ) throws ConvergenceException , MaxCountExceededException { final double n1n2prod = n1 * n2 ; final double EU = n1n2prod / 2.0 ; final double VarU = n1n2prod * ( n1 + n2 + 1 ) / 12.0 ; final double z = ( Umin - EU ) / FastMath . sqrt ( VarU ) ; final NormalDistribution standardNormal = new NormalDistribution ( 0 , 1 ) ; return 2 * standardNormal . cumulativeProbability ( z ) ; }",
        "summary": "Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets",
        "Description": "When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations.\n\nAttached is a patch, including a test, and a fix, which modifies the affected code to use doubles",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-790",
        "comments": [
            "A patch, which adds a unit test for the issue, and a fix in the affected code",
            "Thank you very much for reporting this. I will have a look at this ASAP.",
            "This is fixed in SVN revision 1348024. Thanks again for reporting this issue.",
            "Thomas Neidhart suspect an overflow issues is still present.",
            "As discussed on the ML, there may be still a problem with integer overflow in the code fragment below:\n\n{noformat}\nfinal double n1n2prod = n1 * n2;\n\n// http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation\nfinal double EU = n1n2prod / 2.0;\nfinal double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;\n\nfinal double z = (Umin - EU) / FastMath.sqrt(VarU);\n{noformat}\n\nThe calculation of n1n2prod may still overflow if n1 and n2 are too big as it still does an int multiplication, so I would suggest to do it like that:\n\n{noformat}\nfinal long n1n2prod = (long) n1 * n2;\n\n// http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation\nfinal double EU = n1n2prod / 2.0;\nfinal double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;\n\nfinal double z = (Umin - EU) / FastMath.sqrt(VarU);\n{noformat}\n",
            "Thanks for the details. Why not use a double immediately as below? Is it to avoid precision loss?\n{noformat}\nfinal double n1n2prod = (double) n1 * n2;\n\n// http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation\nfinal double EU = n1n2prod / 2.0;\nfinal double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;\n\nfinal double z = (Umin - EU) / FastMath.sqrt(VarU);\n{noformat}",
            "Actually yes, this was my intention (and long multiplication should be faster too ;-), but in this case it may be negligible.",
            "I agree - I just wanted to have it in writing :-). Thanks for this.",
            "This second issue is fixed in SVN revision 1349372. Thanks again for reporting this issue, too."
        ],
        "summarized_discussion": "\n\nThe bug was an integer overflow issue in the source code. The solution was to use a long multiplication instead of an int multiplication, and this was fixed in SVN revision 1348024. A unit test was also added to ensure that the issue was fixed. A second issue was also fixed in SVN revision 1349372."
    },
    "JacksonCore_23_src/main/java/com/fasterxml/jackson/core/util/DefaultPrettyPrinter.java_253_256": {
        "src": "@Override\n    public DefaultPrettyPrinter createInstance() {\n        return new DefaultPrettyPrinter(this);\n    }",
        "src_wo_comments": "@ Override public DefaultPrettyPrinter createInstance ( ) { return new DefaultPrettyPrinter ( this ) ; }",
        "fixed_src": "@Override\n    public DefaultPrettyPrinter createInstance() {\n        if (getClass() != DefaultPrettyPrinter.class) { // since 2.10\n            throw new IllegalStateException(\"Failed `createInstance()`: \"+getClass().getName()\n                    +\" does not override method; it has to\");\n        }\n        return new DefaultPrettyPrinter(this);\n    }",
        "fixed_src_wo_comments": "@ Override public DefaultPrettyPrinter createInstance ( ) { if ( getClass ( ) != DefaultPrettyPrinter . class ) { throw new IllegalStateException ( \"Failed `createInstance()`: \" + getClass ( ) . getName ( ) + \" does not override method; it has to\" ) ; } return new DefaultPrettyPrinter ( this ) ; }",
        "summary": "Make `DefaultPrettyPrinter.createInstance()` to fail for sub-classes",
        "Description": "Pattern of \"blueprint object\" (that is, having an instance not used as-is, but that has factory method for creating actual instance) is used by Jackson in couple of places; often for things that implement `Instantiatable`. But one problem is that unless method is left abstract, sub-classing can be problematic -- if sub-class does not override method, then calls will result in an instance of wrong type being created.\r\n\r\nAnd this is what can easily happen with `DefaultPrettyPrinter`.\r\n\r\nA simple solution is for base class to make explicit that if base implementation is called, then instance can not be a sub-class (that is, it is only legal to call on `DefaultPrettyPrinter`, but no sub-class). This is not optimal (ideally check would be done compile-time), but better than getting a mysterious failure.\r\n\r\n\r\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug was caused by a missing semicolon at the end of a line of code. The solution to the bug is to add the missing semicolon to the end of the line of code."
    },
    "Math_88_src/java/org/apache/commons/math/optimization/linear/SimplexTableau.java_324_345": {
        "src": "protected RealPointValuePair getSolution() {\n        double[] coefficients = new double[getOriginalNumDecisionVariables()];\n        Integer basicRow =\n            getBasicRow(getNumObjectiveFunctions() + getOriginalNumDecisionVariables());\n        double mostNegative = basicRow == null ? 0 : getEntry(basicRow, getRhsOffset());\n        for (int i = 0; i < coefficients.length; i++) {\n            basicRow = getBasicRow(getNumObjectiveFunctions() + i);\n                // if multiple variables can take a given value \n                // then we choose the first and set the rest equal to 0\n                coefficients[i] =\n                    (basicRow == null ? 0 : getEntry(basicRow, getRhsOffset())) -\n                    (restrictToNonNegative ? 0 : mostNegative);\n            if (basicRow != null) {\n                for (int j = getNumObjectiveFunctions(); j < getNumObjectiveFunctions() + i; j++) {\n                    if (tableau.getEntry(basicRow, j) == 1) {\n                         coefficients[i] = 0;\n                    }\n                }\n            }\n        }\n        return new RealPointValuePair(coefficients, f.getValue(coefficients));\n    }",
        "src_wo_comments": "protected RealPointValuePair getSolution ( ) { double [ ] coefficients = new double [ getOriginalNumDecisionVariables ( ) ] ; Integer basicRow = getBasicRow ( getNumObjectiveFunctions ( ) + getOriginalNumDecisionVariables ( ) ) ; double mostNegative = basicRow == null ? 0 : getEntry ( basicRow , getRhsOffset ( ) ) ; for ( int i = 0 ; i < coefficients . length ; i ++ ) { basicRow = getBasicRow ( getNumObjectiveFunctions ( ) + i ) ; coefficients [ i ] = ( basicRow == null ? 0 : getEntry ( basicRow , getRhsOffset ( ) ) ) - ( restrictToNonNegative ? 0 : mostNegative ) ; if ( basicRow != null ) { for ( int j = getNumObjectiveFunctions ( ) ; j < getNumObjectiveFunctions ( ) + i ; j ++ ) { if ( tableau . getEntry ( basicRow , j ) == 1 ) { coefficients [ i ] = 0 ; } } } } return new RealPointValuePair ( coefficients , f . getValue ( coefficients ) ) ; }",
        "fixed_src": "protected RealPointValuePair getSolution() {\n        double[] coefficients = new double[getOriginalNumDecisionVariables()];\n        Integer basicRow =\n            getBasicRow(getNumObjectiveFunctions() + getOriginalNumDecisionVariables());\n        double mostNegative = basicRow == null ? 0 : getEntry(basicRow, getRhsOffset());\n        Set<Integer> basicRows = new HashSet<Integer>();\n        for (int i = 0; i < coefficients.length; i++) {\n            basicRow = getBasicRow(getNumObjectiveFunctions() + i);\n            if (basicRows.contains(basicRow)) {\n                // if multiple variables can take a given value \n                // then we choose the first and set the rest equal to 0\n                coefficients[i] = 0;\n            } else {\n                basicRows.add(basicRow);\n                coefficients[i] =\n                    (basicRow == null ? 0 : getEntry(basicRow, getRhsOffset())) -\n                    (restrictToNonNegative ? 0 : mostNegative);\n            }\n        }\n        return new RealPointValuePair(coefficients, f.getValue(coefficients));\n    }",
        "fixed_src_wo_comments": "protected RealPointValuePair getSolution ( ) { double [ ] coefficients = new double [ getOriginalNumDecisionVariables ( ) ] ; Integer basicRow = getBasicRow ( getNumObjectiveFunctions ( ) + getOriginalNumDecisionVariables ( ) ) ; double mostNegative = basicRow == null ? 0 : getEntry ( basicRow , getRhsOffset ( ) ) ; Set < Integer > basicRows = new HashSet < Integer > ( ) ; for ( int i = 0 ; i < coefficients . length ; i ++ ) { basicRow = getBasicRow ( getNumObjectiveFunctions ( ) + i ) ; if ( basicRows . contains ( basicRow ) ) { coefficients [ i ] = 0 ; } else { basicRows . add ( basicRow ) ; coefficients [ i ] = ( basicRow == null ? 0 : getEntry ( basicRow , getRhsOffset ( ) ) ) - ( restrictToNonNegative ? 0 : mostNegative ) ; } } return new RealPointValuePair ( coefficients , f . getValue ( coefficients ) ) ; }",
        "summary": "Simplex Solver arrives at incorrect solution",
        "Description": "I have reduced the problem reported to me down to a minimal test case which I will attach.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-272",
        "comments": [
            "Test case to be added to SimplexSolverTest.  It is currently failing and should be fixed.",
            "Previously, there was a bug where we could set one of a number of variables equal to some value.  We were setting all the variables instead of choosing one.  When I patched that bug, I did it incorrectly.  This is a correct implementation, which causes the old bug and the attached test to both pass.",
            "fixed in subversion repository as of r781135\npatch applied\nthanks",
            "Thanks!\n\n\n\n",
            "closing resolved issue for 2.0 release"
        ],
        "summarized_discussion": "\n\nThe bug where multiple variables were being set instead of just one has been fixed in the Subversion repository as of r781135 with the patch applied. The Test case for SimplexSolverTest has been added and is now passing. The issue has been closed for the 2.0 release."
    },
    "Csv_9_src/main/java/org/apache/commons/csv/CSVRecord.java_179_187": {
        "src": "<M extends Map<String, String>> M putIn(final M map) {\n        for (final Entry<String, Integer> entry : mapping.entrySet()) {\n            final int col = entry.getValue().intValue();\n            if (col < values.length) {\n                map.put(entry.getKey(), values[col]);\n            }\n        }\n        return map;\n    }",
        "src_wo_comments": "< M extends Map < String , String > > M putIn ( final M map ) { for ( final Entry < String , Integer > entry : mapping . entrySet ( ) ) { final int col = entry . getValue ( ) . intValue ( ) ; if ( col < values . length ) { map . put ( entry . getKey ( ) , values [ col ] ) ; } } return map ; }",
        "fixed_src": "<M extends Map<String, String>> M putIn(final M map) {\n        if (mapping == null) {\n            return map;\n        }\n        for (final Entry<String, Integer> entry : mapping.entrySet()) {\n            final int col = entry.getValue().intValue();\n            if (col < values.length) {\n                map.put(entry.getKey(), values[col]);\n            }\n        }\n        return map;\n    }",
        "fixed_src_wo_comments": "< M extends Map < String , String > > M putIn ( final M map ) { if ( mapping == null ) { return map ; } for ( final Entry < String , Integer > entry : mapping . entrySet ( ) ) { final int col = entry . getValue ( ) . intValue ( ) ; if ( col < values . length ) { map . put ( entry . getKey ( ) , values [ col ] ) ; } } return map ; }",
        "summary": "CSVRecord.toMap() throws NPE on formats with no headers.",
        "Description": "The method toMap() on CSVRecord throws a NullPointerExcpetion when called on records derived using a format with no headers.\n\nThe method documentation states a null map should be returned instead.\n",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-118",
        "comments": [
            "Patch to main code as well as unit test exposing condition.",
            "{noformat}\ncommit -m \"<action issue=\"CSV-118\" type=\"fix\" dev=\"ggregory\" due-to=\"Enrique Lara\">CSVRecord.toMap() throws NPE on formats with no headers.</action>\" C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVRecordTest.java C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVRecord.java C:/vcs/svn/apache/commons/trunks-proper/csv/src/changes/changes.xml\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/changes/changes.xml\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVRecord.java\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVRecordTest.java\n    Transmitting file data ...\n    Committed revision 1594966.\n{noformat}"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to patch the main code and create a unit test to expose the condition that caused the bug. This was done by committing revision 1594966 to the Apache Commons CSV source code."
    },
    "JacksonDatabind_55_src/main/java/com/fasterxml/jackson/databind/ser/std/StdKeySerializers.java_67_86": {
        "src": "@SuppressWarnings(\"unchecked\")\n    public static JsonSerializer<Object> getFallbackKeySerializer(SerializationConfig config,\n            Class<?> rawKeyType)\n    {\n        if (rawKeyType != null) {\n            // 29-Sep-2015, tatu: Odd case here, of `Enum`, which we may get for `EnumMap`; not sure\n            //   if that is a bug or feature. Regardless, it seems to require dynamic handling\n            //   (compared to getting actual fully typed Enum).\n            //  Note that this might even work from the earlier point, but let's play it safe for now\n            // 11-Aug-2016, tatu: Turns out we get this if `EnumMap` is the root value because\n            //    then there is no static type\n            if (rawKeyType == Enum.class) {\n                return new Dynamic();\n            }\n            if (rawKeyType.isEnum()) {\n                return new Default(Default.TYPE_ENUM, rawKeyType);\n            }\n        }\n        return DEFAULT_KEY_SERIALIZER;\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"unchecked\" ) public static JsonSerializer < Object > getFallbackKeySerializer ( SerializationConfig config , Class < ? > rawKeyType ) { if ( rawKeyType != null ) { if ( rawKeyType == Enum . class ) { return new Dynamic ( ) ; } if ( rawKeyType . isEnum ( ) ) { return new Default ( Default . TYPE_ENUM , rawKeyType ) ; } } return DEFAULT_KEY_SERIALIZER ; }",
        "fixed_src": "@SuppressWarnings(\"unchecked\")\n    public static JsonSerializer<Object> getFallbackKeySerializer(SerializationConfig config,\n            Class<?> rawKeyType)\n    {\n        if (rawKeyType != null) {\n            // 29-Sep-2015, tatu: Odd case here, of `Enum`, which we may get for `EnumMap`; not sure\n            //   if that is a bug or feature. Regardless, it seems to require dynamic handling\n            //   (compared to getting actual fully typed Enum).\n            //  Note that this might even work from the earlier point, but let's play it safe for now\n            // 11-Aug-2016, tatu: Turns out we get this if `EnumMap` is the root value because\n            //    then there is no static type\n            if (rawKeyType == Enum.class) {\n                return new Dynamic();\n            }\n            if (rawKeyType.isEnum()) {\n                return EnumKeySerializer.construct(rawKeyType,\n                        EnumValues.constructFromName(config, (Class<Enum<?>>) rawKeyType));\n            }\n        }\n        return DEFAULT_KEY_SERIALIZER;\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"unchecked\" ) public static JsonSerializer < Object > getFallbackKeySerializer ( SerializationConfig config , Class < ? > rawKeyType ) { if ( rawKeyType != null ) { if ( rawKeyType == Enum . class ) { return new Dynamic ( ) ; } if ( rawKeyType . isEnum ( ) ) { return EnumKeySerializer . construct ( rawKeyType , EnumValues . constructFromName ( config , ( Class < Enum < ? > > ) rawKeyType ) ) ; } } return DEFAULT_KEY_SERIALIZER ; }",
        "summary": "EnumMap keys not using enum's `@JsonProperty` values unlike Enum values",
        "Description": "Based on these issues:\nhttps://github.com/FasterXML/jackson-databind/issues/677\nhttps://github.com/FasterXML/jackson-databind/issues/1148\nhttps://github.com/FasterXML/jackson-annotations/issues/96\n\nI implemented @JsonProperty for my enum constants and they show up nicely when they are property values. But I also have an EnumMap which uses the enum, and it's generated JSON uses the original enum names for the keys and not the JsonProperty values.\n\nUsing 2.8.1 (in spring boot 4.3.2)\n\nThanks!\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this.\n"
            },
            {
                "content": "Ok, looks like use of `@JsonValue` works at least. Next checking `@JsonProperty` part.\n"
            },
            {
                "content": "I can reproduce this, and can see the problem. Fix will take a while, need to add Enum-specific key serializer. Not particularly difficult to do but... some work.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug appears to be related to the use of `@JsonValue` and `@JsonProperty` and will require a key serializer specific to Enums to be added. This fix will take some work, but is not particularly difficult."
    },
    "Cli_11_src/java/org/apache/commons/cli/HelpFormatter.java_613_642": {
        "src": "private static void appendOption(final StringBuffer buff, \n                                     final Option option, \n                                     final boolean required)\n    {\n        if (!required)\n        {\n            buff.append(\"[\");\n        }\n\n        if (option.getOpt() != null)\n        {\n            buff.append(\"-\").append(option.getOpt());\n        }\n        else\n        {\n            buff.append(\"--\").append(option.getLongOpt());\n        }\n\n        // if the Option has a value\n        if (option.hasArg() && (option.getArgName() != null))\n        {\n            buff.append(\" <\").append(option.getArgName()).append(\">\");\n        }\n\n        // if the Option is not a required option\n        if (!required)\n        {\n            buff.append(\"]\");\n        }\n    }",
        "src_wo_comments": "private static void appendOption ( final StringBuffer buff , final Option option , final boolean required ) { if ( ! required ) { buff . append ( \"[\" ) ; } if ( option . getOpt ( ) != null ) { buff . append ( \"-\" ) . append ( option . getOpt ( ) ) ; } else { buff . append ( \"--\" ) . append ( option . getLongOpt ( ) ) ; } if ( option . hasArg ( ) && ( option . getArgName ( ) != null ) ) { buff . append ( \" <\" ) . append ( option . getArgName ( ) ) . append ( \">\" ) ; } if ( ! required ) { buff . append ( \"]\" ) ; } }",
        "fixed_src": "private static void appendOption(final StringBuffer buff, \n                                     final Option option, \n                                     final boolean required)\n    {\n        if (!required)\n        {\n            buff.append(\"[\");\n        }\n\n        if (option.getOpt() != null)\n        {\n            buff.append(\"-\").append(option.getOpt());\n        }\n        else\n        {\n            buff.append(\"--\").append(option.getLongOpt());\n        }\n\n        // if the Option has a value\n        if (option.hasArg() && option.hasArgName())\n        {\n            buff.append(\" <\").append(option.getArgName()).append(\">\");\n        }\n\n        // if the Option is not a required option\n        if (!required)\n        {\n            buff.append(\"]\");\n        }\n    }",
        "fixed_src_wo_comments": "private static void appendOption ( final StringBuffer buff , final Option option , final boolean required ) { if ( ! required ) { buff . append ( \"[\" ) ; } if ( option . getOpt ( ) != null ) { buff . append ( \"-\" ) . append ( option . getOpt ( ) ) ; } else { buff . append ( \"--\" ) . append ( option . getLongOpt ( ) ) ; } if ( option . hasArg ( ) && option . hasArgName ( ) ) { buff . append ( \" <\" ) . append ( option . getArgName ( ) ) . append ( \">\" ) ; } if ( ! required ) { buff . append ( \"]\" ) ; } }",
        "summary": "PosixParser interupts \"-target opt\" as \"-t arget opt\"",
        "Description": "This was posted on the Commons-Developer list and confirmed as a bug.\n\n> Is this a bug?  Or am I using this incorrectly?\n> I have an option with short and long values.  Given code that is \n> essentially what is below, with a PosixParser I see results as \n> follows:\n> \n> A command line with just \"-t\" prints out the results of the catch \n> block\n> (OK)\n> A command line with just \"-target\" prints out the results of the catch\n> block (OK)\n> A command line with just \"-t foobar.com\" prints out \"processing selected\n> target: foobar.com\" (OK)\n> A command line with just \"-target foobar.com\" prints out \"processing\n> selected target: arget\" (ERROR?)\n> \n> ======================================================================\n> ==\n> =======================\n>   private static final String OPTION_TARGET = \"t\";\n>   private static final String OPTION_TARGET_LONG = \"target\";\n>   // ...\n>   Option generateTarget = new Option(OPTION_TARGET, \n>                                      OPTION_TARGET_LONG, \n>                                      true, \n>                                      \"Generate files for the specified\n> target machine\");\n>   // ...\n>   try {\n>         parsedLine = parser.parse(cmdLineOpts, args);\n>       } catch (ParseException pe) {\n>         System.out.println(\"Invalid command: \" + pe.getMessage() +\n> \"\\n\");\n>         HelpFormatter hf = new HelpFormatter();\n>         hf.printHelp(USAGE, cmdLineOpts);\n>         System.exit(-1);\n>       }\n> \n>   if (parsedLine.hasOption(OPTION_TARGET)) {\n>     System.out.println(\"processing selected target: \" +\n> parsedLine.getOptionValue(OPTION_TARGET));        \n>   }\n\nIt is a bug but it is due to well defined behaviour (so that makes me feel a\nlittle better about myself ;).  To support *special* \n(well I call them special anyway) like -Dsystem.property=value we need to be\nable to examine the first character of an option.  If the first character is\nitself defined as an Option then the remainder of the token is used as the\nvalue, e.g. 'D' is the token, it is an option so 'system.property=value' is the\nargument value for that option.  This is the behaviour that we are seeing for\nyour example.  \n't' is the token, it is an options so 'arget' is the argument value.  \n\nI suppose a solution to this could be to have a way to specify properties for\nparsers.  In this case 'posix.special.option == true' for turning \non *special* options. I'll have a look into this and let you know.\n\nJust to keep track of this and to get you used to how we operate, can you log a\nbug in bugzilla for this.\n\nThanks,\n-John K",
        "issue_url": "https://issues.apache.org/jira//browse/cli-1",
        "comments": [
            "Joe,\n\nMy fault on this one.  Its isn't a bug.  I read this first and thought that it\nwas two separate Options i.e. one '-t' and one '-target'.  It is in fact the\nsame Option with '-t' as the shortOpt and '--target' as the long opt.  So I\napologise for my incorrect analysis of the orginal problem.  To use 'target' on\nthe command line you should prefix it with \"--\" which is the way to do this\nusing the PosixParser.  To use options of the style '-target' you need to use\nthe GnuParser.\n\n-John K"
        ],
        "summarized_discussion": "\n\nThe source code bug was not a bug, but rather a misunderstanding of the code. The solution is to use the \"--\" prefix when using the PosixParser and the \"-target\" style when using the GnuParser."
    },
    "Math_10_src/main/java/org/apache/commons/math3/analysis/differentiation/DSCompiler.java_1382_1420": {
        "src": "public void atan2(final double[] y, final int yOffset,\n                      final double[] x, final int xOffset,\n                      final double[] result, final int resultOffset) {\n\n        // compute r = sqrt(x^2+y^2)\n        double[] tmp1 = new double[getSize()];\n        multiply(x, xOffset, x, xOffset, tmp1, 0);      // x^2\n        double[] tmp2 = new double[getSize()];\n        multiply(y, yOffset, y, yOffset, tmp2, 0);      // y^2\n        add(tmp1, 0, tmp2, 0, tmp2, 0);                 // x^2 + y^2\n        rootN(tmp2, 0, 2, tmp1, 0);                     // r = sqrt(x^2 + y^2)\n\n        if (x[xOffset] >= 0) {\n\n            // compute atan2(y, x) = 2 atan(y / (r + x))\n            add(tmp1, 0, x, xOffset, tmp2, 0);          // r + x\n            divide(y, yOffset, tmp2, 0, tmp1, 0);       // y /(r + x)\n            atan(tmp1, 0, tmp2, 0);                     // atan(y / (r + x))\n            for (int i = 0; i < tmp2.length; ++i) {\n                result[resultOffset + i] = 2 * tmp2[i]; // 2 * atan(y / (r + x))\n            }\n\n        } else {\n\n            // compute atan2(y, x) = +/- pi - 2 atan(y / (r - x))\n            subtract(tmp1, 0, x, xOffset, tmp2, 0);     // r - x\n            divide(y, yOffset, tmp2, 0, tmp1, 0);       // y /(r - x)\n            atan(tmp1, 0, tmp2, 0);                     // atan(y / (r - x))\n            result[resultOffset] =\n                    ((tmp2[0] <= 0) ? -FastMath.PI : FastMath.PI) - 2 * tmp2[0]; // +/-pi - 2 * atan(y / (r - x))\n            for (int i = 1; i < tmp2.length; ++i) {\n                result[resultOffset + i] = -2 * tmp2[i]; // +/-pi - 2 * atan(y / (r - x))\n            }\n\n        }\n\n        // fix value to take special cases (+0/+0, +0/-0, -0/+0, -0/-0, +/-infinity) correctly\n\n    }",
        "src_wo_comments": "public void atan2 ( final double [ ] y , final int yOffset , final double [ ] x , final int xOffset , final double [ ] result , final int resultOffset ) { double [ ] tmp1 = new double [ getSize ( ) ] ; multiply ( x , xOffset , x , xOffset , tmp1 , 0 ) ; double [ ] tmp2 = new double [ getSize ( ) ] ; multiply ( y , yOffset , y , yOffset , tmp2 , 0 ) ; add ( tmp1 , 0 , tmp2 , 0 , tmp2 , 0 ) ; rootN ( tmp2 , 0 , 2 , tmp1 , 0 ) ; if ( x [ xOffset ] >= 0 ) { add ( tmp1 , 0 , x , xOffset , tmp2 , 0 ) ; divide ( y , yOffset , tmp2 , 0 , tmp1 , 0 ) ; atan ( tmp1 , 0 , tmp2 , 0 ) ; for ( int i = 0 ; i < tmp2 . length ; ++ i ) { result [ resultOffset + i ] = 2 * tmp2 [ i ] ; } } else { subtract ( tmp1 , 0 , x , xOffset , tmp2 , 0 ) ; divide ( y , yOffset , tmp2 , 0 , tmp1 , 0 ) ; atan ( tmp1 , 0 , tmp2 , 0 ) ; result [ resultOffset ] = ( ( tmp2 [ 0 ] <= 0 ) ? - FastMath . PI : FastMath . PI ) - 2 * tmp2 [ 0 ] ; for ( int i = 1 ; i < tmp2 . length ; ++ i ) { result [ resultOffset + i ] = - 2 * tmp2 [ i ] ; } } }",
        "fixed_src": "public void atan2(final double[] y, final int yOffset,\n                      final double[] x, final int xOffset,\n                      final double[] result, final int resultOffset) {\n\n        // compute r = sqrt(x^2+y^2)\n        double[] tmp1 = new double[getSize()];\n        multiply(x, xOffset, x, xOffset, tmp1, 0);      // x^2\n        double[] tmp2 = new double[getSize()];\n        multiply(y, yOffset, y, yOffset, tmp2, 0);      // y^2\n        add(tmp1, 0, tmp2, 0, tmp2, 0);                 // x^2 + y^2\n        rootN(tmp2, 0, 2, tmp1, 0);                     // r = sqrt(x^2 + y^2)\n\n        if (x[xOffset] >= 0) {\n\n            // compute atan2(y, x) = 2 atan(y / (r + x))\n            add(tmp1, 0, x, xOffset, tmp2, 0);          // r + x\n            divide(y, yOffset, tmp2, 0, tmp1, 0);       // y /(r + x)\n            atan(tmp1, 0, tmp2, 0);                     // atan(y / (r + x))\n            for (int i = 0; i < tmp2.length; ++i) {\n                result[resultOffset + i] = 2 * tmp2[i]; // 2 * atan(y / (r + x))\n            }\n\n        } else {\n\n            // compute atan2(y, x) = +/- pi - 2 atan(y / (r - x))\n            subtract(tmp1, 0, x, xOffset, tmp2, 0);     // r - x\n            divide(y, yOffset, tmp2, 0, tmp1, 0);       // y /(r - x)\n            atan(tmp1, 0, tmp2, 0);                     // atan(y / (r - x))\n            result[resultOffset] =\n                    ((tmp2[0] <= 0) ? -FastMath.PI : FastMath.PI) - 2 * tmp2[0]; // +/-pi - 2 * atan(y / (r - x))\n            for (int i = 1; i < tmp2.length; ++i) {\n                result[resultOffset + i] = -2 * tmp2[i]; // +/-pi - 2 * atan(y / (r - x))\n            }\n\n        }\n\n        // fix value to take special cases (+0/+0, +0/-0, -0/+0, -0/-0, +/-infinity) correctly\n        result[resultOffset] = FastMath.atan2(y[yOffset], x[xOffset]);\n\n    }",
        "fixed_src_wo_comments": "public void atan2 ( final double [ ] y , final int yOffset , final double [ ] x , final int xOffset , final double [ ] result , final int resultOffset ) { double [ ] tmp1 = new double [ getSize ( ) ] ; multiply ( x , xOffset , x , xOffset , tmp1 , 0 ) ; double [ ] tmp2 = new double [ getSize ( ) ] ; multiply ( y , yOffset , y , yOffset , tmp2 , 0 ) ; add ( tmp1 , 0 , tmp2 , 0 , tmp2 , 0 ) ; rootN ( tmp2 , 0 , 2 , tmp1 , 0 ) ; if ( x [ xOffset ] >= 0 ) { add ( tmp1 , 0 , x , xOffset , tmp2 , 0 ) ; divide ( y , yOffset , tmp2 , 0 , tmp1 , 0 ) ; atan ( tmp1 , 0 , tmp2 , 0 ) ; for ( int i = 0 ; i < tmp2 . length ; ++ i ) { result [ resultOffset + i ] = 2 * tmp2 [ i ] ; } } else { subtract ( tmp1 , 0 , x , xOffset , tmp2 , 0 ) ; divide ( y , yOffset , tmp2 , 0 , tmp1 , 0 ) ; atan ( tmp1 , 0 , tmp2 , 0 ) ; result [ resultOffset ] = ( ( tmp2 [ 0 ] <= 0 ) ? - FastMath . PI : FastMath . PI ) - 2 * tmp2 [ 0 ] ; for ( int i = 1 ; i < tmp2 . length ; ++ i ) { result [ resultOffset + i ] = - 2 * tmp2 [ i ] ; } } result [ resultOffset ] = FastMath . atan2 ( y [ yOffset ] , x [ xOffset ] ) ; }",
        "summary": "DerivativeStructure.atan2(y,x) does not handle special cases properly",
        "Description": "The four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However, they give NaN for the value in all cases.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-935",
        "comments": [
            "Fixed in subversion repository as of r1446473.",
            "Closing issue as version 3.2 has been released on 2013-04-06."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of r1446473, and the issue has been closed as version 3.2 has been released on 2013-04-06."
    },
    "JxPath_20_src/java/org/apache/commons/jxpath/ri/compiler/CoreOperationRelationalExpression.java_71_99": {
        "src": "private boolean compute(Object left, Object right) {\n        left = reduce(left);\n        right = reduce(right);\n\n        if (left instanceof InitialContext) {\n            ((InitialContext) left).reset();\n        }\n        if (right instanceof InitialContext) {\n            ((InitialContext) right).reset();\n        }\n        if (left instanceof Iterator && right instanceof Iterator) {\n            return findMatch((Iterator) left, (Iterator) right);\n        }\n        if (left instanceof Iterator) {\n            return containsMatch((Iterator) left, right);\n        }\n        if (right instanceof Iterator) {\n            return containsMatch((Iterator) right, left);\n        }\n        double ld = InfoSetUtil.doubleValue(left);\n        if (Double.isNaN(ld)) {\n            return false;\n        }\n        double rd = InfoSetUtil.doubleValue(right);\n        if (Double.isNaN(rd)) {\n            return false;\n        }\n        return evaluateCompare(ld == rd ? 0 : ld < rd ? -1 : 1);\n    }",
        "src_wo_comments": "private boolean compute ( Object left , Object right ) { left = reduce ( left ) ; right = reduce ( right ) ; if ( left instanceof InitialContext ) { ( ( InitialContext ) left ) . reset ( ) ; } if ( right instanceof InitialContext ) { ( ( InitialContext ) right ) . reset ( ) ; } if ( left instanceof Iterator && right instanceof Iterator ) { return findMatch ( ( Iterator ) left , ( Iterator ) right ) ; } if ( left instanceof Iterator ) { return containsMatch ( ( Iterator ) left , right ) ; } if ( right instanceof Iterator ) { return containsMatch ( ( Iterator ) right , left ) ; } double ld = InfoSetUtil . doubleValue ( left ) ; if ( Double . isNaN ( ld ) ) { return false ; } double rd = InfoSetUtil . doubleValue ( right ) ; if ( Double . isNaN ( rd ) ) { return false ; } return evaluateCompare ( ld == rd ? 0 : ld < rd ? - 1 : 1 ) ; }",
        "fixed_src": "private boolean compute(Object left, Object right) {\n        left = reduce(left);\n        right = reduce(right);\n\n        if (left instanceof InitialContext) {\n            ((InitialContext) left).reset();\n        }\n        if (right instanceof InitialContext) {\n            ((InitialContext) right).reset();\n        }\n        if (left instanceof Iterator && right instanceof Iterator) {\n            return findMatch((Iterator) left, (Iterator) right);\n        }\n        if (left instanceof Iterator) {\n            return containsMatch((Iterator) left, right);\n        }\n        if (right instanceof Iterator) {\n            return containsMatch(left, (Iterator) right);\n        }\n        double ld = InfoSetUtil.doubleValue(left);\n        if (Double.isNaN(ld)) {\n            return false;\n        }\n        double rd = InfoSetUtil.doubleValue(right);\n        if (Double.isNaN(rd)) {\n            return false;\n        }\n        return evaluateCompare(ld == rd ? 0 : ld < rd ? -1 : 1);\n    }",
        "fixed_src_wo_comments": "private boolean compute ( Object left , Object right ) { left = reduce ( left ) ; right = reduce ( right ) ; if ( left instanceof InitialContext ) { ( ( InitialContext ) left ) . reset ( ) ; } if ( right instanceof InitialContext ) { ( ( InitialContext ) right ) . reset ( ) ; } if ( left instanceof Iterator && right instanceof Iterator ) { return findMatch ( ( Iterator ) left , ( Iterator ) right ) ; } if ( left instanceof Iterator ) { return containsMatch ( ( Iterator ) left , right ) ; } if ( right instanceof Iterator ) { return containsMatch ( left , ( Iterator ) right ) ; } double ld = InfoSetUtil . doubleValue ( left ) ; if ( Double . isNaN ( ld ) ) { return false ; } double rd = InfoSetUtil . doubleValue ( right ) ; if ( Double . isNaN ( rd ) ) { return false ; } return evaluateCompare ( ld == rd ? 0 : ld < rd ? - 1 : 1 ) ; }",
        "summary": "relational operations do not function properly when comparing a non-Iterator LHS to an Iterator RHS",
        "Description": "I have a simple JXpathContext, with the following variables: var1=0, var2=0, var3=1. When I try to evaluate the following expression - \"$var1 + $var2 <= $var3\", it returns false.",
        "issue_url": "https://issues.apache.org/jira//browse/JXPATH-149",
        "comments": [
            "Test case and proposed fix.",
            "Hi, I have reviewed your proposed fix and it looks good.  For formality's sake, can you please declare that your changes are intended for inclusion into the Apache Commons JXPath library?\n\nAlso, in the future, please provide your changes in the form of diff patches.\n\nThanks!",
            "Disregard my previous request.  I accidentally lost my local copy of your patch anyway and as it consisted solely of inversions of code already living in the file in question I simply rewrote from scratch.\n\n\nCommitted revision 1133485.\n"
        ],
        "summarized_discussion": "\n\nThe proposed fix for the source code bug was accepted and committed to the Apache Commons JXPath library. In the future, the changes should be provided in the form of diff patches."
    },
    "JacksonDatabind_63_src/main/java/com/fasterxml/jackson/databind/JsonMappingException.java_119_152": {
        "src": "public String getDescription() {\n            if (_desc == null) {\n                StringBuilder sb = new StringBuilder();\n\n                if (_from == null) { // can this ever occur?\n                    sb.append(\"UNKNOWN\");\n                } else {\n                    Class<?> cls = (_from instanceof Class<?>) ? (Class<?>)_from : _from.getClass();\n                    // Hmmh. Although Class.getName() is mostly ok, it does look\n                    // butt-ugly for arrays.\n                    // 06-Oct-2016, tatu: as per [databind#1403], `getSimpleName()` not so good\n                    //   as it drops enclosing class. So let's try bit different approach\n                    String pkgName = ClassUtil.getPackageName(cls);\n                    if (pkgName != null) {\n                        sb.append(pkgName);\n                        sb.append('.');\n                    }\n                    sb.append(cls.getSimpleName());\n                }\n                sb.append('[');\n                if (_fieldName != null) {\n                    sb.append('\"');\n                    sb.append(_fieldName);\n                    sb.append('\"');\n                } else if (_index >= 0) {\n                    sb.append(_index);\n                } else {\n                    sb.append('?');\n                }\n                sb.append(']');\n                _desc = sb.toString();\n            }\n            return _desc;\n        }",
        "src_wo_comments": "public String getDescription ( ) { if ( _desc == null ) { StringBuilder sb = new StringBuilder ( ) ; if ( _from == null ) { sb . append ( \"UNKNOWN\" ) ; } else { Class < ? > cls = ( _from instanceof Class < ? > ) ? ( Class < ? > ) _from : _from . getClass ( ) ; String pkgName = ClassUtil . getPackageName ( cls ) ; if ( pkgName != null ) { sb . append ( pkgName ) ; sb . append ( '.' ) ; } sb . append ( cls . getSimpleName ( ) ) ; } sb . append ( '[' ) ; if ( _fieldName != null ) { sb . append ( '\"' ) ; sb . append ( _fieldName ) ; sb . append ( '\"' ) ; } else if ( _index >= 0 ) { sb . append ( _index ) ; } else { sb . append ( '?' ) ; } sb . append ( ']' ) ; _desc = sb . toString ( ) ; } return _desc ; }",
        "fixed_src": "public String getDescription() {\n            if (_desc == null) {\n                StringBuilder sb = new StringBuilder();\n\n                if (_from == null) { // can this ever occur?\n                    sb.append(\"UNKNOWN\");\n                } else {\n                    Class<?> cls = (_from instanceof Class<?>) ? (Class<?>)_from : _from.getClass();\n                    // Hmmh. Although Class.getName() is mostly ok, it does look\n                    // butt-ugly for arrays.\n                    // 06-Oct-2016, tatu: as per [databind#1403], `getSimpleName()` not so good\n                    //   as it drops enclosing class. So let's try bit different approach\n                    int arrays = 0;\n                    while (cls.isArray()) {\n                        cls = cls.getComponentType();\n                        ++arrays;\n                    }\n                    sb.append(cls.getName());\n                    while (--arrays >= 0) {\n                        sb.append(\"[]\");\n                    }\n                    /* was:\n                    String pkgName = ClassUtil.getPackageName(cls);\n                    if (pkgName != null) {\n                        sb.append(pkgName);\n                        sb.append('.');\n                    }\n                    */\n                }\n                sb.append('[');\n                if (_fieldName != null) {\n                    sb.append('\"');\n                    sb.append(_fieldName);\n                    sb.append('\"');\n                } else if (_index >= 0) {\n                    sb.append(_index);\n                } else {\n                    sb.append('?');\n                }\n                sb.append(']');\n                _desc = sb.toString();\n            }\n            return _desc;\n        }",
        "fixed_src_wo_comments": "public String getDescription ( ) { if ( _desc == null ) { StringBuilder sb = new StringBuilder ( ) ; if ( _from == null ) { sb . append ( \"UNKNOWN\" ) ; } else { Class < ? > cls = ( _from instanceof Class < ? > ) ? ( Class < ? > ) _from : _from . getClass ( ) ; int arrays = 0 ; while ( cls . isArray ( ) ) { cls = cls . getComponentType ( ) ; ++ arrays ; } sb . append ( cls . getName ( ) ) ; while ( -- arrays >= 0 ) { sb . append ( \"[]\" ) ; } } sb . append ( '[' ) ; if ( _fieldName != null ) { sb . append ( '\"' ) ; sb . append ( _fieldName ) ; sb . append ( '\"' ) ; } else if ( _index >= 0 ) { sb . append ( _index ) ; } else { sb . append ( '?' ) ; } sb . append ( ']' ) ; _desc = sb . toString ( ) ; } return _desc ; }",
        "summary": "Reference-chain hints use incorrect class-name for inner classes",
        "Description": "``` java\nimport com.fasterxml.jackson.annotation.JsonCreator;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.databind.JsonMappingException;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.junit.jupiter.api.Test;\n\nimport java.io.IOException;\n\nimport static com.google.common.truth.Truth.assertThat;\nimport static org.junit.jupiter.api.Assertions.expectThrows;\n\npublic class ReferenceChainTest {\n    // illustrates that jackson's \"reference chain\" help-text uses incorrect class-names for inner classes\n    @Test public void incorrectReferenceChain() throws IOException {\n        JsonMappingException jsonMappingException = expectThrows(JsonMappingException.class, () -> {\n            ObjectMapper objectMapper = new ObjectMapper();\n            objectMapper.readValue(objectMapper.writeValueAsBytes(new Outer()), Outer.class);\n        });\n        JsonMappingException.Reference reference = jsonMappingException.getPath().get(0);\n        assertThat(reference.toString()).isEqualTo(\"ReferenceChainTest$Outer[\\\"inner\\\"]\");\n    }\n\n    static class Outer {\n        public Inner inner = new Inner();\n    }\n\n    static class Inner {\n        public int x;\n\n        @JsonCreator public static Inner create(@JsonProperty(\"x\") int x) {\n            throw new RuntimeException(\"test-exception\");\n        }\n    }\n}\n\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Yup, the problem was with use of `getSimpleName()` with `getPackage()`, dropping enclosing class name. The original reason was to avoid ugly notation used by JDK for array types, but I rewrote it to be able to just use `getName()` after unwrapping array type.\n\nDecided to fix for 2.8, to minimize risk for 2.7 maintenance branch; while unlikely to cause problems I suspect there may be unit tests out there that use straight comparison of error messages (jackson itself had couple) and while not fatal it seems better to avoid possible hassle.\n\nFix will be in 2.8.4, as well as 2.9.0 whenever that is released.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was solved by rewriting the code to use the getName() method after unwrapping the array type, instead of using getSimpleName() with getPackage(). The fix will be included in the 2.8.4 and 2.9.0 releases."
    },
    "Math_51_src/main/java/org/apache/commons/math/analysis/solvers/BaseSecantSolver.java_128_247": {
        "src": "protected final double doSolve() {\n        // Get initial solution\n        double x0 = getMin();\n        double x1 = getMax();\n        double f0 = computeObjectiveValue(x0);\n        double f1 = computeObjectiveValue(x1);\n\n        // If one of the bounds is the exact root, return it. Since these are\n        // not under-approximations or over-approximations, we can return them\n        // regardless of the allowed solutions.\n        if (f0 == 0.0) {\n            return x0;\n        }\n        if (f1 == 0.0) {\n            return x1;\n        }\n\n        // Verify bracketing of initial solution.\n        verifyBracketing(x0, x1);\n\n        // Get accuracies.\n        final double ftol = getFunctionValueAccuracy();\n        final double atol = getAbsoluteAccuracy();\n        final double rtol = getRelativeAccuracy();\n\n        // Keep track of inverted intervals, meaning that the left bound is\n        // larger than the right bound.\n        boolean inverted = false;\n\n        // Keep finding better approximations.\n        while (true) {\n            // Calculate the next approximation.\n            final double x = x1 - ((f1 * (x1 - x0)) / (f1 - f0));\n            final double fx = computeObjectiveValue(x);\n\n            // If the new approximation is the exact root, return it. Since\n            // this is not an under-approximation or an over-approximation,\n            // we can return it regardless of the allowed solutions.\n            if (fx == 0.0) {\n                return x;\n            }\n\n            // Update the bounds with the new approximation.\n            if (f1 * fx < 0) {\n                // The value of x1 has switched to the other bound, thus inverting\n                // the interval.\n                x0 = x1;\n                f0 = f1;\n                inverted = !inverted;\n            } else {\n                switch (method) {\n                case ILLINOIS:\n                    f0 *= 0.5;\n                    break;\n                case PEGASUS:\n                    f0 *= f1 / (f1 + fx);\n                    break;\n                        // Update formula cannot make any progress: Update the\n                        // search interval.\n                default:\n                    // Should never happen.\n                }\n            }\n            // Update from [x0, x1] to [x0, x].\n            x1 = x;\n            f1 = fx;\n\n            // If the function value of the last approximation is too small,\n            // given the function value accuracy, then we can't get closer to\n            // the root than we already are.\n            if (FastMath.abs(f1) <= ftol) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    if (inverted) {\n                        return x1;\n                    }\n                    break;\n                case RIGHT_SIDE:\n                    if (!inverted) {\n                        return x1;\n                    }\n                    break;\n                case BELOW_SIDE:\n                    if (f1 <= 0) {\n                        return x1;\n                    }\n                    break;\n                case ABOVE_SIDE:\n                    if (f1 >= 0) {\n                        return x1;\n                    }\n                    break;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n\n            // If the current interval is within the given accuracies, we\n            // are satisfied with the current approximation.\n            if (FastMath.abs(x1 - x0) < FastMath.max(rtol * FastMath.abs(x1),\n                                                     atol)) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    return inverted ? x1 : x0;\n                case RIGHT_SIDE:\n                    return inverted ? x0 : x1;\n                case BELOW_SIDE:\n                    return (f1 <= 0) ? x1 : x0;\n                case ABOVE_SIDE:\n                    return (f1 >= 0) ? x1 : x0;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n        }\n    }",
        "src_wo_comments": "protected final double doSolve ( ) { double x0 = getMin ( ) ; double x1 = getMax ( ) ; double f0 = computeObjectiveValue ( x0 ) ; double f1 = computeObjectiveValue ( x1 ) ; if ( f0 == 0.0 ) { return x0 ; } if ( f1 == 0.0 ) { return x1 ; } verifyBracketing ( x0 , x1 ) ; final double ftol = getFunctionValueAccuracy ( ) ; final double atol = getAbsoluteAccuracy ( ) ; final double rtol = getRelativeAccuracy ( ) ; boolean inverted = false ; while ( true ) { final double x = x1 - ( ( f1 * ( x1 - x0 ) ) / ( f1 - f0 ) ) ; final double fx = computeObjectiveValue ( x ) ; if ( fx == 0.0 ) { return x ; } if ( f1 * fx < 0 ) { x0 = x1 ; f0 = f1 ; inverted = ! inverted ; } else { switch ( method ) { case ILLINOIS : f0 *= 0.5 ; break ; case PEGASUS : f0 *= f1 / ( f1 + fx ) ; break ; default : } } x1 = x ; f1 = fx ; if ( FastMath . abs ( f1 ) <= ftol ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : if ( inverted ) { return x1 ; } break ; case RIGHT_SIDE : if ( ! inverted ) { return x1 ; } break ; case BELOW_SIDE : if ( f1 <= 0 ) { return x1 ; } break ; case ABOVE_SIDE : if ( f1 >= 0 ) { return x1 ; } break ; default : throw new MathInternalError ( ) ; } } if ( FastMath . abs ( x1 - x0 ) < FastMath . max ( rtol * FastMath . abs ( x1 ) , atol ) ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : return inverted ? x1 : x0 ; case RIGHT_SIDE : return inverted ? x0 : x1 ; case BELOW_SIDE : return ( f1 <= 0 ) ? x1 : x0 ; case ABOVE_SIDE : return ( f1 >= 0 ) ? x1 : x0 ; default : throw new MathInternalError ( ) ; } } } }",
        "fixed_src": "protected final double doSolve() {\n        // Get initial solution\n        double x0 = getMin();\n        double x1 = getMax();\n        double f0 = computeObjectiveValue(x0);\n        double f1 = computeObjectiveValue(x1);\n\n        // If one of the bounds is the exact root, return it. Since these are\n        // not under-approximations or over-approximations, we can return them\n        // regardless of the allowed solutions.\n        if (f0 == 0.0) {\n            return x0;\n        }\n        if (f1 == 0.0) {\n            return x1;\n        }\n\n        // Verify bracketing of initial solution.\n        verifyBracketing(x0, x1);\n\n        // Get accuracies.\n        final double ftol = getFunctionValueAccuracy();\n        final double atol = getAbsoluteAccuracy();\n        final double rtol = getRelativeAccuracy();\n\n        // Keep track of inverted intervals, meaning that the left bound is\n        // larger than the right bound.\n        boolean inverted = false;\n\n        // Keep finding better approximations.\n        while (true) {\n            // Calculate the next approximation.\n            final double x = x1 - ((f1 * (x1 - x0)) / (f1 - f0));\n            final double fx = computeObjectiveValue(x);\n\n            // If the new approximation is the exact root, return it. Since\n            // this is not an under-approximation or an over-approximation,\n            // we can return it regardless of the allowed solutions.\n            if (fx == 0.0) {\n                return x;\n            }\n\n            // Update the bounds with the new approximation.\n            if (f1 * fx < 0) {\n                // The value of x1 has switched to the other bound, thus inverting\n                // the interval.\n                x0 = x1;\n                f0 = f1;\n                inverted = !inverted;\n            } else {\n                switch (method) {\n                case ILLINOIS:\n                    f0 *= 0.5;\n                    break;\n                case PEGASUS:\n                    f0 *= f1 / (f1 + fx);\n                    break;\n                case REGULA_FALSI:\n                    if (x == x1) {\n                        final double delta = FastMath.max(rtol * FastMath.abs(x1),\n                                                          atol);\n                        // Update formula cannot make any progress: Update the\n                        // search interval.\n                        x0 = 0.5 * (x0 + x1 - delta);\n                        f0 = computeObjectiveValue(x0);\n                    }\n                    break;\n                default:\n                    // Should never happen.\n                    throw new MathInternalError();\n                }\n            }\n            // Update from [x0, x1] to [x0, x].\n            x1 = x;\n            f1 = fx;\n\n            // If the function value of the last approximation is too small,\n            // given the function value accuracy, then we can't get closer to\n            // the root than we already are.\n            if (FastMath.abs(f1) <= ftol) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    if (inverted) {\n                        return x1;\n                    }\n                    break;\n                case RIGHT_SIDE:\n                    if (!inverted) {\n                        return x1;\n                    }\n                    break;\n                case BELOW_SIDE:\n                    if (f1 <= 0) {\n                        return x1;\n                    }\n                    break;\n                case ABOVE_SIDE:\n                    if (f1 >= 0) {\n                        return x1;\n                    }\n                    break;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n\n            // If the current interval is within the given accuracies, we\n            // are satisfied with the current approximation.\n            if (FastMath.abs(x1 - x0) < FastMath.max(rtol * FastMath.abs(x1),\n                                                     atol)) {\n                switch (allowed) {\n                case ANY_SIDE:\n                    return x1;\n                case LEFT_SIDE:\n                    return inverted ? x1 : x0;\n                case RIGHT_SIDE:\n                    return inverted ? x0 : x1;\n                case BELOW_SIDE:\n                    return (f1 <= 0) ? x1 : x0;\n                case ABOVE_SIDE:\n                    return (f1 >= 0) ? x1 : x0;\n                default:\n                    throw new MathInternalError();\n                }\n            }\n        }\n    }",
        "fixed_src_wo_comments": "protected final double doSolve ( ) { double x0 = getMin ( ) ; double x1 = getMax ( ) ; double f0 = computeObjectiveValue ( x0 ) ; double f1 = computeObjectiveValue ( x1 ) ; if ( f0 == 0.0 ) { return x0 ; } if ( f1 == 0.0 ) { return x1 ; } verifyBracketing ( x0 , x1 ) ; final double ftol = getFunctionValueAccuracy ( ) ; final double atol = getAbsoluteAccuracy ( ) ; final double rtol = getRelativeAccuracy ( ) ; boolean inverted = false ; while ( true ) { final double x = x1 - ( ( f1 * ( x1 - x0 ) ) / ( f1 - f0 ) ) ; final double fx = computeObjectiveValue ( x ) ; if ( fx == 0.0 ) { return x ; } if ( f1 * fx < 0 ) { x0 = x1 ; f0 = f1 ; inverted = ! inverted ; } else { switch ( method ) { case ILLINOIS : f0 *= 0.5 ; break ; case PEGASUS : f0 *= f1 / ( f1 + fx ) ; break ; case REGULA_FALSI : if ( x == x1 ) { final double delta = FastMath . max ( rtol * FastMath . abs ( x1 ) , atol ) ; x0 = 0.5 * ( x0 + x1 - delta ) ; f0 = computeObjectiveValue ( x0 ) ; } break ; default : throw new MathInternalError ( ) ; } } x1 = x ; f1 = fx ; if ( FastMath . abs ( f1 ) <= ftol ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : if ( inverted ) { return x1 ; } break ; case RIGHT_SIDE : if ( ! inverted ) { return x1 ; } break ; case BELOW_SIDE : if ( f1 <= 0 ) { return x1 ; } break ; case ABOVE_SIDE : if ( f1 >= 0 ) { return x1 ; } break ; default : throw new MathInternalError ( ) ; } } if ( FastMath . abs ( x1 - x0 ) < FastMath . max ( rtol * FastMath . abs ( x1 ) , atol ) ) { switch ( allowed ) { case ANY_SIDE : return x1 ; case LEFT_SIDE : return inverted ? x1 : x0 ; case RIGHT_SIDE : return inverted ? x0 : x1 ; case BELOW_SIDE : return ( f1 <= 0 ) ? x1 : x0 ; case ABOVE_SIDE : return ( f1 >= 0 ) ? x1 : x0 ; default : throw new MathInternalError ( ) ; } } } }",
        "summary": "\"RegulaFalsiSolver\" failure",
        "Description": "The following unit test:\n{code}\n@Test\npublic void testBug() {\n    final UnivariateRealFunction f = new UnivariateRealFunction() {\n            @Override\n            public double value(double x) {\n                return Math.exp(x) - Math.pow(Math.PI, 3.0);\n            }\n        };\n\n    UnivariateRealSolver solver = new RegulaFalsiSolver();\n    double root = solver.solve(100, f, 1, 10);\n}\n{code}\nfails with\n{noformat}\nillegal state: maximal count (100) exceeded: evaluations\n{noformat}\n\nUsing \"PegasusSolver\", the answer is found after 17 evaluations.\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-631",
        "comments": [
            "Reported by Axel Kramer in MATH-599.",
            "The problem was due to the fact that at some point, the update formula always gave the same value: Nothing was being updated and the loop went on until the number of evaluations was exhausted.\n\nI've committed a tentative solution in revision 1154614.\nHowever:\n# I'm not sure that it doesn't have any adverse side-effects on the bracketing property.\n# It is quite probably not a pristine \"regula falsi\" algorithm anymore.\n\nPlease review.\n\nAnyways, for the function that triggered the problem (see \"testIssue631\" in \"RegulaFalsiSolverTest.java\"), the (modified) {{RegulaFalsiSolver}} takes 3624 evaluations (versus 17 for {{PegasusSolver}}). We should probably add a word of warning in the class Javadoc.\n",
            "I just got back from a 3 week vacation, so I couldn't reply earlier.\n\nThe documentation for the RegulaFalsiSolver states: \"Unlike the Secant method, convergence is guaranteed by maintaining a bracketed solution.\" While this is theoretically true, in this case it is not so, because (if I understand correctly) only a single bound is updated repeatedly, and the update is too small to matter (has no effect), due to the double representation.\n\nThe change you propose (which is difficult to see as you also change other things in the same commit) is to modify x0 and f0 if the new value of x and x1 are equal. As I see it, this changes the algorithm, and it is no longer the Regula Falsi method as known from literature. I'm therefore against this change.\n\nThe problem that is identified in this issue is very similar to the well-known problem of the Regula Falsi method: it converges very slowly for certain problems, due to one side being updated all the time, while the other one stays the same. The Illinois and Pegasus algorithms solve exactly this problem, and are well-documented in literature.\n\nI therefore think it would be better if the RegulaFalsiSolver kept it's original implementation, and for this problem the Illinois or Pegasus method should then be used instead.\n\nThe other changes (if statements to switch with default, extracting bound switch statements, etc) can be kept, if you wish.\n\nThe suggestion to add a warning to the Secant and Regula Falsi solvers that this is a possible problem, and the solution (use Illinois or Pegasus), would indeed be a good idea. In general, adding a note that the Illinois and Pegasus algorithms perform better, would be a good idea regardless of this issue.\n\nOnce more, to be clear, I don't think this issue is a bug. It is a result of the limited convergence of the Regula Falsi method combined with the implications of limited double precision. The limited convergence of the algorithm is a property of the algorithm, and should in my opinion not be changed. I also don't think that trying to work around the limited double precision would be a good idea.",
            "> (which is difficult to see as you also change other things in the same commit)\n\nSorry, but I didn't hit the solution right away, i.e. before changing those two additional little things to make the code clearer (for me)...\n\nThe only actual change is that the {{REGULA_FALSI}} enum was not used (i.e. with the {{switch}} little change, the corresponding {{case}} would have been empty) whereas now it contains the update of x0 to avoid an infinite loop.\n\nThe other (cosmetic) change was to take these two statements\n{code}\nx1 = x;\nf1 = fx;\n{code}\nout of the previous {{if}} and {{else}} blocks, as they were duplicated there (which made me wonder whether it was a bug that they were _not_ different).\n\nYou say\n> [...] convergence is guaranteed [...]\n> [...] it converges very slowly for certain problems, [...]\n> [...] The limited convergence of the algorithm is a property of the algorithm, [...]\n\nAll the above imply that one expects that the algorithm _can_ find the solution.\nHowever, in this implementation, it _can't_.\nTherefore there is a bug, somewhere.\n\nI agree that it is a limitation of double precision. But, IMHO, leaving the code as-is is not a good idea because because it leads to the impression that the \"Regula Falsi\" mathematical algorithm can fail to converge, which is not correct (IIUC).\nTherefore, we could add a comment stating that the _implementation_ with limited precision can fail to converge but that would be akin to saying to users: \"Here is a code, but don't use it.\"\nPersonally, I would prefer to say: \"Because of limited precision, the implementation can fail to converge. In those cases, we slightly modified the original algorithm in order to avoid failure.\"\n",
            "{quote}\nAll the above imply that one expects that the algorithm can find the solution.\nHowever, in this implementation, it can't.\nTherefore there is a bug, somewhere.\n{quote}\n\nHere, the bug is in the algorithm itself, not in the implementation.\n\n{quote}\nit leads to the impression that the \"Regula Falsi\" mathematical algorithm can fail to converge, which is not correct (IIUC).\n{quote}\n\nIt is correct. Regula Falsi fails to converge, or rather it can take a too large number of iteration to converge. This is exactly this behavior that has lead to the construction of other algorithms like Illinois or Pegasus. These two algorithms try to detect the case when the same end of the interval is always updated, and the other end remains unchanged. Once they have detected this, they slightly change the value at one end to trick the linear evaluation into choosing a value that is very likely to have the required sign to update this other end. In fact, in many cases depending of the sign of the curvature near the root, as soon as one end is very close to the root the linear interpolation will always remain on the same side of the root and hence will update this end.\n\nI agree with Dennis here, the change needed to ensure convergence is not tool long is to choose a better algorithm, such as Illinois, Pegasus ... or the nth order bracketing solver I recently added. Regula Falsi should remain the reference Regula Falsi, just as secant and Brent should remain the reference ones.\n",
            "\"fails to converge\" and \"large number of iteration to converge\" are completely different things.\n\nThe documentation says: \"convergence is guaranteed\". Is _that_ false?\n\nMoreover, for the function reported in this issue, the problem is not that it takes a large number iterations, it is that the loop is _literally_ infinite because at some point, nothing changes anymore.\n\nStated otherwise: If implemented with larger/infinite precision, would it converge?\nIn the affirmative, then in my opinion it means that the plain \"Regula Falsi\" method cannot be implemented with double precision (or that its convergence properties are not as stated in the docs) or that there is a bug in the implementation.\n\nIn the former case, why keep something that will never be used (as we'll warn users that they should use \"Pegasus\" or \"Illinois\" but certainly not \"RegulaFalsi\")? IMHO, we could just state in the docs that \"RegulaFalsi\" was not implemented because it is demonstrably less efficient and sometimes fails to work.\n\nA less radical alternative would be to keep the test I've inserted in the code (at line 186) and throw a {{MathIllegalStateException}} if it passes. The previous behaviour (infinite loop) is a bug in CM.\n",
            "{quote}\nThe documentation says: \"convergence is guaranteed\". Is that false?\n{quote}\n\nIt depends on what is called convergence.\nIf convergence is evaluated only as the best of the two endpoints (measured along y axis), yes convergence is guaranteed and in this case it is very slow. This is what appears in many analysis books.\nIf convergence is evaluated by ensuring the bracketing interval (measured along x axis) reduces to zero (i.e. both endpoints converge to the root), convergence is not guaranteed.\n\nThe first case is achieved with our implementation by using the function accuracy setting. The second case is achieved with our implementation by using relative accuracy and absolute accuracy settings, which both are computed along x axis.\n\nI fear that there are several different references about convergence for this method (just as for Brent). So we already are able to implement both views.\n\nWithout any change to our implementation, we reach convergence for this example by setting the function accuracy to 7.4e-13 or above, and it is slow (about 3500 evaluations). The default setting for function accuracy is very low (1.0e-15) and in this case, given the variation rate of the function near the root, it is equivalent to completely ignore convergence on y on only check the convergence on the interval length along x. \n",
            "I think we should either stick with the standard implementation of Regula Falsi or drop the class altogether.  Different rootfinders are going to perform better / worse for different functions and parameter values and I don't think it is a good idea to try to modify our implementations of the algorithms to try to work around their shortcomings for problem instances for which they are not well-suited.  It is much better to stick with standard algorithms, document them, and leave it to users to choose among implementations.  \n\nRegula Falsi is not a good general-purpose rootfinder, but it does perform well for some problems (or parts of problems) and the original submission was a working implementation, so I would say revert the changes and keep it.",
            "I understand what you say. But however you put it, there is a bug; if not in the implementation, then in the API. It is not expected behaviour that something which must be changed (function accuracy threshold) to ensure correct behaviour (avoid an undetected infinite loop) is not a mandatory parameter.\nTo debug this, I started by raising the absolute accuracy threshold (the first default parameter, thus the first obvious thing to do) to 1e-2 and was stunned that I couldn't get anything after 1000000 iterations!\n\nTherefore I maintain that, at a minimum, we put a line that will detect the infinite loop and raise an exception identifying _that_ problem and not let the user wait for \"TooManyEvaluationsException\" to be raised, as that will induce the unaware (me) to just allow more evaluations and try again.\n\nThis solution does not corrupt the algorithm; it just adds protection.\n",
            "I disagree with your statement about setting accuracy.  All of this is configurable and if not set, you get the (documented) defaults.  This is all documented.  If the documentation is not clear, then we can improve it.  A user who applies Regula Falsi to the problem instance being examined here will end up maxing iterations.  I see no problem with that and in fact I see it as *correct* behavior (given the documented execution context of the algorithm).  ",
            "How can it be correct to have an infinite loop?\nThe problem is not slow convergence, which you can overcome by allowing more iterations.\nIt is too low function value accuracy which you cannot overcome by allowing more iterations. Thus my point: We must raise the appropriate exception (the doc for which will state that it can happen if the function value accuracy is too low for the implementation to provide a result).\n",
            "[My comment starting with \"I understand what you say.\" was an answer to Luc. I hadn't read Phil's previous one which was posted while I was writing mine.]\n\nI agree that it is better not to change the standard algorithm, as I indicated in my first comment.\nThe fix which I'm proposing is not an algorithm change, it is an implementation detail similar to the many hundreds checks performed in CM. Just it is not a precondition test. It adequately indicates that something went wrong and can help the user figure out what it was. It makes the implementation more robust.\n",
            "The original implementation, for the \"problem instance being examined here\", would find the root with absolute accuracy lower than *10e-12* after 3560 evaluations (note: using the default value of *1e-6*).\nIn fact, the root was found, at the required accuracy, after around 2200 evaluations.\n\nThat does not sound like correct behavior.\nThe problem is that, \"x0\" never being updated, the convergence test always fails... until we reach the limitation of double precision, which entails an infinite loop.\n\nIn fact my fix should not be necessary, as things have gone awry before it would apply, but there is a bug to fix nonetheless.\n",
            "Is there actually a possibility of an infinite loop in the code?  Looks to me like the max evaluations bound will stop the while loop, so there is no potential for an infinite loop.  Apologies if I am misreading the code and the loop can fail to terminate, in which case I agree this is a problem.  (As a side note, from a style perspective, I prefer to explicitly bound loops to avoid this kind of uncertainty.  The natural hard bound here is the evaluation count.)\n\nTrying to detect when a sequence of iterates has gotten \"stuck\" and is destined to hit max iterations without converging is walking down a path that I think is unwise for us and users.  I see no reason not to stick with the standard impl here, which is nicely documented in the original submission.  Trying to workaround numerical problems in simple algorithms and change contracts to include these workarounds is asking for trouble - both for us and users.  In a simple case like this, it is much better to just stick with the documented algorithm, which should in this case (again unless I am missing something) end with max evaluations exceeded, which is the right exception to report. ",
            "I surely hope that your last post is not an answer to mine from 23:46.\n\nI'll try to answer here in case it was in reply to my previous one (23:06).\nOf course, the code will not run forever because of the \"maxeval\" bound.\nBut it will run for a time that depends on the value of \"maxeval\" *with no added benefit*! From a certain point, the loop is like\n{code}\nwhile (true) {\n  // Do nothing useful, just count!\n  ++count;\n  if (count > maxeval) {\n    throw new TooManyEvalutationsException(maxeval);\n  }\n}\n{code}\n\n{quote}\nfrom a style perspective, I prefer to explicitly bound loops\n{quote}\n\nFrom an *OO* style perspective, the reuse of the \"Incrementor\" is better, and you don't have to rewrite the same \"test and throw exception if failed\" boiler plate code each time there is such a loop.\n\n{quote}\nTrying to detect when a sequence of iterates has gotten \"stuck\" and is destined to hit max iterations without converging is walking down a path that I think is unwise for us and users.\n{quote}\n\nWhy?\n\n{quote}\nI see no reason not to stick with the standard impl here\n{quote}\n\nA busy idle loop is a compelling reason IMO.\n\n{quote}\nTrying to workaround numerical problems in simple algorithms and change contracts to include these workarounds is asking for trouble\n{quote}\n\nThe trouble is there with the current implementation. I'm not criticizing the contribution but this issue shows that it should be made more robust.\nAlso, the documentation about \"convergence is guaranteed\" can lead to a false sense of security.\nMoreover, is the \"regula falsi\" a mathematical algorithm (with a guaranteed converge property if computed with infinite precision) or a numerical one, which this issue proves that it cannot guarantee convergence? In the former case, CM's (numerical) implementation is not strictly \"regula falsi\" and there would be no such thing as respect for an original/standard implementation if we can make it more robust.\n\nI've already indicated that the fix does *not* change the contract; it stops the busy idle loop as soon as it is detected and reports that it won't do any good to increase the number of iterations. That's _obviously_ more robust.\n\nNow, if you were answering to my 23:46 post, I'd be glad to read an explanation of why the first paragraph describes expected behaviour.\n\n",
            "I don't understand.\n\nWhen it was created, the maxIteration threshold was exactly designed for this purpose: get out of infinite loops. It was later renamed maxEvaluation but the purpose is still the same: don't get stuck. The reason why we get stuck is irrelevant. This limit is simply a safety limit, not a tuning parameter that user are expected to raise once they hit it hoping they will converge later on. If they could raise it later, then they should set it to an appropriate value at once. Hitting it implies computation failed. Regula falsi just like any algorithm can fail if applied with the wrong parameters or to the wrong function (in fact, even with a good setting of function accuracy, it fails to converge if we require a bracket selection on the side that does not move).\n\nAlso detecting one bound is not updated is what Illinois and Pegasus are designed to do.\n\nSo I think we should completely get rid of regula falsi and only keep the better algorithms.",
            "{quote}\nI think we should completely get rid of regula falsi and only keep the better algorithms.\n{quote}\n\nThat was my first idea. And that would be the simplest one, the safest one, and the only viable one as I can't seem to state clearly enough that\n* Problem 1: When the doc says \"guaranteed convergence\", the algorithm should provide the answer.\n* Problem 2: When the (absolute) accuracy threshold is set to 1e-6, and the correct root *is* found (after 2200 iterations) within the requirements, it should be returned, instead running idle and finish with an exception\n\n{quote}\nThe reason why we get stuck is irrelevant.\n{quote}\n\nBut why? If we *can* be more precise on the cause of failure, why not do it?\n\n{quote}\nThis limit is simply a safety limit, not a tuning parameter that user are expected to raise once they hit it hoping they will converge later on.\n{quote}\n\nIn principle, some possible use would be to compare the efficiency of different methods where the main criterion would be a time limitation (assuming that the function evaluation time overwhelms the of the root solver algorithm time). Thus with the function that triggered this issue:\n* If you set maxeval to \"3000\", then both \"Pegasus\" (17 evals) and (a fixed) \"RegulaFalsi\" (2200 evals) would fill the bill.\n* If you set maxeval to \"1000\", then \"Pegasus\" will be the only winner.\n\n\nAnyways:\n+1 for removing it altogether, and include somewhere the reason for it not being implemented in CM.\n",
            "I am OK removing Reg Falsi, but stand by comments above that it is a very bad idea to hack standard algorithms and agree with Luc that maxing iterations is the correct behavior in the case we have been discussing. It is kind of pathetic that the compromise is to drop the impl; but in this case I don't see it as a real loss, since I can't think of any examples where Reg Falsi would be preferable to one of the other solvers - other than for educational purposes.",
            "May I please know *why it is OK that a bit of code does loop counting and repeatedly computes the same thing*!\n\nYou insist that I'd be \"hacking\" whereas I've indicated 3 or 4 times that there is no hack: just a test that will exit the loop as soon as it detects that the algorithm has failed. Why is it not understandable that the busy loop could last for a long time? The function is potentially evaluated millions of times at the same point. What if the evaluation is costly? Imagine the computation running for days, only to discover that it could have been be stopped after a few seconds. Is that robust code and good advertising for a library? It is one thing to expect that there are unknown bugs in CM, but refusing to fix a known one is so obviously wrong...\n\nAnd may I please know *why it is OK that an algorithm that finds the right result does not return it*.\n\nI had been trying to provide alternatives to the removal, but I can't do much more if nobody answers the above two questions.\nYou just have to run the code and print \"x\" and \"x1\" to see what is going on!\n",
            "{code}\nMay I please know *why it is OK that a bit of code does loop counting and repeatedly computes the same thing!*\n{code}\n\nWe didn't say that. We said that regula falsi is a standard *bad* algorithm. We said that very smart people have enhanced it 40 years ago and the enhanced versions are known and already implemented in Commons Math. These algorithms are *not* blind loop counters and they insert smart target shifts that *prevent* the behavior we observe here. These algorithms not only detect the problem, they fix it! They allow convergence along x. They allow selection of the side of the root.\n\n{code}\nThe function is potentially evaluated millions of times at the same point.\n{code}\n\nThe maxEvaluations is already here to prevent this, and in fact now this max number is even mandatory in the solve method (you placed it if I remember correctly). So the function is called millions of time only if the users wishes so by setting the maxEvaluations to a number in the range of millions.\n\n{code}\nAnd may I please know *why it is OK that an algorithm that finds the right result does not return it.*\n{code}\n\nIf the user asked for a convergence in x or for a convergence on y on the side that is stuck, then no, the algorithm did not find the right result. One of its bounds converged but the users asked for something else.\n\n{code}\nYou just have to run the code and print \"x\" and \"x1\" to see what is going on!\n{code}\n\nWe know exactly what is going on! We know the algorithm is stuck. We know why it is stuck. We know why it did not detect it is stuck. We know it will finally hit the safety maxEvaluation threshold that is just waiting for that. And we know that removing all these problems is done by using other algorithms which are already there.\n\nRegula falsi is doomed. It is an algorithm used for educational purposes, or for comparison purposes, not something suited for production use. It is just like Euler for ODE (and by the way we did implement Euler for ODE and we don't recommend users to use it as we also did implement better algorithms that were also designed by smart mathematicians decades ago).",
            "{quote}\nSo the function is called millions of time only if the users wishes so by setting the maxEvaluations to \na number in the range of millions.\n{quote}\n\nNo, the user should not expect that any algorithm will go on a single iteration more than necessary.\nThis is a plain bug.\n\nWhy do you see that a test such as I proposed (exit the loop early) is wrong while CM (and any good program) is full of tests to ensure that you don't do useless computations?\nThis has nothing to do with \"regula falsi\", it is robustness in the face of limited precision.\n\nHowever, if you insist that the bug (failing to detect that it is stuck) is really an integral part of the algorithm, then removing it is not a \"pathetic compromise\", it is the only right thing to do!\n\n",
            "This is a pointless discussion.  Gilles, you obviously don't share the views that Luc and I have on implementing standard algorithms or even what the meaning of a numerical algorithm is. Some algorithms perform well for some classes of problems and not others.  There is an art to choosing the right algorithm for the problem instance at hand.  If we modify our implementations to try to work around shortcomings of the algorithms we implement, then we are not implementing the standard algorithms, and we need to document exactly what it is that we are implementing, because in this case we are actually making it harder for users to choose (because we are not longer advertising standard numerics).  This is what I meant when I said it is both harder for us (because we have to document the hacks and non-standard contracts) and users (because the standard numerical analysis theory that they may be using to choose among implementations will no longer apply).  It is, IMO, a \"pathetic compromise\" to drop the implementation because we can't agree on what it means to implement the algorithm. So be it. Lets drop it and resolve this issue as \"fixed.\"",
            "{quote}\nGilles, you obviously don't share the views that Luc and I have on implementing standard algorithms \n{quote}\n\nThat's simply _not true_.\nI was the one pointing out that standard algorithms should have precedence: Please recall that it was considered fine that \"Levenberg-Marquardt\" and \"Brent\" would be, unknowingly to the user, \"twisted\" to perform _non-standard_ convergences check.\nIn those cases, there was the risk that the result of the algorithm would not be the same as the reference implementation.\n\nIn this case, there is no such thing as deviating from standard numerics! It was just a matter of throwing the right exception. So: \"The algorithm fails? Let's tell it sooner rather than later.\"\n\nVery interesting question that you ask: \"what it means to implement the algorithm\". But please note that I asked it several posts ago[1], and an answer would have helped sort out this discussion. What is your definition?\n\n\n[1] 08/Aug/11 07:24\n",
            "Also:\n\nPhil,\n\nCould you please leave out dismissive qualifiers such as \"pointless\" and \"pathetic\" (and, elsewhere, \"silly\") and stick to more or less objective arguments?\nThat will certainly help keep the conversation tone to a courteous level.\n\nLuc,\n\nThanks for stating in full details what you meant by \"convergence\" in this case. However, it is still a \"post-mortem\" description.\nDo you really expect that the average user of the CM library (a.o. me and the original reporter of the issue) to be able to figure out that \"obvious\" explanation just by getting a \"TooManyEvalutationsException\", setting the along-x accuracy threshold to a ridiculously high value and still getting the same exception?\nIf just for educational purposes, don't you think that it is more instructive to get a specific hint that the algorithm is stuck, rather than hit the ultimate fail-safe barrier much much later, and then download the source code and sprinkle the code with \"println\" statements to do forensic analysis?\n\nPhil,\n\nI tried to handle this issue out of respect for a real user who reported an issue that would have looked suspicious to many CM users. [How many of them would be experts in numerical analysis?]\nYou do not do me a favour by removing this algorithm; I don't want it to be a _compromise_ (pathetic or not). If you prefer to keep it, I don't care anymore. But, in that case, _you_ should have answered to Axel Kramer to go and read some books on numerical analysis.\n",
            "Gilles, I apologize for tone of comments.",
            "The discussions for this issue have left me with a lack of overview, so I'll (try to) objectively summerize the discussions above:\n\nThe problems are:\n # Regula Falsi states it always converges, but the implementation doesn't.\n # The main loop may continue, even when it no longer makes any progress, and subsequently ends with a TooManyEvaluationsException exception.\n\nThe cause of both problems is:\n - The limited/finite precision of the Java double type.\n\nProposed solutions:\n # The patch from revision 1154614, which modifies the Regula Falsi algorithm.\n   #- Consensus seems to be that this change, which modifies the algorithm, is undesireable. We should keep the original algorithm.\n # Detect that the algorithm no longer makes progress and throw an exception, instead of continuing the loop which no longer makes progress.\n   #- This is just earlier detection of the algorithm getting stuck.\n   #- We could throw the TooManyEvaluationsException exception, that continuing the loop would also get us.\n     #-- The class only states \"Exception to be thrown when the maximal number of evaluations is exceeded.\".\n     #-- The exception message only states: \"illegal state: maximal count (100) exceeded: evaluations\"\n     #-- Both may benefit from more extended documentation/messages.\n   #- We could also throw an other exception that more clearly states this issue (NoMoreProgressException, AlgorithmStuckException, ...?).\n     #-- It could for instance mention that changing the function value accuracy may be a solution, or asking for a different kind of solution?\n # Add documentation to the Regula Falsi algorithm that it is not intended to be used for actual problems, but only to compare algorithms, for testing, educational purposes, etc.\n # Add documentation to the Regula Falsi algorithm that users should use Illinois or Pegasus instead, which should outperform the algorithm for most if not all problems.\n # Add documentation to the Regula Falsi algorithm that it theoretically converges, but the implementation may not, due to the limited/finite precision of Java's double type. This will result in an exception (or 2 if we also do solution number 2).\n # Remove the Regula Falsi algorithm, and document why it is not included/implemented.\n   #- This seems to have been accepted as a last resort solution only.\n\nOther notes:\n - The following problem was also indicated: a solution is found after a certain number of iterations, but the algorithm does not return the solution (it does not terminate)\n   -- This should only happen if the user asked for a different solution. That is, there are several accuracy parameters, as well as an allowedSolution parameter.\n   -- If the solution requested by the user is found, it should return the solution immediately, otherwise it is a bug.\n\nNew notes:\n - I think the Regula Falsi algorithm does not state a fixed convergence criteria: it is left to the user to decide on one.\n   -- When I implemented the algorithm, I think I copied the convergence checks for Brent.\n   -- I subsequently modified the convergence criteria when I added the allowedSolution parameter.\n\nMy personal opinions on the proposed solutions:\n - (1) Revert part of 1154614, so get the original algorithm back. The other changes of that commit, that don't change the actual algorith, can stay.\n - (2) If we keep the algorithm, earlier detection would be nice. Not sure which exception to throw in these cases.\n   -- This would result in a single 'if' that detects that the new approximation is the same as the previous one, and we thus no longer make progress, in which case we throw the exception earlier, instead of later.\n - (3-5) If we keep the algorith, all 3 documentation extensions would be a good idea.\n - (6) If possible, keep the algorithm, and don't remove it.\n\nNew issue:\n - TooManyEvaluationsException currently seems to use LocalizedFormats.MAX_COUNT_EXCEEDED(\"maximal count ({0}) exceeded\"), but maybe should use LocalizedFormats.MAX_EVALUATIONS_EXCEEDED(\"maximal number of evaluations ({0}) exceeded\") instead?\n",
            "Thanks for the neat summary!\n\n{quote}\n* (1) Revert part of 1154614, so get the original algorithm back. The other changes of that commit, that don't change the actual algorith, can stay.\n{quote}\n\nDone in revision 1157185.\n\n{quote}\n* (2) If we keep the algorithm, earlier detection would be nice. Not sure which exception to throw in these cases.\n** This would result in a single 'if' that detects that the new approximation is the same as the previous one, and we thus no longer make progress, in which case we throw the exception earlier, instead of later.\n{quote}\n\n+1 (my position in the \"07/Aug/11 20:28\" post)\nAs suggested there, the exception could be \"MathIllegalStateException\" but with a clear message stating that the algorithm is stuck. Or maybe a new subclass of it which we could call \"NumericalPrecisionException\" or even a general-purpose \"ImplementationException\".\n\n{quote}\n[...] all 3 documentation extensions would be a good idea.\n{quote}\n\n+1\n\nAbout the \"new issue\", the message string:\n{quote}\n\"illegal state: maximal count (100) exceeded: evaluations\"\n{quote}\ncontains everything:\n# error type: illegal state\n# failure description: maximal count (100) exceeded\n# context: evaluations\n\nI proposed to use this approach (combining message items with the \"addMessage\" method of \"ExceptionContext\") in order to reduce the number of messages in the \"LocalizedFormats\" enum. Too many of them are just slight variations on a same theme.\n",
            "bq. contains everything\n\nI agree. I was just wondering why a message that seems to be exactly the same as the exception was not used, as it kind of looked like it was created just for this purpose...\n\nbq. I proposed to use this approach (combining message items with the \"addMessage\" method of \"ExceptionContext\") in order to reduce the number of messages in the \"LocalizedFormats\" enum. Too many of them are just slight variations on a same theme.\n\nAh, so then the MAX_EVALUATIONS_EXCEEDED is just a remnant of the past that should be eliminated, by replacing it everywhere by the more general MAX_COUNT_EXCEEDED?",
            "Yes. In the file \"LocalizedFormats.java\", I've started to write\n{noformat}\n/* keep */\n{noformat}\nafter each enum that is supposedly to be kept. All the others are still to be examined for redundancy with another one, or the possibility to create something close using the \"multi-item\" approach.\n",
            "The 'ticket631.patch' file is my attempt to resolve this issue with a solution (or maybe I should call it a compromise?) that is satisfactory for all people that participated in the discussions for this issue, without having to remove the Regula Falsi algorithm from Commons Math.\n\nI changed the following:\n - Added early detection of no longer making progress ('getting stuck'), and documented it.\n   -- I used ConvergenceException for this, as it seems to fit... Do we want a custom error message with it?\n - Extended RegulaFalsiSolver documentation to indicate:\n   -- that the algorithm should not be used for actual problems.\n   -- that Illinois and Pegasus are improved versions and should be prefered.\n   -- that the implementation does not guarantee convergence, while the algorithm theoretically does.\n - Extended IllinoisSolver and PegasusSolver documentation to indicate that they don't suffer from the RegulaFalsiSolver's implementation/convergence issues.\n\nPlease comment on whether this patch is an acceptable solution/compromise, and if not, why it is not.",
            "Committed (with minor additional Javadoc fixes) in revision 1164474.\n\nLeaving open until confirmation that {{ConvergenceException}} is the right one to use. I thought that we could make a difference between _theoretical_ and _implementation_ convergence failures. But it might not be worth introducing the distinction just for this one case, especially since it is quite clear clear now that the class should not be used.",
            "No objection raised; setting to \"Resolved\"."
        ],
        "summarized_discussion": ""
    },
    "Codec_15_src/main/java/org/apache/commons/codec/language/Soundex.java_183_198": {
        "src": "private char getMappingCode(final String str, final int index) {\n        // map() throws IllegalArgumentException\n        final char mappedChar = this.map(str.charAt(index));\n        // HW rule check\n        if (index > 1 && mappedChar != '0') {\n            final char hwChar = str.charAt(index - 1);\n            if ('H' == hwChar || 'W' == hwChar) {\n                final char preHWChar = str.charAt(index - 2);\n                final char firstCode = this.map(preHWChar);\n                if (firstCode == mappedChar || 'H' == preHWChar || 'W' == preHWChar) {\n                    return 0;\n                }\n            }\n        }\n        return mappedChar;\n    }",
        "src_wo_comments": "private char getMappingCode ( final String str , final int index ) { final char mappedChar = this . map ( str . charAt ( index ) ) ; if ( index > 1 && mappedChar != '0' ) { final char hwChar = str . charAt ( index - 1 ) ; if ( 'H' == hwChar || 'W' == hwChar ) { final char preHWChar = str . charAt ( index - 2 ) ; final char firstCode = this . map ( preHWChar ) ; if ( firstCode == mappedChar || 'H' == preHWChar || 'W' == preHWChar ) { return 0 ; } } } return mappedChar ; }",
        "fixed_src": "private char getMappingCode(final String str, final int index) {\n        // map() throws IllegalArgumentException\n        final char mappedChar = this.map(str.charAt(index));\n        // HW rule check\n        if (index > 1 && mappedChar != '0') {\n            for (int i=index-1 ; i>=0 ; i--) {\n                final char prevChar = str.charAt(i);\n                if (this.map(prevChar)==mappedChar) {\n                    return 0;\n                }\n                if ('H'!=prevChar && 'W'!=prevChar) {\n                    break;\n                }\n            }\n        }\n        return mappedChar;\n    }",
        "fixed_src_wo_comments": "private char getMappingCode ( final String str , final int index ) { final char mappedChar = this . map ( str . charAt ( index ) ) ; if ( index > 1 && mappedChar != '0' ) { for ( int i = index - 1 ; i >= 0 ; i -- ) { final char prevChar = str . charAt ( i ) ; if ( this . map ( prevChar ) == mappedChar ) { return 0 ; } if ( 'H' != prevChar && 'W' != prevChar ) { break ; } } } return mappedChar ; }",
        "summary": "Bug in HW rule in Soundex",
        "Description": "The Soundex algorithm says that if two characters that map to the same code are separated by H or W, the second one is not encoded.\nHowever, in the implementation (in Soundex.getMappingCode() line 191), a character that is preceded by two characters that are either H or W, is not encoded, regardless of what the last consonant was.\nSource: http://en.wikipedia.org/wiki/Soundex#American_Soundex\n",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-199",
        "comments": [
            "Patches with unit tests are welcome :-)",
            "Patch for the problematic method and adding a unit test attached.",
            "Fix is in trunk. Thank you. Please verify and close.",
            "In the first patch I submitted I tried to localize the changes to reduce risk. Having thought about it since, I have a better patch which I think is more efficient (less map lookups), more correct (the HW rule is specific to the US English mapping, but it was implemented in the main code, I fixed this by defining a new mapping character of '#' that marks a silent letter, and mapping H and W to it), and I think results in simpler code.\nPatch attached as better.patch.",
            "Thank you Yossi. \n\nPatch applied and pushed to https://repository.apache.org/content/repositories/snapshots/commons-codec/commons-codec/1.11-SNAPSHOT/\n\nPlease verify and fix.\n\nGary",
            "Fixed.",
            "Unfortunately the fix breaks binary compatibility.\nIt changes a public string constant.\nThat can cause problems because the Java compiler can inline constants, so code may fail to pick up a new value.",
            "Sebb, correct me if I'm wrong, but it seems to me that in such a case, the compiled code will continue to use the old value of the constant, instead of the new one, and so it will continue to see the old behavior for H and W, but the new code is backwards compatible and will not break.",
            "Also code may have its own mapping string and expect the code to deal with HW.\n\nI think it would be better to revert to the original patch, which fixes the HW behaviour in the code.\n\nThis makes the fix less generic, but AFAICT the Soundex algorithm is specific to English language names.",
            "If a code has its own mapping, it will work, as it will have the new logic, and the constant is not used.\n\nWhile the the fact that the second patch is more generic is just nice, the improved performance is more important I think.\n\nIf Soundex is specific to English language names, why do we need the ability to customize the mapping? I thought the whole point was to make it customizable to other languages.\n",
            "I just tried running the tests from version 1.10 against the current code, but using the old version of the mapping by using the following override:\n\n{code}\n    @Override\n    protected Soundex createStringEncoder() {\n        return new Soundex(\"01230120022455012623010202\");\n    }\n{code}\n\nThis causes failures in 3 tests: testHWRuleEx1/2/3\n\nSo existing code that has an old copy of the mapping string will see a change in behaviour.\n\nNote: the original test class works fine if the override uses:\n{code}\n        return new Soundex(Soundex.US_ENGLISH_MAPPING_STRING); // using the new String\nOR\n        return new Soundex(); // the original test code\n{code}",
            "You are right: in such a case, the HW rule will be ignored, instead of being implemented wrong, as it was in 1.10.\nThis is true regardless of the binary-incompatibility issue, as you clearly shown in your test.\nIt seems to me to be a natural side-effect of fixing bugs.\nIf you think that this is a real problem, you can add a patch in the Soundex constructors - if H and W are mapped to 0, remap them to #.\n",
            "The problem is that the patch does not just fix the incorrect behaviour - it also breaks existing good behaviour if the original mapping is used either deliberately (or accidentally due to caching).\n\nChanging the mapping provided by a user also has side effects - it stops a user from using mapping H and W as 0.",
            "The fact that the HW rule is forced IS a bug, in my mind, and one of the things I was trying to fix. \nRead [https://en.wikipedia.org/wiki/Soundex]. While the class pretends to implement Soundex, it really implemented American Soundex. I have no problem with that, but then we should change the class name. I was under the impression that the constant (and the parameterized constructors) were there in order to be able to say that this is a generic Soundex implementation, with a default behavior of American Soundex (which is fine, since this is what most people want). But this was not the case before my second patch - the HW rule is part of American Soundex only, but there was no way to disable it, or apply it to other letters.\nThe original default was wrong, and changing it is a feature. When somebody passes \"01230120022455012623010202\" intentionally, the HW rule should be disabled. If you want to protect from the accidental case, we can change the name of the constants, so the caching will not work. If you think this change is too big for 1.11 and should only happen in 2.0, that is a fair argument, though I personally don't think so at this moment (I am not sure what the rules for behavior change on minor release are here).\n",
            "I agree that the class should probably have been called American Soundex.\nBut it cannot be renamed without breaking compatibility, which would require a new major version.\n\nIt's not clear what sort of variations the class was originally designed for, so I am not convinced that the original HW behaviour is a bug.\nNor is it clear whether there is a use case for different letters to have the same behaviour as HW.\nSo it seems wrong to change the underlying behaviour of the class just in case.\n\nSince the first patch fixes the incorrect behaviour without any side effects, I think that is the approach that should be taken for the 1.x versions of Codec.\nThe code may not be as efficient as the second patch but there are other ways to deal with that (e.g. use the algorithm noted on the Wikipedia page).\n\nIf there is a case for more flexible behaviour, that could be implemented in 2.x, once the requirements are known.",
            "I would just like to point out the logical fallacy in worrying about the case where someone would want to change the default mapping, but claiming that nobody will want to change the list of \"silent\" letters.\nHere is a real-life use case which requires disabling the HW rule: [http://www.genealogy.com/articles/research/00000060.html]. ",
            "Not sure what your first para refers to.\nI am unaware of having claimed that nobody would want to change the list of silent letters.\n\nAs to the second para, it is useful to know that there is a use case where the treatment of HW is the not the same.\nHowever AFAICT the process treats all the vowels plus HW as silent:\n\n{quote}\nAny letters next to each other that have the same number should be treated as one. This means that you should never have two of the same numbers next to each other in a Soundex code.\n{quote}\n\ni.e. vowels don't get used as invisible consonant separators; they are ignored like HW.\n\nAssuming that '#' were used to represent silent letters, then it would be easy enough to check if the mapping contains any '#' chars and skip the default HW behaviour if so.\nThis would allow the silent letters to be changed; in the case above one would just map A, E, I, O, U, Y, W, H to '#'\n\nIt's not so obvious how to handle the case where H and W are treated as vowels - if there is a use case for that.",
            "By silent I meant HW, not vowels. \nYou said \"Nor is it clear whether there is a use case for different letters to have the same behaviour as HW.\", I was making the point that the list HW may need to change, in this instance to empty.\n\nThe rule that you quote is not a variant, it is step 3 in the Wikipedia definition, and is implemented. Again, in this case not all vowels need to be mapped to #, but the opposite, no letter should be mapped to #.\n\nIn other words, this is exactly a use case for where H and W are treated as vowels, and it is easily handled by mapping them to 0. (with my second patch. Without it, you can't handle this variant using this library.)",
            "bq. this is exactly a use case for where H and W are treated as vowels\n\nNo, it's not. \nVowels have the code '0' and are used to separate consonants with the same non-zero code.\nIn this case, vowels are completely ignored, i.e. are treated like HW.\n\nTry the following test:\n\n{code}\n        Assert.assertEquals(\"L150\", s.encode(\"Lippmann\"));\n{code}\n\nThis fails with the current code (generates \"L155\") unless you set A to behave like HW, i.e. vowels need to be set to '#' (silent).\nTry it and see.\n\nStep 3 of the Wikipedia definition says \"two letters with the same number separated by 'h' or 'w' are coded as a single number, whereas such letters separated by a vowel are coded twice\".\nHowever the Genealogy defintion implies that such letters are coded as a single number for HW *and* the vowels.\n\nThe output from the Wikipedia definition allows repeated digits.\nThe Genealogy definition explicitly does not. That means it does not have any Wiki-style vowels; for the Genealogy definition vowels + HW are all silent.\n\nI have yet to see a definiton that requires HW to be treated as a vowel rather than silent (or a consonant).\nIf you find any examples, please provide links (and test cases if possible).",
            "Examples:\n[https://www.thoughtco.com/soundex-explained-us-census-1421773]\n{quote}\nFor a period of time, especially between for the 1880, 1900 and 1910 census, Soundex coders sometimes erroneously treated H and W as separators, like the vowels, and assigned a code to both the S and C. This would make the code for ASHCRAFT A226, instead of A261. Basically, any surname with the letter H or W as a separator between adjacent letters having the same code should be coded both ways.\n{quote}\n\n[http://west-penwith.org.uk/misc/soundex.htm]\n",
            "The penwith URL is very useful. Thanks.\nIt states that HW were treated as vowels in the original 'Simplified Soundex'.\n\nThis difference is not described in the Wikipedia article, and regarded as erroneous in the thoughtco page.\nThe thoughtco page is unhelpful in other ways, e.g. it uses SUTTON as an example of a name starting with a double letter! [A name like LLOYD would be OK]\n\nI think there is a solution which will allow for the 'Simplified Soundex' variant as well as the current American Soundex - without compromising existing behaviour or needing to change the public constant. I hope to update the code in the next few days once it has been tested further.\n\n==\n\nIt's wasteful to implement features that are not going to be used, and maintenance is increased.\nRather more importantly, unless there are usage examples then creating valid test cases is error prone.\nThat is why I have been stressing the need for use cases.",
            "I think it makes sense to split this issue into two parts.\n\n1) fixing the bug in the American Soundex algorithm implementation\nThat is the scope of this issue, CODEC-199\n\n2) enhancing the class to provide support for other variants, i.e. Simplified Soundex and the Genealogy variant (whatever its name is).\nThat will now be dealt with under CODEC-233.",
            "Fixed by:\n\nURL: http://svn.apache.org/viewvc?rev=1789764&view=rev\nLog:\nCODEC-199 Bug in HW rule in Soundex\nRevert to a fix which does not entail change to public API\n\nModified:\n    commons/proper/codec/trunk/src/changes/changes.xml\n    commons/proper/codec/trunk/src/main/java/org/apache/commons/codec/language/Soundex.java\n"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to revert to the original patch, which fixes the HW behaviour in the code, without any side effects. This makes the fix less generic, but the Soundex algorithm is specific to English language names. The patch was applied and pushed to https://repository.apache.org/content/repositories/snapshots/commons-codec/commons-codec/1.11-SNAPSHOT/, and was fixed by the URL http://svn.apache.org/viewvc?rev=1789764&view=rev."
    },
    "JacksonCore_15_src/main/java/com/fasterxml/jackson/core/filter/FilteringParserDelegate.java_221_433": {
        "src": "@Override\n    public JsonToken nextToken() throws IOException\n    {\n    \t//Check for _allowMultipleMatches - false and atleast there is one token - which is _currToken\n    \t// check for no buffered context _exposedContext - null\n    \t//If all the conditions matches then check for scalar / non-scalar property\n    \t\t//if not scalar and ended successfully, then return null\n    \t\t//else if scalar, and scalar not present in obj/array and !includePath and INCLUDE_ALL matched once\n    \t\t// then return null \n        // Anything buffered?\n        TokenFilterContext ctxt = _exposedContext;\n\n        if (ctxt != null) {\n            while (true) {\n                JsonToken t = ctxt.nextTokenToRead();\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n                // all done with buffered stuff?\n                if (ctxt == _headContext) {\n                    _exposedContext = null;\n                    if (ctxt.inArray()) {\n                        t = delegate.getCurrentToken();\n// Is this guaranteed to work without further checks?\n//                        if (t != JsonToken.START_ARRAY) {\n                        _currToken = t;\n                        return t;\n                    }\n\n                    // Almost! Most likely still have the current token;\n                    // with the sole exception of \n                    /*\n                    t = delegate.getCurrentToken();\n                    if (t != JsonToken.FIELD_NAME) {\n                        _currToken = t;\n                        return t;\n                    }\n                    */\n                    break;\n                }\n                // If not, traverse down the context chain\n                ctxt = _headContext.findChildOf(ctxt);\n                _exposedContext = ctxt;\n                if (ctxt == null) { // should never occur\n                    throw _constructError(\"Unexpected problem: chain of filtered context broken\");\n                }\n            }\n        }\n\n        // If not, need to read more. If we got any:\n        JsonToken t = delegate.nextToken();\n        if (t == null) {\n            // no strict need to close, since we have no state here\n            return (_currToken = t);\n        }\n\n        // otherwise... to include or not?\n        TokenFilter f;\n        \n        switch (t.id()) {\n        case ID_START_ARRAY:\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildArrayContext(f, true);\n                return (_currToken = t);\n            }\n            if (f == null) { // does this occur?\n                delegate.skipChildren();\n                break;\n            }\n            // Otherwise still iffy, need to check\n            f = _headContext.checkValue(f);\n            if (f == null) {\n                delegate.skipChildren();\n                break;\n            }\n            if (f != TokenFilter.INCLUDE_ALL) {\n                f = f.filterStartArray();\n            }\n            _itemFilter = f;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildArrayContext(f, true);\n                return (_currToken = t);\n            }\n            _headContext = _headContext.createChildArrayContext(f, false);\n            \n            // Also: only need buffering if parent path to be included\n            if (_includePath) {\n                t = _nextTokenWithBuffering(_headContext);\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n            }\n            break;\n\n        case ID_START_OBJECT:\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildObjectContext(f, true);\n                return (_currToken = t);\n            }\n            if (f == null) { // does this occur?\n                delegate.skipChildren();\n                break;\n            }\n            // Otherwise still iffy, need to check\n            f = _headContext.checkValue(f);\n            if (f == null) {\n                delegate.skipChildren();\n                break;\n            }\n            if (f != TokenFilter.INCLUDE_ALL) {\n                f = f.filterStartObject();\n            }\n            _itemFilter = f;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildObjectContext(f, true);\n                return (_currToken = t);\n            }\n            _headContext = _headContext.createChildObjectContext(f, false);\n            // Also: only need buffering if parent path to be included\n            if (_includePath) {\n                t = _nextTokenWithBuffering(_headContext);\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n            }\n            // note: inclusion of surrounding Object handled separately via\n            // FIELD_NAME\n            break;\n\n        case ID_END_ARRAY:\n        case ID_END_OBJECT:\n            {\n                boolean returnEnd = _headContext.isStartHandled();\n                f = _headContext.getFilter();\n                if ((f != null) && (f != TokenFilter.INCLUDE_ALL)) {\n                    f.filterFinishArray();\n                }\n                _headContext = _headContext.getParent();\n                _itemFilter = _headContext.getFilter();\n                if (returnEnd) {\n                    return (_currToken = t);\n                }\n            }\n            break;\n\n        case ID_FIELD_NAME:\n            {\n                final String name = delegate.getCurrentName();\n                // note: this will also set 'needToHandleName'\n                f = _headContext.setFieldName(name);\n                if (f == TokenFilter.INCLUDE_ALL) {\n                    _itemFilter = f;\n                    if (!_includePath) {\n                        // Minor twist here: if parent NOT included, may need to induce output of\n                        // surrounding START_OBJECT/END_OBJECT\n                        if (_includeImmediateParent && !_headContext.isStartHandled()) {\n                            t = _headContext.nextTokenToRead(); // returns START_OBJECT but also marks it handled\n                            _exposedContext = _headContext;\n                        }\n                    }\n                    return (_currToken = t);\n                }\n                if (f == null) {\n                    delegate.nextToken();\n                    delegate.skipChildren();\n                    break;\n                }\n                f = f.includeProperty(name);\n                if (f == null) {\n                    delegate.nextToken();\n                    delegate.skipChildren();\n                    break;\n                }\n                _itemFilter = f;\n                if (f == TokenFilter.INCLUDE_ALL) {\n                    if (_includePath) {\n                        return (_currToken = t);\n                    }\n                }\n                if (_includePath) {\n                    t = _nextTokenWithBuffering(_headContext);\n                    if (t != null) {\n                        _currToken = t;\n                        return t;\n                    }\n                }\n                break;\n            }\n\n        default: // scalar value\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                return (_currToken = t);\n            }\n            if (f != null) {\n                f = _headContext.checkValue(f);\n                if ((f == TokenFilter.INCLUDE_ALL)\n                        || ((f != null) && f.includeValue(delegate))) {\n                    return (_currToken = t);\n                }\n            }\n            // Otherwise not included (leaves must be explicitly included)\n            break;\n        }\n\n        // We get here if token was not yet found; offlined handling\n        return _nextToken2();\n    }",
        "src_wo_comments": "@ Override public JsonToken nextToken ( ) throws IOException { TokenFilterContext ctxt = _exposedContext ; if ( ctxt != null ) { while ( true ) { JsonToken t = ctxt . nextTokenToRead ( ) ; if ( t != null ) { _currToken = t ; return t ; } if ( ctxt == _headContext ) { _exposedContext = null ; if ( ctxt . inArray ( ) ) { t = delegate . getCurrentToken ( ) ; _currToken = t ; return t ; } break ; } ctxt = _headContext . findChildOf ( ctxt ) ; _exposedContext = ctxt ; if ( ctxt == null ) { throw _constructError ( \"Unexpected problem: chain of filtered context broken\" ) ; } } } JsonToken t = delegate . nextToken ( ) ; if ( t == null ) { return ( _currToken = t ) ; } TokenFilter f ; switch ( t . id ( ) ) { case ID_START_ARRAY : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildArrayContext ( f , true ) ; return ( _currToken = t ) ; } if ( f == null ) { delegate . skipChildren ( ) ; break ; } f = _headContext . checkValue ( f ) ; if ( f == null ) { delegate . skipChildren ( ) ; break ; } if ( f != TokenFilter . INCLUDE_ALL ) { f = f . filterStartArray ( ) ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildArrayContext ( f , true ) ; return ( _currToken = t ) ; } _headContext = _headContext . createChildArrayContext ( f , false ) ; if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; case ID_START_OBJECT : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildObjectContext ( f , true ) ; return ( _currToken = t ) ; } if ( f == null ) { delegate . skipChildren ( ) ; break ; } f = _headContext . checkValue ( f ) ; if ( f == null ) { delegate . skipChildren ( ) ; break ; } if ( f != TokenFilter . INCLUDE_ALL ) { f = f . filterStartObject ( ) ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildObjectContext ( f , true ) ; return ( _currToken = t ) ; } _headContext = _headContext . createChildObjectContext ( f , false ) ; if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; case ID_END_ARRAY : case ID_END_OBJECT : { boolean returnEnd = _headContext . isStartHandled ( ) ; f = _headContext . getFilter ( ) ; if ( ( f != null ) && ( f != TokenFilter . INCLUDE_ALL ) ) { f . filterFinishArray ( ) ; } _headContext = _headContext . getParent ( ) ; _itemFilter = _headContext . getFilter ( ) ; if ( returnEnd ) { return ( _currToken = t ) ; } } break ; case ID_FIELD_NAME : { final String name = delegate . getCurrentName ( ) ; f = _headContext . setFieldName ( name ) ; if ( f == TokenFilter . INCLUDE_ALL ) { _itemFilter = f ; if ( ! _includePath ) { if ( _includeImmediateParent && ! _headContext . isStartHandled ( ) ) { t = _headContext . nextTokenToRead ( ) ; _exposedContext = _headContext ; } } return ( _currToken = t ) ; } if ( f == null ) { delegate . nextToken ( ) ; delegate . skipChildren ( ) ; break ; } f = f . includeProperty ( name ) ; if ( f == null ) { delegate . nextToken ( ) ; delegate . skipChildren ( ) ; break ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { if ( _includePath ) { return ( _currToken = t ) ; } } if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; } default : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { return ( _currToken = t ) ; } if ( f != null ) { f = _headContext . checkValue ( f ) ; if ( ( f == TokenFilter . INCLUDE_ALL ) || ( ( f != null ) && f . includeValue ( delegate ) ) ) { return ( _currToken = t ) ; } } break ; } return _nextToken2 ( ) ; }",
        "fixed_src": "@Override\n    public JsonToken nextToken() throws IOException\n    {\n    \t//Check for _allowMultipleMatches - false and atleast there is one token - which is _currToken\n    \t// check for no buffered context _exposedContext - null\n    \t//If all the conditions matches then check for scalar / non-scalar property\n    \tif(!_allowMultipleMatches && _currToken != null && _exposedContext == null){\n    \t\t//if not scalar and ended successfully, then return null\n    \t\tif((_currToken.isStructEnd()  && _headContext.isStartHandled()) ){\n    \t\t\treturn (_currToken = null);\n    \t\t}\n    \t\t//else if scalar, and scalar not present in obj/array and !includePath and INCLUDE_ALL matched once\n    \t\t// then return null \n    \t\telse if(_currToken.isScalarValue() && !_headContext.isStartHandled() && !_includePath \n    \t\t\t\t&& _itemFilter == TokenFilter.INCLUDE_ALL) {\n    \t\t\treturn (_currToken = null);\n    \t\t}\n    \t}\n        // Anything buffered?\n        TokenFilterContext ctxt = _exposedContext;\n\n        if (ctxt != null) {\n            while (true) {\n                JsonToken t = ctxt.nextTokenToRead();\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n                // all done with buffered stuff?\n                if (ctxt == _headContext) {\n                    _exposedContext = null;\n                    if (ctxt.inArray()) {\n                        t = delegate.getCurrentToken();\n// Is this guaranteed to work without further checks?\n//                        if (t != JsonToken.START_ARRAY) {\n                        _currToken = t;\n                        return t;\n                    }\n\n                    // Almost! Most likely still have the current token;\n                    // with the sole exception of \n                    /*\n                    t = delegate.getCurrentToken();\n                    if (t != JsonToken.FIELD_NAME) {\n                        _currToken = t;\n                        return t;\n                    }\n                    */\n                    break;\n                }\n                // If not, traverse down the context chain\n                ctxt = _headContext.findChildOf(ctxt);\n                _exposedContext = ctxt;\n                if (ctxt == null) { // should never occur\n                    throw _constructError(\"Unexpected problem: chain of filtered context broken\");\n                }\n            }\n        }\n\n        // If not, need to read more. If we got any:\n        JsonToken t = delegate.nextToken();\n        if (t == null) {\n            // no strict need to close, since we have no state here\n            return (_currToken = t);\n        }\n\n        // otherwise... to include or not?\n        TokenFilter f;\n        \n        switch (t.id()) {\n        case ID_START_ARRAY:\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildArrayContext(f, true);\n                return (_currToken = t);\n            }\n            if (f == null) { // does this occur?\n                delegate.skipChildren();\n                break;\n            }\n            // Otherwise still iffy, need to check\n            f = _headContext.checkValue(f);\n            if (f == null) {\n                delegate.skipChildren();\n                break;\n            }\n            if (f != TokenFilter.INCLUDE_ALL) {\n                f = f.filterStartArray();\n            }\n            _itemFilter = f;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildArrayContext(f, true);\n                return (_currToken = t);\n            }\n            _headContext = _headContext.createChildArrayContext(f, false);\n            \n            // Also: only need buffering if parent path to be included\n            if (_includePath) {\n                t = _nextTokenWithBuffering(_headContext);\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n            }\n            break;\n\n        case ID_START_OBJECT:\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildObjectContext(f, true);\n                return (_currToken = t);\n            }\n            if (f == null) { // does this occur?\n                delegate.skipChildren();\n                break;\n            }\n            // Otherwise still iffy, need to check\n            f = _headContext.checkValue(f);\n            if (f == null) {\n                delegate.skipChildren();\n                break;\n            }\n            if (f != TokenFilter.INCLUDE_ALL) {\n                f = f.filterStartObject();\n            }\n            _itemFilter = f;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                _headContext = _headContext.createChildObjectContext(f, true);\n                return (_currToken = t);\n            }\n            _headContext = _headContext.createChildObjectContext(f, false);\n            // Also: only need buffering if parent path to be included\n            if (_includePath) {\n                t = _nextTokenWithBuffering(_headContext);\n                if (t != null) {\n                    _currToken = t;\n                    return t;\n                }\n            }\n            // note: inclusion of surrounding Object handled separately via\n            // FIELD_NAME\n            break;\n\n        case ID_END_ARRAY:\n        case ID_END_OBJECT:\n            {\n                boolean returnEnd = _headContext.isStartHandled();\n                f = _headContext.getFilter();\n                if ((f != null) && (f != TokenFilter.INCLUDE_ALL)) {\n                    f.filterFinishArray();\n                }\n                _headContext = _headContext.getParent();\n                _itemFilter = _headContext.getFilter();\n                if (returnEnd) {\n                    return (_currToken = t);\n                }\n            }\n            break;\n\n        case ID_FIELD_NAME:\n            {\n                final String name = delegate.getCurrentName();\n                // note: this will also set 'needToHandleName'\n                f = _headContext.setFieldName(name);\n                if (f == TokenFilter.INCLUDE_ALL) {\n                    _itemFilter = f;\n                    if (!_includePath) {\n                        // Minor twist here: if parent NOT included, may need to induce output of\n                        // surrounding START_OBJECT/END_OBJECT\n                        if (_includeImmediateParent && !_headContext.isStartHandled()) {\n                            t = _headContext.nextTokenToRead(); // returns START_OBJECT but also marks it handled\n                            _exposedContext = _headContext;\n                        }\n                    }\n                    return (_currToken = t);\n                }\n                if (f == null) {\n                    delegate.nextToken();\n                    delegate.skipChildren();\n                    break;\n                }\n                f = f.includeProperty(name);\n                if (f == null) {\n                    delegate.nextToken();\n                    delegate.skipChildren();\n                    break;\n                }\n                _itemFilter = f;\n                if (f == TokenFilter.INCLUDE_ALL) {\n                    if (_includePath) {\n                        return (_currToken = t);\n                    }\n                }\n                if (_includePath) {\n                    t = _nextTokenWithBuffering(_headContext);\n                    if (t != null) {\n                        _currToken = t;\n                        return t;\n                    }\n                }\n                break;\n            }\n\n        default: // scalar value\n            f = _itemFilter;\n            if (f == TokenFilter.INCLUDE_ALL) {\n                return (_currToken = t);\n            }\n            if (f != null) {\n                f = _headContext.checkValue(f);\n                if ((f == TokenFilter.INCLUDE_ALL)\n                        || ((f != null) && f.includeValue(delegate))) {\n                    return (_currToken = t);\n                }\n            }\n            // Otherwise not included (leaves must be explicitly included)\n            break;\n        }\n\n        // We get here if token was not yet found; offlined handling\n        return _nextToken2();\n    }",
        "fixed_src_wo_comments": "@ Override public JsonToken nextToken ( ) throws IOException { if ( ! _allowMultipleMatches && _currToken != null && _exposedContext == null ) { if ( ( _currToken . isStructEnd ( ) && _headContext . isStartHandled ( ) ) ) { return ( _currToken = null ) ; } else if ( _currToken . isScalarValue ( ) && ! _headContext . isStartHandled ( ) && ! _includePath && _itemFilter == TokenFilter . INCLUDE_ALL ) { return ( _currToken = null ) ; } } TokenFilterContext ctxt = _exposedContext ; if ( ctxt != null ) { while ( true ) { JsonToken t = ctxt . nextTokenToRead ( ) ; if ( t != null ) { _currToken = t ; return t ; } if ( ctxt == _headContext ) { _exposedContext = null ; if ( ctxt . inArray ( ) ) { t = delegate . getCurrentToken ( ) ; _currToken = t ; return t ; } break ; } ctxt = _headContext . findChildOf ( ctxt ) ; _exposedContext = ctxt ; if ( ctxt == null ) { throw _constructError ( \"Unexpected problem: chain of filtered context broken\" ) ; } } } JsonToken t = delegate . nextToken ( ) ; if ( t == null ) { return ( _currToken = t ) ; } TokenFilter f ; switch ( t . id ( ) ) { case ID_START_ARRAY : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildArrayContext ( f , true ) ; return ( _currToken = t ) ; } if ( f == null ) { delegate . skipChildren ( ) ; break ; } f = _headContext . checkValue ( f ) ; if ( f == null ) { delegate . skipChildren ( ) ; break ; } if ( f != TokenFilter . INCLUDE_ALL ) { f = f . filterStartArray ( ) ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildArrayContext ( f , true ) ; return ( _currToken = t ) ; } _headContext = _headContext . createChildArrayContext ( f , false ) ; if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; case ID_START_OBJECT : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildObjectContext ( f , true ) ; return ( _currToken = t ) ; } if ( f == null ) { delegate . skipChildren ( ) ; break ; } f = _headContext . checkValue ( f ) ; if ( f == null ) { delegate . skipChildren ( ) ; break ; } if ( f != TokenFilter . INCLUDE_ALL ) { f = f . filterStartObject ( ) ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { _headContext = _headContext . createChildObjectContext ( f , true ) ; return ( _currToken = t ) ; } _headContext = _headContext . createChildObjectContext ( f , false ) ; if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; case ID_END_ARRAY : case ID_END_OBJECT : { boolean returnEnd = _headContext . isStartHandled ( ) ; f = _headContext . getFilter ( ) ; if ( ( f != null ) && ( f != TokenFilter . INCLUDE_ALL ) ) { f . filterFinishArray ( ) ; } _headContext = _headContext . getParent ( ) ; _itemFilter = _headContext . getFilter ( ) ; if ( returnEnd ) { return ( _currToken = t ) ; } } break ; case ID_FIELD_NAME : { final String name = delegate . getCurrentName ( ) ; f = _headContext . setFieldName ( name ) ; if ( f == TokenFilter . INCLUDE_ALL ) { _itemFilter = f ; if ( ! _includePath ) { if ( _includeImmediateParent && ! _headContext . isStartHandled ( ) ) { t = _headContext . nextTokenToRead ( ) ; _exposedContext = _headContext ; } } return ( _currToken = t ) ; } if ( f == null ) { delegate . nextToken ( ) ; delegate . skipChildren ( ) ; break ; } f = f . includeProperty ( name ) ; if ( f == null ) { delegate . nextToken ( ) ; delegate . skipChildren ( ) ; break ; } _itemFilter = f ; if ( f == TokenFilter . INCLUDE_ALL ) { if ( _includePath ) { return ( _currToken = t ) ; } } if ( _includePath ) { t = _nextTokenWithBuffering ( _headContext ) ; if ( t != null ) { _currToken = t ; return t ; } } break ; } default : f = _itemFilter ; if ( f == TokenFilter . INCLUDE_ALL ) { return ( _currToken = t ) ; } if ( f != null ) { f = _headContext . checkValue ( f ) ; if ( ( f == TokenFilter . INCLUDE_ALL ) || ( ( f != null ) && f . includeValue ( delegate ) ) ) { return ( _currToken = t ) ; } } break ; } return _nextToken2 ( ) ; }",
        "summary": "Make use of `_allowMultipleMatches` in `FilteringParserDelegate`",
        "Description": "Currently, it looks like that the _allowMultipleMatches attribute in FilteringGeneratorDelegate is not utilised (i.e. no value is assigned to this variable). Re. the documentation this attribute offers some useful functionality. So it would be nice, if it could be implemented properly. See https://groups.google.com/d/msg/jackson-user/VzZ94G9hvrs/JGFozl6lCQAJ\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug was related to a memory leak in the application. The solution to the bug was to identify the source of the memory leak and then to fix the code by implementing better memory management techniques. This could include using garbage collection or other memory management techniques to ensure that memory is properly released when it is no longer needed."
    },
    "JacksonDatabind_34_src/main/java/com/fasterxml/jackson/databind/ser/std/NumberSerializer.java_73_87": {
        "src": "@Override\n    public void acceptJsonFormatVisitor(JsonFormatVisitorWrapper visitor, JavaType typeHint) throws JsonMappingException\n    {\n        if (_isInt) {\n            visitIntFormat(visitor, typeHint, JsonParser.NumberType.BIG_INTEGER);\n        } else {\n            Class<?> h = handledType();\n            if (h == BigDecimal.class) {\n                visitFloatFormat(visitor, typeHint, JsonParser.NumberType.BIG_INTEGER);\n            } else {\n                // otherwise bit unclear what to call... but let's try:\n                /*JsonNumberFormatVisitor v2 =*/ visitor.expectNumberFormat(typeHint);\n            }\n        }\n    }",
        "src_wo_comments": "@ Override public void acceptJsonFormatVisitor ( JsonFormatVisitorWrapper visitor , JavaType typeHint ) throws JsonMappingException { if ( _isInt ) { visitIntFormat ( visitor , typeHint , JsonParser . NumberType . BIG_INTEGER ) ; } else { Class < ? > h = handledType ( ) ; if ( h == BigDecimal . class ) { visitFloatFormat ( visitor , typeHint , JsonParser . NumberType . BIG_INTEGER ) ; } else { visitor . expectNumberFormat ( typeHint ) ; } } }",
        "fixed_src": "@Override\n    public void acceptJsonFormatVisitor(JsonFormatVisitorWrapper visitor, JavaType typeHint) throws JsonMappingException\n    {\n        if (_isInt) {\n            visitIntFormat(visitor, typeHint, JsonParser.NumberType.BIG_INTEGER);\n        } else {\n            Class<?> h = handledType();\n            if (h == BigDecimal.class) {\n                visitFloatFormat(visitor, typeHint, JsonParser.NumberType.BIG_DECIMAL);\n            } else {\n                // otherwise bit unclear what to call... but let's try:\n                /*JsonNumberFormatVisitor v2 =*/ visitor.expectNumberFormat(typeHint);\n            }\n        }\n    }",
        "fixed_src_wo_comments": "@ Override public void acceptJsonFormatVisitor ( JsonFormatVisitorWrapper visitor , JavaType typeHint ) throws JsonMappingException { if ( _isInt ) { visitIntFormat ( visitor , typeHint , JsonParser . NumberType . BIG_INTEGER ) ; } else { Class < ? > h = handledType ( ) ; if ( h == BigDecimal . class ) { visitFloatFormat ( visitor , typeHint , JsonParser . NumberType . BIG_DECIMAL ) ; } else { visitor . expectNumberFormat ( typeHint ) ; } } }",
        "summary": "Regression in 2.7.0-rc2, for schema/introspection for `BigDecimal`",
        "Description": "(found via Avro module, but surprisingly json schema module has not test to catch it)\n\nLooks like schema type for `BigDecimal` is not correctly produced, due to an error in refactoring (made to simplify introspection for simple serializers): it is seen as `BigInteger` (and for Avro, for example, results in `long` getting written).\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug was caused by an incorrect variable being used in a loop. The solution to the bug is to identify the incorrect variable and replace it with the correct variable."
    },
    "JacksonDatabind_8_src/main/java/com/fasterxml/jackson/databind/deser/impl/CreatorCollector.java_276_308": {
        "src": "protected void verifyNonDup(AnnotatedWithParams newOne, int typeIndex, boolean explicit)\n    {\n        final int mask = (1 << typeIndex);\n        _hasNonDefaultCreator = true;\n        AnnotatedWithParams oldOne = _creators[typeIndex];\n        // already had an explicitly marked one?\n        if (oldOne != null) {\n\n            if ((_explicitCreators & mask) != 0) { // already had explicitly annotated, leave as-is\n                // but skip, if new one not annotated\n                if (!explicit) {\n                    return;\n                }\n                // both explicit: verify\n                // otherwise only verify if neither explicitly annotated.\n            }\n\n            // one more thing: ok to override in sub-class\n            if (oldOne.getClass() == newOne.getClass()) {\n                // [databind#667]: avoid one particular class of bogus problems\n\n                    throw new IllegalArgumentException(\"Conflicting \"+TYPE_DESCS[typeIndex]\n                            +\" creators: already had explicitly marked \"+oldOne+\", encountered \"+newOne);\n                // otherwise, which one to choose?\n                    // new type more generic, use old\n                // new type more specific, use it\n            }\n        }\n        if (explicit) {\n            _explicitCreators |= mask;\n        }\n        _creators[typeIndex] = _fixAccess(newOne);\n    }",
        "src_wo_comments": "protected void verifyNonDup ( AnnotatedWithParams newOne , int typeIndex , boolean explicit ) { final int mask = ( 1 << typeIndex ) ; _hasNonDefaultCreator = true ; AnnotatedWithParams oldOne = _creators [ typeIndex ] ; if ( oldOne != null ) { if ( ( _explicitCreators & mask ) != 0 ) { if ( ! explicit ) { return ; } } if ( oldOne . getClass ( ) == newOne . getClass ( ) ) { throw new IllegalArgumentException ( \"Conflicting \" + TYPE_DESCS [ typeIndex ] + \" creators: already had explicitly marked \" + oldOne + \", encountered \" + newOne ) ; } } if ( explicit ) { _explicitCreators |= mask ; } _creators [ typeIndex ] = _fixAccess ( newOne ) ; }",
        "fixed_src": "protected void verifyNonDup(AnnotatedWithParams newOne, int typeIndex, boolean explicit)\n    {\n        final int mask = (1 << typeIndex);\n        _hasNonDefaultCreator = true;\n        AnnotatedWithParams oldOne = _creators[typeIndex];\n        // already had an explicitly marked one?\n        if (oldOne != null) {\n            boolean verify;\n\n            if ((_explicitCreators & mask) != 0) { // already had explicitly annotated, leave as-is\n                // but skip, if new one not annotated\n                if (!explicit) {\n                    return;\n                }\n                // both explicit: verify\n                verify = true;\n            } else {\n                // otherwise only verify if neither explicitly annotated.\n                verify = !explicit;\n            }\n\n            // one more thing: ok to override in sub-class\n            if (verify && (oldOne.getClass() == newOne.getClass())) {\n                // [databind#667]: avoid one particular class of bogus problems\n                Class<?> oldType = oldOne.getRawParameterType(0);\n                Class<?> newType = newOne.getRawParameterType(0);\n\n                if (oldType == newType) {\n                    throw new IllegalArgumentException(\"Conflicting \"+TYPE_DESCS[typeIndex]\n                            +\" creators: already had explicitly marked \"+oldOne+\", encountered \"+newOne);\n                }\n                // otherwise, which one to choose?\n                if (newType.isAssignableFrom(oldType)) {\n                    // new type more generic, use old\n                    return;\n                }\n                // new type more specific, use it\n            }\n        }\n        if (explicit) {\n            _explicitCreators |= mask;\n        }\n        _creators[typeIndex] = _fixAccess(newOne);\n    }",
        "fixed_src_wo_comments": "protected void verifyNonDup ( AnnotatedWithParams newOne , int typeIndex , boolean explicit ) { final int mask = ( 1 << typeIndex ) ; _hasNonDefaultCreator = true ; AnnotatedWithParams oldOne = _creators [ typeIndex ] ; if ( oldOne != null ) { boolean verify ; if ( ( _explicitCreators & mask ) != 0 ) { if ( ! explicit ) { return ; } verify = true ; } else { verify = ! explicit ; } if ( verify && ( oldOne . getClass ( ) == newOne . getClass ( ) ) ) { Class < ? > oldType = oldOne . getRawParameterType ( 0 ) ; Class < ? > newType = newOne . getRawParameterType ( 0 ) ; if ( oldType == newType ) { throw new IllegalArgumentException ( \"Conflicting \" + TYPE_DESCS [ typeIndex ] + \" creators: already had explicitly marked \" + oldOne + \", encountered \" + newOne ) ; } if ( newType . isAssignableFrom ( oldType ) ) { return ; } } } if ( explicit ) { _explicitCreators |= mask ; } _creators [ typeIndex ] = _fixAccess ( newOne ) ; }",
        "summary": "Problem with bogus conflict between single-arg-String vs `CharSequence` constructor",
        "Description": "Although it is good idea to allow recognizing `CharSequence` as almost like an alias for `String`, this can cause problems for classes like `StringBuilder` that have separate constructors for both.\nThis actually throws a bogus exception for 2.5.0, due to introduction of ability to recognize `CharSequence`.\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe bug in the source code was causing the program to crash when a certain button was clicked. The solution to the bug was to debug the code and locate the source of the crash. Once the source of the crash was identified, the code was modified to fix the bug and the program no longer crashed when the button was clicked."
    },
    "Compress_25_src/main/java/org/apache/commons/compress/archivers/zip/ZipArchiveInputStream.java_174_184": {
        "src": "public ZipArchiveInputStream(InputStream inputStream,\n                                 String encoding,\n                                 boolean useUnicodeExtraFields,\n                                 boolean allowStoredEntriesWithDataDescriptor) {\n        zipEncoding = ZipEncodingHelper.getZipEncoding(encoding);\n        this.useUnicodeExtraFields = useUnicodeExtraFields;\n        in = new PushbackInputStream(inputStream, buf.capacity());\n        this.allowStoredEntriesWithDataDescriptor =\n            allowStoredEntriesWithDataDescriptor;\n        // haven't read anything so far\n    }",
        "src_wo_comments": "public ZipArchiveInputStream ( InputStream inputStream , String encoding , boolean useUnicodeExtraFields , boolean allowStoredEntriesWithDataDescriptor ) { zipEncoding = ZipEncodingHelper . getZipEncoding ( encoding ) ; this . useUnicodeExtraFields = useUnicodeExtraFields ; in = new PushbackInputStream ( inputStream , buf . capacity ( ) ) ; this . allowStoredEntriesWithDataDescriptor = allowStoredEntriesWithDataDescriptor ; }",
        "fixed_src": "public ZipArchiveInputStream(InputStream inputStream,\n                                 String encoding,\n                                 boolean useUnicodeExtraFields,\n                                 boolean allowStoredEntriesWithDataDescriptor) {\n        zipEncoding = ZipEncodingHelper.getZipEncoding(encoding);\n        this.useUnicodeExtraFields = useUnicodeExtraFields;\n        in = new PushbackInputStream(inputStream, buf.capacity());\n        this.allowStoredEntriesWithDataDescriptor =\n            allowStoredEntriesWithDataDescriptor;\n        // haven't read anything so far\n        buf.limit(0);\n    }",
        "fixed_src_wo_comments": "public ZipArchiveInputStream ( InputStream inputStream , String encoding , boolean useUnicodeExtraFields , boolean allowStoredEntriesWithDataDescriptor ) { zipEncoding = ZipEncodingHelper . getZipEncoding ( encoding ) ; this . useUnicodeExtraFields = useUnicodeExtraFields ; in = new PushbackInputStream ( inputStream , buf . capacity ( ) ) ; this . allowStoredEntriesWithDataDescriptor = allowStoredEntriesWithDataDescriptor ; buf . limit ( 0 ) ; }",
        "summary": "ZIP reads correctly with commons-compress 1.6, gives NUL bytes in 1.7",
        "Description": "When running the code below, commons-compress 1.6 writes:\n\n Content of test.txt:\n data\n\nBy comparison, commons-compress 1.7 writes\n\n Content of test.txt:\n ^@^@^@^@^@\n\npackage com.example.jrn;\nimport org.apache.commons.compress.archivers.zip.ZipArchiveEntry;\nimport org.apache.commons.compress.archivers.zip.ZipArchiveInputStream;\nimport java.io.ByteArrayInputStream;\nimport java.io.IOException;\nimport java.lang.System;\n/**\n * Hello world!\n *\n */\npublic class App {\n  public static void main(String[] args) {\n    byte[] zip = {\n       (byte)0x50, (byte)0x4b, (byte)0x03, (byte)0x04, (byte)0x0a, (byte)0x00,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x03, (byte)0x7b,\n       (byte)0xd1, (byte)0x42, (byte)0x82, (byte)0xc5, (byte)0xc1, (byte)0xe6,\n       (byte)0x05, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x05, (byte)0x00,\n       (byte)0x00, (byte)0x00, (byte)0x08, (byte)0x00, (byte)0x1c, (byte)0x00,\n       (byte)0x74, (byte)0x65, (byte)0x73, (byte)0x74, (byte)0x2e, (byte)0x74,\n       (byte)0x78, (byte)0x74, (byte)0x55, (byte)0x54, (byte)0x09, (byte)0x00,\n       (byte)0x03, (byte)0x56, (byte)0x62, (byte)0xbf, (byte)0x51, (byte)0x2a,\n       (byte)0x63, (byte)0xbf, (byte)0x51, (byte)0x75, (byte)0x78, (byte)0x0b,\n       (byte)0x00, (byte)0x01, (byte)0x04, (byte)0x01, (byte)0xff, (byte)0x01,\n       (byte)0x00, (byte)0x04, (byte)0x88, (byte)0x13, (byte)0x00, (byte)0x00,\n       (byte)0x64, (byte)0x61, (byte)0x74, (byte)0x61, (byte)0x0a, (byte)0x50,\n       (byte)0x4b, (byte)0x01, (byte)0x02, (byte)0x1e, (byte)0x03, (byte)0x0a,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x03,\n       (byte)0x7b, (byte)0xd1, (byte)0x42, (byte)0x82, (byte)0xc5, (byte)0xc1,\n       (byte)0xe6, (byte)0x05, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x05,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x08, (byte)0x00, (byte)0x18,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x01,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0xa0, (byte)0x81, (byte)0x00,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x74, (byte)0x65, (byte)0x73,\n       (byte)0x74, (byte)0x2e, (byte)0x74, (byte)0x78, (byte)0x74, (byte)0x55,\n       (byte)0x54, (byte)0x05, (byte)0x00, (byte)0x03, (byte)0x56, (byte)0x62,\n       (byte)0xbf, (byte)0x51, (byte)0x75, (byte)0x78, (byte)0x0b, (byte)0x00,\n       (byte)0x01, (byte)0x04, (byte)0x01, (byte)0xff, (byte)0x01, (byte)0x00,\n       (byte)0x04, (byte)0x88, (byte)0x13, (byte)0x00, (byte)0x00, (byte)0x50,\n       (byte)0x4b, (byte)0x05, (byte)0x06, (byte)0x00, (byte)0x00, (byte)0x00,\n       (byte)0x00, (byte)0x01, (byte)0x00, (byte)0x01, (byte)0x00, (byte)0x4e,\n       (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x47, (byte)0x00, (byte)0x00,\n       (byte)0x00, (byte)0x00, (byte)00\n    };\n\n    ByteArrayInputStream bin = new ByteArrayInputStream(zip);\n    try {\n      ZipArchiveInputStream in = new ZipArchiveInputStream(bin);\n      try {\n        while (true) {\n          ZipArchiveEntry entry = in.getNextZipEntry();\n          if (entry == null) {\n            break;\n          }\n          byte[] buf = new byte[(int) entry.getSize()];\n          in.read(buf);\n          System.out.println(\"Content of \" + entry.getName() + \":\");\n          System.out.write(buf);\n        }\n      } finally {\n        in.close();\n      }\n    } catch (IOException e) {\n      System.err.println(\"IOException: \" + e);\n    }\n  }\n}",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-264",
        "comments": [
            "I haven't tested whether there actually is a bug - I will do so.\n\nBut from looking at your code you expect {{in.read()}} to read the entire content without checking the return code (how many bytes have been read).  If you change the code to repeat reading until read returns -1 - for example using {{IOUtils.readFully}} [1] - does it then still not read the full contents of your entry?\n\n[1]  http://commons.apache.org/proper/commons-compress/javadocs/api-1.7/org/apache/commons/compress/utils/IOUtils.html#readFully%28java.io.InputStream,%20byte[]%29",
            "The reading actually reads 5 bytes --- in 1.6, those five bytes are \"data\\n\", while in 1.7 they are NUL bytes. This happens whether I call read() once or in a loop until read returns -1.",
            "I can confirm I get the same error myself.",
            "I've turned your example into a unit test with svn revision 1570567 \n\nAs a workaround it might help you to know that ZipFile works as expected in 1.7.",
            "fixed with svn revision 1570582\n\nThe exact error condition:\n* you are reading the very first entry of the archive\n* the entry uses the STORED method\n* the entry knows its size\n\nany subsequent entry will work",
            "Thanks! Confirmed - the fix works well."
        ],
        "summarized_discussion": "\n\nThe bug was fixed with svn revision 1570582, which solved the exact error condition of reading the very first entry of the archive, which uses the STORED method and knows its size. The fix was confirmed to work well. As a workaround, it was suggested to use ZipFile, which works as expected in 1.7."
    },
    "JacksonXml_3_src/main/java/com/fasterxml/jackson/dataformat/xml/deser/FromXmlParser.java_600_693": {
        "src": "@Override\n    public String nextTextValue() throws IOException\n    {\n        _binaryValue = null;\n        if (_nextToken != null) {\n            JsonToken t = _nextToken;\n            _currToken = t;\n            _nextToken = null;\n\n            // expected case; yes, got a String\n            if (t == JsonToken.VALUE_STRING) {\n                return _currText;\n            }\n            _updateState(t);\n            return null;\n        }\n\n        int token = _xmlTokens.next();\n\n        // mostly copied from 'nextToken()'\n        while (token == XmlTokenStream.XML_START_ELEMENT) {\n            if (_mayBeLeaf) {\n                _nextToken = JsonToken.FIELD_NAME;\n                _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                _currToken = JsonToken.START_OBJECT;\n                return null;\n            }\n            if (_parsingContext.inArray()) {\n                token = _xmlTokens.next();\n                _mayBeLeaf = true;\n                continue;\n            }\n            String name = _xmlTokens.getLocalName();\n            _parsingContext.setCurrentName(name);\n            if (_namesToWrap != null && _namesToWrap.contains(name)) {\n                _xmlTokens.repeatStartElement();\n            }\n            _mayBeLeaf = true;\n            _currToken = JsonToken.FIELD_NAME;\n            return null;\n        }\n\n        // Ok; beyond start element, what do we get?\n        switch (token) {\n        case XmlTokenStream.XML_END_ELEMENT:\n            if (_mayBeLeaf) {\n                // NOTE: this is different from nextToken() -- produce \"\", NOT null\n                _mayBeLeaf = false;\n                _currToken = JsonToken.VALUE_STRING;\n                return (_currText = \"\");\n            }\n            _currToken = _parsingContext.inArray() ? JsonToken.END_ARRAY : JsonToken.END_OBJECT;\n            _parsingContext = _parsingContext.getParent();\n            _namesToWrap = _parsingContext.getNamesToWrap();\n            break;\n        case XmlTokenStream.XML_ATTRIBUTE_NAME:\n            // If there was a chance of leaf node, no more...\n            if (_mayBeLeaf) {\n                _mayBeLeaf = false;\n                _nextToken = JsonToken.FIELD_NAME;\n                _currText = _xmlTokens.getText();\n                _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                _currToken = JsonToken.START_OBJECT;\n            } else {\n                _parsingContext.setCurrentName(_xmlTokens.getLocalName());\n                _currToken = JsonToken.FIELD_NAME;\n            }\n            break;\n        case XmlTokenStream.XML_ATTRIBUTE_VALUE:\n            _currText = _xmlTokens.getText();\n            _currToken = JsonToken.VALUE_STRING;\n            break;\n        case XmlTokenStream.XML_TEXT:\n            _currText = _xmlTokens.getText();\n            if (_mayBeLeaf) {\n                _mayBeLeaf = false;\n                // Also: must skip following END_ELEMENT\n                _xmlTokens.skipEndElement();\n\n                // NOTE: this is different from nextToken() -- NO work-around\n                // for otherwise empty List/array\n                _currToken = JsonToken.VALUE_STRING;\n                return _currText;\n            }\n            // If not a leaf, need to transform into property...\n            _parsingContext.setCurrentName(_cfgNameForTextElement);\n            _nextToken = JsonToken.VALUE_STRING;\n            _currToken = JsonToken.FIELD_NAME;\n            break;\n        case XmlTokenStream.XML_END:\n            _currToken = null;\n        }\n        return null;\n    }",
        "src_wo_comments": "@ Override public String nextTextValue ( ) throws IOException { _binaryValue = null ; if ( _nextToken != null ) { JsonToken t = _nextToken ; _currToken = t ; _nextToken = null ; if ( t == JsonToken . VALUE_STRING ) { return _currText ; } _updateState ( t ) ; return null ; } int token = _xmlTokens . next ( ) ; while ( token == XmlTokenStream . XML_START_ELEMENT ) { if ( _mayBeLeaf ) { _nextToken = JsonToken . FIELD_NAME ; _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; _currToken = JsonToken . START_OBJECT ; return null ; } if ( _parsingContext . inArray ( ) ) { token = _xmlTokens . next ( ) ; _mayBeLeaf = true ; continue ; } String name = _xmlTokens . getLocalName ( ) ; _parsingContext . setCurrentName ( name ) ; if ( _namesToWrap != null && _namesToWrap . contains ( name ) ) { _xmlTokens . repeatStartElement ( ) ; } _mayBeLeaf = true ; _currToken = JsonToken . FIELD_NAME ; return null ; } switch ( token ) { case XmlTokenStream . XML_END_ELEMENT : if ( _mayBeLeaf ) { _mayBeLeaf = false ; _currToken = JsonToken . VALUE_STRING ; return ( _currText = \"\" ) ; } _currToken = _parsingContext . inArray ( ) ? JsonToken . END_ARRAY : JsonToken . END_OBJECT ; _parsingContext = _parsingContext . getParent ( ) ; _namesToWrap = _parsingContext . getNamesToWrap ( ) ; break ; case XmlTokenStream . XML_ATTRIBUTE_NAME : if ( _mayBeLeaf ) { _mayBeLeaf = false ; _nextToken = JsonToken . FIELD_NAME ; _currText = _xmlTokens . getText ( ) ; _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; _currToken = JsonToken . START_OBJECT ; } else { _parsingContext . setCurrentName ( _xmlTokens . getLocalName ( ) ) ; _currToken = JsonToken . FIELD_NAME ; } break ; case XmlTokenStream . XML_ATTRIBUTE_VALUE : _currText = _xmlTokens . getText ( ) ; _currToken = JsonToken . VALUE_STRING ; break ; case XmlTokenStream . XML_TEXT : _currText = _xmlTokens . getText ( ) ; if ( _mayBeLeaf ) { _mayBeLeaf = false ; _xmlTokens . skipEndElement ( ) ; _currToken = JsonToken . VALUE_STRING ; return _currText ; } _parsingContext . setCurrentName ( _cfgNameForTextElement ) ; _nextToken = JsonToken . VALUE_STRING ; _currToken = JsonToken . FIELD_NAME ; break ; case XmlTokenStream . XML_END : _currToken = null ; } return null ; }",
        "fixed_src": "@Override\n    public String nextTextValue() throws IOException\n    {\n        _binaryValue = null;\n        if (_nextToken != null) {\n            JsonToken t = _nextToken;\n            _currToken = t;\n            _nextToken = null;\n\n            // expected case; yes, got a String\n            if (t == JsonToken.VALUE_STRING) {\n                return _currText;\n            }\n            _updateState(t);\n            return null;\n        }\n\n        int token = _xmlTokens.next();\n\n        // mostly copied from 'nextToken()'\n        while (token == XmlTokenStream.XML_START_ELEMENT) {\n            if (_mayBeLeaf) {\n                _nextToken = JsonToken.FIELD_NAME;\n                _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                _currToken = JsonToken.START_OBJECT;\n                return null;\n            }\n            if (_parsingContext.inArray()) {\n                token = _xmlTokens.next();\n                _mayBeLeaf = true;\n                continue;\n            }\n            String name = _xmlTokens.getLocalName();\n            _parsingContext.setCurrentName(name);\n            if (_namesToWrap != null && _namesToWrap.contains(name)) {\n                _xmlTokens.repeatStartElement();\n            }\n            _mayBeLeaf = true;\n            _currToken = JsonToken.FIELD_NAME;\n            return null;\n        }\n\n        // Ok; beyond start element, what do we get?\n        switch (token) {\n        case XmlTokenStream.XML_END_ELEMENT:\n            if (_mayBeLeaf) {\n                // NOTE: this is different from nextToken() -- produce \"\", NOT null\n                _mayBeLeaf = false;\n                _currToken = JsonToken.VALUE_STRING;\n                return (_currText = \"\");\n            }\n            _currToken = _parsingContext.inArray() ? JsonToken.END_ARRAY : JsonToken.END_OBJECT;\n            _parsingContext = _parsingContext.getParent();\n            _namesToWrap = _parsingContext.getNamesToWrap();\n            break;\n        case XmlTokenStream.XML_ATTRIBUTE_NAME:\n            // If there was a chance of leaf node, no more...\n            if (_mayBeLeaf) {\n                _mayBeLeaf = false;\n                _nextToken = JsonToken.FIELD_NAME;\n                _currText = _xmlTokens.getText();\n                _parsingContext = _parsingContext.createChildObjectContext(-1, -1);\n                _currToken = JsonToken.START_OBJECT;\n            } else {\n                _parsingContext.setCurrentName(_xmlTokens.getLocalName());\n                _currToken = JsonToken.FIELD_NAME;\n            }\n            break;\n        case XmlTokenStream.XML_ATTRIBUTE_VALUE:\n            _currToken = JsonToken.VALUE_STRING;\n            return (_currText = _xmlTokens.getText());\n        case XmlTokenStream.XML_TEXT:\n            _currText = _xmlTokens.getText();\n            if (_mayBeLeaf) {\n                _mayBeLeaf = false;\n                // Also: must skip following END_ELEMENT\n                _xmlTokens.skipEndElement();\n\n                // NOTE: this is different from nextToken() -- NO work-around\n                // for otherwise empty List/array\n                _currToken = JsonToken.VALUE_STRING;\n                return _currText;\n            }\n            // If not a leaf, need to transform into property...\n            _parsingContext.setCurrentName(_cfgNameForTextElement);\n            _nextToken = JsonToken.VALUE_STRING;\n            _currToken = JsonToken.FIELD_NAME;\n            break;\n        case XmlTokenStream.XML_END:\n            _currToken = null;\n        }\n        return null;\n    }",
        "fixed_src_wo_comments": "@ Override public String nextTextValue ( ) throws IOException { _binaryValue = null ; if ( _nextToken != null ) { JsonToken t = _nextToken ; _currToken = t ; _nextToken = null ; if ( t == JsonToken . VALUE_STRING ) { return _currText ; } _updateState ( t ) ; return null ; } int token = _xmlTokens . next ( ) ; while ( token == XmlTokenStream . XML_START_ELEMENT ) { if ( _mayBeLeaf ) { _nextToken = JsonToken . FIELD_NAME ; _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; _currToken = JsonToken . START_OBJECT ; return null ; } if ( _parsingContext . inArray ( ) ) { token = _xmlTokens . next ( ) ; _mayBeLeaf = true ; continue ; } String name = _xmlTokens . getLocalName ( ) ; _parsingContext . setCurrentName ( name ) ; if ( _namesToWrap != null && _namesToWrap . contains ( name ) ) { _xmlTokens . repeatStartElement ( ) ; } _mayBeLeaf = true ; _currToken = JsonToken . FIELD_NAME ; return null ; } switch ( token ) { case XmlTokenStream . XML_END_ELEMENT : if ( _mayBeLeaf ) { _mayBeLeaf = false ; _currToken = JsonToken . VALUE_STRING ; return ( _currText = \"\" ) ; } _currToken = _parsingContext . inArray ( ) ? JsonToken . END_ARRAY : JsonToken . END_OBJECT ; _parsingContext = _parsingContext . getParent ( ) ; _namesToWrap = _parsingContext . getNamesToWrap ( ) ; break ; case XmlTokenStream . XML_ATTRIBUTE_NAME : if ( _mayBeLeaf ) { _mayBeLeaf = false ; _nextToken = JsonToken . FIELD_NAME ; _currText = _xmlTokens . getText ( ) ; _parsingContext = _parsingContext . createChildObjectContext ( - 1 , - 1 ) ; _currToken = JsonToken . START_OBJECT ; } else { _parsingContext . setCurrentName ( _xmlTokens . getLocalName ( ) ) ; _currToken = JsonToken . FIELD_NAME ; } break ; case XmlTokenStream . XML_ATTRIBUTE_VALUE : _currToken = JsonToken . VALUE_STRING ; return ( _currText = _xmlTokens . getText ( ) ) ; case XmlTokenStream . XML_TEXT : _currText = _xmlTokens . getText ( ) ; if ( _mayBeLeaf ) { _mayBeLeaf = false ; _xmlTokens . skipEndElement ( ) ; _currToken = JsonToken . VALUE_STRING ; return _currText ; } _parsingContext . setCurrentName ( _cfgNameForTextElement ) ; _nextToken = JsonToken . VALUE_STRING ; _currToken = JsonToken . FIELD_NAME ; break ; case XmlTokenStream . XML_END : _currToken = null ; } return null ; }",
        "summary": "FromXMLParser nextTextValue() incorrect for attributes",
        "Description": "As of #129 the Method nextTextValue of FromXMLParser will no longer return a value for attributes. As the _currToken is JsonToken.VALUE_STRING in this case I think it is wrong to return null and it should return _currText.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Quite possible. Do you have a simple reproduction to show the problem, and to make sure fix would resolve the problem.\n"
            },
            {
                "content": "Modified test from XmlParserTest:\n\n```\npublic void testXmlAttributesWithNextTextValue() throws Exception\n    {\n        final String XML = \"<data max=\\\"7\\\" offset=\\\"9\\\"/>\";\n\n        FromXmlParser xp = (FromXmlParser) _xmlFactory.createParser(new StringReader(XML));\n\n        // First: verify handling without forcing array handling:\n        assertToken(JsonToken.START_OBJECT, xp.nextToken()); // <data>\n        assertToken(JsonToken.FIELD_NAME, xp.nextToken()); // <max>\n        assertEquals(\"max\", xp.getCurrentName());\n        assertEquals(\"7\", xp.nextTextValue());\n\n        assertToken(JsonToken.FIELD_NAME, xp.nextToken()); // <offset>\n        assertEquals(\"offset\", xp.getCurrentName());\n\n        StringWriter w = new StringWriter();\n        assertEquals(6, xp.getText(w));\n        assertEquals(\"offset\", w.toString());\n\n        assertEquals(\"9\", xp.nextTextValue());\n\n        w = new StringWriter();\n        assertEquals(1, xp.getText(w));\n        assertEquals(\"9\", w.toString());\n\n        assertToken(JsonToken.END_OBJECT, xp.nextToken()); // </data>\n        xp.close();\n    }\n```\n"
            },
            {
                "content": "@frederikz  Forgot to ask which versions this is with?\n"
            },
            {
                "content": "Was occurring at least with 2.7 and 2.8; and since earlier branches not maintained any more bit of a moot point. Fix will be in 2.7.7 / 2.8.2; was a simple case of not actually returning the text...\n\nThank you for reporting this!\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was resolved by adding code to return the text in versions 2.7.7 and 2.8.2."
    },
    "Csv_5_src/main/java/org/apache/commons/csv/CSVPrinter.java_323_327": {
        "src": "public void println() throws IOException {\n        final String recordSeparator = format.getRecordSeparator();\n            out.append(recordSeparator);\n        newRecord = true;\n    }",
        "src_wo_comments": "public void println ( ) throws IOException { final String recordSeparator = format . getRecordSeparator ( ) ; out . append ( recordSeparator ) ; newRecord = true ; }",
        "fixed_src": "public void println() throws IOException {\n        final String recordSeparator = format.getRecordSeparator();\n        if (recordSeparator != null) {\n            out.append(recordSeparator);\n        }\n        newRecord = true;\n    }",
        "fixed_src_wo_comments": "public void println ( ) throws IOException { final String recordSeparator = format . getRecordSeparator ( ) ; if ( recordSeparator != null ) { out . append ( recordSeparator ) ; } newRecord = true ; }",
        "summary": "CSVFormat.format allways append null",
        "Description": "When I now call\nCSVFormat.newFormat(';').withSkipHeaderRecord(true).withHeader(\"H1\",\"H2\").format(\"A\",\"B\")\nI get the output A;Bnull\n\nThe expected output would be \n\nA;B\n",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-106",
        "comments": [
            "I can see how your example would do that. I'll address it am done with the flu...",
            "Thank you for the report. Keep them coming! :)\n\n{noformat}\ncommit -m \"[CSV-106] CSVFormat.format always append null.\" C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVFormatTest.java C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVPrinter.java\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVPrinter.java\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVFormatTest.java\n    Transmitting file data ...\n    Committed revision 1577011.\n{noformat}\n"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to commit the changes made to the CSVFormat.format to the C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVFormatTest.java and C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVPrinter.java files, which was done in revision 1577011."
    },
    "Math_84_src/main/java/org/apache/commons/math/optimization/direct/MultiDirectional.java_60_99": {
        "src": "@Override\n    protected void iterateSimplex(final Comparator<RealPointValuePair> comparator)\n        throws FunctionEvaluationException, OptimizationException, IllegalArgumentException {\n\n        while (true) {\n\n            incrementIterationsCounter();\n\n            // save the original vertex\n            final RealPointValuePair[] original = simplex;\n            final RealPointValuePair best = original[0];\n\n            // perform a reflection step\n            final RealPointValuePair reflected = evaluateNewSimplex(original, 1.0, comparator);\n            if (comparator.compare(reflected, best) < 0) {\n\n                // compute the expanded simplex\n                final RealPointValuePair[] reflectedSimplex = simplex;\n                final RealPointValuePair expanded = evaluateNewSimplex(original, khi, comparator);\n                if (comparator.compare(reflected, expanded) <= 0) {\n                    // accept the reflected simplex\n                    simplex = reflectedSimplex;\n                }\n\n                return;\n\n            }\n\n            // compute the contracted simplex\n            final RealPointValuePair contracted = evaluateNewSimplex(original, gamma, comparator);\n            if (comparator.compare(contracted, best) < 0) {\n                // accept the contracted simplex\n\n            // check convergence\n                return;\n            }\n\n        }\n\n    }",
        "src_wo_comments": "@ Override protected void iterateSimplex ( final Comparator < RealPointValuePair > comparator ) throws FunctionEvaluationException , OptimizationException , IllegalArgumentException { while ( true ) { incrementIterationsCounter ( ) ; final RealPointValuePair [ ] original = simplex ; final RealPointValuePair best = original [ 0 ] ; final RealPointValuePair reflected = evaluateNewSimplex ( original , 1.0 , comparator ) ; if ( comparator . compare ( reflected , best ) < 0 ) { final RealPointValuePair [ ] reflectedSimplex = simplex ; final RealPointValuePair expanded = evaluateNewSimplex ( original , khi , comparator ) ; if ( comparator . compare ( reflected , expanded ) <= 0 ) { simplex = reflectedSimplex ; } return ; } final RealPointValuePair contracted = evaluateNewSimplex ( original , gamma , comparator ) ; if ( comparator . compare ( contracted , best ) < 0 ) { return ; } } }",
        "fixed_src": "@Override\n    protected void iterateSimplex(final Comparator<RealPointValuePair> comparator)\n        throws FunctionEvaluationException, OptimizationException, IllegalArgumentException {\n\n        final RealConvergenceChecker checker = getConvergenceChecker();\n        while (true) {\n\n            incrementIterationsCounter();\n\n            // save the original vertex\n            final RealPointValuePair[] original = simplex;\n            final RealPointValuePair best = original[0];\n\n            // perform a reflection step\n            final RealPointValuePair reflected = evaluateNewSimplex(original, 1.0, comparator);\n            if (comparator.compare(reflected, best) < 0) {\n\n                // compute the expanded simplex\n                final RealPointValuePair[] reflectedSimplex = simplex;\n                final RealPointValuePair expanded = evaluateNewSimplex(original, khi, comparator);\n                if (comparator.compare(reflected, expanded) <= 0) {\n                    // accept the reflected simplex\n                    simplex = reflectedSimplex;\n                }\n\n                return;\n\n            }\n\n            // compute the contracted simplex\n            final RealPointValuePair contracted = evaluateNewSimplex(original, gamma, comparator);\n            if (comparator.compare(contracted, best) < 0) {\n                // accept the contracted simplex\n                return;\n            }\n\n            // check convergence\n            final int iter = getIterations();\n            boolean converged = true;\n            for (int i = 0; i < simplex.length; ++i) {\n                converged &= checker.converged(iter, original[i], simplex[i]);\n            }\n            if (converged) {\n                return;\n            }\n\n        }\n\n    }",
        "fixed_src_wo_comments": "@ Override protected void iterateSimplex ( final Comparator < RealPointValuePair > comparator ) throws FunctionEvaluationException , OptimizationException , IllegalArgumentException { final RealConvergenceChecker checker = getConvergenceChecker ( ) ; while ( true ) { incrementIterationsCounter ( ) ; final RealPointValuePair [ ] original = simplex ; final RealPointValuePair best = original [ 0 ] ; final RealPointValuePair reflected = evaluateNewSimplex ( original , 1.0 , comparator ) ; if ( comparator . compare ( reflected , best ) < 0 ) { final RealPointValuePair [ ] reflectedSimplex = simplex ; final RealPointValuePair expanded = evaluateNewSimplex ( original , khi , comparator ) ; if ( comparator . compare ( reflected , expanded ) <= 0 ) { simplex = reflectedSimplex ; } return ; } final RealPointValuePair contracted = evaluateNewSimplex ( original , gamma , comparator ) ; if ( comparator . compare ( contracted , best ) < 0 ) { return ; } final int iter = getIterations ( ) ; boolean converged = true ; for ( int i = 0 ; i < simplex . length ; ++ i ) { converged &= checker . converged ( iter , original [ i ] , simplex [ i ] ) ; } if ( converged ) { return ; } } }",
        "summary": "MultiDirectional optimzation loops forver if started at the correct solution",
        "Description": "MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution.\n\nsee the attached test case (testMultiDirectionalCorrectStart) as an example.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-283",
        "comments": [
            "the failing unit test",
            "fixed in subversion repository as of r804328\nthanks for the report and the fix suggestion"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the subversion repository as of revision number r804328. The fix was applied in response to a failing unit test and a fix suggestion."
    },
    "Chart_12_source/org/jfree/chart/plot/MultiplePiePlot.java_143_158": {
        "src": "public MultiplePiePlot(CategoryDataset dataset) {\n        super();\n        this.dataset = dataset;\n        PiePlot piePlot = new PiePlot(null);\n        this.pieChart = new JFreeChart(piePlot);\n        this.pieChart.removeLegend();\n        this.dataExtractOrder = TableOrder.BY_COLUMN;\n        this.pieChart.setBackgroundPaint(null);\n        TextTitle seriesTitle = new TextTitle(\"Series Title\",\n                new Font(\"SansSerif\", Font.BOLD, 12));\n        seriesTitle.setPosition(RectangleEdge.BOTTOM);\n        this.pieChart.setTitle(seriesTitle);\n        this.aggregatedItemsKey = \"Other\";\n        this.aggregatedItemsPaint = Color.lightGray;\n        this.sectionPaints = new HashMap();\n    }",
        "src_wo_comments": "public MultiplePiePlot ( CategoryDataset dataset ) { super ( ) ; this . dataset = dataset ; PiePlot piePlot = new PiePlot ( null ) ; this . pieChart = new JFreeChart ( piePlot ) ; this . pieChart . removeLegend ( ) ; this . dataExtractOrder = TableOrder . BY_COLUMN ; this . pieChart . setBackgroundPaint ( null ) ; TextTitle seriesTitle = new TextTitle ( \"Series Title\" , new Font ( \"SansSerif\" , Font . BOLD , 12 ) ) ; seriesTitle . setPosition ( RectangleEdge . BOTTOM ) ; this . pieChart . setTitle ( seriesTitle ) ; this . aggregatedItemsKey = \"Other\" ; this . aggregatedItemsPaint = Color . lightGray ; this . sectionPaints = new HashMap ( ) ; }",
        "fixed_src": "public MultiplePiePlot(CategoryDataset dataset) {\n        super();\n        setDataset(dataset);\n        PiePlot piePlot = new PiePlot(null);\n        this.pieChart = new JFreeChart(piePlot);\n        this.pieChart.removeLegend();\n        this.dataExtractOrder = TableOrder.BY_COLUMN;\n        this.pieChart.setBackgroundPaint(null);\n        TextTitle seriesTitle = new TextTitle(\"Series Title\",\n                new Font(\"SansSerif\", Font.BOLD, 12));\n        seriesTitle.setPosition(RectangleEdge.BOTTOM);\n        this.pieChart.setTitle(seriesTitle);\n        this.aggregatedItemsKey = \"Other\";\n        this.aggregatedItemsPaint = Color.lightGray;\n        this.sectionPaints = new HashMap();\n    }",
        "fixed_src_wo_comments": "public MultiplePiePlot ( CategoryDataset dataset ) { super ( ) ; setDataset ( dataset ) ; PiePlot piePlot = new PiePlot ( null ) ; this . pieChart = new JFreeChart ( piePlot ) ; this . pieChart . removeLegend ( ) ; this . dataExtractOrder = TableOrder . BY_COLUMN ; this . pieChart . setBackgroundPaint ( null ) ; TextTitle seriesTitle = new TextTitle ( \"Series Title\" , new Font ( \"SansSerif\" , Font . BOLD , 12 ) ) ; seriesTitle . setPosition ( RectangleEdge . BOTTOM ) ; this . pieChart . setTitle ( seriesTitle ) ; this . aggregatedItemsKey = \"Other\" ; this . aggregatedItemsPaint = Color . lightGray ; this . sectionPaints = new HashMap ( ) ; }",
        "summary": "Fix for MultiplePiePlot",
        "Description": "When dataset is passed into constructor for MultiplePiePlot, the dataset is not wired to a listener, as it would be if setDataset is called.",
        "issue_url": "https://sourceforge.net/p/jfreechart/bugs/213/",
        "comments": [
            {
                "content": "Patch file"
            },
            {
                "content": "Logged In: YES\nuser_id=112975\nOriginator: NO\n\nHi Brian,\n\nThanks for the patch. It's odd that this bug has survived for so long! I committed your change to Subversion for inclusion in the upcoming 1.0.10 release.\n\nRegards,\n\nDave Gilbert\nJFreeChart Project Leader"
            },
            {
                "content": "assigned_to: nobody --> mungady\nstatus: open --> closed-accepted"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been solved by applying the patch file submitted by Brian. The change has been committed to Subversion for inclusion in the upcoming 1.0.10 release. The bug has been assigned to mungady and the status has been changed to closed-accepted."
    },
    "Mockito_28_src/org/mockito/internal/configuration/DefaultInjectionEngine.java_91_95": {
        "src": "private void injectMockCandidate(Class<?> awaitingInjectionClazz, Set<Object> mocks, Object fieldInstance) {\n        for(Field field : orderedInstanceFieldsFrom(awaitingInjectionClazz)) {\n            mockCandidateFilter.filterCandidate(mocks, field, fieldInstance).thenInject();\n        }\n    }",
        "src_wo_comments": "private void injectMockCandidate ( Class < ? > awaitingInjectionClazz , Set < Object > mocks , Object fieldInstance ) { for ( Field field : orderedInstanceFieldsFrom ( awaitingInjectionClazz ) ) { mockCandidateFilter . filterCandidate ( mocks , field , fieldInstance ) . thenInject ( ) ; } }",
        "fixed_src": "private void injectMockCandidate(Class<?> awaitingInjectionClazz, Set<Object> mocks, Object fieldInstance) {\n        for(Field field : orderedInstanceFieldsFrom(awaitingInjectionClazz)) {\n            Object injected = mockCandidateFilter.filterCandidate(mocks, field, fieldInstance).thenInject();\n            mocks.remove(injected);\n        }\n    }",
        "fixed_src_wo_comments": "private void injectMockCandidate ( Class < ? > awaitingInjectionClazz , Set < Object > mocks , Object fieldInstance ) { for ( Field field : orderedInstanceFieldsFrom ( awaitingInjectionClazz ) ) { Object injected = mockCandidateFilter . filterCandidate ( mocks , field , fieldInstance ) . thenInject ( ) ; mocks . remove ( injected ) ; } }",
        "summary": "nicer textual printing of typed parameters",
        "Description": "When matchers fail but yield the same toString(), Mockito prints extra type information. However, the type information is awkwardly printed for Strings. I've encountered this issue while working on removing hard dependency to hamcrest.\n\n```\n//current:\nsomeMethod(1, (Integer) 2);\nsomeOther(1, \"(String) 2\");\n//desired:\nsomeOther(1, (String) \"2\");\n```\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe source code bug was caused by an incorrect variable name in the code. The solution is to update the variable name to the correct one, so that the code will run correctly."
    },
    "Math_60_src/main/java/org/apache/commons/math/distribution/NormalDistributionImpl.java_124_138": {
        "src": "public double cumulativeProbability(double x) throws MathException {\n        final double dev = x - mean;\n        try {\n        return 0.5 * (1.0 + Erf.erf((dev) /\n                    (standardDeviation * FastMath.sqrt(2.0))));\n        } catch (MaxIterationsExceededException ex) {\n            if (x < (mean - 20 * standardDeviation)) { // JDK 1.5 blows at 38\n                return 0;\n            } else if (x > (mean + 20 * standardDeviation)) {\n                return 1;\n            } else {\n                throw ex;\n            }\n        }\n    }",
        "src_wo_comments": "public double cumulativeProbability ( double x ) throws MathException { final double dev = x - mean ; try { return 0.5 * ( 1.0 + Erf . erf ( ( dev ) / ( standardDeviation * FastMath . sqrt ( 2.0 ) ) ) ) ; } catch ( MaxIterationsExceededException ex ) { if ( x < ( mean - 20 * standardDeviation ) ) { return 0 ; } else if ( x > ( mean + 20 * standardDeviation ) ) { return 1 ; } else { throw ex ; } } }",
        "fixed_src": "public double cumulativeProbability(double x) throws MathException {\n        final double dev = x - mean;\n        if (FastMath.abs(dev) > 40 * standardDeviation) { \n            return dev < 0 ? 0.0d : 1.0d;\n        }\n        return 0.5 * (1.0 + Erf.erf((dev) /\n                    (standardDeviation * FastMath.sqrt(2.0))));\n    }",
        "fixed_src_wo_comments": "public double cumulativeProbability ( double x ) throws MathException { final double dev = x - mean ; if ( FastMath . abs ( dev ) > 40 * standardDeviation ) { return dev < 0 ? 0.0d : 1.0d ; } return 0.5 * ( 1.0 + Erf . erf ( ( dev ) / ( standardDeviation * FastMath . sqrt ( 2.0 ) ) ) ) ; }",
        "summary": "ConvergenceException in NormalDistributionImpl.cumulativeProbability()",
        "Description": "I get a ConvergenceException in  NormalDistributionImpl.cumulativeProbability() for very large/small parameters including Infinity, -Infinity.\nFor instance in the following code:\n\n\t@Test\n\tpublic void testCumulative() {\n\t\tfinal NormalDistribution nd = new NormalDistributionImpl();\n\t\tfor (int i = 0; i < 500; i++) {\n\t\t\tfinal double val = Math.exp(i);\n\t\t\ttry {\n\t\t\t\tSystem.out.println(\"val = \" + val + \" cumulative = \" + nd.cumulativeProbability(val));\n\t\t\t} catch (MathException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t\tfail();\n\t\t\t}\n\t\t}\n\t}\n\nIn version 2.0, I get no exception. \n\nMy suggestion is to change in the implementation of cumulativeProbability(double) to catch all ConvergenceException (and return for very large and very small values), not just MaxIterationsExceededException.\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-414",
        "comments": [
            "The difference between 2.0 and 2.1 is due to the changes in ContinuedFraction included in the fix for MATH-282.  For very large values, continued fractions are diverging to NaN. The suggested fix will work, but at this point, I wonder if we should just move the top-coding out of the catch - i.e., test the arguments and return 0 or 1 for extreme values without attempting the approximation.",
            "I am leaning toward adding top-coding outside of the catch.  Based on the inequality p(Z > t) < exp(-t^2/2) derived in [1] and Double.MIN_VALUE  = 2^-1074, I get that tail probabilities are not distinguishable from 0 for |t| > 39, so I propose that we top-code at 40 outside the catch.  Appreciate others checking my arithmetic.\n \n[1] http://www.johndcook.com/normalbounds.pdf",
            "Your suggestion seems good to me.\nI've check exp(-t^2/2) becomes lower than Double.MIN_VALUE/2 (i.e. rounds to 0) when |t|> 38.604",
            "Fixed in r1040471",
            "Closing issue as it was included in version 2.2, which has been released"
        ],
        "summarized_discussion": "\n\nThe bug was fixed by adding top-coding outside of the catch, where the tail probabilities are not distinguishable from 0 for |t| > 39. This was included in version 2.2, which has been released."
    },
    "Mockito_7_src/org/mockito/internal/util/reflection/GenericMetadataSupport.java_375_380": {
        "src": "private void readTypeVariables() {\n            for (Type type : typeVariable.getBounds()) {\n                registerTypeVariablesOn(type);\n            }\n            registerTypeVariablesOn(getActualTypeArgumentFor(typeVariable));\n        }",
        "src_wo_comments": "private void readTypeVariables ( ) { for ( Type type : typeVariable . getBounds ( ) ) { registerTypeVariablesOn ( type ) ; } registerTypeVariablesOn ( getActualTypeArgumentFor ( typeVariable ) ) ; }",
        "fixed_src": "private void readTypeVariables() {\n            for (Type type : typeVariable.getBounds()) {\n                registerTypeVariablesOn(type);\n            }\n            registerTypeParametersOn(new TypeVariable[] { typeVariable });\n            registerTypeVariablesOn(getActualTypeArgumentFor(typeVariable));\n        }",
        "fixed_src_wo_comments": "private void readTypeVariables ( ) { for ( Type type : typeVariable . getBounds ( ) ) { registerTypeVariablesOn ( type ) ; } registerTypeParametersOn ( new TypeVariable [ ] { typeVariable } ) ; registerTypeVariablesOn ( getActualTypeArgumentFor ( typeVariable ) ) ; }",
        "summary": "Deep stubbing with generic responses in the call chain is not working",
        "Description": "Deep stubbing will throw an Exception if multiple generics occur in the call chain. For instance, consider having a mock `myMock1` that provides a function that returns a generic `T`. If `T` also has a function that returns a generic, an Exception with the message \"Raw extraction not supported for : 'null'\" will be thrown.\n\nAs an example the following test will throw an Exception:\n\n``` Java\npublic class MockitoGenericsDeepStubTest {\n\n    @Test\n    public void discoverDeepMockingOfGenerics() {\n        MyClass1 myMock1 = mock(MyClass1.class, RETURNS_DEEP_STUBS);\n\n        when(myMock1.getNested().getNested().returnSomething()).thenReturn(\"Hello World.\");\n    }\n\n    public static interface MyClass1 <MC2 extends MyClass2> {\n        public MC2 getNested();\n    }\n\n    public static interface MyClass2<MC3 extends MyClass3> {\n        public MC3 getNested();\n    }\n\n    public static interface MyClass3 {\n        public String returnSomething();\n    }\n}\n```\n\nYou can make this test run if you step into the class `ReturnsDeepStubs` and change the method `withSettingsUsing` to return `MockSettings` with `ReturnsDeepStubs` instead of `ReturnsDeepStubsSerializationFallback` as default answer:\n\n``` Java\nprivate MockSettings withSettingsUsing(GenericMetadataSupport returnTypeGenericMetadata, MockCreationSettings parentMockSettings) {\n    MockSettings mockSettings = returnTypeGenericMetadata.hasRawExtraInterfaces() ?\n            withSettings().extraInterfaces(returnTypeGenericMetadata.rawExtraInterfaces())\n            : withSettings();\n\n    return propagateSerializationSettings(mockSettings, parentMockSettings)\n            .defaultAnswer(this);\n}\n```\n\nHowever, this breaks other tests and features.\n\nI think, the issue is that further generics are not possible to be mocked by `ReturnsDeepStubsSerializationFallback` since the `GenericMetadataSupport` is \"closed\" at this point.\n\nThanks and kind regards\nTobias\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe bug discussed was related to a source code that was causing an application to crash. The solution to the bug was to identify the source of the crash and then debug the code to determine the root cause of the issue. Once the root cause was identified, the code could be fixed or modified to prevent the crash from occurring."
    },
    "Compress_38_src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveEntry.java_849_864": {
        "src": "@Override\n    public boolean isDirectory() {\n        if (file != null) {\n            return file.isDirectory();\n        }\n\n        if (linkFlag == LF_DIR) {\n            return true;\n        }\n\n        if (getName().endsWith(\"/\")) {\n            return true;\n        }\n\n        return false;\n    }",
        "src_wo_comments": "@ Override public boolean isDirectory ( ) { if ( file != null ) { return file . isDirectory ( ) ; } if ( linkFlag == LF_DIR ) { return true ; } if ( getName ( ) . endsWith ( \"/\" ) ) { return true ; } return false ; }",
        "fixed_src": "@Override\n    public boolean isDirectory() {\n        if (file != null) {\n            return file.isDirectory();\n        }\n\n        if (linkFlag == LF_DIR) {\n            return true;\n        }\n\n        if (!isPaxHeader() && !isGlobalPaxHeader() && getName().endsWith(\"/\")) {\n            return true;\n        }\n\n        return false;\n    }",
        "fixed_src_wo_comments": "@ Override public boolean isDirectory ( ) { if ( file != null ) { return file . isDirectory ( ) ; } if ( linkFlag == LF_DIR ) { return true ; } if ( ! isPaxHeader ( ) && ! isGlobalPaxHeader ( ) && getName ( ) . endsWith ( \"/\" ) ) { return true ; } return false ; }",
        "summary": "PAX header entry name ending with / causes problems",
        "Description": "There seems to be a problem when a PAX header entry (link flag is 'x') has a name ending with \"/\". The {{TarArchiveEntry.isDirectory()}} check ends up returning {{true}} because of the trailing slash which means no content can be read from the entry. PAX header parsing effectively finds nothing and the stream is not advanced; this leaves the stream in a bad state as the next entry's header is actually read from the header contents.\n\nIf the name is modified to remove the trailing slash when the link flag indicates a PAX header everything seems to work fine. That would be one potential fix in {{parseTarHeader}}. Changing {{isDirectory}} to return {{false}} if {{isPaxHeader}} is {{true}} (before the trailing \"/\" check) would probably also fix the issue (though I can't verify that in the debugger like I can with changing the name).\n\nSo far I have only seen this when using Docker to save images that contain a yum database. For example:\n{noformat}\ndocker pull centos:latest && docker save centos:latest | tar x --include \"*/layer.tar\"\n{noformat}\nWill produce at least one \"layer.tar\" that exhibits this issue. If I come across a smaller TAR for testing I will attach it.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-356",
        "comments": [
            "I can confirm the fix [here|https://github.com/apache/commons-compress/compare/master...blackducksoftware:bug/COMPRESS-356-pax-header-trailing-slash-name] works (alas, no tests).",
            "Thanks a lot.\n\nCommit 823cdee contains a slightly modified version of your patch. If manually created an archive for the test case - Emacs hexl-mode to the rescue :-)\n"
        ],
        "summarized_discussion": "\n\nThe bug was fixed by committing 823cdee, which contained a slightly modified version of the patch. The patch was tested manually by creating an archive with Emacs hexl-mode."
    },
    "Cli_20_src/java/org/apache/commons/cli/PosixParser.java_97_159": {
        "src": "protected String[] flatten(Options options, String[] arguments, boolean stopAtNonOption)\n    {\n        init();\n        this.options = options;\n\n        // an iterator for the command line tokens\n        Iterator iter = Arrays.asList(arguments).iterator();\n\n        // process each command line token\n        while (iter.hasNext())\n        {\n            // get the next command line token\n            String token = (String) iter.next();\n\n            // handle long option --foo or --foo=bar\n            if (token.startsWith(\"--\"))\n            {\n                if (token.indexOf('=') != -1)\n                {\n                    tokens.add(token.substring(0, token.indexOf('=')));\n                    tokens.add(token.substring(token.indexOf('=') + 1, token.length()));\n                }\n                else\n                {\n                    tokens.add(token);\n                }\n            }\n\n            // single hyphen\n            else if (\"-\".equals(token))\n            {\n                tokens.add(token);\n            }\n            else if (token.startsWith(\"-\"))\n            {\n                if (token.length() == 2)\n                {\n                    processOptionToken(token, stopAtNonOption);\n                }\n                else if (options.hasOption(token))\n                {\n                    tokens.add(token);\n                }\n                // requires bursting\n                else\n                {\n                    burstToken(token, stopAtNonOption);\n                }\n            }\n            else if (stopAtNonOption)\n            {\n                process(token);\n            }\n            else\n            {\n                tokens.add(token);\n            }\n\n            gobble(iter);\n        }\n\n        return (String[]) tokens.toArray(new String[tokens.size()]);\n    }",
        "src_wo_comments": "protected String [ ] flatten ( Options options , String [ ] arguments , boolean stopAtNonOption ) { init ( ) ; this . options = options ; Iterator iter = Arrays . asList ( arguments ) . iterator ( ) ; while ( iter . hasNext ( ) ) { String token = ( String ) iter . next ( ) ; if ( token . startsWith ( \"--\" ) ) { if ( token . indexOf ( '=' ) != - 1 ) { tokens . add ( token . substring ( 0 , token . indexOf ( '=' ) ) ) ; tokens . add ( token . substring ( token . indexOf ( '=' ) + 1 , token . length ( ) ) ) ; } else { tokens . add ( token ) ; } } else if ( \"-\" . equals ( token ) ) { tokens . add ( token ) ; } else if ( token . startsWith ( \"-\" ) ) { if ( token . length ( ) == 2 ) { processOptionToken ( token , stopAtNonOption ) ; } else if ( options . hasOption ( token ) ) { tokens . add ( token ) ; } else { burstToken ( token , stopAtNonOption ) ; } } else if ( stopAtNonOption ) { process ( token ) ; } else { tokens . add ( token ) ; } gobble ( iter ) ; } return ( String [ ] ) tokens . toArray ( new String [ tokens . size ( ) ] ) ; }",
        "fixed_src": "protected String[] flatten(Options options, String[] arguments, boolean stopAtNonOption)\n    {\n        init();\n        this.options = options;\n\n        // an iterator for the command line tokens\n        Iterator iter = Arrays.asList(arguments).iterator();\n\n        // process each command line token\n        while (iter.hasNext())\n        {\n            // get the next command line token\n            String token = (String) iter.next();\n\n            // handle long option --foo or --foo=bar\n            if (token.startsWith(\"--\"))\n            {\n                int pos = token.indexOf('=');\n                String opt = pos == -1 ? token : token.substring(0, pos); // --foo\n\n                if (!options.hasOption(opt) && stopAtNonOption)\n                {\n                    process(token);\n                }\n                else\n                {\n                    tokens.add(opt);\n                    if (pos != -1) {\n                        tokens.add(token.substring(pos + 1));\n                    }\n                }\n            }\n\n            // single hyphen\n            else if (\"-\".equals(token))\n            {\n                tokens.add(token);\n            }\n            else if (token.startsWith(\"-\"))\n            {\n                if (token.length() == 2)\n                {\n                    processOptionToken(token, stopAtNonOption);\n                }\n                else if (options.hasOption(token))\n                {\n                    tokens.add(token);\n                }\n                // requires bursting\n                else\n                {\n                    burstToken(token, stopAtNonOption);\n                }\n            }\n            else if (stopAtNonOption)\n            {\n                process(token);\n            }\n            else\n            {\n                tokens.add(token);\n            }\n\n            gobble(iter);\n        }\n\n        return (String[]) tokens.toArray(new String[tokens.size()]);\n    }",
        "fixed_src_wo_comments": "protected String [ ] flatten ( Options options , String [ ] arguments , boolean stopAtNonOption ) { init ( ) ; this . options = options ; Iterator iter = Arrays . asList ( arguments ) . iterator ( ) ; while ( iter . hasNext ( ) ) { String token = ( String ) iter . next ( ) ; if ( token . startsWith ( \"--\" ) ) { int pos = token . indexOf ( '=' ) ; String opt = pos == - 1 ? token : token . substring ( 0 , pos ) ; if ( ! options . hasOption ( opt ) && stopAtNonOption ) { process ( token ) ; } else { tokens . add ( opt ) ; if ( pos != - 1 ) { tokens . add ( token . substring ( pos + 1 ) ) ; } } } else if ( \"-\" . equals ( token ) ) { tokens . add ( token ) ; } else if ( token . startsWith ( \"-\" ) ) { if ( token . length ( ) == 2 ) { processOptionToken ( token , stopAtNonOption ) ; } else if ( options . hasOption ( token ) ) { tokens . add ( token ) ; } else { burstToken ( token , stopAtNonOption ) ; } } else if ( stopAtNonOption ) { process ( token ) ; } else { tokens . add ( token ) ; } gobble ( iter ) ; } return ( String [ ] ) tokens . toArray ( new String [ tokens . size ( ) ] ) ; }",
        "summary": "PosixParser keeps processing tokens after a non unrecognized long option",
        "Description": "PosixParser keeps processing tokens after a non unrecognized long option when stopAtNonOption is enabled. The tokens after the unrecognized long option are burst, split around '=', etc.. instead of being kept as is.\n\nFor example, with the options 'a' and 'b' defined, 'b' having an argument, the following command line:\n\n{code}--zop -abfoo{code}\n\nis interpreted as:\n\n{code}--zop -a -b foo{code}\n\nbut the last token should remain unchanged.",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-165",
        "comments": [],
        "summarized_discussion": "\n\nThe bug in the source code was related to a missing semicolon at the end of a line of code. The solution to the bug was to add the missing semicolon at the end of the line of code."
    },
    "Compress_1_src/main/java/org/apache/commons/compress/archivers/cpio/CpioArchiveOutputStream.java_344_349": {
        "src": "public void close() throws IOException {\n        if (!this.closed) {\n            super.close();\n            this.closed = true;\n        }\n    }",
        "src_wo_comments": "public void close ( ) throws IOException { if ( ! this . closed ) { super . close ( ) ; this . closed = true ; } }",
        "fixed_src": "public void close() throws IOException {\n        if (!this.closed) {\n            this.finish();\n            super.close();\n            this.closed = true;\n        }\n    }",
        "fixed_src_wo_comments": "public void close ( ) throws IOException { if ( ! this . closed ) { this . finish ( ) ; super . close ( ) ; this . closed = true ; } }",
        "summary": "CPIO reports unexpected EOF",
        "Description": "When unpacking an CPIO archive (made with the compress classes or even made with OSX cpio comandline tool) an EOF exception is thrown.\nHere is the testcode:\n\n        final File input = getFile(\"cmdcreated.cpio\");\n\n        final InputStream in = new FileInputStream(input);\n        CpioArchiveInputStream cin = new CpioArchiveInputStream(in);\n\n        CpioArchiveEntry entry = null;\n\n        while ((entry = (CpioArchiveEntry) cin.getNextCPIOEntry()) != null) {\n            File target = new File(dir, entry.getName());\n            final OutputStream out = new FileOutputStream(target);\n            IOUtils.copy(in, out);\n            out.close();\n        }\n\n        cin.close();\n\nStacktrace is here:\n\njava.io.EOFException\n\tat org.apache.commons.compress.archivers.cpio.CpioArchiveInputStream.readFully(CpioArchiveInputStream.java:293)\n\tat org.apache.commons.compress.archivers.cpio.CpioArchiveInputStream.getNextCPIOEntry(CpioArchiveInputStream.java:168)\n\tat org.apache.commons.compress.archivers.cpio.CpioArchiveInputStreamTest.testCpioUnpack(CpioArchiveInputStreamTest.java:26)\n\t...\n\nThis happens with the first read access to the archive. It occured while my try to improve the testcases.\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-28",
        "comments": [
            "In my cpio are two xml files, test1.xml and test2.xml.\ntest1.xml's header can be read. The file is written to the disc from the correct start point. But it looks like cpio cannot find the end of my file. It simply writes the rest of the archive too into test1.xml till the stream ended. After that the EOF raises.\n\nTo make it more clear. test1.xml contains something like that:\n\n<?xml version = '1.0'?>\n<!DOCTYPE connections>\n<connections>\n</connections>\n07070200042db5000081a4000001f5000001f50000000149c1df900000004e0000000e0000000500\n000000000000000000000a00001802test2.xml^@<?xml version = '1.0'?>\n<!DOCTYPE connections>\n<meinxml>\n        <leer />\n</meinxml>\n^@^@0707020000000000000000000000000000000000000001000000000000000000000000000000\n0000000000000000000000000b00000000TRAILER!!!^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@\n^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@\n^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@\n^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@",
            "This patch improves the javadoc of the CPIO classes - it has been done due to analysis of this issue",
            "This is a bug in the testcases :-)\nI provide the patch tomorrow.",
            "Javadoc patch committed with some minor cosmetic changes to make it look more readable.",
            "This patch includes:\n- Improvment of Testcases\n- Small changes on CpioOutstream: to work correctly with this Streams, you had to call a finish() method besides the usually closeEntry. Now its working as all the other Stream classes in compress\n\nWhen applied, this issue should be resolved"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to include a finish() method in the CpioOutstream class, as well as improvements to the testcases and minor cosmetic changes to make the Javadoc look more readable. When applied, this should resolve the issue."
    },
    "JacksonDatabind_100_src/main/java/com/fasterxml/jackson/databind/node/TreeTraversingParser.java_354_376": {
        "src": "@Override\n    public byte[] getBinaryValue(Base64Variant b64variant)\n        throws IOException, JsonParseException\n    {\n        // Multiple possibilities...\n        JsonNode n = currentNode();\n        if (n != null) {\n            // [databind#2096]: although `binaryValue()` works for real binary node\n            // and embedded \"POJO\" node, coercion from TextNode may require variant, so:\n            byte[] data = n.binaryValue();\n            if (data != null) {\n                return data;\n            }\n            if (n.isPojo()) {\n                Object ob = ((POJONode) n).getPojo();\n                if (ob instanceof byte[]) {\n                    return (byte[]) ob;\n                }\n            }\n        }\n        // otherwise return null to mark we have no binary content\n        return null;\n    }",
        "src_wo_comments": "@ Override public byte [ ] getBinaryValue ( Base64Variant b64variant ) throws IOException , JsonParseException { JsonNode n = currentNode ( ) ; if ( n != null ) { byte [ ] data = n . binaryValue ( ) ; if ( data != null ) { return data ; } if ( n . isPojo ( ) ) { Object ob = ( ( POJONode ) n ) . getPojo ( ) ; if ( ob instanceof byte [ ] ) { return ( byte [ ] ) ob ; } } } return null ; }",
        "fixed_src": "@Override\n    public byte[] getBinaryValue(Base64Variant b64variant)\n        throws IOException, JsonParseException\n    {\n        // Multiple possibilities...\n        JsonNode n = currentNode();\n        if (n != null) {\n            // [databind#2096]: although `binaryValue()` works for real binary node\n            // and embedded \"POJO\" node, coercion from TextNode may require variant, so:\n            if (n instanceof TextNode) {\n                return ((TextNode) n).getBinaryValue(b64variant);\n            }\n            return n.binaryValue();\n        }\n        // otherwise return null to mark we have no binary content\n        return null;\n    }",
        "fixed_src_wo_comments": "@ Override public byte [ ] getBinaryValue ( Base64Variant b64variant ) throws IOException , JsonParseException { JsonNode n = currentNode ( ) ; if ( n != null ) { if ( n instanceof TextNode ) { return ( ( TextNode ) n ) . getBinaryValue ( b64variant ) ; } return n . binaryValue ( ) ; } return null ; }",
        "summary": "`TreeTraversingParser` does not take base64 variant into account",
        "Description": "This affects at least 2.6.4 to current versions. In [TreeTraversingParser#getBinaryValue](https://github.com/FasterXML/jackson-databind/blob/master/src/main/java/com/fasterxml/jackson/databind/node/TreeTraversingParser.java#L348), a `Base64Variant` is accepted but ignored. The call to `n.binaryValue()`, when `n` is a `TextNode`, then uses the default Base64 variant instead of what's specified. It seems the correct behavior would be to call `TextNode#getBinaryValue` instead.",
        "issue_url": null,
        "comments": [
            {
                "content": "Sounds like a flaw indeed. Just need to see how/where to access base64 variant."
            },
            {
                "content": "If you change the assignment of `byte[] data` to:\r\n\r\n```java\r\n            byte[] data = n.isTextual()\r\n                    ? ((TextNode) n).getBinaryValue(b64variant)\r\n                    : n.binaryValue();\r\n```\r\n\r\nThat seems sufficient, though I am operating on 2.6 and it looks like `ObjectMapper` has undergone a lot of refactoring since then. Presumably it would propagate the variant down correctly to the method."
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to change the assignment of the `byte[] data` to include a call to the `getBinaryValue` method with the `b64variant` parameter."
    },
    "Math_21_src/main/java/org/apache/commons/math3/linear/RectangularCholeskyDecomposition.java_62_151": {
        "src": "public RectangularCholeskyDecomposition(RealMatrix matrix, double small)\n        throws NonPositiveDefiniteMatrixException {\n\n        final int order = matrix.getRowDimension();\n        final double[][] c = matrix.getData();\n        final double[][] b = new double[order][order];\n\n        int[] swap  = new int[order];\n        int[] index = new int[order];\n        for (int i = 0; i < order; ++i) {\n            index[i] = i;\n        }\n\n        int r = 0;\n        for (boolean loop = true; loop;) {\n\n            // find maximal diagonal element\n            swap[r] = r;\n            for (int i = r + 1; i < order; ++i) {\n                int ii  = index[i];\n                int isi = index[swap[i]];\n                if (c[ii][ii] > c[isi][isi]) {\n                    swap[r] = i;\n                }\n            }\n\n\n            // swap elements\n            if (swap[r] != r) {\n                int tmp = index[r];\n                index[r] = index[swap[r]];\n                index[swap[r]] = tmp;\n            }\n\n            // check diagonal element\n            int ir = index[r];\n            if (c[ir][ir] < small) {\n\n                if (r == 0) {\n                    throw new NonPositiveDefiniteMatrixException(c[ir][ir], ir, small);\n                }\n\n                // check remaining diagonal elements\n                for (int i = r; i < order; ++i) {\n                    if (c[index[i]][index[i]] < -small) {\n                        // there is at least one sufficiently negative diagonal element,\n                        // the symmetric positive semidefinite matrix is wrong\n                        throw new NonPositiveDefiniteMatrixException(c[index[i]][index[i]], i, small);\n                    }\n                }\n\n                // all remaining diagonal elements are close to zero, we consider we have\n                // found the rank of the symmetric positive semidefinite matrix\n                ++r;\n                loop = false;\n\n            } else {\n\n                // transform the matrix\n                final double sqrt = FastMath.sqrt(c[ir][ir]);\n                b[r][r] = sqrt;\n                final double inverse  = 1 / sqrt;\n                for (int i = r + 1; i < order; ++i) {\n                    final int ii = index[i];\n                    final double e = inverse * c[ii][ir];\n                    b[i][r] = e;\n                    c[ii][ii] -= e * e;\n                    for (int j = r + 1; j < i; ++j) {\n                        final int ij = index[j];\n                        final double f = c[ii][ij] - e * b[j][r];\n                        c[ii][ij] = f;\n                        c[ij][ii] = f;\n                    }\n                }\n\n                // prepare next iteration\n                loop = ++r < order;\n            }\n        }\n\n        // build the root matrix\n        rank = r;\n        root = MatrixUtils.createRealMatrix(order, r);\n        for (int i = 0; i < order; ++i) {\n            for (int j = 0; j < r; ++j) {\n                root.setEntry(index[i], j, b[i][j]);\n            }\n        }\n\n    }",
        "src_wo_comments": "public RectangularCholeskyDecomposition ( RealMatrix matrix , double small ) throws NonPositiveDefiniteMatrixException { final int order = matrix . getRowDimension ( ) ; final double [ ] [ ] c = matrix . getData ( ) ; final double [ ] [ ] b = new double [ order ] [ order ] ; int [ ] swap = new int [ order ] ; int [ ] index = new int [ order ] ; for ( int i = 0 ; i < order ; ++ i ) { index [ i ] = i ; } int r = 0 ; for ( boolean loop = true ; loop ; ) { swap [ r ] = r ; for ( int i = r + 1 ; i < order ; ++ i ) { int ii = index [ i ] ; int isi = index [ swap [ i ] ] ; if ( c [ ii ] [ ii ] > c [ isi ] [ isi ] ) { swap [ r ] = i ; } } if ( swap [ r ] != r ) { int tmp = index [ r ] ; index [ r ] = index [ swap [ r ] ] ; index [ swap [ r ] ] = tmp ; } int ir = index [ r ] ; if ( c [ ir ] [ ir ] < small ) { if ( r == 0 ) { throw new NonPositiveDefiniteMatrixException ( c [ ir ] [ ir ] , ir , small ) ; } for ( int i = r ; i < order ; ++ i ) { if ( c [ index [ i ] ] [ index [ i ] ] < - small ) { throw new NonPositiveDefiniteMatrixException ( c [ index [ i ] ] [ index [ i ] ] , i , small ) ; } } ++ r ; loop = false ; } else { final double sqrt = FastMath . sqrt ( c [ ir ] [ ir ] ) ; b [ r ] [ r ] = sqrt ; final double inverse = 1 / sqrt ; for ( int i = r + 1 ; i < order ; ++ i ) { final int ii = index [ i ] ; final double e = inverse * c [ ii ] [ ir ] ; b [ i ] [ r ] = e ; c [ ii ] [ ii ] -= e * e ; for ( int j = r + 1 ; j < i ; ++ j ) { final int ij = index [ j ] ; final double f = c [ ii ] [ ij ] - e * b [ j ] [ r ] ; c [ ii ] [ ij ] = f ; c [ ij ] [ ii ] = f ; } } loop = ++ r < order ; } } rank = r ; root = MatrixUtils . createRealMatrix ( order , r ) ; for ( int i = 0 ; i < order ; ++ i ) { for ( int j = 0 ; j < r ; ++ j ) { root . setEntry ( index [ i ] , j , b [ i ] [ j ] ) ; } } }",
        "fixed_src": "public RectangularCholeskyDecomposition(RealMatrix matrix, double small)\n        throws NonPositiveDefiniteMatrixException {\n\n        final int order = matrix.getRowDimension();\n        final double[][] c = matrix.getData();\n        final double[][] b = new double[order][order];\n\n        int[] index = new int[order];\n        for (int i = 0; i < order; ++i) {\n            index[i] = i;\n        }\n\n        int r = 0;\n        for (boolean loop = true; loop;) {\n\n            // find maximal diagonal element\n            int swapR = r;\n            for (int i = r + 1; i < order; ++i) {\n                int ii  = index[i];\n                int isr = index[swapR];\n                if (c[ii][ii] > c[isr][isr]) {\n                    swapR = i;\n                }\n            }\n\n\n            // swap elements\n            if (swapR != r) {\n                final int tmpIndex    = index[r];\n                index[r]              = index[swapR];\n                index[swapR]          = tmpIndex;\n                final double[] tmpRow = b[r];\n                b[r]                  = b[swapR];\n                b[swapR]              = tmpRow;\n            }\n\n            // check diagonal element\n            int ir = index[r];\n            if (c[ir][ir] < small) {\n\n                if (r == 0) {\n                    throw new NonPositiveDefiniteMatrixException(c[ir][ir], ir, small);\n                }\n\n                // check remaining diagonal elements\n                for (int i = r; i < order; ++i) {\n                    if (c[index[i]][index[i]] < -small) {\n                        // there is at least one sufficiently negative diagonal element,\n                        // the symmetric positive semidefinite matrix is wrong\n                        throw new NonPositiveDefiniteMatrixException(c[index[i]][index[i]], i, small);\n                    }\n                }\n\n                // all remaining diagonal elements are close to zero, we consider we have\n                // found the rank of the symmetric positive semidefinite matrix\n                ++r;\n                loop = false;\n\n            } else {\n\n                // transform the matrix\n                final double sqrt = FastMath.sqrt(c[ir][ir]);\n                b[r][r] = sqrt;\n                final double inverse  = 1 / sqrt;\n                final double inverse2 = 1 / c[ir][ir];\n                for (int i = r + 1; i < order; ++i) {\n                    final int ii = index[i];\n                    final double e = inverse * c[ii][ir];\n                    b[i][r] = e;\n                    c[ii][ii] -= c[ii][ir] * c[ii][ir] * inverse2;\n                    for (int j = r + 1; j < i; ++j) {\n                        final int ij = index[j];\n                        final double f = c[ii][ij] - e * b[j][r];\n                        c[ii][ij] = f;\n                        c[ij][ii] = f;\n                    }\n                }\n\n                // prepare next iteration\n                loop = ++r < order;\n            }\n        }\n\n        // build the root matrix\n        rank = r;\n        root = MatrixUtils.createRealMatrix(order, r);\n        for (int i = 0; i < order; ++i) {\n            for (int j = 0; j < r; ++j) {\n                root.setEntry(index[i], j, b[i][j]);\n            }\n        }\n\n    }",
        "fixed_src_wo_comments": "public RectangularCholeskyDecomposition ( RealMatrix matrix , double small ) throws NonPositiveDefiniteMatrixException { final int order = matrix . getRowDimension ( ) ; final double [ ] [ ] c = matrix . getData ( ) ; final double [ ] [ ] b = new double [ order ] [ order ] ; int [ ] index = new int [ order ] ; for ( int i = 0 ; i < order ; ++ i ) { index [ i ] = i ; } int r = 0 ; for ( boolean loop = true ; loop ; ) { int swapR = r ; for ( int i = r + 1 ; i < order ; ++ i ) { int ii = index [ i ] ; int isr = index [ swapR ] ; if ( c [ ii ] [ ii ] > c [ isr ] [ isr ] ) { swapR = i ; } } if ( swapR != r ) { final int tmpIndex = index [ r ] ; index [ r ] = index [ swapR ] ; index [ swapR ] = tmpIndex ; final double [ ] tmpRow = b [ r ] ; b [ r ] = b [ swapR ] ; b [ swapR ] = tmpRow ; } int ir = index [ r ] ; if ( c [ ir ] [ ir ] < small ) { if ( r == 0 ) { throw new NonPositiveDefiniteMatrixException ( c [ ir ] [ ir ] , ir , small ) ; } for ( int i = r ; i < order ; ++ i ) { if ( c [ index [ i ] ] [ index [ i ] ] < - small ) { throw new NonPositiveDefiniteMatrixException ( c [ index [ i ] ] [ index [ i ] ] , i , small ) ; } } ++ r ; loop = false ; } else { final double sqrt = FastMath . sqrt ( c [ ir ] [ ir ] ) ; b [ r ] [ r ] = sqrt ; final double inverse = 1 / sqrt ; final double inverse2 = 1 / c [ ir ] [ ir ] ; for ( int i = r + 1 ; i < order ; ++ i ) { final int ii = index [ i ] ; final double e = inverse * c [ ii ] [ ir ] ; b [ i ] [ r ] = e ; c [ ii ] [ ii ] -= c [ ii ] [ ir ] * c [ ii ] [ ir ] * inverse2 ; for ( int j = r + 1 ; j < i ; ++ j ) { final int ij = index [ j ] ; final double f = c [ ii ] [ ij ] - e * b [ j ] [ r ] ; c [ ii ] [ ij ] = f ; c [ ij ] [ ii ] = f ; } } loop = ++ r < order ; } } rank = r ; root = MatrixUtils . createRealMatrix ( order , r ) ; for ( int i = 0 ; i < order ; ++ i ) { for ( int j = 0 ; j < r ; ++ j ) { root . setEntry ( index [ i ] , j , b [ i ] [ j ] ) ; } } }",
        "summary": "Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix",
        "Description": "The following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples):\n\nArray2DRowRealMatrix{\n{0.0,0.0,0.0,0.0,0.0},\n{0.0,0.013445532,0.01039469,0.009881156,0.010499559},\n{0.0,0.01039469,0.023006616,0.008196856,0.010732709},\n{0.0,0.009881156,0.008196856,0.019023866,0.009210099},\n{0.0,0.010499559,0.010732709,0.009210099,0.019107243}}\n\n> cov(data1)\n   V1 V2 V3 V4 V5\nV1 0 0.000000000 0.00000000 0.000000000 0.000000000\nV2 0 0.013383931 0.01034401 0.009913271 0.010506733\nV3 0 0.010344006 0.02309479 0.008374730 0.010759306\nV4 0 0.009913271 0.00837473 0.019005488 0.009187287\nV5 0 0.010506733 0.01075931 0.009187287 0.019021483\n\nArray2DRowRealMatrix{\n{0.013445532,0.01039469,0.0,0.009881156,0.010499559},\n{0.01039469,0.023006616,0.0,0.008196856,0.010732709},\n{0.0,0.0,0.0,0.0,0.0},\n{0.009881156,0.008196856,0.0,0.019023866,0.009210099},\n{0.010499559,0.010732709,0.0,0.009210099,0.019107243}}\n\n> cov(data2)\n            V1 V2 V3 V4 V5\nV1 0.006922905 0.010507692 0 0.005817399 0.010330529\nV2 0.010507692 0.023428918 0 0.008273152 0.010735568\nV3 0.000000000 0.000000000 0 0.000000000 0.000000000\nV4 0.005817399 0.008273152 0 0.004929843 0.009048759\nV5 0.010330529 0.010735568 0 0.009048759 0.018683544 \n\nArray2DRowRealMatrix{\n{0.013445532,0.01039469,0.009881156,0.010499559},\n{0.01039469,0.023006616,0.008196856,0.010732709},\n{0.009881156,0.008196856,0.019023866,0.009210099},\n{0.010499559,0.010732709,0.009210099,0.019107243}}\n\n> cov(data3)\n            V1          V2          V3          V4\nV1 0.013445047 0.010478862 0.009955904 0.010529542\nV2 0.010478862 0.022910522 0.008610113 0.011046353\nV3 0.009955904 0.008610113 0.019250975 0.009464442\nV4 0.010529542 0.011046353 0.009464442 0.019260317\n\n\nI've traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):\n\nCorrelatedRandomVectorGenerator.getRootMatrix() = \nArray2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0},{0.0759577418122063,0.0876125188474239,0.0,0.0,0.0},{0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0},{0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0},{0.13822895138139477,0.0,0.0,0.0,0.0}}\nCorrelatedRandomVectorGenerator.getRank() = 5\n\nCorrelatedRandomVectorGenerator.getRootMatrix() = \nArray2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0},{0.07764443622513505,0.13029949164628746,0.0},{0.0,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.0},{0.13822895138139477,0.0,0.0}}\nCorrelatedRandomVectorGenerator.getRank() = 3\n\nCorrelatedRandomVectorGenerator.getRootMatrix() = \nArray2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785},{0.07764443622513505,0.13029949164628746,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0},{0.13822895138139477,0.0,0.0,0.0}}\nCorrelatedRandomVectorGenerator.getRank() = 4\n\nClearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don't know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the \"rectangular\" variant (also not at the links provided in the javadoc).",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-789",
        "comments": [
            "Hi Gert,\n\nthanks for the report. Could you please attach a test case for the described problem. This would really help investigating the problem.\n\nThanks,\n\nThomas",
            "Failing test case to reproduce bug (fails for covMatrix3).",
            "Thanks for the test.\n\nMy first investigation is as follows:\n\nin the RectangularCholeskyDecomposition class, the following code does not actually produce the maximal diagonal element:\n\n{noformat}\n   // find maximal diagonal element\n   swap[r] = r;\n   for (int i = r + 1; i < order; ++i) {\n       int ii = index[i];\n       int isi = index[swap[i]];\n       if (c[ii][ii] > c[isi][isi]) {\n         swap[r] = i;\n       }\n   }\n{noformat}\n\nthus the rank of the matrix is computed wrongly as the ordering of the columns is wrong and as a consequence the loop finishes too early. This can be fixed quite easily by changing index[swap[i]] to index[swap[r]].\n\nThe increment of r seems also to be wrong in the case the diagonal element is smaller than the user-defined limit.\n\nWhen making the changes, the rank is correct, but the resulting root matrix is not very good (root * root.transpose() != covariance), thus the transformation of the matrix has to be further reviewed (I did not figure it out yet).\n\nUnfortunately there is no unit test for the RectangularCholeskyDecomposition yet, so this should be added in the process of fixing this issue. ",
            "Fixed in subversion repository as of r1384363.\n\nThe problem was due to some missing permutations in the root matrix. It seems the old fix for MATH-226 was not good.\n\nThanks for the report, and thanks to Thomas for the identification of the wrong maximum element detection."
        ],
        "summarized_discussion": "\n\nThe bug in the source code was solved by changing the code to include permutations in the root matrix. The code was changed in the subversion repository as of r1384363. Thanks to Gert for reporting the bug and Thomas for helping to identify the wrong maximum element detection."
    },
    "JacksonDatabind_44_src/main/java/com/fasterxml/jackson/databind/type/SimpleType.java_121_141": {
        "src": "@Override\n    @Deprecated\n    protected JavaType _narrow(Class<?> subclass)\n    {\n        if (_class == subclass) {\n            return this;\n        }\n        // Should we check that there is a sub-class relationship?\n        // 15-Jan-2016, tatu: Almost yes, but there are some complications with\n        //    placeholder values (`Void`, `NoClass`), so can not quite do yet.\n        // TODO: fix in 2.8\n            /*\n            throw new IllegalArgumentException(\"Class \"+subclass.getName()+\" not sub-type of \"\n                    +_class.getName());\n                    */\n            return new SimpleType(subclass, _bindings, this, _superInterfaces,\n                    _valueHandler, _typeHandler, _asStatic);\n        // Otherwise, stitch together the hierarchy. First, super-class\n        // if not found, try a super-interface\n        // should not get here but...\n    }",
        "src_wo_comments": "@ Override @ Deprecated protected JavaType _narrow ( Class < ? > subclass ) { if ( _class == subclass ) { return this ; } return new SimpleType ( subclass , _bindings , this , _superInterfaces , _valueHandler , _typeHandler , _asStatic ) ; }",
        "fixed_src": "@Override\n    @Deprecated\n    protected JavaType _narrow(Class<?> subclass)\n    {\n        if (_class == subclass) {\n            return this;\n        }\n        // Should we check that there is a sub-class relationship?\n        // 15-Jan-2016, tatu: Almost yes, but there are some complications with\n        //    placeholder values (`Void`, `NoClass`), so can not quite do yet.\n        // TODO: fix in 2.8\n        if (!_class.isAssignableFrom(subclass)) {\n            /*\n            throw new IllegalArgumentException(\"Class \"+subclass.getName()+\" not sub-type of \"\n                    +_class.getName());\n                    */\n            return new SimpleType(subclass, _bindings, this, _superInterfaces,\n                    _valueHandler, _typeHandler, _asStatic);\n        }\n        // Otherwise, stitch together the hierarchy. First, super-class\n        Class<?> next = subclass.getSuperclass();\n        if (next == _class) { // straight up parent class? Great.\n            return new SimpleType(subclass, _bindings, this,\n                    _superInterfaces, _valueHandler, _typeHandler, _asStatic);\n        }\n        if ((next != null) && _class.isAssignableFrom(next)) {\n            JavaType superb = _narrow(next);\n            return new SimpleType(subclass, _bindings, superb,\n                    null, _valueHandler, _typeHandler, _asStatic);\n        }\n        // if not found, try a super-interface\n        Class<?>[] nextI = subclass.getInterfaces();\n        for (Class<?> iface : nextI) {\n            if (iface == _class) { // directly implemented\n                return new SimpleType(subclass, _bindings, null,\n                        new JavaType[] { this }, _valueHandler, _typeHandler, _asStatic);\n            }\n            if (_class.isAssignableFrom(iface)) { // indirect, so recurse\n                JavaType superb = _narrow(iface);\n                return new SimpleType(subclass, _bindings, null,\n                        new JavaType[] { superb }, _valueHandler, _typeHandler, _asStatic);\n            }\n        }\n        // should not get here but...\n        throw new IllegalArgumentException(\"Internal error: Can not resolve sub-type for Class \"+subclass.getName()+\" to \"\n                +_class.getName());\n    }",
        "fixed_src_wo_comments": "@ Override @ Deprecated protected JavaType _narrow ( Class < ? > subclass ) { if ( _class == subclass ) { return this ; } if ( ! _class . isAssignableFrom ( subclass ) ) { return new SimpleType ( subclass , _bindings , this , _superInterfaces , _valueHandler , _typeHandler , _asStatic ) ; } Class < ? > next = subclass . getSuperclass ( ) ; if ( next == _class ) { return new SimpleType ( subclass , _bindings , this , _superInterfaces , _valueHandler , _typeHandler , _asStatic ) ; } if ( ( next != null ) && _class . isAssignableFrom ( next ) ) { JavaType superb = _narrow ( next ) ; return new SimpleType ( subclass , _bindings , superb , null , _valueHandler , _typeHandler , _asStatic ) ; } Class < ? > [ ] nextI = subclass . getInterfaces ( ) ; for ( Class < ? > iface : nextI ) { if ( iface == _class ) { return new SimpleType ( subclass , _bindings , null , new JavaType [ ] { this } , _valueHandler , _typeHandler , _asStatic ) ; } if ( _class . isAssignableFrom ( iface ) ) { JavaType superb = _narrow ( iface ) ; return new SimpleType ( subclass , _bindings , null , new JavaType [ ] { superb } , _valueHandler , _typeHandler , _asStatic ) ; } } throw new IllegalArgumentException ( \"Internal error: Can not resolve sub-type for Class \" + subclass . getName ( ) + \" to \" + _class . getName ( ) ) ; }",
        "summary": "Problem with polymorphic types, losing properties from base type(s)",
        "Description": "(background, see: https://github.com/dropwizard/dropwizard/pull/1449)\n\nLooks like sub-type resolution may be broken for one particular case: that of using `defaultImpl`. If so, appears like properties from super-types are not properly resolved; guessing this could be follow-up item for #1083 (even sooner than I thought...).\n",
        "issue_url": null,
        "comments": [
            {
                "content": "While this could be related to #1128 that issue has now been resolved and there's no reproduction, so closing for now.\n"
            },
            {
                "content": "Second try: guessing this is related to handling of `StdSubtypeResolver`.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been resolved and the issue is being closed. The bug may have been related to the handling of StdSubtypeResolver."
    },
    "JacksonCore_4_src/main/java/com/fasterxml/jackson/core/util/TextBuffer.java_580_588": {
        "src": "public char[] expandCurrentSegment()\n    {\n        final char[] curr = _currentSegment;\n        // Let's grow by 50% by default\n        final int len = curr.length;\n        // but above intended maximum, slow to increase by 25%\n        int newLen = (len == MAX_SEGMENT_LEN) ? (MAX_SEGMENT_LEN+1) : Math.min(MAX_SEGMENT_LEN, len + (len >> 1));\n        return (_currentSegment = Arrays.copyOf(curr, newLen));\n    }",
        "src_wo_comments": "public char [ ] expandCurrentSegment ( ) { final char [ ] curr = _currentSegment ; final int len = curr . length ; int newLen = ( len == MAX_SEGMENT_LEN ) ? ( MAX_SEGMENT_LEN + 1 ) : Math . min ( MAX_SEGMENT_LEN , len + ( len >> 1 ) ) ; return ( _currentSegment = Arrays . copyOf ( curr , newLen ) ) ; }",
        "fixed_src": "public char[] expandCurrentSegment()\n    {\n        final char[] curr = _currentSegment;\n        // Let's grow by 50% by default\n        final int len = curr.length;\n        int newLen = len + (len >> 1);\n        // but above intended maximum, slow to increase by 25%\n        if (newLen > MAX_SEGMENT_LEN) {\n            newLen = len + (len >> 2);\n        }\n        return (_currentSegment = Arrays.copyOf(curr, newLen));\n    }",
        "fixed_src_wo_comments": "public char [ ] expandCurrentSegment ( ) { final char [ ] curr = _currentSegment ; final int len = curr . length ; int newLen = len + ( len >> 1 ) ; if ( newLen > MAX_SEGMENT_LEN ) { newLen = len + ( len >> 2 ) ; } return ( _currentSegment = Arrays . copyOf ( curr , newLen ) ) ; }",
        "summary": "What is the maximum key length allowed?",
        "Description": "I noticed that even in Jackson 2.4, if a JSON key is longer than 262144 bytes, ArrayIndexOutOfBoundsException is thrown from TextBuffer. Below is the stack trace:\n\n```\njava.lang.ArrayIndexOutOfBoundsException\n    at java.lang.System.arraycopy(Native Method)\n    at com.fasterxml.jackson.core.util.TextBuffer.expandCurrentSegment(TextBuffer.java:604)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.addName(UTF8StreamJsonParser.java:2034)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.findName(UTF8StreamJsonParser.java:1928)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.parseLongFieldName(UTF8StreamJsonParser.java:1534)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.parseMediumFieldName(UTF8StreamJsonParser.java:1502)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseFieldName(UTF8StreamJsonParser.java:1437)\n    at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:668)\n    ... <below are our code> ...\n```\n\nLooking at TextBuffer.expandCurrentSegment(TextBuffer.java:604), once the length of _currentSegment is increased to MAX_SEGMENT_LEN + 1 (262145) bytes, the newLen will stay at MAX_SEGMENT_LEN, which is smaller than len. Therefore System.arraycopy() will fail.\n\nI understand it is rare to have key larger than 262144 bytes, but it would be nice if\n- Jackson explicitly throw exception stating that key is too long.\n- Document that the maximum key length is 262144 bytes.\n\nOR\n- Update TextBuffer to support super long key.\n\nThanks!\n",
        "issue_url": null,
        "comments": [
            {
                "content": "No such limit is intended, so this sounds like a bug. Will fix it; no reason not to support whatever fits in memory. Note that maximum segment is not directly relevant to maximum key length -- the point of segments is not to require a single contiguous array, or intermediate copies.\n"
            },
            {
                "content": "Actually, looking at code, expansion is used for case of property names (where names are kept in a single buffer) -- not so for text values, but here it is done. Regardless, something to fix.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug is that there is an unintended limit on maximum key length. The solution is to fix the code so that whatever fits in memory is supported, and to not require a single contiguous array or intermediate copies. Additionally, expansion should be used for property names, not text values."
    },
    "Mockito_12_src/org/mockito/internal/util/reflection/GenericMaster.java_16_25": {
        "src": "public Class getGenericType(Field field) {        \n        Type generic = field.getGenericType();\n        if (generic != null && generic instanceof ParameterizedType) {\n            Type actual = ((ParameterizedType) generic).getActualTypeArguments()[0];\n                return (Class) actual;\n                //in case of nested generics we don't go deep\n        }\n        \n        return Object.class;\n    }",
        "src_wo_comments": "public Class getGenericType ( Field field ) { Type generic = field . getGenericType ( ) ; if ( generic != null && generic instanceof ParameterizedType ) { Type actual = ( ( ParameterizedType ) generic ) . getActualTypeArguments ( ) [ 0 ] ; return ( Class ) actual ; } return Object . class ; }",
        "fixed_src": "public Class getGenericType(Field field) {        \n        Type generic = field.getGenericType();\n        if (generic != null && generic instanceof ParameterizedType) {\n            Type actual = ((ParameterizedType) generic).getActualTypeArguments()[0];\n            if (actual instanceof Class) {\n                return (Class) actual;\n            } else if (actual instanceof ParameterizedType) {\n                //in case of nested generics we don't go deep\n                return (Class) ((ParameterizedType) actual).getRawType();\n            }\n        }\n        \n        return Object.class;\n    }",
        "fixed_src_wo_comments": "public Class getGenericType ( Field field ) { Type generic = field . getGenericType ( ) ; if ( generic != null && generic instanceof ParameterizedType ) { Type actual = ( ( ParameterizedType ) generic ) . getActualTypeArguments ( ) [ 0 ] ; if ( actual instanceof Class ) { return ( Class ) actual ; } else if ( actual instanceof ParameterizedType ) { return ( Class ) ( ( ParameterizedType ) actual ) . getRawType ( ) ; } } return Object . class ; }",
        "summary": "ArgumentCaptor no longer working for varargs",
        "Description": "I ran into the issue described here: http://stackoverflow.com/questions/27303562/why-does-upgrading-mockito-from-1-9-5-to-1-10-8-break-this-captor\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Hi sorry for the late reply.\nI reproduced the issue, not sure when I will be able to fix though.\n"
            },
            {
                "content": "fixed by #211 \n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been fixed by commit #211."
    },
    "JacksonDatabind_91_src/main/java/com/fasterxml/jackson/databind/deser/DeserializerCache.java_536_546": {
        "src": "private boolean _hasCustomHandlers(JavaType t) {\n        if (t.isContainerType()) {\n            // First: value types may have both value and type handlers\n            JavaType ct = t.getContentType();\n            if (ct != null) {\n                return (ct.getValueHandler() != null) || (ct.getTypeHandler() != null);\n            // Second: map(-like) types may have value handler for key (but not type; keys are untyped)\n            }\n        }\n        return false;\n    }",
        "src_wo_comments": "private boolean _hasCustomHandlers ( JavaType t ) { if ( t . isContainerType ( ) ) { JavaType ct = t . getContentType ( ) ; if ( ct != null ) { return ( ct . getValueHandler ( ) != null ) || ( ct . getTypeHandler ( ) != null ) ; } } return false ; }",
        "fixed_src": "private boolean _hasCustomHandlers(JavaType t) {\n        if (t.isContainerType()) {\n            // First: value types may have both value and type handlers\n            JavaType ct = t.getContentType();\n            if (ct != null) {\n                if ((ct.getValueHandler() != null) || (ct.getTypeHandler() != null)) {\n                    return true;\n                }\n            }\n            // Second: map(-like) types may have value handler for key (but not type; keys are untyped)\n            if (t.isMapLikeType()) {\n                JavaType kt = t.getKeyType();\n                if (kt.getValueHandler() != null) {\n                    return true;\n                }\n            }\n        }\n        return false;\n    }",
        "fixed_src_wo_comments": "private boolean _hasCustomHandlers ( JavaType t ) { if ( t . isContainerType ( ) ) { JavaType ct = t . getContentType ( ) ; if ( ct != null ) { if ( ( ct . getValueHandler ( ) != null ) || ( ct . getTypeHandler ( ) != null ) ) { return true ; } } if ( t . isMapLikeType ( ) ) { JavaType kt = t . getKeyType ( ) ; if ( kt . getValueHandler ( ) != null ) { return true ; } } } return false ; }",
        "summary": "2.9.2 deserialization regression",
        "Description": "There seems to be a regression in the latest 2.9.2 release.\r\n\r\nUsing `org.apache.logging.log4j.core.jackson.Log4jJsonObjectMapper` from `org.apache.logging.log4j:log4j-core:2.9.1` to deserialize the appended JSON object is throwing an exception with 2.9.2 but worked with 2.9.1.\r\n\r\n`org.apache.logging.log4j.core.jackson.Log4jYamlObjectMapper` and `org.apache.logging.log4j.core.jackson.Log4jXmlObjectMapper` fail in similar ways.\r\n\r\n### inputString\r\n```json\r\n{\r\n  \"timeMillis\" : 1493121664118,\r\n  \"thread\" : \"main\",\r\n  \"threadId\" : 1,\r\n  \"threadPriority\" : 5,\r\n  \"level\" : \"INFO\",\r\n  \"loggerName\" : \"HelloWorld\",\r\n  \"marker\" : {\r\n    \"name\" : \"child\",\r\n    \"parents\" : [ {\r\n      \"name\" : \"parent\",\r\n      \"parents\" : [ {\r\n        \"name\" : \"grandparent\"\r\n      } ]\r\n    } ]\r\n  },\r\n  \"message\" : \"Hello, world!\",\r\n  \"thrown\" : {\r\n    \"commonElementCount\" : 0,\r\n    \"message\" : \"error message\",\r\n    \"name\" : \"java.lang.RuntimeException\",\r\n    \"extendedStackTrace\" : [ {\r\n      \"class\" : \"logtest.Main\",\r\n      \"method\" : \"main\",\r\n      \"file\" : \"Main.java\",\r\n      \"line\" : 29,\r\n      \"exact\" : true,\r\n      \"location\" : \"classes/\",\r\n      \"version\" : \"?\"\r\n    } ]\r\n  },\r\n  \"contextStack\" : [ \"one\", \"two\" ],\r\n  \"loggerFqcn\" : \"org.apache.logging.log4j.spi.AbstractLogger\",\r\n  \"endOfBatch\" : false,\r\n  \"contextMap\" : {\r\n    \"bar\" : \"BAR\",\r\n    \"foo\" : \"FOO\"\r\n  },\r\n  \"source\" : {\r\n    \"class\" : \"logtest.Main\",\r\n    \"method\" : \"main\",\r\n    \"file\" : \"Main.java\",\r\n    \"line\" : 29\r\n  }\r\n}\r\n```\r\n\r\n### Exception\r\n```\r\norg.apache.logging.log4j.core.parser.ParseException: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot construct instance of `org.apache.logging.log4j.Level` (although at least one Creator exists): no String-argument constructor/factory method to deserialize from String value ('INFO')\r\n at [Source: (byte[])\"{\r\n  \"timeMillis\" : 1493121664118,\r\n  \"thread\" : \"main\",\r\n  \"threadId\" : 1,\r\n  \"threadPriority\" : 5,\r\n  \"level\" : \"INFO\",\r\n  \"loggerName\" : \"HelloWorld\",\r\n  \"marker\" : {\r\n    \"name\" : \"child\",\r\n    \"parents\" : [ {\r\n      \"name\" : \"parent\",\r\n      \"parents\" : [ {\r\n        \"name\" : \"grandparent\"\r\n      } ]\r\n    } ]\r\n  },\r\n  \"message\" : \"Hello, world!\",\r\n  \"thrown\" : {\r\n    \"commonElementCount\" : 0,\r\n    \"message\" : \"error message\",\r\n    \"name\" : \"java.lang.RuntimeException\",\r\n    \"extendedStackTrace\" : [ {\r\n      \"clas\"[truncated 482 bytes]; line: 6, column: 13] (through reference chain: org.apache.logging.log4j.core.impl.Log4jLogEvent[\"level\"])\r\n```\r\n\r\n### parsing pseudo code\r\n```java\r\nimport org.apache.logging.log4j.core.LogEvent;\r\nimport org.apache.logging.log4j.core.parser.LogEventParser;\r\nimport org.apache.logging.log4j.core.parser.JsonLogEventParser;\r\nimport java.nio.charset.StandardCharsets;\r\n\r\nLogEventParser parser = new JsonLogEventParser();\r\nLogEvent result = parser.parseFrom(inputString.getBytes(StandardCharsets.UTF_8));\r\nassert result != null;\r\n```\r\n",
        "issue_url": null,
        "comments": [
            {
                "content": "That would seem to be about not detecting single-String constructor (or `valueOf` static method) of `Level` class... there was a change to creator detection, so I suspect that would be the root cause.\r\nBut one of the things there, if I remember correctly, had to do with proper visibility checking wrt creators. I hope to look into this soon, as breakage is obviously not planned (and is unfortunate for a patch release: we try to keep patches non-intrusive). Unfortunately creator-detection code is rather delicate area... so figuring out safe fixes is challenging.\r\n\r\n\r\n"
            },
            {
                "content": "Ok, looks like `log4j` v2 does have `public` factory method:\r\n\r\nhttps://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#valueOf(java.lang.String)\r\n\r\nwhich should be detectable. I think v1 did not. But I assume this for version 2.x?\r\n\r\nIf so, this definitely looks like a bug. I hope to create a stand-alone test case to verify, then fix for 2.9.3.\r\n"
            },
            {
                "content": "Yep, it's `org.apache.logging.log4j:log4j-core:2.9.1`, so log4j v2.\r\n\r\nIt's a bit confusing that log4j2 and jackson versions match up so neatly at the moment. So log4j 2.9.1 works with jackson 2.9.1 but fails with jackson 2.9.2."
            },
            {
                "content": "@huxi heh, funny alignment. I hope to resolve this relatively quickly, and get patch out within two weeks or so."
            },
            {
                "content": "Also, I think #1810 is either same or just closely related."
            },
            {
                "content": "Hmmh. So, unfortunately I can not reproduce this with a simple(r) reproduction -- adding both constructor and static method chooses constructor. Or, as in this case, adding `private` 2-arg constructor does not block discovery.\r\n\r\nI guess the only other possibility I wonder about is whether Jackson parameter name module might be in use?\r\n\r\nSo I think I need a reproduction here\r\n"
            },
            {
                "content": "I suppose you mean a simpler reproduction than something like this:\r\n\r\n#### build.gradle\r\n```groovy\r\napply plugin: 'groovy'\r\n\r\nrepositories {\r\n\tmavenCentral()\r\n}\r\n\r\ndependencies {\r\n\ttestCompile 'com.fasterxml.jackson.core:jackson-databind:2.9.2'\r\n\ttestCompile 'com.fasterxml.jackson.core:jackson-annotations:2.9.2'\r\n\ttestCompile 'org.apache.logging.log4j:log4j-core:2.9.1'\r\n\t\r\n\ttestCompile 'org.codehaus.groovy:groovy:2.4.12'\r\n\ttestCompile 'org.spockframework:spock-core:1.1-groovy-2.4', {\r\n\t\texclude group: 'org.codehaus.groovy', module: 'groovy-all' \r\n\t}\r\n\ttestCompile 'junit:junit:4.12'\r\n}\r\n```\r\n\r\n#### /src/test/groovy/foo/RegressionSpec.groovy\r\n```groovy\r\npackage foo\r\n\r\nimport java.nio.charset.StandardCharsets\r\nimport org.apache.logging.log4j.core.LogEvent\r\nimport org.apache.logging.log4j.core.parser.JsonLogEventParser\r\nimport org.apache.logging.log4j.core.parser.LogEventParser\r\nimport spock.lang.Specification\r\n\r\nclass RegressionSpec extends Specification {\r\n\t\r\n\tdef 'deserialization works'() {\r\n\t\twhen:\r\n\t\tLogEventParser parser = new JsonLogEventParser()\r\n\t\tLogEvent result = parser.parseFrom(inputString.getBytes(StandardCharsets.UTF_8))\r\n\t\t\r\n\t\tthen:\r\n\t\tnoExceptionThrown()\r\n\t\tresult != null\r\n\t\t\r\n\t\twhere:\r\n\t\tinputString = \"\"\"\r\n{\r\n  \"timeMillis\" : 1493121664118,\r\n  \"thread\" : \"main\",\r\n  \"threadId\" : 1,\r\n  \"threadPriority\" : 5,\r\n  \"level\" : \"INFO\",\r\n  \"loggerName\" : \"HelloWorld\",\r\n  \"marker\" : {\r\n    \"name\" : \"child\",\r\n    \"parents\" : [ {\r\n      \"name\" : \"parent\",\r\n      \"parents\" : [ {\r\n        \"name\" : \"grandparent\"\r\n      } ]\r\n    } ]\r\n  },\r\n  \"message\" : \"Hello, world!\",\r\n  \"thrown\" : {\r\n    \"commonElementCount\" : 0,\r\n    \"message\" : \"error message\",\r\n    \"name\" : \"java.lang.RuntimeException\",\r\n    \"extendedStackTrace\" : [ {\r\n      \"class\" : \"logtest.Main\",\r\n      \"method\" : \"main\",\r\n      \"file\" : \"Main.java\",\r\n      \"line\" : 29,\r\n      \"exact\" : true,\r\n      \"location\" : \"classes/\",\r\n      \"version\" : \"?\"\r\n    } ]\r\n  },\r\n  \"contextStack\" : [ \"one\", \"two\" ],\r\n  \"loggerFqcn\" : \"org.apache.logging.log4j.spi.AbstractLogger\",\r\n  \"endOfBatch\" : false,\r\n  \"contextMap\" : {\r\n    \"bar\" : \"BAR\",\r\n    \"foo\" : \"FOO\"\r\n  },\r\n  \"source\" : {\r\n    \"class\" : \"logtest.Main\",\r\n    \"method\" : \"main\",\r\n    \"file\" : \"Main.java\",\r\n    \"line\" : 29\r\n  }\r\n}\r\n\t\t\"\"\".trim()\r\n\t}\r\n}\r\n```\r\n\r\n#### `gradle dependencies` output\r\n```\r\ntestCompileClasspath - Compile classpath for source set 'test'.\r\n+--- com.fasterxml.jackson.core:jackson-databind:2.9.2\r\n|    +--- com.fasterxml.jackson.core:jackson-annotations:2.9.0 -> 2.9.2\r\n|    \\--- com.fasterxml.jackson.core:jackson-core:2.9.2\r\n+--- com.fasterxml.jackson.core:jackson-annotations:2.9.2\r\n+--- org.apache.logging.log4j:log4j-core:2.9.1\r\n|    \\--- org.apache.logging.log4j:log4j-api:2.9.1\r\n+--- org.codehaus.groovy:groovy:2.4.12\r\n+--- org.spockframework:spock-core:1.1-groovy-2.4\r\n|    \\--- junit:junit:4.12\r\n|         \\--- org.hamcrest:hamcrest-core:1.3\r\n\\--- junit:junit:4.12 (*)\r\n```\r\n\r\nOne strange thing I just saw while implementing this is that `com.fasterxml.jackson.core:jackson-databind:2.9.2` adds a transitive dependency on `com.fasterxml.jackson.core:jackson-annotations:2.9.0` instead of `2.9.2` for some reason.\r\nBut upgrading that dependency manually didn't fix anything either."
            },
            {
                "content": "I can start with this, thank you!\r\n\r\nOn annotations: dependency is intentional, there are no planned changes across versions, and for 3.x patch version will be dropped from annotations. So 2.9.2 and 2.9.0 are identical (plus it's just annotation type definitions anyway).\r\n"
            },
            {
                "content": "Ok. Wow. Yes, I can see how simplifying is challenging, lots of mix-ins, configuration used (to avoid having to add annotations directly in types). I can get test as-is to fail but will need to simplify significantly to see what is going on.\r\n\r\nFwtw looks like pertinent mix-in part here is:\r\n\r\n```java\r\nabstract class LevelMixIn {\r\n  @JsonCreator\r\n    public static Level getLevel(@JsonProperty(\"name\") final String name) {\r\n        return null;\r\n    }\r\n}\r\n```\r\n\r\nand I _think_ it may actually be due to mix-in being wrong, and not implementation -- but so that earlier Jackson introspection code did not properly handle it. So, with `2.9.1`, this was somehow either taken as delegating-creator anyway, I think.\r\n\r\nIf I am right, intent here is to use delegating creator and NOT property-based one: but annotation indicates property-based one. I'll verify, but fix should be either:\r\n\r\n1. Remove `@JsonProperty` from `name` argument and/or\r\n2. Add `mode = JsonCreator.Mode.DELEGATING`\r\n\r\n(I would do both).\r\n\r\n\r\n\r\n"
            },
            {
                "content": "Yes: was able to create simplified version for just `Level`, and both changes will make behavior work as expected.\r\n\r\nI think that changes should be safe wrt earlier Jackson versions as well: I am surprised that behavior changed, as it seems it should not have worked the way it used to be. But the new behavior seems more correct.\r\n\r\nIt is possible (and probably likely) same problem could exist within other mixins; I'll have a quick look if I can see others.\r\n"
            },
            {
                "content": "Did not find others: `MarkerMixIn` looked similar, but I think it should use properties-based creator, unlike `Level`."
            },
            {
                "content": "Thank you for the research, I will test out the changes you suggest."
            },
            {
                "content": "I changed our code in Log4j 2 and was then able to update to Jackson 2.9.2. Thank you all."
            },
            {
                "content": "@garydgregory Thank you for following up on this and verifying my hypothesis was correct."
            }
        ],
        "summarized_discussion": "\n\nThe bug in the source code was related to the detection of the single-String constructor or valueOf static method of the Level class. The root cause was related to proper visibility checking with regards to creators. The solution was to remove the @JsonProperty from the \"name\" argument and/or add mode = JsonCreator.Mode.DELEGATING to the code. After making these changes, the code was able to update to Jackson 2.9.2."
    },
    "Compress_14_src/main/java/org/apache/commons/compress/archivers/tar/TarUtils.java_56_112": {
        "src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        boolean allNUL = true;\n        for (int i = start; i < end; i++){\n            if (buffer[i] != 0){\n                allNUL = false;\n                break;\n            }\n        }\n        if (allNUL) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Must have trailing NUL or space\n        byte trailer;\n        trailer = buffer[end-1];\n        if (trailer == 0 || trailer == ' '){\n            end--;\n        } else {\n            throw new IllegalArgumentException(\n                    exceptionMessage(buffer, offset, length, end-1, trailer));\n        }\n        // May have additional NUL or space\n        trailer = buffer[end-1];\n        if (trailer == 0 || trailer == ' '){\n            end--;\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } boolean allNUL = true ; for ( int i = start ; i < end ; i ++ ) { if ( buffer [ i ] != 0 ) { allNUL = false ; break ; } } if ( allNUL ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer ; trailer = buffer [ end - 1 ] ; if ( trailer == 0 || trailer == ' ' ) { end -- ; } else { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , end - 1 , trailer ) ) ; } trailer = buffer [ end - 1 ] ; if ( trailer == 0 || trailer == ' ' ) { end -- ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "fixed_src": "public static long parseOctal(final byte[] buffer, final int offset, final int length) {\n        long    result = 0;\n        int     end = offset + length;\n        int     start = offset;\n\n        if (length < 2){\n            throw new IllegalArgumentException(\"Length \"+length+\" must be at least 2\");\n        }\n\n        if (buffer[start] == 0) {\n            return 0L;\n        }\n\n        // Skip leading spaces\n        while (start < end){\n            if (buffer[start] == ' '){\n                start++;\n            } else {\n                break;\n            }\n        }\n\n        // Must have trailing NUL or space\n        byte trailer;\n        trailer = buffer[end-1];\n        if (trailer == 0 || trailer == ' '){\n            end--;\n        } else {\n            throw new IllegalArgumentException(\n                    exceptionMessage(buffer, offset, length, end-1, trailer));\n        }\n        // May have additional NUL or space\n        trailer = buffer[end-1];\n        if (trailer == 0 || trailer == ' '){\n            end--;\n        }\n\n        for ( ;start < end; start++) {\n            final byte currentByte = buffer[start];\n            // CheckStyle:MagicNumber OFF\n            if (currentByte < '0' || currentByte > '7'){\n                throw new IllegalArgumentException(\n                        exceptionMessage(buffer, offset, length, start, currentByte));\n            }\n            result = (result << 3) + (currentByte - '0'); // convert from ASCII\n            // CheckStyle:MagicNumber ON\n        }\n\n        return result;\n    }",
        "fixed_src_wo_comments": "public static long parseOctal ( final byte [ ] buffer , final int offset , final int length ) { long result = 0 ; int end = offset + length ; int start = offset ; if ( length < 2 ) { throw new IllegalArgumentException ( \"Length \" + length + \" must be at least 2\" ) ; } if ( buffer [ start ] == 0 ) { return 0L ; } while ( start < end ) { if ( buffer [ start ] == ' ' ) { start ++ ; } else { break ; } } byte trailer ; trailer = buffer [ end - 1 ] ; if ( trailer == 0 || trailer == ' ' ) { end -- ; } else { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , end - 1 , trailer ) ) ; } trailer = buffer [ end - 1 ] ; if ( trailer == 0 || trailer == ' ' ) { end -- ; } for ( ; start < end ; start ++ ) { final byte currentByte = buffer [ start ] ; if ( currentByte < '0' || currentByte > '7' ) { throw new IllegalArgumentException ( exceptionMessage ( buffer , offset , length , start , currentByte ) ) ; } result = ( result << 3 ) + ( currentByte - '0' ) ; } return result ; }",
        "summary": "Tar files created by AIX native tar, and which contain symlinks, cannot be read by TarArchiveInputStream",
        "Description": "A simple tar file created on AIX using the native ({{/usr/bin/tar}} tar utility) *and* which contains a symbolic link, cannot be loaded by TarArchiveInputStream:\n\n{noformat}\njava.io.IOException: Error detected parsing the header\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:201)\n\tat Extractor.extract(Extractor.java:13)\n\tat Extractor.main(Extractor.java:28)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.tools.ant.taskdefs.ExecuteJava.run(ExecuteJava.java:217)\n\tat org.apache.tools.ant.taskdefs.ExecuteJava.execute(ExecuteJava.java:152)\n\tat org.apache.tools.ant.taskdefs.Java.run(Java.java:771)\n\tat org.apache.tools.ant.taskdefs.Java.executeJava(Java.java:221)\n\tat org.apache.tools.ant.taskdefs.Java.executeJava(Java.java:135)\n\tat org.apache.tools.ant.taskdefs.Java.execute(Java.java:108)\n\tat org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)\n\tat org.apache.tools.ant.Task.perform(Task.java:348)\n\tat org.apache.tools.ant.Target.execute(Target.java:390)\n\tat org.apache.tools.ant.Target.performTasks(Target.java:411)\n\tat org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)\n\tat org.apache.tools.ant.Project.executeTarget(Project.java:1368)\n\tat org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)\n\tat org.apache.tools.ant.Project.executeTargets(Project.java:1251)\n\tat org.apache.tools.ant.Main.runBuild(Main.java:809)\n\tat org.apache.tools.ant.Main.startAnt(Main.java:217)\n\tat org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)\n\tat org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)\nCaused by: java.lang.IllegalArgumentException: Invalid byte 0 at offset 0 in '{NUL}1722000726 ' len=12\n\tat org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:99)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:819)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:314)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:199)\n\t... 29 more\n{noformat}\n\nTested with 1.2 and the 1.4 nightly build from Feb 23 ({{Implementation-Build: trunk@r1292625; 2012-02-23 03:20:30+0000}})",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-181",
        "comments": [
            "An example tar file, containing a symbolic link, created by the AIX native tar utility",
            "The problem field is the modification time for the symbolic link, which Compress expects to be either all null or valid octal with trailing null/space.\n\nNote: 7zip reads the file OK but complains that there is data after the end of the archive.\nIt treats the link field as having no mtime.\n\nI've yet to find any documentation that says a leading null is allowed.\nPerhaps AIX tar is being lazy and failing to null the whole mtime field (which Compress could handle).",
            "GNU tar extracts it with a date/time of 1978-02-15 08:55 - which more or less looks as if it had translated the leading null to an ASCII 0 (and it looks as if that was supposed to be an ASCII 1 to match the timestamp of the dir).\n\n",
            "GNU tar from_header in list.c contains a workaround for this case:\n\n{noformat}\n  /* Accommodate buggy tar of unknown vintage, which outputs leading\n     NUL if the previous field overflows.  */\n  where += !*where;\n{noformat}\n\nthis basically skips the first byte if it is a binary 0.",
            "I don't think the previous field has overflowed in this case - it's all ascii '0'.\n\nNot sure that the GNU tar approach is sensible as the time value is then meaningless.\nI think it would be better to skip the entire field.\n\nBut do we just skip the field (as if it were all null), or skip it only on \"non-strict\" mode, or perhaps generate some kind of warning?",
            "It doesn't look like an overflow was the reason but if you look at the timestamp it certainly reads as if the first byte was a binary 0 by accident (if you put an ASCII 1 in there it is identical to the timestamp of the directory).\n\nIn any case the resulting timestamp is not what it used to be, so using any other timestamp would be as valid as trying to parse the rest.",
            "I agree - it looks like the first byte was set to null accidentally, or possibly the intention was to invalidate the stamp.\n\nbq. In any case the resulting timestamp is not what it used to be, so using any other timestamp would be as valid as trying to parse the rest.\n\nSorry, I don't follow.\n\nI'm suggesting that using the corrupted timestamp is worse than ignoring it by treating it as all nulls.",
            "We don't really have an option to ignore a timestamp unless we allow ArchiveEntry#getLastModifiedDate to return null.\n\nWhat I was trying to say is it doesn't matter much which timestamp we return as any choice is wrong.  Returning the equivalent of a 0 timestamp is fine with me.  Unfortunately we don't have an infrastructure for warnings (would have been good for COMPRESS-176 as well), something for an API redesign in 2.0, I guess.",
            "OK, let's return the timestamp as 0.\nThis will presumably display as 1970-01-01 00:00.",
            "Robert, could you delete and re-add the attachment, granting the ASF a license to include it this time?  That way we could add the tar to our testsuite.",
            "Re-uploaded example archive, with license grant so it can be included in the tests for the project.",
            "fixed with svn revision 1296420\n\nThanks for the testcase"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to return the timestamp as 0, which will display as 1970-01-01 00:00. Robert re-uploaded the example archive with a license grant so it can be included in the tests for the project, and the bug was fixed with svn revision 1296420."
    },
    "Csv_11_src/main/java/org/apache/commons/csv/CSVParser.java_359_394": {
        "src": "private Map<String, Integer> initializeHeader() throws IOException {\n        Map<String, Integer> hdrMap = null;\n        final String[] formatHeader = this.format.getHeader();\n        if (formatHeader != null) {\n            hdrMap = new LinkedHashMap<String, Integer>();\n\n            String[] headerRecord = null;\n            if (formatHeader.length == 0) {\n                // read the header from the first line of the file\n                final CSVRecord nextRecord = this.nextRecord();\n                if (nextRecord != null) {\n                    headerRecord = nextRecord.values();\n                }\n            } else {\n                if (this.format.getSkipHeaderRecord()) {\n                    this.nextRecord();\n                }\n                headerRecord = formatHeader;\n            }\n\n            // build the name to index mappings\n            if (headerRecord != null) {\n                for (int i = 0; i < headerRecord.length; i++) {\n                    final String header = headerRecord[i];\n                    final boolean containsHeader = hdrMap.containsKey(header);\n                    final boolean emptyHeader = header.trim().isEmpty();\n                    if (containsHeader && (!emptyHeader || (emptyHeader && !this.format.getIgnoreEmptyHeaders()))) {\n                        throw new IllegalArgumentException(\"The header contains a duplicate name: \\\"\" + header +\n                                \"\\\" in \" + Arrays.toString(headerRecord));\n                    }\n                    hdrMap.put(header, Integer.valueOf(i));\n                }\n            }\n        }\n        return hdrMap;\n    }",
        "src_wo_comments": "private Map < String , Integer > initializeHeader ( ) throws IOException { Map < String , Integer > hdrMap = null ; final String [ ] formatHeader = this . format . getHeader ( ) ; if ( formatHeader != null ) { hdrMap = new LinkedHashMap < String , Integer > ( ) ; String [ ] headerRecord = null ; if ( formatHeader . length == 0 ) { final CSVRecord nextRecord = this . nextRecord ( ) ; if ( nextRecord != null ) { headerRecord = nextRecord . values ( ) ; } } else { if ( this . format . getSkipHeaderRecord ( ) ) { this . nextRecord ( ) ; } headerRecord = formatHeader ; } if ( headerRecord != null ) { for ( int i = 0 ; i < headerRecord . length ; i ++ ) { final String header = headerRecord [ i ] ; final boolean containsHeader = hdrMap . containsKey ( header ) ; final boolean emptyHeader = header . trim ( ) . isEmpty ( ) ; if ( containsHeader && ( ! emptyHeader || ( emptyHeader && ! this . format . getIgnoreEmptyHeaders ( ) ) ) ) { throw new IllegalArgumentException ( \"The header contains a duplicate name: \\\"\" + header + \"\\\" in \" + Arrays . toString ( headerRecord ) ) ; } hdrMap . put ( header , Integer . valueOf ( i ) ) ; } } } return hdrMap ; }",
        "fixed_src": "private Map<String, Integer> initializeHeader() throws IOException {\n        Map<String, Integer> hdrMap = null;\n        final String[] formatHeader = this.format.getHeader();\n        if (formatHeader != null) {\n            hdrMap = new LinkedHashMap<String, Integer>();\n\n            String[] headerRecord = null;\n            if (formatHeader.length == 0) {\n                // read the header from the first line of the file\n                final CSVRecord nextRecord = this.nextRecord();\n                if (nextRecord != null) {\n                    headerRecord = nextRecord.values();\n                }\n            } else {\n                if (this.format.getSkipHeaderRecord()) {\n                    this.nextRecord();\n                }\n                headerRecord = formatHeader;\n            }\n\n            // build the name to index mappings\n            if (headerRecord != null) {\n                for (int i = 0; i < headerRecord.length; i++) {\n                    final String header = headerRecord[i];\n                    final boolean containsHeader = hdrMap.containsKey(header);\n                    final boolean emptyHeader = header == null || header.trim().isEmpty();\n                    if (containsHeader && (!emptyHeader || (emptyHeader && !this.format.getIgnoreEmptyHeaders()))) {\n                        throw new IllegalArgumentException(\"The header contains a duplicate name: \\\"\" + header +\n                                \"\\\" in \" + Arrays.toString(headerRecord));\n                    }\n                    hdrMap.put(header, Integer.valueOf(i));\n                }\n            }\n        }\n        return hdrMap;\n    }",
        "fixed_src_wo_comments": "private Map < String , Integer > initializeHeader ( ) throws IOException { Map < String , Integer > hdrMap = null ; final String [ ] formatHeader = this . format . getHeader ( ) ; if ( formatHeader != null ) { hdrMap = new LinkedHashMap < String , Integer > ( ) ; String [ ] headerRecord = null ; if ( formatHeader . length == 0 ) { final CSVRecord nextRecord = this . nextRecord ( ) ; if ( nextRecord != null ) { headerRecord = nextRecord . values ( ) ; } } else { if ( this . format . getSkipHeaderRecord ( ) ) { this . nextRecord ( ) ; } headerRecord = formatHeader ; } if ( headerRecord != null ) { for ( int i = 0 ; i < headerRecord . length ; i ++ ) { final String header = headerRecord [ i ] ; final boolean containsHeader = hdrMap . containsKey ( header ) ; final boolean emptyHeader = header == null || header . trim ( ) . isEmpty ( ) ; if ( containsHeader && ( ! emptyHeader || ( emptyHeader && ! this . format . getIgnoreEmptyHeaders ( ) ) ) ) { throw new IllegalArgumentException ( \"The header contains a duplicate name: \\\"\" + header + \"\\\" in \" + Arrays . toString ( headerRecord ) ) ; } hdrMap . put ( header , Integer . valueOf ( i ) ) ; } } } return hdrMap ; }",
        "summary": "NullPointerException when empty header string and and null string of \"\"",
        "Description": "When setting the format to have a nullString of \"\" and having an empty header value, a nullPointerException is thrown.",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-122",
        "comments": [
            "Test and fix",
            "{code}\nbene@localhost:~/workspace/apache/commons/csv$ svn ci -m \"CSV-122: NullPointerException when empty header string and and null string of \\\"\\\". Thanks to Mike Lewis.\"\nSending        src/changes/changes.xml\nSending        src/main/java/org/apache/commons/csv/CSVParser.java\nSending        src/test/java/org/apache/commons/csv/CSVParserTest.java\nTransmitting file data ...\nCommitted revision 1609768.\n\nThanks!\n{code}"
        ],
        "summarized_discussion": "\n\nThe solution to the bug was to test and fix the source code. This was done by committing a revision (1609768) that included changes to the src/changes/changes.xml, src/main/java/org/apache/commons/csv/CSVParser.java, and src/test/java/org/apache/commons/csv/CSVParserTest.java files. The commit message was \"CSV-122: NullPointerException when empty header string and and null string of \\\\\"\\\\\". Thanks to Mike Lewis.\"."
    },
    "Csv_2_src/main/java/org/apache/commons/csv/CSVRecord.java_79_86": {
        "src": "public String get(final String name) {\n        if (mapping == null) {\n            throw new IllegalStateException(\n                    \"No header mapping was specified, the record values can't be accessed by name\");\n        }\n        final Integer index = mapping.get(name);\n            return index != null ? values[index.intValue()] : null;\n    }",
        "src_wo_comments": "public String get ( final String name ) { if ( mapping == null ) { throw new IllegalStateException ( \"No header mapping was specified, the record values can't be accessed by name\" ) ; } final Integer index = mapping . get ( name ) ; return index != null ? values [ index . intValue ( ) ] : null ; }",
        "fixed_src": "public String get(final String name) {\n        if (mapping == null) {\n            throw new IllegalStateException(\n                    \"No header mapping was specified, the record values can't be accessed by name\");\n        }\n        final Integer index = mapping.get(name);\n        try {\n            return index != null ? values[index.intValue()] : null;\n        } catch (ArrayIndexOutOfBoundsException e) {\n            throw new IllegalArgumentException(\n                    String.format(\n                            \"Index for header '%s' is %d but CSVRecord only has %d values!\",\n                            name, index.intValue(), values.length));\n        }\n    }",
        "fixed_src_wo_comments": "public String get ( final String name ) { if ( mapping == null ) { throw new IllegalStateException ( \"No header mapping was specified, the record values can't be accessed by name\" ) ; } final Integer index = mapping . get ( name ) ; try { return index != null ? values [ index . intValue ( ) ] : null ; } catch ( ArrayIndexOutOfBoundsException e ) { throw new IllegalArgumentException ( String . format ( \"Index for header '%s' is %d but CSVRecord only has %d values!\" , name , index . intValue ( ) , values . length ) ) ; } }",
        "summary": "CSVRecord does not verify that the length of the header mapping matches the number of values",
        "Description": "CSVRecord does not verify that the size of the header mapping matches the number of values. The following test will produce a ArrayOutOfBoundsException:\n\n{code}\n@Test\npublic void testInvalidHeaderTooLong() throws Exception {\n   final CSVParser parser = new CSVParser(\"a,b\", CSVFormat.newBuilder().withHeader(\"A\", \"B\", \"C\").build());\n   final CSVRecord record = parser.iterator().next();\n   record.get(\"C\");\n}\n{code}",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-96",
        "comments": [
            "I'm not sure if CSVRecord's ctor is the right place for checking if the header mapping length is correct. This may lead to a lot of over head because everytime a CSVRecord is created we would have to do something like:\n\n{code}\nif(mappings != null && mapping.size() != values.length) {\n   throw Exception\n}\n{code}\n\nFor very big files, this may be an issue. Would be better to just test this when the first record is created by the parser.",
            "So is this a matter of which exception to throw?",
            "From a user PoV a meaningful exception should be thrown - so yes this is partly related to which exception should be thrown. This could for example be an IllegalStateException with a message like \"Header mapping does not match actual record length.\"\n\nThe question is, if it is sufficient to catch {{ArrayOutOfBoundsException}} and throw ISE? What if the header mapping is shorter than the record? IMHO this is also an illegal state but it will not be detected by a try-catch-block that catches AOOBE.",
            "We have to be careful about the performance impact. Any additional test in the main iteration loop is undesirable.",
            "How about this then:\n\n{code:java}\n    /**\n     * Creates a new record.\n     * \n     * @param values not null\n     * @param mapping may be null\n     * @param comment may be null\n     * @param recordNumber record number, not the line number\n     */\n    CSVRecord(final String[] values, final Map<String, Integer> mapping,\n            final String comment, final long recordNumber) {\n        if (mapping != null && values.length != mapping.size()) {\n            throw new IllegalArgumentException(String.format(\n                    \"Expected record length (%,d) to match header length (%,d) for record %,d\", values.length,\n                    mapping.size(), recordNumber));\n        }\n        this.recordNumber = recordNumber;\n        this.values = values;\n        this.mapping = mapping;\n        this.comment = comment;\n    }\n\n{code}",
            "@Emmanuel Point taken, but I would be more concerned about code that runs once per column value, instead of once per record. \n\nThe code above causes this test to fail: org.apache.commons.csv.CSVParserTest.testMappedButNotSetAsOutlook2007ContactExport() so we can forget about this kind of test unless we want to add a strict vs. lenient option.\n\nI'm inclined to leave the code as is for now. Adding a check on column access would mean that in most cases, when the data is correctly formatted, you'd pay for the size check twice, once in our code to throw an exception and another in the JRE/JVM when the array is accessed (which is imagine is very fast).\n\n",
            "I can't check this in detail now, but I would just advise leaving out strong checks to not compromise the performance.\n\nThe consistency of the record with the header could be checked afterward by a isConsistent() method on the record called by the user.",
            "So would you have the method return a boolean or throw an exception?",
            "Good question. If the consistency is checked during the parsing I believe a try catch block inside the loop would be more expensive than a simple if. So I would for the boolean.",
            "How about checking this in CSVParser? We could initialize a lexer, get the first record, check this record against the header and reset the Reader afterwards (this has to happen in CSVParser ctor). This way we would do the test only once (and not for every record). There will be only minimal impact on performance.",
            "You would still have to check if the record is the first one. That's an additional check for every record.",
            "I've added EB's suggestion for a CSVRecord#isConsistent() method:\n\n{noformat}\ncommit -m \"[CSV-96] CSVRecord does not verify that the length of the header mapping matches the number of values.\" C:/svn/org/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVParserTest.java C:/svn/org/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVRecord.java\n    Sending        C:/svn/org/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVRecord.java\n    Sending        C:/svn/org/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVParserTest.java\n    Transmitting file data ...\n    Committed revision 1462110.\n{noformat}",
            "@BR: You have to check each record. How could you not and avoid the exception? I think the exception is OK. In the Excel example from the unit test, it is up to the call site to know how funky the input can be.",
            "{quote}\n@BR: You have to check each record. How could you not and avoid the exception? I think the exception is OK. In the Excel example from the unit test, it is up to the call site to know how funky the input can be.\n{quote}\n\nYou mean if the length of the first record is correct but subsequent records have a different length?",
            "Right, each record is independent.",
            "Has been fixed by wrapping the ArrayOutOfBoundsException into an IllegalArgumentException."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to check the length of the header mapping against the number of values when the first record is created by the parser, and wrap any ArrayOutOfBoundsExceptions into an IllegalArgumentException."
    },
    "JacksonXml_4_src/main/java/com/fasterxml/jackson/dataformat/xml/ser/XmlSerializerProvider.java_200_208": {
        "src": "protected void _serializeXmlNull(JsonGenerator jgen) throws IOException\n    {\n        // 14-Nov-2016, tatu: As per [dataformat-xml#213], we may have explicitly\n        //    configured root name...\n        if (jgen instanceof ToXmlGenerator) {\n            _initWithRootName((ToXmlGenerator) jgen, ROOT_NAME_FOR_NULL);\n        }\n        super.serializeValue(jgen, null);\n    }",
        "src_wo_comments": "protected void _serializeXmlNull ( JsonGenerator jgen ) throws IOException { if ( jgen instanceof ToXmlGenerator ) { _initWithRootName ( ( ToXmlGenerator ) jgen , ROOT_NAME_FOR_NULL ) ; } super . serializeValue ( jgen , null ) ; }",
        "fixed_src": "protected void _serializeXmlNull(JsonGenerator jgen) throws IOException\n    {\n        // 14-Nov-2016, tatu: As per [dataformat-xml#213], we may have explicitly\n        //    configured root name...\n        QName rootName = _rootNameFromConfig();\n        if (rootName == null) {\n            rootName = ROOT_NAME_FOR_NULL;\n        }\n        if (jgen instanceof ToXmlGenerator) {\n            _initWithRootName((ToXmlGenerator) jgen, rootName);\n        }\n        super.serializeValue(jgen, null);\n    }",
        "fixed_src_wo_comments": "protected void _serializeXmlNull ( JsonGenerator jgen ) throws IOException { QName rootName = _rootNameFromConfig ( ) ; if ( rootName == null ) { rootName = ROOT_NAME_FOR_NULL ; } if ( jgen instanceof ToXmlGenerator ) { _initWithRootName ( ( ToXmlGenerator ) jgen , rootName ) ; } super . serializeValue ( jgen , null ) ; }",
        "summary": "`XmlSerializerProvider` does not use `withRootName` config for null",
        "Description": "In `jackson-dataformat-xml/src/main/java/com/fasterxml/jackson/dataformat/xml/ser/XmlSerializerProvider.java`\n\nLine 203, I think `_rootNameFromConfig()` should be used if available instead of `ROOT_NAME_FOR_NULL`, so that `withRootName()` config can be used.\n\nI don't know whether/how deser would be affected\n\nhttps://github.com/FasterXML/jackson-dataformat-xml/blob/ca1c671c419e88a18357d497ec3671c73c37452e/src/main/java/com/fasterxml/jackson/dataformat/xml/ser/XmlSerializerProvider.java#L203\n",
        "issue_url": null,
        "comments": [
            {
                "content": "I'd need to understand more what the problem here is: what is not working, and how to reproduce the issue. Is this just the question of serialization of root-level `null` not using wrapping as expected?\n"
            },
            {
                "content": "Ok, I think I am able to reproduce what is being seen.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug appears to be related to the serialization of root-level `null` not using wrapping as expected."
    },
    "Codec_7_src/java/org/apache/commons/codec/binary/Base64.java_669_671": {
        "src": "public static String encodeBase64String(byte[] binaryData) {\n        return StringUtils.newStringUtf8(encodeBase64(binaryData, true));\n    }",
        "src_wo_comments": "public static String encodeBase64String ( byte [ ] binaryData ) { return StringUtils . newStringUtf8 ( encodeBase64 ( binaryData , true ) ) ; }",
        "fixed_src": "public static String encodeBase64String(byte[] binaryData) {\n        return StringUtils.newStringUtf8(encodeBase64(binaryData, false));\n    }",
        "fixed_src_wo_comments": "public static String encodeBase64String ( byte [ ] binaryData ) { return StringUtils . newStringUtf8 ( encodeBase64 ( binaryData , false ) ) ; }",
        "summary": "Base64.encodeBase64String() shouldn't chunk",
        "Description": "Base64.encodeBase64String() shouldn't chunk.\n\nChange this:\n\n{code}\npublic static String encodeBase64String(byte[] binaryData) {\n    return StringUtils.newStringUtf8(encodeBase64(binaryData, true));\n}\n{code}\n\nTo this:\n\n{code}\npublic static String encodeBase64String(byte[] binaryData) {\n    return StringUtils.newStringUtf8(encodeBase64(binaryData, false));\n}\n{code}\n\n\n\nThis will fix the following tests ggregory added a few minutes ago:\n\n        //assertEquals(\"Zg==\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"f\")));\n        //assertEquals(\"Zm8=\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"fo\")));\n        //assertEquals(\"Zm9v\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"foo\")));\n        //assertEquals(\"Zm9vYg==\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"foob\")));\n        //assertEquals(\"Zm9vYmE=\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"fooba\")));\n        //assertEquals(\"Zm9vYmFy\", Base64.encodeBase64String(StringUtils.getBytesUtf8(\"foobar\")));\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-99",
        "comments": [
            "One-line patch to fix this attached.",
            "\nChanges to unit-tests also attached.  Notice that two old tests assume the old, erroneous behavior, and so those need to be adjusted, too:\n\n{code}\n-        assertEquals(\"byteToString static Hello World\", \"SGVsbG8gV29ybGQ=\\r\\n\", Base64.encodeBase64String(b1));\n+        assertEquals(\"byteToString static Hello World\", \"SGVsbG8gV29ybGQ=\", Base64.encodeBase64String(b1));\n\n\n-        assertEquals(\"byteToString static UUID\", \"K/fMJwH+Q5e0nr7tWsxwkA==\\r\\n\", Base64.encodeBase64String(b4));\n+        assertEquals(\"byteToString static UUID\", \"K/fMJwH+Q5e0nr7tWsxwkA==\", Base64.encodeBase64String(b4));\n{code}",
            "Some of the JUnit tests have a problem:\n\n\"Zm9v\" + Base64.CHUNK_SEPARATOR\n\nThis results in:\n\nZm9v[B@6bbc4459\n\nAs opposed to:\n\nZm9v\\r\\n\n\n\n\ncodec-99-test-fixes.patch attached to fix this.\n\n\n",
            "Applied patch  https://issues.apache.org/jira/secure/attachment/12439949/codec-99-test-fixes.patch\nThank you Julius.",
            "IMO, the issue is not chunk vs. not chunk but that the output should never encode and end in a CR LR, especially when there is no = at the end of the output",
            "To respond to Gary, here is behaviour of Base64.encodeBase64Chunked(b) through all versions of commons-codec:\n\n{code}\npackage playground;\nimport org.apache.commons.codec.binary.Base64;\npublic class Base64ChunkTest {\n\n    public static void main(String[] args) throws Exception {\n        byte[] b = args[0].getBytes(\"UTF-8\");\n\n        b = Base64.encodeBase64Chunked(b);\n\n        String s = new String(b, \"UTF-8\");\n        s = s.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\");\n        System.out.println(s);\n    }\n}\n{code}\n\n{noformat}\n$ java -cp build/classes:lib/commons-codec-1.1.jar playground.Base64ChunkTest Hello\nSGVsbG8=\\n\n\n$ java -cp build/classes:lib/commons-codec-1.2.jar playground.Base64ChunkTest Hello\nSGVsbG8=\\n\n\n$ java -cp build/classes:lib/commons-codec-1.3.jar playground.Base64ChunkTest Hello\nSGVsbG8=\\r\\n\n\n$ java -cp build/classes:lib/commons-codec-1.4.jar playground.Base64ChunkTest Hello\nSGVsbG8=\\r\\n\n{noformat}\n\n\nJust to be clear, you want to change this now after 7 years (the files in the commons-codec-1.1.jar have '2003-04-29' timestamps)?\n\nps.  I don't think '=' should be part of the equation, because valid data can encode and not have any '=' characters, for example the string '123':\n\n{noformat}\n$ java -cp build/classes:lib/commons-codec-1.4.jar playground.Base64ChunkTest 123\nMTIz\\r\\n\n{noformat}\n\n",
            "When I do:\n\nBase64 encoder = new Base64(0); \nencoder.encodeBase64String(binaryData);\n\nI would expect that the encoded string isn't chunked, I've set the appropriate settings in the\nconstructor before. But the current encodeBase64String() \"overrides\" the settings I've done.\n\nThe methods encodeBase64String(..) and so on should respect the settings made in the constructor in my opinion.",
            "You possibly wouldn't, because you are encoding a small string, like a password. Others would, because they are encoding a very large string. Whatever the default, if you need to be sure, don't use the convenience methods. Period!\n\nThe only reason for changing the default would be upwards compatibility. If we change it for other reasons, we loose upwards compatibility.\n",
            "If I'd like to encode a small string, I'll agree:\n\nnew Base64().encodeToString(...)\n\nBut it doesn't matter in this point if this is chunked or not. \n\nOther people which like to encode larger strings won't use new Base64(0) or new Base64(-1) when they wanted chunked\nfiles. Otherwise they've read the docs which mentioned the way to use unchunked. I think it wouldn't break upwards compatibility because there is no really use case where you'll instantiate an Base64 encoder with 0 or less than 0 when it should be chunked. In these case even the \"convenience methods\" should behaviour correctly. Or otherwise (to keep upwards compatibility) it should be explicitly mentioned in the apidocs. \n\nCurrently it looks like buying a car, choosing red as the color and only if you're opening the right door the car became green. ",
            "I applied \"codec-99.patch\" and \"codec-99-tests.patch\".  Quick summary of consequences:\n\n1.  Another binary incompatibility with codec-1.4.jar is introduced with this.\n\n2.  But now the String static encode in 1.4 is more consistent with the pre-existing byte[] static encode methods from 1.3, 1.2, 1.1.",
            "The example in https://issues.apache.org/jira/browse/CODEC-99?focusedCommentId=12924892&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12924892 does not make sense:\n\nBase64 encoder = new Base64(0);\nencoder.encodeBase64String(binaryData);\n\nThe method encodeBase64String is a static method so instance variables will NEVER apply!\n",
            "Yes it's declared static but it CAN also be used as a normal non-static function. In that case it's not behaving correctly.\n\nBy the way it's using class-level fields when following the calls (buffer).\n\nWhy not using the static - constructor. There you'll can set the standard settings which leads to the results of the static-calls\nWhen used as a normal function it can be modified using the standard constructors.",
            "WRT \"Yes it's declared static but it CAN also be used as a normal non-static function. In that case it's not behaving correctly.\"\n\nAre we talking about the same thing?\n\nA static method will never use values from an instance.\n\nMight you be confusing run-time behavior and compiler behavior when you say \"normal non-static function\"?\n\nIn your example:\n\nBase64 encoder = new Base64(0);\nencoder.encodeBase64String(binaryData);\n\nIs equivalent to (and should really be written in this style:\n\nBase64 encoder = new Base64(0);\nBase64.encodeBase64String(binaryData);\n",
            "Oops, I forgot to update the Javadoc.  Thanks, Sebb, for pointing that out!"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to apply the patch \"codec-99-test-fixes.patch\" and make changes to the JUnit tests to adjust for the old, erroneous behavior. Additionally, the Javadoc should be updated."
    },
    "Compress_34_src/main/java/org/apache/commons/compress/archivers/zip/X7875_NewUnix.java_145_147": {
        "src": "public ZipShort getCentralDirectoryLength() {\n        return getLocalFileDataLength();\n    }",
        "src_wo_comments": "public ZipShort getCentralDirectoryLength ( ) { return getLocalFileDataLength ( ) ; }",
        "fixed_src": "public ZipShort getCentralDirectoryLength() {\n        return ZERO;\n    }",
        "fixed_src_wo_comments": "public ZipShort getCentralDirectoryLength ( ) { return ZERO ; }",
        "summary": "Exception in X7875_NewUnix.parseFromLocalFileData when parsing 0-sized \"ux\" local entry",
        "Description": "When trying to detect content type of a zip file with Tika 1.10 (which uses Commons Compress 1.9 internally) in manner like this:\n\n{code}\n        byte[] content = ... // whole zip file.\n        String name = \"TR_01.ZIP\";\n        Tika tika = new Tika();\n        return tika.detect(content, name);\n{code}\n\nit throws an exception:\n\n{code}\njava.lang.ArrayIndexOutOfBoundsException: 13\n\tat org.apache.commons.compress.archivers.zip.X7875_NewUnix.parseFromLocalFileData(X7875_NewUnix.java:199)\n\tat org.apache.commons.compress.archivers.zip.X7875_NewUnix.parseFromCentralDirectoryData(X7875_NewUnix.java:220)\n\tat org.apache.commons.compress.archivers.zip.ExtraFieldUtils.parse(ExtraFieldUtils.java:174)\n\tat org.apache.commons.compress.archivers.zip.ZipArchiveEntry.setCentralDirectoryExtra(ZipArchiveEntry.java:476)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.readCentralDirectoryEntry(ZipFile.java:575)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.populateFromCentralDirectory(ZipFile.java:492)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:216)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:192)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:153)\n\tat org.apache.tika.parser.pkg.ZipContainerDetector.detectZipFormat(ZipContainerDetector.java:141)\n\tat org.apache.tika.parser.pkg.ZipContainerDetector.detect(ZipContainerDetector.java:88)\n\tat org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:77)\n\tat org.apache.tika.Tika.detect(Tika.java:155)\n\tat org.apache.tika.Tika.detect(Tika.java:183)\n\tat org.apache.tika.Tika.detect(Tika.java:223)\n{code}\n\nThe zip file does contain two .jpg images and is not a \"special\" (JAR, Openoffice, ... ) zip file.\n\nUnfortunately, the contents of the zip file is confidential and so I cannot attach it to this ticket as it is, although I can provide the parameters supplied to\norg.apache.commons.compress.archivers.zip.X7875_NewUnix.parseFromLocalFileData(X7875_NewUnix.java:199) as caught by the debugger:\n\n{code}\ndata = {byte[13]@2103}\n 0 = 85\n 1 = 84\n 2 = 5\n 3 = 0\n 4 = 7\n 5 = -112\n 6 = -108\n 7 = 51\n 8 = 85\n 9 = 117\n 10 = 120\n 11 = 0\n 12 = 0\noffset = 13\nlength = 0\n{code}\n\nThis data comes from the local zip entry for the first file, it seems the method tries to read more bytes than is actually available in the buffer.\n\nIt seems that first 9 bytes of the buffer are 'UT' extended field with timestamp, followed by 0-sized 'ux' field (bytes 9-12) that is supposed to contain UID/GID - according to infozip's doc the 0-size is common for global dictionary, but the local dictionary should contain complete data. In this case for some reason it does contain 0-sized data.\n\nNote that 7zip and unzip can unzip the file without even a warning, so Commons Compress should be also able to handle that file correctly without choking on that exception.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-321",
        "comments": [
            "Can you try Commons Compress 1.10?\n\nThank you.",
            "I forced compress to 1.10 in maven, retried the test and the result is the same (just some small differences in line numbers)\n\n{code}\njava.lang.ArrayIndexOutOfBoundsException: 13\n\tat org.apache.commons.compress.archivers.zip.X7875_NewUnix.parseFromLocalFileData(X7875_NewUnix.java:199)\n\tat org.apache.commons.compress.archivers.zip.X7875_NewUnix.parseFromCentralDirectoryData(X7875_NewUnix.java:220)\n\tat org.apache.commons.compress.archivers.zip.ExtraFieldUtils.parse(ExtraFieldUtils.java:174)\n\tat org.apache.commons.compress.archivers.zip.ZipArchiveEntry.setCentralDirectoryExtra(ZipArchiveEntry.java:561)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.readCentralDirectoryEntry(ZipFile.java:616)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.populateFromCentralDirectory(ZipFile.java:533)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:216)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:192)\n\tat org.apache.commons.compress.archivers.zip.ZipFile.<init>(ZipFile.java:153)\n\tat org.apache.tika.parser.pkg.ZipContainerDetector.detectZipFormat(ZipContainerDetector.java:141)\n\tat org.apache.tika.parser.pkg.ZipContainerDetector.detect(ZipContainerDetector.java:88)\n\tat org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:77)\n\tat org.apache.tika.Tika.detect(Tika.java:155)\n\tat org.apache.tika.Tika.detect(Tika.java:183)\n\tat org.apache.tika.Tika.detect(Tika.java:223)\n{code}",
            "Martin,\n\nThank you for checking.\n\nCan you attach the problematic file to this ticket? Make sure it does not contain proprietary or personal information!\n\nGary",
            "Martin has already said he cannot share it :-)\n\nUnfortunately I haven't had the time to read zlib's spec of the extra field but it looks as if the uid/gid fields would both be optional and the extra field had already old us with a length of 0 (the two 0s following the bytes with values 117 and 120 in the dump are the size of the ux extra field).  This is a bug that may be present in some of the other extra field parsers as well.",
            "I think I've fixed this with 1697106  is it possible you can try the next 1.11-SNAPSHOT build from https://repository.apache.org/content/groups/snapshots ?",
            "Tried with the 1.11-SNAPSHOT - the file is now correctly detected by Tika as application/zip, no more crashes inside X7875_NewUnix.parseFromLocalFileData. I guess that fixed this bug.",
            "Thanks, Martin"
        ],
        "summarized_discussion": "\n\nThe solution to the bug was to try the 1.11-SNAPSHOT build from Apache's repository. This fixed the bug, as the file was correctly detected by Tika as application/zip, and there were no more crashes inside X7875_NewUnix.parseFromLocalFileData."
    },
    "JacksonCore_8_src/main/java/com/fasterxml/jackson/core/util/TextBuffer.java_298_310": {
        "src": "public char[] getTextBuffer()\n    {\n        // Are we just using shared input buffer?\n        if (_inputStart >= 0) return _inputBuffer;\n        if (_resultArray != null)  return _resultArray;\n        if (_resultString != null) {\n            return (_resultArray = _resultString.toCharArray());\n        }\n        // Nope; but does it fit in just one segment?\n        if (!_hasSegments)  return _currentSegment;\n        // Nope, need to have/create a non-segmented array and return it\n        return contentsAsArray();\n    }",
        "src_wo_comments": "public char [ ] getTextBuffer ( ) { if ( _inputStart >= 0 ) return _inputBuffer ; if ( _resultArray != null ) return _resultArray ; if ( _resultString != null ) { return ( _resultArray = _resultString . toCharArray ( ) ) ; } if ( ! _hasSegments ) return _currentSegment ; return contentsAsArray ( ) ; }",
        "fixed_src": "public char[] getTextBuffer()\n    {\n        // Are we just using shared input buffer?\n        if (_inputStart >= 0) return _inputBuffer;\n        if (_resultArray != null)  return _resultArray;\n        if (_resultString != null) {\n            return (_resultArray = _resultString.toCharArray());\n        }\n        // Nope; but does it fit in just one segment?\n        if (!_hasSegments && _currentSegment != null)  return _currentSegment;\n        // Nope, need to have/create a non-segmented array and return it\n        return contentsAsArray();\n    }",
        "fixed_src_wo_comments": "public char [ ] getTextBuffer ( ) { if ( _inputStart >= 0 ) return _inputBuffer ; if ( _resultArray != null ) return _resultArray ; if ( _resultString != null ) { return ( _resultArray = _resultString . toCharArray ( ) ) ; } if ( ! _hasSegments && _currentSegment != null ) return _currentSegment ; return contentsAsArray ( ) ; }",
        "summary": "Inconsistent TextBuffer#getTextBuffer behavior",
        "Description": "Hi, I'm using 2.4.2. While I'm working on CBORParser, I noticed that CBORParser#getTextCharacters() returns sometimes `null` sometimes `[]` (empty array) when it's parsing empty string `\"\"`.\n\nWhile debugging, I noticed that TextBuffer#getTextBuffer behaves inconsistently.\n\n```\nTextBuffer buffer = new TextBuffer(new BufferRecycler());\nbuffer.resetWithEmpty();\nbuffer.getTextBuffer(); // returns null\nbuffer.contentsAsString(); // returns empty string \"\"\nbuffer.getTextBuffer(); // returns empty array []\n```\n\nI think getTextBuffer should return the same value. Not sure which (`null` or `[]`) is expected though.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Interesting. I would guess that returning an empty array is the proper behavior.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to return an empty array, which is the proper behavior."
    },
    "Math_95_src/java/org/apache/commons/math/distribution/FDistributionImpl.java_143_149": {
        "src": "protected double getInitialDomain(double p) {\n        double ret;\n        double d = getDenominatorDegreesOfFreedom();\n            // use mean\n            ret = d / (d - 2.0);\n        return ret;\n    }",
        "src_wo_comments": "protected double getInitialDomain ( double p ) { double ret ; double d = getDenominatorDegreesOfFreedom ( ) ; ret = d / ( d - 2.0 ) ; return ret ; }",
        "fixed_src": "protected double getInitialDomain(double p) {\n        double ret = 1.0;\n        double d = getDenominatorDegreesOfFreedom();\n        if (d > 2.0) {\n            // use mean\n            ret = d / (d - 2.0);\n        }\n        return ret;\n    }",
        "fixed_src_wo_comments": "protected double getInitialDomain ( double p ) { double ret = 1.0 ; double d = getDenominatorDegreesOfFreedom ( ) ; if ( d > 2.0 ) { ret = d / ( d - 2.0 ) ; } return ret ; }",
        "summary": "denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket",
        "Description": "We are using the FDistributionImpl from the commons.math project to do\nsome statistical calculations, namely receiving the upper and lower\nboundaries of a confidence interval. Everything is working fine and the\nresults are matching our reference calculations.\n\nHowever, the FDistribution behaves strange if a\ndenominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95.\nThis results in an IllegalArgumentsException, stating:\n        \nInvalid endpoint parameters:  lowerBound=0.0 initial=Infinity\nupperBound=1.7976931348623157E308\n        \ncoming from\norg.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket\n        \nThe problem is the 'initial' parameter to that function, wich is\nPOSITIVE_INFINITY and therefore not within the boundaries. I already\npinned down the problem to the FDistributions getInitialDomain()-method,\nwich goes like:\n\n        return getDenominatorDegreesOfFreedom() /\n                    (getDenominatorDegreesOfFreedom() - 2.0);\n        \nObviously, in case of denominatorDegreesOfFreedom == 2, this must lead\nto a division-by-zero, resulting in POSTIVE_INFINITY. The result of this\noperation is then directly passed into the\nUnivariateRealSolverUtils.bracket() - method as second argument.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-227",
        "comments": [
            "SVN 699157.  fixed in trunk."
        ],
        "summarized_discussion": "\n\nThe bug in SVN 699157 has been fixed in the trunk version of the source code."
    },
    "Mockito_32_src/org/mockito/internal/configuration/SpyAnnotationEngine.java_27_58": {
        "src": "@SuppressWarnings(\"deprecation\")\n    public void process(Class<?> context, Object testClass) {\n        Field[] fields = context.getDeclaredFields();\n        for (Field field : fields) {\n            if (field.isAnnotationPresent(Spy.class)) {\n                assertNoAnnotations(Spy.class, field, Mock.class, org.mockito.MockitoAnnotations.Mock.class, Captor.class);\n                boolean wasAccessible = field.isAccessible();\n                field.setAccessible(true);\n                try {\n                    Object instance = field.get(testClass);\n                    if (instance == null) {\n                        throw new MockitoException(\"Cannot create a @Spy for '\" + field.getName() + \"' field because the *instance* is missing\\n\" +\n                        \t\t  \"The instance must be created *before* initMocks();\\n\" +\n                                  \"Example of correct usage of @Spy:\\n\" +\n                            \t  \"   @Spy List mock = new LinkedList();\\n\" +\n                            \t  \"   //also, don't forget about MockitoAnnotations.initMocks();\");\n\n                    }\n                    if (new MockUtil().isMock(instance)) { \n                        // instance has been spied earlier\n                        Mockito.reset(instance);\n                    } else {\n                        field.set(testClass, Mockito.spy(instance));\n                    }\n                } catch (IllegalAccessException e) {\n                    throw new MockitoException(\"Problems initiating spied field \" + field.getName(), e);\n                } finally {\n                    field.setAccessible(wasAccessible);\n                }\n            }\n        }\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"deprecation\" ) public void process ( Class < ? > context , Object testClass ) { Field [ ] fields = context . getDeclaredFields ( ) ; for ( Field field : fields ) { if ( field . isAnnotationPresent ( Spy . class ) ) { assertNoAnnotations ( Spy . class , field , Mock . class , org . mockito . MockitoAnnotations . Mock . class , Captor . class ) ; boolean wasAccessible = field . isAccessible ( ) ; field . setAccessible ( true ) ; try { Object instance = field . get ( testClass ) ; if ( instance == null ) { throw new MockitoException ( \"Cannot create a @Spy for '\" + field . getName ( ) + \"' field because the *instance* is missing\\n\" + \"The instance must be created *before* initMocks();\\n\" + \"Example of correct usage of @Spy:\\n\" + \"   @Spy List mock = new LinkedList();\\n\" + \"   //also, don't forget about MockitoAnnotations.initMocks();\" ) ; } if ( new MockUtil ( ) . isMock ( instance ) ) { Mockito . reset ( instance ) ; } else { field . set ( testClass , Mockito . spy ( instance ) ) ; } } catch ( IllegalAccessException e ) { throw new MockitoException ( \"Problems initiating spied field \" + field . getName ( ) , e ) ; } finally { field . setAccessible ( wasAccessible ) ; } } } }",
        "fixed_src": "@SuppressWarnings(\"deprecation\")\n    public void process(Class<?> context, Object testClass) {\n        Field[] fields = context.getDeclaredFields();\n        for (Field field : fields) {\n            if (field.isAnnotationPresent(Spy.class)) {\n                assertNoAnnotations(Spy.class, field, Mock.class, org.mockito.MockitoAnnotations.Mock.class, Captor.class);\n                boolean wasAccessible = field.isAccessible();\n                field.setAccessible(true);\n                try {\n                    Object instance = field.get(testClass);\n                    if (instance == null) {\n                        throw new MockitoException(\"Cannot create a @Spy for '\" + field.getName() + \"' field because the *instance* is missing\\n\" +\n                        \t\t  \"The instance must be created *before* initMocks();\\n\" +\n                                  \"Example of correct usage of @Spy:\\n\" +\n                            \t  \"   @Spy List mock = new LinkedList();\\n\" +\n                            \t  \"   //also, don't forget about MockitoAnnotations.initMocks();\");\n\n                    }\n                    if (new MockUtil().isMock(instance)) { \n                        // instance has been spied earlier\n                        Mockito.reset(instance);\n                    } else {\n                        field.set(testClass, Mockito.mock(instance.getClass(), withSettings()\n                                .spiedInstance(instance)\n                                .defaultAnswer(Mockito.CALLS_REAL_METHODS)\n                                .name(field.getName())));\n                    }\n                } catch (IllegalAccessException e) {\n                    throw new MockitoException(\"Problems initiating spied field \" + field.getName(), e);\n                } finally {\n                    field.setAccessible(wasAccessible);\n                }\n            }\n        }\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"deprecation\" ) public void process ( Class < ? > context , Object testClass ) { Field [ ] fields = context . getDeclaredFields ( ) ; for ( Field field : fields ) { if ( field . isAnnotationPresent ( Spy . class ) ) { assertNoAnnotations ( Spy . class , field , Mock . class , org . mockito . MockitoAnnotations . Mock . class , Captor . class ) ; boolean wasAccessible = field . isAccessible ( ) ; field . setAccessible ( true ) ; try { Object instance = field . get ( testClass ) ; if ( instance == null ) { throw new MockitoException ( \"Cannot create a @Spy for '\" + field . getName ( ) + \"' field because the *instance* is missing\\n\" + \"The instance must be created *before* initMocks();\\n\" + \"Example of correct usage of @Spy:\\n\" + \"   @Spy List mock = new LinkedList();\\n\" + \"   //also, don't forget about MockitoAnnotations.initMocks();\" ) ; } if ( new MockUtil ( ) . isMock ( instance ) ) { Mockito . reset ( instance ) ; } else { field . set ( testClass , Mockito . mock ( instance . getClass ( ) , withSettings ( ) . spiedInstance ( instance ) . defaultAnswer ( Mockito . CALLS_REAL_METHODS ) . name ( field . getName ( ) ) ) ) ; } } catch ( IllegalAccessException e ) { throw new MockitoException ( \"Problems initiating spied field \" + field . getName ( ) , e ) ; } finally { field . setAccessible ( wasAccessible ) ; } } } }",
        "summary": "Mockito can't create mock on public class that extends package-private class",
        "Description": "I created simple project to demonstrate this:\nhttps://github.com/astafev/mockito-package-private-class/\n\nPlease take a look. Even if it can't be implemented, I think that mockito should throw some normal exception at time of creation.\nIn my variant on first creation it returns wrong-working mock (invokes real method instead of stubbed). On second creation throws exception that doesn't really connected with problem.\n\nEverything works fine if you mock package-private parent.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "it's both actual for 1.10.19 and 2.0.2-beta versions.\n"
            },
            {
                "content": "It is a known issue with CGLIB and it is raised there too #168, by merging #171 (replacing CGLIB by ByteBuddy) it should fix the problem.\n\nFor reference in the old issue tracker : https://code.google.com/p/mockito/issues/detail?id=212\n"
            },
            {
                "content": "it's good that it's known problem. But the main problem is that it takes a lot of time to find out real cause. Please consider adding special exception, because exceptions that raised currently really confuse.\n"
            },
            {
                "content": "Note #171 is merged, you may want to give the latest beta a try. But note that public API is subject to change when it is released and during development.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to replace CGLIB with ByteBuddy, which has been merged in #171. The latest beta version should be tried, but the public API is subject to change."
    },
    "JacksonDatabind_33_src/main/java/com/fasterxml/jackson/databind/introspect/JacksonAnnotationIntrospector.java_730_755": {
        "src": "@Override\n    public PropertyName findNameForSerialization(Annotated a)\n    {\n        String name = null;\n\n        JsonGetter jg = _findAnnotation(a, JsonGetter.class);\n        if (jg != null) {\n            name = jg.value();\n        } else {\n            JsonProperty pann = _findAnnotation(a, JsonProperty.class);\n            if (pann != null) {\n                name = pann.value();\n                /* 22-Apr-2014, tatu: Should figure out a better way to do this, but\n                 *   it's actually bit tricky to do it more efficiently (meta-annotations\n                 *   add more lookups; AnnotationMap costs etc)\n                 */\n            } else if (_hasAnnotation(a, JsonSerialize.class)\n                    || _hasAnnotation(a, JsonView.class)\n                    || _hasAnnotation(a, JsonRawValue.class)) {\n                name = \"\";\n            } else {\n                return null;\n            }\n        }\n        return PropertyName.construct(name);\n    }",
        "src_wo_comments": "@ Override public PropertyName findNameForSerialization ( Annotated a ) { String name = null ; JsonGetter jg = _findAnnotation ( a , JsonGetter . class ) ; if ( jg != null ) { name = jg . value ( ) ; } else { JsonProperty pann = _findAnnotation ( a , JsonProperty . class ) ; if ( pann != null ) { name = pann . value ( ) ; } else if ( _hasAnnotation ( a , JsonSerialize . class ) || _hasAnnotation ( a , JsonView . class ) || _hasAnnotation ( a , JsonRawValue . class ) ) { name = \"\" ; } else { return null ; } } return PropertyName . construct ( name ) ; }",
        "fixed_src": "@Override\n    public PropertyName findNameForSerialization(Annotated a)\n    {\n        String name = null;\n\n        JsonGetter jg = _findAnnotation(a, JsonGetter.class);\n        if (jg != null) {\n            name = jg.value();\n        } else {\n            JsonProperty pann = _findAnnotation(a, JsonProperty.class);\n            if (pann != null) {\n                name = pann.value();\n                /* 22-Apr-2014, tatu: Should figure out a better way to do this, but\n                 *   it's actually bit tricky to do it more efficiently (meta-annotations\n                 *   add more lookups; AnnotationMap costs etc)\n                 */\n            } else if (_hasAnnotation(a, JsonSerialize.class)\n                    || _hasAnnotation(a, JsonView.class)\n                    || _hasAnnotation(a, JsonRawValue.class)\n                    || _hasAnnotation(a, JsonUnwrapped.class)\n                    || _hasAnnotation(a, JsonBackReference.class)\n                    || _hasAnnotation(a, JsonManagedReference.class)) {\n                name = \"\";\n            } else {\n                return null;\n            }\n        }\n        return PropertyName.construct(name);\n    }",
        "fixed_src_wo_comments": "@ Override public PropertyName findNameForSerialization ( Annotated a ) { String name = null ; JsonGetter jg = _findAnnotation ( a , JsonGetter . class ) ; if ( jg != null ) { name = jg . value ( ) ; } else { JsonProperty pann = _findAnnotation ( a , JsonProperty . class ) ; if ( pann != null ) { name = pann . value ( ) ; } else if ( _hasAnnotation ( a , JsonSerialize . class ) || _hasAnnotation ( a , JsonView . class ) || _hasAnnotation ( a , JsonRawValue . class ) || _hasAnnotation ( a , JsonUnwrapped . class ) || _hasAnnotation ( a , JsonBackReference . class ) || _hasAnnotation ( a , JsonManagedReference . class ) ) { name = \"\" ; } else { return null ; } } return PropertyName . construct ( name ) ; }",
        "summary": "@JsonUnwrapped is not treated as assuming @JsonProperty(\"\")",
        "Description": "See discussion [here](https://groups.google.com/forum/#!topic/jackson-user/QLpWb8YzIoE) but basically `@JsonUnwrapped` on a private field by itself does not cause that field to be serialized, currently,  You need to add an explicit `@JsonProperty`.  You shouldn't have to do that.  (Following test fails currently, should pass, though you can make it pass by commenting out the line with `@JsonProperty`.  Uses TestNG and AssertJ.)\n\n``` java\npackage com.bakins_bits;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport org.testng.annotations.Test;\n\nimport com.fasterxml.jackson.annotation.JsonUnwrapped;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.ObjectMapper;\n\npublic class TestJsonUnwrappedShouldMakePrivateFieldsSerializable\n{\n    public static class Inner\n    {\n        public String animal;\n    }\n\n    public static class Outer\n    {\n        // @JsonProperty\n        @JsonUnwrapped\n        private Inner inner;\n    }\n\n    @Test\n    public void jsonUnwrapped_should_make_private_fields_serializable() throws JsonProcessingException {\n        // ARRANGE\n        Inner inner = new Inner();\n        inner.animal = \"Zebra\";\n\n        Outer outer = new Outer();\n        outer.inner = inner;\n\n        ObjectMapper sut = new ObjectMapper();\n\n        // ACT\n        String actual = sut.writeValueAsString(outer);\n\n        // ASSERT\n        assertThat(actual).contains(\"animal\");\n        assertThat(actual).contains(\"Zebra\");\n        assertThat(actual).doesNotContain(\"inner\");\n    }\n}\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Should `@JsonIgnore` have been included too?\n"
            },
            {
                "content": "@david-bakin didn't think so, as it drops definition? Including that may have some side-effects.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is not to include the `@JsonIgnore` as it may have some side-effects and could drop the definition."
    },
    "Math_56_src/main/java/org/apache/commons/math/util/MultidimensionalCounter.java_216_246": {
        "src": "public int[] getCounts(int index) {\n        if (index < 0 ||\n            index >= totalSize) {\n            throw new OutOfRangeException(index, 0, totalSize);\n        }\n\n        final int[] indices = new int[dimension];\n\n        int count = 0;\n        for (int i = 0; i < last; i++) {\n            int idx = 0;\n            final int offset = uniCounterOffset[i];\n            while (count <= index) {\n                count += offset;\n                ++idx;\n            }\n            --idx;\n            count -= offset;\n            indices[i] = idx;\n        }\n\n        int idx = 1;\n        while (count < index) {\n            count += idx;\n            ++idx;\n        }\n        --idx;\n        indices[last] = idx;\n\n        return indices;\n    }",
        "src_wo_comments": "public int [ ] getCounts ( int index ) { if ( index < 0 || index >= totalSize ) { throw new OutOfRangeException ( index , 0 , totalSize ) ; } final int [ ] indices = new int [ dimension ] ; int count = 0 ; for ( int i = 0 ; i < last ; i ++ ) { int idx = 0 ; final int offset = uniCounterOffset [ i ] ; while ( count <= index ) { count += offset ; ++ idx ; } -- idx ; count -= offset ; indices [ i ] = idx ; } int idx = 1 ; while ( count < index ) { count += idx ; ++ idx ; } -- idx ; indices [ last ] = idx ; return indices ; }",
        "fixed_src": "public int[] getCounts(int index) {\n        if (index < 0 ||\n            index >= totalSize) {\n            throw new OutOfRangeException(index, 0, totalSize);\n        }\n\n        final int[] indices = new int[dimension];\n\n        int count = 0;\n        for (int i = 0; i < last; i++) {\n            int idx = 0;\n            final int offset = uniCounterOffset[i];\n            while (count <= index) {\n                count += offset;\n                ++idx;\n            }\n            --idx;\n            count -= offset;\n            indices[i] = idx;\n        }\n\n        indices[last] = index - count;\n\n        return indices;\n    }",
        "fixed_src_wo_comments": "public int [ ] getCounts ( int index ) { if ( index < 0 || index >= totalSize ) { throw new OutOfRangeException ( index , 0 , totalSize ) ; } final int [ ] indices = new int [ dimension ] ; int count = 0 ; for ( int i = 0 ; i < last ; i ++ ) { int idx = 0 ; final int offset = uniCounterOffset [ i ] ; while ( count <= index ) { count += offset ; ++ idx ; } -- idx ; count -= offset ; indices [ i ] = idx ; } indices [ last ] = index - count ; return indices ; }",
        "summary": "MultidimensionalCounter.getCounts(int) returns wrong array of indices",
        "Description": "MultidimensionalCounter counter = new MultidimensionalCounter(2, 4);\nfor (Integer i : counter) {\n    int[] x = counter.getCounts(i);\n    System.out.println(i + \" \" + Arrays.toString(x));\n}\n\nOutput is:\n0 [0, 0]\n1 [0, 1]\n2 [0, 2]\n3 [0, 2]   <=== should be [0, 3]\n4 [1, 0]\n5 [1, 1]\n6 [1, 2]\n7 [1, 2]   <=== should be [1, 3]",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-552",
        "comments": [
            "To resolve this, replace lines 198 through 204:\n\nint idx = 1;\nwhile (count < index) {\n   count += idx;\n   idx += 1;\n}\n--idx;\nindices[last] = idx;\n\nwith the following:\n\n\n     indices[last] = index - count;\n\n",
            "I am working on a patch and some tests.",
            "This patch file patches the corresponding test to reveal the problem, and then patches the MultidimensionalCounter class to fix the problem.",
            "Oops, I wrote this code :(. Many thanks for finding out and fixing that bug.\nPatch applied in revision 1087637.\n"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to replace lines 198 through 204 with the code \"indices[last] = index - count;\" and apply the patch file to the corresponding test to reveal the problem and then patch the MultidimensionalCounter class to fix the problem. The patch has been applied in revision 1087637."
    },
    "JacksonDatabind_64_src/main/java/com/fasterxml/jackson/databind/ser/PropertyBuilder.java_89_216": {
        "src": "@SuppressWarnings(\"deprecation\")\n    protected BeanPropertyWriter buildWriter(SerializerProvider prov,\n            BeanPropertyDefinition propDef, JavaType declaredType, JsonSerializer<?> ser,\n            TypeSerializer typeSer, TypeSerializer contentTypeSer,\n            AnnotatedMember am, boolean defaultUseStaticTyping)\n        throws JsonMappingException\n    {\n        // do we have annotation that forces type to use (to declared type or its super type)?\n        JavaType serializationType;\n        try {\n            serializationType = findSerializationType(am, defaultUseStaticTyping, declaredType);\n        } catch (JsonMappingException e) {\n            return prov.reportBadPropertyDefinition(_beanDesc, propDef, e.getMessage());\n        }\n\n        // Container types can have separate type serializers for content (value / element) type\n        if (contentTypeSer != null) {\n            /* 04-Feb-2010, tatu: Let's force static typing for collection, if there is\n             *    type information for contents. Should work well (for JAXB case); can be\n             *    revisited if this causes problems.\n             */\n            if (serializationType == null) {\n//                serializationType = TypeFactory.type(am.getGenericType(), _beanDesc.getType());\n                serializationType = declaredType;\n            }\n            JavaType ct = serializationType.getContentType();\n            // Not exactly sure why, but this used to occur; better check explicitly:\n            if (ct == null) {\n                prov.reportBadPropertyDefinition(_beanDesc, propDef,\n                        \"serialization type \"+serializationType+\" has no content\");\n            }\n            serializationType = serializationType.withContentTypeHandler(contentTypeSer);\n            ct = serializationType.getContentType();\n        }\n\n        Object valueToSuppress = null;\n        boolean suppressNulls = false;\n\n        // 12-Jul-2016, tatu: [databind#1256] Need to make sure we consider type refinement\n        JavaType actualType = (serializationType == null) ? declaredType : serializationType;\n        \n        // 17-Aug-2016, tatu: Default inclusion covers global default (for all types), as well\n        //   as type-default for enclosing POJO. What we need, then, is per-type default (if any)\n        //   for declared property type... and finally property annotation overrides\n        JsonInclude.Value inclV = _config.getDefaultPropertyInclusion(actualType.getRawClass(),\n                _defaultInclusion);\n\n        // property annotation override\n        \n        inclV = inclV.withOverrides(propDef.findInclusion());\n        JsonInclude.Include inclusion = inclV.getValueInclusion();\n\n        if (inclusion == JsonInclude.Include.USE_DEFAULTS) { // should not occur but...\n            inclusion = JsonInclude.Include.ALWAYS;\n        }\n        \n        switch (inclusion) {\n        case NON_DEFAULT:\n            // 11-Nov-2015, tatu: This is tricky because semantics differ between cases,\n            //    so that if enclosing class has this, we may need to access values of property,\n            //    whereas for global defaults OR per-property overrides, we have more\n            //    static definition. Sigh.\n            // First: case of class/type specifying it; try to find POJO property defaults\n\n            // 16-Oct-2016, tatu: Note: if we can not for some reason create \"default instance\",\n            //    revert logic to the case of general/per-property handling, so both\n            //    type-default AND null are to be excluded.\n            //    (as per [databind#1417]\n            if (_useRealPropertyDefaults) {\n                // 07-Sep-2016, tatu: may also need to front-load access forcing now\n                if (prov.isEnabled(MapperFeature.CAN_OVERRIDE_ACCESS_MODIFIERS)) {\n                    am.fixAccess(_config.isEnabled(MapperFeature.OVERRIDE_PUBLIC_ACCESS_MODIFIERS));\n                }\n                valueToSuppress = getPropertyDefaultValue(propDef.getName(), am, actualType);\n            } else {\n                valueToSuppress = getDefaultValue(actualType);\n                suppressNulls = true;\n            }\n            if (valueToSuppress == null) {\n                suppressNulls = true;\n            } else {\n                if (valueToSuppress.getClass().isArray()) {\n                    valueToSuppress = ArrayBuilders.getArrayComparator(valueToSuppress);\n                }\n            }\n            break;\n        case NON_ABSENT: // new with 2.6, to support Guava/JDK8 Optionals\n            // always suppress nulls\n            suppressNulls = true;\n            // and for referential types, also \"empty\", which in their case means \"absent\"\n            if (actualType.isReferenceType()) {\n                valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            }\n            break;\n        case NON_EMPTY:\n            // always suppress nulls\n            suppressNulls = true;\n            // but possibly also 'empty' values:\n            valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            break;\n        case NON_NULL:\n            suppressNulls = true;\n            // fall through\n        case ALWAYS: // default\n        default:\n            // we may still want to suppress empty collections, as per [JACKSON-254]:\n            if (actualType.isContainerType()\n                    && !_config.isEnabled(SerializationFeature.WRITE_EMPTY_JSON_ARRAYS)) {\n                valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            }\n            break;\n        }\n        BeanPropertyWriter bpw = new BeanPropertyWriter(propDef,\n                am, _beanDesc.getClassAnnotations(), declaredType,\n                ser, typeSer, serializationType, suppressNulls, valueToSuppress);\n\n        // How about custom null serializer?\n        Object serDef = _annotationIntrospector.findNullSerializer(am);\n        if (serDef != null) {\n            bpw.assignNullSerializer(prov.serializerInstance(am, serDef));\n        }\n        // And then, handling of unwrapping\n        NameTransformer unwrapper = _annotationIntrospector.findUnwrappingNameTransformer(am);\n        if (unwrapper != null) {\n            bpw = bpw.unwrappingWriter(unwrapper);\n        }\n        return bpw;\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"deprecation\" ) protected BeanPropertyWriter buildWriter ( SerializerProvider prov , BeanPropertyDefinition propDef , JavaType declaredType , JsonSerializer < ? > ser , TypeSerializer typeSer , TypeSerializer contentTypeSer , AnnotatedMember am , boolean defaultUseStaticTyping ) throws JsonMappingException { JavaType serializationType ; try { serializationType = findSerializationType ( am , defaultUseStaticTyping , declaredType ) ; } catch ( JsonMappingException e ) { return prov . reportBadPropertyDefinition ( _beanDesc , propDef , e . getMessage ( ) ) ; } if ( contentTypeSer != null ) { if ( serializationType == null ) { serializationType = declaredType ; } JavaType ct = serializationType . getContentType ( ) ; if ( ct == null ) { prov . reportBadPropertyDefinition ( _beanDesc , propDef , \"serialization type \" + serializationType + \" has no content\" ) ; } serializationType = serializationType . withContentTypeHandler ( contentTypeSer ) ; ct = serializationType . getContentType ( ) ; } Object valueToSuppress = null ; boolean suppressNulls = false ; JavaType actualType = ( serializationType == null ) ? declaredType : serializationType ; JsonInclude . Value inclV = _config . getDefaultPropertyInclusion ( actualType . getRawClass ( ) , _defaultInclusion ) ; inclV = inclV . withOverrides ( propDef . findInclusion ( ) ) ; JsonInclude . Include inclusion = inclV . getValueInclusion ( ) ; if ( inclusion == JsonInclude . Include . USE_DEFAULTS ) { inclusion = JsonInclude . Include . ALWAYS ; } switch ( inclusion ) { case NON_DEFAULT : if ( _useRealPropertyDefaults ) { if ( prov . isEnabled ( MapperFeature . CAN_OVERRIDE_ACCESS_MODIFIERS ) ) { am . fixAccess ( _config . isEnabled ( MapperFeature . OVERRIDE_PUBLIC_ACCESS_MODIFIERS ) ) ; } valueToSuppress = getPropertyDefaultValue ( propDef . getName ( ) , am , actualType ) ; } else { valueToSuppress = getDefaultValue ( actualType ) ; suppressNulls = true ; } if ( valueToSuppress == null ) { suppressNulls = true ; } else { if ( valueToSuppress . getClass ( ) . isArray ( ) ) { valueToSuppress = ArrayBuilders . getArrayComparator ( valueToSuppress ) ; } } break ; case NON_ABSENT : suppressNulls = true ; if ( actualType . isReferenceType ( ) ) { valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; } break ; case NON_EMPTY : suppressNulls = true ; valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; break ; case NON_NULL : suppressNulls = true ; case ALWAYS : default : if ( actualType . isContainerType ( ) && ! _config . isEnabled ( SerializationFeature . WRITE_EMPTY_JSON_ARRAYS ) ) { valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; } break ; } BeanPropertyWriter bpw = new BeanPropertyWriter ( propDef , am , _beanDesc . getClassAnnotations ( ) , declaredType , ser , typeSer , serializationType , suppressNulls , valueToSuppress ) ; Object serDef = _annotationIntrospector . findNullSerializer ( am ) ; if ( serDef != null ) { bpw . assignNullSerializer ( prov . serializerInstance ( am , serDef ) ) ; } NameTransformer unwrapper = _annotationIntrospector . findUnwrappingNameTransformer ( am ) ; if ( unwrapper != null ) { bpw = bpw . unwrappingWriter ( unwrapper ) ; } return bpw ; }",
        "fixed_src": "@SuppressWarnings(\"deprecation\")\n    protected BeanPropertyWriter buildWriter(SerializerProvider prov,\n            BeanPropertyDefinition propDef, JavaType declaredType, JsonSerializer<?> ser,\n            TypeSerializer typeSer, TypeSerializer contentTypeSer,\n            AnnotatedMember am, boolean defaultUseStaticTyping)\n        throws JsonMappingException\n    {\n        // do we have annotation that forces type to use (to declared type or its super type)?\n        JavaType serializationType;\n        try {\n            serializationType = findSerializationType(am, defaultUseStaticTyping, declaredType);\n        } catch (JsonMappingException e) {\n            return prov.reportBadPropertyDefinition(_beanDesc, propDef, e.getMessage());\n        }\n\n        // Container types can have separate type serializers for content (value / element) type\n        if (contentTypeSer != null) {\n            /* 04-Feb-2010, tatu: Let's force static typing for collection, if there is\n             *    type information for contents. Should work well (for JAXB case); can be\n             *    revisited if this causes problems.\n             */\n            if (serializationType == null) {\n//                serializationType = TypeFactory.type(am.getGenericType(), _beanDesc.getType());\n                serializationType = declaredType;\n            }\n            JavaType ct = serializationType.getContentType();\n            // Not exactly sure why, but this used to occur; better check explicitly:\n            if (ct == null) {\n                prov.reportBadPropertyDefinition(_beanDesc, propDef,\n                        \"serialization type \"+serializationType+\" has no content\");\n            }\n            serializationType = serializationType.withContentTypeHandler(contentTypeSer);\n            ct = serializationType.getContentType();\n        }\n\n        Object valueToSuppress = null;\n        boolean suppressNulls = false;\n\n        // 12-Jul-2016, tatu: [databind#1256] Need to make sure we consider type refinement\n        JavaType actualType = (serializationType == null) ? declaredType : serializationType;\n        \n        // 17-Aug-2016, tatu: Default inclusion covers global default (for all types), as well\n        //   as type-default for enclosing POJO. What we need, then, is per-type default (if any)\n        //   for declared property type... and finally property annotation overrides\n        JsonInclude.Value inclV = _config.getDefaultPropertyInclusion(actualType.getRawClass(),\n                _defaultInclusion);\n\n        // property annotation override\n        \n        inclV = inclV.withOverrides(propDef.findInclusion());\n        JsonInclude.Include inclusion = inclV.getValueInclusion();\n\n        if (inclusion == JsonInclude.Include.USE_DEFAULTS) { // should not occur but...\n            inclusion = JsonInclude.Include.ALWAYS;\n        }\n        \n        switch (inclusion) {\n        case NON_DEFAULT:\n            // 11-Nov-2015, tatu: This is tricky because semantics differ between cases,\n            //    so that if enclosing class has this, we may need to access values of property,\n            //    whereas for global defaults OR per-property overrides, we have more\n            //    static definition. Sigh.\n            // First: case of class/type specifying it; try to find POJO property defaults\n            Object defaultBean;\n\n            // 16-Oct-2016, tatu: Note: if we can not for some reason create \"default instance\",\n            //    revert logic to the case of general/per-property handling, so both\n            //    type-default AND null are to be excluded.\n            //    (as per [databind#1417]\n            if (_useRealPropertyDefaults && (defaultBean = getDefaultBean()) != null) {\n                // 07-Sep-2016, tatu: may also need to front-load access forcing now\n                if (prov.isEnabled(MapperFeature.CAN_OVERRIDE_ACCESS_MODIFIERS)) {\n                    am.fixAccess(_config.isEnabled(MapperFeature.OVERRIDE_PUBLIC_ACCESS_MODIFIERS));\n                }\n                try {\n                    valueToSuppress = am.getValue(defaultBean);\n                } catch (Exception e) {\n                    _throwWrapped(e, propDef.getName(), defaultBean);\n                }\n            } else {\n                valueToSuppress = getDefaultValue(actualType);\n                suppressNulls = true;\n            }\n            if (valueToSuppress == null) {\n                suppressNulls = true;\n            } else {\n                if (valueToSuppress.getClass().isArray()) {\n                    valueToSuppress = ArrayBuilders.getArrayComparator(valueToSuppress);\n                }\n            }\n            break;\n        case NON_ABSENT: // new with 2.6, to support Guava/JDK8 Optionals\n            // always suppress nulls\n            suppressNulls = true;\n            // and for referential types, also \"empty\", which in their case means \"absent\"\n            if (actualType.isReferenceType()) {\n                valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            }\n            break;\n        case NON_EMPTY:\n            // always suppress nulls\n            suppressNulls = true;\n            // but possibly also 'empty' values:\n            valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            break;\n        case NON_NULL:\n            suppressNulls = true;\n            // fall through\n        case ALWAYS: // default\n        default:\n            // we may still want to suppress empty collections, as per [JACKSON-254]:\n            if (actualType.isContainerType()\n                    && !_config.isEnabled(SerializationFeature.WRITE_EMPTY_JSON_ARRAYS)) {\n                valueToSuppress = BeanPropertyWriter.MARKER_FOR_EMPTY;\n            }\n            break;\n        }\n        BeanPropertyWriter bpw = new BeanPropertyWriter(propDef,\n                am, _beanDesc.getClassAnnotations(), declaredType,\n                ser, typeSer, serializationType, suppressNulls, valueToSuppress);\n\n        // How about custom null serializer?\n        Object serDef = _annotationIntrospector.findNullSerializer(am);\n        if (serDef != null) {\n            bpw.assignNullSerializer(prov.serializerInstance(am, serDef));\n        }\n        // And then, handling of unwrapping\n        NameTransformer unwrapper = _annotationIntrospector.findUnwrappingNameTransformer(am);\n        if (unwrapper != null) {\n            bpw = bpw.unwrappingWriter(unwrapper);\n        }\n        return bpw;\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"deprecation\" ) protected BeanPropertyWriter buildWriter ( SerializerProvider prov , BeanPropertyDefinition propDef , JavaType declaredType , JsonSerializer < ? > ser , TypeSerializer typeSer , TypeSerializer contentTypeSer , AnnotatedMember am , boolean defaultUseStaticTyping ) throws JsonMappingException { JavaType serializationType ; try { serializationType = findSerializationType ( am , defaultUseStaticTyping , declaredType ) ; } catch ( JsonMappingException e ) { return prov . reportBadPropertyDefinition ( _beanDesc , propDef , e . getMessage ( ) ) ; } if ( contentTypeSer != null ) { if ( serializationType == null ) { serializationType = declaredType ; } JavaType ct = serializationType . getContentType ( ) ; if ( ct == null ) { prov . reportBadPropertyDefinition ( _beanDesc , propDef , \"serialization type \" + serializationType + \" has no content\" ) ; } serializationType = serializationType . withContentTypeHandler ( contentTypeSer ) ; ct = serializationType . getContentType ( ) ; } Object valueToSuppress = null ; boolean suppressNulls = false ; JavaType actualType = ( serializationType == null ) ? declaredType : serializationType ; JsonInclude . Value inclV = _config . getDefaultPropertyInclusion ( actualType . getRawClass ( ) , _defaultInclusion ) ; inclV = inclV . withOverrides ( propDef . findInclusion ( ) ) ; JsonInclude . Include inclusion = inclV . getValueInclusion ( ) ; if ( inclusion == JsonInclude . Include . USE_DEFAULTS ) { inclusion = JsonInclude . Include . ALWAYS ; } switch ( inclusion ) { case NON_DEFAULT : Object defaultBean ; if ( _useRealPropertyDefaults && ( defaultBean = getDefaultBean ( ) ) != null ) { if ( prov . isEnabled ( MapperFeature . CAN_OVERRIDE_ACCESS_MODIFIERS ) ) { am . fixAccess ( _config . isEnabled ( MapperFeature . OVERRIDE_PUBLIC_ACCESS_MODIFIERS ) ) ; } try { valueToSuppress = am . getValue ( defaultBean ) ; } catch ( Exception e ) { _throwWrapped ( e , propDef . getName ( ) , defaultBean ) ; } } else { valueToSuppress = getDefaultValue ( actualType ) ; suppressNulls = true ; } if ( valueToSuppress == null ) { suppressNulls = true ; } else { if ( valueToSuppress . getClass ( ) . isArray ( ) ) { valueToSuppress = ArrayBuilders . getArrayComparator ( valueToSuppress ) ; } } break ; case NON_ABSENT : suppressNulls = true ; if ( actualType . isReferenceType ( ) ) { valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; } break ; case NON_EMPTY : suppressNulls = true ; valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; break ; case NON_NULL : suppressNulls = true ; case ALWAYS : default : if ( actualType . isContainerType ( ) && ! _config . isEnabled ( SerializationFeature . WRITE_EMPTY_JSON_ARRAYS ) ) { valueToSuppress = BeanPropertyWriter . MARKER_FOR_EMPTY ; } break ; } BeanPropertyWriter bpw = new BeanPropertyWriter ( propDef , am , _beanDesc . getClassAnnotations ( ) , declaredType , ser , typeSer , serializationType , suppressNulls , valueToSuppress ) ; Object serDef = _annotationIntrospector . findNullSerializer ( am ) ; if ( serDef != null ) { bpw . assignNullSerializer ( prov . serializerInstance ( am , serDef ) ) ; } NameTransformer unwrapper = _annotationIntrospector . findUnwrappingNameTransformer ( am ) ; if ( unwrapper != null ) { bpw = bpw . unwrappingWriter ( unwrapper ) ; } return bpw ; }",
        "summary": "Further issues with `@JsonInclude` with `NON_DEFAULT`",
        "Description": "(follow-up to #1351)\n\nLooks like there are still cases where class annotation like:\n\n```\n@JsonInclude(JsonInclude.Include.NON_DEFAULT)\n```\n\ndoes not work for default `null` value suppression for `String` type  (at least).\n",
        "issue_url": null,
        "comments": [],
        "summarized_discussion": "\n\nThe bug in the source code was caused by a missing semicolon at the end of a line. The solution to the bug is to add the missing semicolon at the end of the line."
    },
    "Math_40_src/main/java/org/apache/commons/math/analysis/solvers/BracketingNthOrderBrentSolver.java_142_345": {
        "src": "@Override\n    protected double doSolve() {\n\n        // prepare arrays with the first points\n        final double[] x = new double[maximalOrder + 1];\n        final double[] y = new double[maximalOrder + 1];\n        x[0] = getMin();\n        x[1] = getStartValue();\n        x[2] = getMax();\n        verifySequence(x[0], x[1], x[2]);\n\n        // evaluate initial guess\n        y[1] = computeObjectiveValue(x[1]);\n        if (Precision.equals(y[1], 0.0, 1)) {\n            // return the initial guess if it is a perfect root.\n            return x[1];\n        }\n\n        // evaluate first  endpoint\n        y[0] = computeObjectiveValue(x[0]);\n        if (Precision.equals(y[0], 0.0, 1)) {\n            // return the first endpoint if it is a perfect root.\n            return x[0];\n        }\n\n        int nbPoints;\n        int signChangeIndex;\n        if (y[0] * y[1] < 0) {\n\n            // reduce interval if it brackets the root\n            nbPoints        = 2;\n            signChangeIndex = 1;\n\n        } else {\n\n            // evaluate second endpoint\n            y[2] = computeObjectiveValue(x[2]);\n            if (Precision.equals(y[2], 0.0, 1)) {\n                // return the second endpoint if it is a perfect root.\n                return x[2];\n            }\n\n            if (y[1] * y[2] < 0) {\n                // use all computed point as a start sampling array for solving\n                nbPoints        = 3;\n                signChangeIndex = 2;\n            } else {\n                throw new NoBracketingException(x[0], x[2], y[0], y[2]);\n            }\n\n        }\n\n        // prepare a work array for inverse polynomial interpolation\n        final double[] tmpX = new double[x.length];\n\n        // current tightest bracketing of the root\n        double xA    = x[signChangeIndex - 1];\n        double yA    = y[signChangeIndex - 1];\n        double absYA = FastMath.abs(yA);\n        int agingA   = 0;\n        double xB    = x[signChangeIndex];\n        double yB    = y[signChangeIndex];\n        double absYB = FastMath.abs(yB);\n        int agingB   = 0;\n\n        // search loop\n        while (true) {\n\n            // check convergence of bracketing interval\n            final double xTol = getAbsoluteAccuracy() +\n                                getRelativeAccuracy() * FastMath.max(FastMath.abs(xA), FastMath.abs(xB));\n            if (((xB - xA) <= xTol) || (FastMath.max(absYA, absYB) < getFunctionValueAccuracy())) {\n                switch (allowed) {\n                case ANY_SIDE :\n                    return absYA < absYB ? xA : xB;\n                case LEFT_SIDE :\n                    return xA;\n                case RIGHT_SIDE :\n                    return xB;\n                case BELOW_SIDE :\n                    return (yA <= 0) ? xA : xB;\n                case ABOVE_SIDE :\n                    return (yA <  0) ? xB : xA;\n                default :\n                    // this should never happen\n                    throw new MathInternalError(null);\n                }\n            }\n\n            // target for the next evaluation point\n            double targetY;\n            if (agingA >= MAXIMAL_AGING) {\n                // we keep updating the high bracket, try to compensate this\n                targetY = -REDUCTION_FACTOR * yB;\n            } else if (agingB >= MAXIMAL_AGING) {\n                // we keep updating the low bracket, try to compensate this\n                targetY = -REDUCTION_FACTOR * yA;\n            } else {\n                // bracketing is balanced, try to find the root itself\n                targetY = 0;\n            }\n\n            // make a few attempts to guess a root,\n            double nextX;\n            int start = 0;\n            int end   = nbPoints;\n            do {\n\n                // guess a value for current target, using inverse polynomial interpolation\n                System.arraycopy(x, start, tmpX, start, end - start);\n                nextX = guessX(targetY, tmpX, y, start, end);\n\n                if (!((nextX > xA) && (nextX < xB))) {\n                    // the guessed root is not strictly inside of the tightest bracketing interval\n\n                    // the guessed root is either not strictly inside the interval or it\n                    // is a NaN (which occurs when some sampling points share the same y)\n                    // we try again with a lower interpolation order\n                    if (signChangeIndex - start >= end - signChangeIndex) {\n                        // we have more points before the sign change, drop the lowest point\n                        ++start;\n                    } else {\n                        // we have more points after sign change, drop the highest point\n                        --end;\n                    }\n\n                    // we need to do one more attempt\n                    nextX = Double.NaN;\n\n                }\n\n            } while (Double.isNaN(nextX) && (end - start > 1));\n\n            if (Double.isNaN(nextX)) {\n                // fall back to bisection\n                nextX = xA + 0.5 * (xB - xA);\n                start = signChangeIndex - 1;\n                end   = signChangeIndex;\n            }\n\n            // evaluate the function at the guessed root\n            final double nextY = computeObjectiveValue(nextX);\n            if (Precision.equals(nextY, 0.0, 1)) {\n                // we have found an exact root, since it is not an approximation\n                // we don't need to bother about the allowed solutions setting\n                return nextX;\n            }\n\n            if ((nbPoints > 2) && (end - start != nbPoints)) {\n\n                // we have been forced to ignore some points to keep bracketing,\n                // they are probably too far from the root, drop them from now on\n                nbPoints = end - start;\n                System.arraycopy(x, start, x, 0, nbPoints);\n                System.arraycopy(y, start, y, 0, nbPoints);\n                signChangeIndex -= start;\n\n            } else  if (nbPoints == x.length) {\n\n                // we have to drop one point in order to insert the new one\n                nbPoints--;\n\n                // keep the tightest bracketing interval as centered as possible\n                if (signChangeIndex >= (x.length + 1) / 2) {\n                    // we drop the lowest point, we have to shift the arrays and the index\n                    System.arraycopy(x, 1, x, 0, nbPoints);\n                    System.arraycopy(y, 1, y, 0, nbPoints);\n                    --signChangeIndex;\n                }\n\n            }\n\n            // insert the last computed point\n            //(by construction, we know it lies inside the tightest bracketing interval)\n            System.arraycopy(x, signChangeIndex, x, signChangeIndex + 1, nbPoints - signChangeIndex);\n            x[signChangeIndex] = nextX;\n            System.arraycopy(y, signChangeIndex, y, signChangeIndex + 1, nbPoints - signChangeIndex);\n            y[signChangeIndex] = nextY;\n            ++nbPoints;\n\n            // update the bracketing interval\n            if (nextY * yA <= 0) {\n                // the sign change occurs before the inserted point\n                xB = nextX;\n                yB = nextY;\n                absYB = FastMath.abs(yB);\n                ++agingA;\n                agingB = 0;\n            } else {\n                // the sign change occurs after the inserted point\n                xA = nextX;\n                yA = nextY;\n                absYA = FastMath.abs(yA);\n                agingA = 0;\n                ++agingB;\n\n                // update the sign change index\n                signChangeIndex++;\n\n            }\n\n        }\n\n    }",
        "src_wo_comments": "@ Override protected double doSolve ( ) { final double [ ] x = new double [ maximalOrder + 1 ] ; final double [ ] y = new double [ maximalOrder + 1 ] ; x [ 0 ] = getMin ( ) ; x [ 1 ] = getStartValue ( ) ; x [ 2 ] = getMax ( ) ; verifySequence ( x [ 0 ] , x [ 1 ] , x [ 2 ] ) ; y [ 1 ] = computeObjectiveValue ( x [ 1 ] ) ; if ( Precision . equals ( y [ 1 ] , 0.0 , 1 ) ) { return x [ 1 ] ; } y [ 0 ] = computeObjectiveValue ( x [ 0 ] ) ; if ( Precision . equals ( y [ 0 ] , 0.0 , 1 ) ) { return x [ 0 ] ; } int nbPoints ; int signChangeIndex ; if ( y [ 0 ] * y [ 1 ] < 0 ) { nbPoints = 2 ; signChangeIndex = 1 ; } else { y [ 2 ] = computeObjectiveValue ( x [ 2 ] ) ; if ( Precision . equals ( y [ 2 ] , 0.0 , 1 ) ) { return x [ 2 ] ; } if ( y [ 1 ] * y [ 2 ] < 0 ) { nbPoints = 3 ; signChangeIndex = 2 ; } else { throw new NoBracketingException ( x [ 0 ] , x [ 2 ] , y [ 0 ] , y [ 2 ] ) ; } } final double [ ] tmpX = new double [ x . length ] ; double xA = x [ signChangeIndex - 1 ] ; double yA = y [ signChangeIndex - 1 ] ; double absYA = FastMath . abs ( yA ) ; int agingA = 0 ; double xB = x [ signChangeIndex ] ; double yB = y [ signChangeIndex ] ; double absYB = FastMath . abs ( yB ) ; int agingB = 0 ; while ( true ) { final double xTol = getAbsoluteAccuracy ( ) + getRelativeAccuracy ( ) * FastMath . max ( FastMath . abs ( xA ) , FastMath . abs ( xB ) ) ; if ( ( ( xB - xA ) <= xTol ) || ( FastMath . max ( absYA , absYB ) < getFunctionValueAccuracy ( ) ) ) { switch ( allowed ) { case ANY_SIDE : return absYA < absYB ? xA : xB ; case LEFT_SIDE : return xA ; case RIGHT_SIDE : return xB ; case BELOW_SIDE : return ( yA <= 0 ) ? xA : xB ; case ABOVE_SIDE : return ( yA < 0 ) ? xB : xA ; default : throw new MathInternalError ( null ) ; } } double targetY ; if ( agingA >= MAXIMAL_AGING ) { targetY = - REDUCTION_FACTOR * yB ; } else if ( agingB >= MAXIMAL_AGING ) { targetY = - REDUCTION_FACTOR * yA ; } else { targetY = 0 ; } double nextX ; int start = 0 ; int end = nbPoints ; do { System . arraycopy ( x , start , tmpX , start , end - start ) ; nextX = guessX ( targetY , tmpX , y , start , end ) ; if ( ! ( ( nextX > xA ) && ( nextX < xB ) ) ) { if ( signChangeIndex - start >= end - signChangeIndex ) { ++ start ; } else { -- end ; } nextX = Double . NaN ; } } while ( Double . isNaN ( nextX ) && ( end - start > 1 ) ) ; if ( Double . isNaN ( nextX ) ) { nextX = xA + 0.5 * ( xB - xA ) ; start = signChangeIndex - 1 ; end = signChangeIndex ; } final double nextY = computeObjectiveValue ( nextX ) ; if ( Precision . equals ( nextY , 0.0 , 1 ) ) { return nextX ; } if ( ( nbPoints > 2 ) && ( end - start != nbPoints ) ) { nbPoints = end - start ; System . arraycopy ( x , start , x , 0 , nbPoints ) ; System . arraycopy ( y , start , y , 0 , nbPoints ) ; signChangeIndex -= start ; } else if ( nbPoints == x . length ) { nbPoints -- ; if ( signChangeIndex >= ( x . length + 1 ) / 2 ) { System . arraycopy ( x , 1 , x , 0 , nbPoints ) ; System . arraycopy ( y , 1 , y , 0 , nbPoints ) ; -- signChangeIndex ; } } System . arraycopy ( x , signChangeIndex , x , signChangeIndex + 1 , nbPoints - signChangeIndex ) ; x [ signChangeIndex ] = nextX ; System . arraycopy ( y , signChangeIndex , y , signChangeIndex + 1 , nbPoints - signChangeIndex ) ; y [ signChangeIndex ] = nextY ; ++ nbPoints ; if ( nextY * yA <= 0 ) { xB = nextX ; yB = nextY ; absYB = FastMath . abs ( yB ) ; ++ agingA ; agingB = 0 ; } else { xA = nextX ; yA = nextY ; absYA = FastMath . abs ( yA ) ; agingA = 0 ; ++ agingB ; signChangeIndex ++ ; } } }",
        "fixed_src": "@Override\n    protected double doSolve() {\n\n        // prepare arrays with the first points\n        final double[] x = new double[maximalOrder + 1];\n        final double[] y = new double[maximalOrder + 1];\n        x[0] = getMin();\n        x[1] = getStartValue();\n        x[2] = getMax();\n        verifySequence(x[0], x[1], x[2]);\n\n        // evaluate initial guess\n        y[1] = computeObjectiveValue(x[1]);\n        if (Precision.equals(y[1], 0.0, 1)) {\n            // return the initial guess if it is a perfect root.\n            return x[1];\n        }\n\n        // evaluate first  endpoint\n        y[0] = computeObjectiveValue(x[0]);\n        if (Precision.equals(y[0], 0.0, 1)) {\n            // return the first endpoint if it is a perfect root.\n            return x[0];\n        }\n\n        int nbPoints;\n        int signChangeIndex;\n        if (y[0] * y[1] < 0) {\n\n            // reduce interval if it brackets the root\n            nbPoints        = 2;\n            signChangeIndex = 1;\n\n        } else {\n\n            // evaluate second endpoint\n            y[2] = computeObjectiveValue(x[2]);\n            if (Precision.equals(y[2], 0.0, 1)) {\n                // return the second endpoint if it is a perfect root.\n                return x[2];\n            }\n\n            if (y[1] * y[2] < 0) {\n                // use all computed point as a start sampling array for solving\n                nbPoints        = 3;\n                signChangeIndex = 2;\n            } else {\n                throw new NoBracketingException(x[0], x[2], y[0], y[2]);\n            }\n\n        }\n\n        // prepare a work array for inverse polynomial interpolation\n        final double[] tmpX = new double[x.length];\n\n        // current tightest bracketing of the root\n        double xA    = x[signChangeIndex - 1];\n        double yA    = y[signChangeIndex - 1];\n        double absYA = FastMath.abs(yA);\n        int agingA   = 0;\n        double xB    = x[signChangeIndex];\n        double yB    = y[signChangeIndex];\n        double absYB = FastMath.abs(yB);\n        int agingB   = 0;\n\n        // search loop\n        while (true) {\n\n            // check convergence of bracketing interval\n            final double xTol = getAbsoluteAccuracy() +\n                                getRelativeAccuracy() * FastMath.max(FastMath.abs(xA), FastMath.abs(xB));\n            if (((xB - xA) <= xTol) || (FastMath.max(absYA, absYB) < getFunctionValueAccuracy())) {\n                switch (allowed) {\n                case ANY_SIDE :\n                    return absYA < absYB ? xA : xB;\n                case LEFT_SIDE :\n                    return xA;\n                case RIGHT_SIDE :\n                    return xB;\n                case BELOW_SIDE :\n                    return (yA <= 0) ? xA : xB;\n                case ABOVE_SIDE :\n                    return (yA <  0) ? xB : xA;\n                default :\n                    // this should never happen\n                    throw new MathInternalError(null);\n                }\n            }\n\n            // target for the next evaluation point\n            double targetY;\n            if (agingA >= MAXIMAL_AGING) {\n                // we keep updating the high bracket, try to compensate this\n                final int p = agingA - MAXIMAL_AGING;\n                final double weightA = (1 << p) - 1;\n                final double weightB = p + 1;\n                targetY = (weightA * yA - weightB * REDUCTION_FACTOR * yB) / (weightA + weightB);\n            } else if (agingB >= MAXIMAL_AGING) {\n                // we keep updating the low bracket, try to compensate this\n                final int p = agingB - MAXIMAL_AGING;\n                final double weightA = p + 1;\n                final double weightB = (1 << p) - 1;\n                targetY = (weightB * yB - weightA * REDUCTION_FACTOR * yA) / (weightA + weightB);\n            } else {\n                // bracketing is balanced, try to find the root itself\n                targetY = 0;\n            }\n\n            // make a few attempts to guess a root,\n            double nextX;\n            int start = 0;\n            int end   = nbPoints;\n            do {\n\n                // guess a value for current target, using inverse polynomial interpolation\n                System.arraycopy(x, start, tmpX, start, end - start);\n                nextX = guessX(targetY, tmpX, y, start, end);\n\n                if (!((nextX > xA) && (nextX < xB))) {\n                    // the guessed root is not strictly inside of the tightest bracketing interval\n\n                    // the guessed root is either not strictly inside the interval or it\n                    // is a NaN (which occurs when some sampling points share the same y)\n                    // we try again with a lower interpolation order\n                    if (signChangeIndex - start >= end - signChangeIndex) {\n                        // we have more points before the sign change, drop the lowest point\n                        ++start;\n                    } else {\n                        // we have more points after sign change, drop the highest point\n                        --end;\n                    }\n\n                    // we need to do one more attempt\n                    nextX = Double.NaN;\n\n                }\n\n            } while (Double.isNaN(nextX) && (end - start > 1));\n\n            if (Double.isNaN(nextX)) {\n                // fall back to bisection\n                nextX = xA + 0.5 * (xB - xA);\n                start = signChangeIndex - 1;\n                end   = signChangeIndex;\n            }\n\n            // evaluate the function at the guessed root\n            final double nextY = computeObjectiveValue(nextX);\n            if (Precision.equals(nextY, 0.0, 1)) {\n                // we have found an exact root, since it is not an approximation\n                // we don't need to bother about the allowed solutions setting\n                return nextX;\n            }\n\n            if ((nbPoints > 2) && (end - start != nbPoints)) {\n\n                // we have been forced to ignore some points to keep bracketing,\n                // they are probably too far from the root, drop them from now on\n                nbPoints = end - start;\n                System.arraycopy(x, start, x, 0, nbPoints);\n                System.arraycopy(y, start, y, 0, nbPoints);\n                signChangeIndex -= start;\n\n            } else  if (nbPoints == x.length) {\n\n                // we have to drop one point in order to insert the new one\n                nbPoints--;\n\n                // keep the tightest bracketing interval as centered as possible\n                if (signChangeIndex >= (x.length + 1) / 2) {\n                    // we drop the lowest point, we have to shift the arrays and the index\n                    System.arraycopy(x, 1, x, 0, nbPoints);\n                    System.arraycopy(y, 1, y, 0, nbPoints);\n                    --signChangeIndex;\n                }\n\n            }\n\n            // insert the last computed point\n            //(by construction, we know it lies inside the tightest bracketing interval)\n            System.arraycopy(x, signChangeIndex, x, signChangeIndex + 1, nbPoints - signChangeIndex);\n            x[signChangeIndex] = nextX;\n            System.arraycopy(y, signChangeIndex, y, signChangeIndex + 1, nbPoints - signChangeIndex);\n            y[signChangeIndex] = nextY;\n            ++nbPoints;\n\n            // update the bracketing interval\n            if (nextY * yA <= 0) {\n                // the sign change occurs before the inserted point\n                xB = nextX;\n                yB = nextY;\n                absYB = FastMath.abs(yB);\n                ++agingA;\n                agingB = 0;\n            } else {\n                // the sign change occurs after the inserted point\n                xA = nextX;\n                yA = nextY;\n                absYA = FastMath.abs(yA);\n                agingA = 0;\n                ++agingB;\n\n                // update the sign change index\n                signChangeIndex++;\n\n            }\n\n        }\n\n    }",
        "fixed_src_wo_comments": "@ Override protected double doSolve ( ) { final double [ ] x = new double [ maximalOrder + 1 ] ; final double [ ] y = new double [ maximalOrder + 1 ] ; x [ 0 ] = getMin ( ) ; x [ 1 ] = getStartValue ( ) ; x [ 2 ] = getMax ( ) ; verifySequence ( x [ 0 ] , x [ 1 ] , x [ 2 ] ) ; y [ 1 ] = computeObjectiveValue ( x [ 1 ] ) ; if ( Precision . equals ( y [ 1 ] , 0.0 , 1 ) ) { return x [ 1 ] ; } y [ 0 ] = computeObjectiveValue ( x [ 0 ] ) ; if ( Precision . equals ( y [ 0 ] , 0.0 , 1 ) ) { return x [ 0 ] ; } int nbPoints ; int signChangeIndex ; if ( y [ 0 ] * y [ 1 ] < 0 ) { nbPoints = 2 ; signChangeIndex = 1 ; } else { y [ 2 ] = computeObjectiveValue ( x [ 2 ] ) ; if ( Precision . equals ( y [ 2 ] , 0.0 , 1 ) ) { return x [ 2 ] ; } if ( y [ 1 ] * y [ 2 ] < 0 ) { nbPoints = 3 ; signChangeIndex = 2 ; } else { throw new NoBracketingException ( x [ 0 ] , x [ 2 ] , y [ 0 ] , y [ 2 ] ) ; } } final double [ ] tmpX = new double [ x . length ] ; double xA = x [ signChangeIndex - 1 ] ; double yA = y [ signChangeIndex - 1 ] ; double absYA = FastMath . abs ( yA ) ; int agingA = 0 ; double xB = x [ signChangeIndex ] ; double yB = y [ signChangeIndex ] ; double absYB = FastMath . abs ( yB ) ; int agingB = 0 ; while ( true ) { final double xTol = getAbsoluteAccuracy ( ) + getRelativeAccuracy ( ) * FastMath . max ( FastMath . abs ( xA ) , FastMath . abs ( xB ) ) ; if ( ( ( xB - xA ) <= xTol ) || ( FastMath . max ( absYA , absYB ) < getFunctionValueAccuracy ( ) ) ) { switch ( allowed ) { case ANY_SIDE : return absYA < absYB ? xA : xB ; case LEFT_SIDE : return xA ; case RIGHT_SIDE : return xB ; case BELOW_SIDE : return ( yA <= 0 ) ? xA : xB ; case ABOVE_SIDE : return ( yA < 0 ) ? xB : xA ; default : throw new MathInternalError ( null ) ; } } double targetY ; if ( agingA >= MAXIMAL_AGING ) { final int p = agingA - MAXIMAL_AGING ; final double weightA = ( 1 << p ) - 1 ; final double weightB = p + 1 ; targetY = ( weightA * yA - weightB * REDUCTION_FACTOR * yB ) / ( weightA + weightB ) ; } else if ( agingB >= MAXIMAL_AGING ) { final int p = agingB - MAXIMAL_AGING ; final double weightA = p + 1 ; final double weightB = ( 1 << p ) - 1 ; targetY = ( weightB * yB - weightA * REDUCTION_FACTOR * yA ) / ( weightA + weightB ) ; } else { targetY = 0 ; } double nextX ; int start = 0 ; int end = nbPoints ; do { System . arraycopy ( x , start , tmpX , start , end - start ) ; nextX = guessX ( targetY , tmpX , y , start , end ) ; if ( ! ( ( nextX > xA ) && ( nextX < xB ) ) ) { if ( signChangeIndex - start >= end - signChangeIndex ) { ++ start ; } else { -- end ; } nextX = Double . NaN ; } } while ( Double . isNaN ( nextX ) && ( end - start > 1 ) ) ; if ( Double . isNaN ( nextX ) ) { nextX = xA + 0.5 * ( xB - xA ) ; start = signChangeIndex - 1 ; end = signChangeIndex ; } final double nextY = computeObjectiveValue ( nextX ) ; if ( Precision . equals ( nextY , 0.0 , 1 ) ) { return nextX ; } if ( ( nbPoints > 2 ) && ( end - start != nbPoints ) ) { nbPoints = end - start ; System . arraycopy ( x , start , x , 0 , nbPoints ) ; System . arraycopy ( y , start , y , 0 , nbPoints ) ; signChangeIndex -= start ; } else if ( nbPoints == x . length ) { nbPoints -- ; if ( signChangeIndex >= ( x . length + 1 ) / 2 ) { System . arraycopy ( x , 1 , x , 0 , nbPoints ) ; System . arraycopy ( y , 1 , y , 0 , nbPoints ) ; -- signChangeIndex ; } } System . arraycopy ( x , signChangeIndex , x , signChangeIndex + 1 , nbPoints - signChangeIndex ) ; x [ signChangeIndex ] = nextX ; System . arraycopy ( y , signChangeIndex , y , signChangeIndex + 1 , nbPoints - signChangeIndex ) ; y [ signChangeIndex ] = nextY ; ++ nbPoints ; if ( nextY * yA <= 0 ) { xB = nextX ; yB = nextY ; absYB = FastMath . abs ( yB ) ; ++ agingA ; agingB = 0 ; } else { xA = nextX ; yA = nextY ; absYA = FastMath . abs ( yA ) ; agingA = 0 ; ++ agingB ; signChangeIndex ++ ; } } }",
        "summary": "BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary",
        "Description": "In some cases, the aging feature in BracketingNthOrderBrentSolver fails.\nIt attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket.\nIn the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-716",
        "comments": [
            "The problem Pascal (who works with me) describes has already been encountered during the development of the algorithm. The solution found at that time seems to be insufficient. What happens is that in order to still gain a few digits while rebalancing the bracketing interval, we base our retargeting on the currently best solution. In this case, we set targetY = -yA/16 and fail to rebalance. Using the other bracket would improve rebalancing but waste evaluations as was observed during development. So its not a perfect solution either.\n\nI think a compromise would be to attempt rebalancing with a progressively more aggressive target. We could start from the current setting (i.e -1/16 of the best bracket), and if we still update the same side, move towards larger targets.",
            "The following simple test reproduces the bad behavior with a trivial function:\n{code}\n    @Test\n    public void testIssue716() {\n        BracketingNthOrderBrentSolver solver =\n                new BracketingNthOrderBrentSolver(1.0e-12, 1.0e-10, 0.0, 5);\n        UnivariateFunction sharpTurn = new UnivariateFunction() {\n            public double value(double x) {\n                return (2 * x + 1) / (1.0e9 * (x + 1));\n            }\n        };\n        double result = solver.solve(100, sharpTurn, -0.9999999, 30, 15, AllowedSolution.RIGHT_SIDE);\n        Assert.assertEquals(0, sharpTurn.value(result), solver.getFunctionValueAccuracy());\n        Assert.assertTrue(sharpTurn.value(result) >= 0);\n        Assert.assertEquals(-0.5, result, 1.0e-10);\n    }\n{code}\nThe test fails with TooManyEvaluationsException. In fact, only the right side of the bracketing interval is updated and very slowly decreases from 15.0 to 14.999677603318897 while the left side of the bracketing interval is stuck at -0.9999999.",
            "Fixed in subversion repository as of r1209307.\n\nThanks for the report."
        ],
        "summarized_discussion": "\n\nThe bug was solved by implementing a compromise solution of attempting rebalancing with a progressively more aggressive target, starting from the current setting (-1/16 of the best bracket). This was fixed in the subversion repository as of r1209307."
    },
    "Math_17_src/main/java/org/apache/commons/math3/dfp/Dfp.java_1602_1604": {
        "src": "public Dfp multiply(final int x) {\n            return multiplyFast(x);\n    }",
        "src_wo_comments": "public Dfp multiply ( final int x ) { return multiplyFast ( x ) ; }",
        "fixed_src": "public Dfp multiply(final int x) {\n        if (x >= 0 && x < RADIX) {\n            return multiplyFast(x);\n        } else {\n            return multiply(newInstance(x));\n        }\n    }",
        "fixed_src_wo_comments": "public Dfp multiply ( final int x ) { if ( x >= 0 && x < RADIX ) { return multiplyFast ( x ) ; } else { return multiply ( newInstance ( x ) ) ; } }",
        "summary": "Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n)",
        "Description": "In class {{org.apache.commons.math3.Dfp}},  the method {{multiply(int n)}} is limited to {{0 <= n <= 9999}}. This is not consistent with the general contract of {{FieldElement.multiply(int n)}}, where there should be no limitation on the values of {{n}}.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-778",
        "comments": [
            "Hi,\n\nI looked at this issue, and if I understand it correctly, the current multiply(int) method is using a performance shortcut for values of x between 0 and RADIX.\n\nI did a very simple patch to implement the following logic:\n\n * if 0<=x<RADIX: call multiplyFast with x\n * otherwise create a new Dfp instance with x and call multiply(Dfp) with it",
            "Thomas, S\u00e9bastien,\n\nAny reluctance to apply this patch?\n",
            "I looked at it as it is one of the few open issues for 3.1. As I am no expert for the Dfp implementation, I did not want to commit it without at least a comment from Sebastien or somebody else who is more proficient in this area than I am.",
            "The patch seems good to me.",
            "Seems good to me too. Using shift-right operations would result in faster implementations. I remember having tried that at the time I created this issue, but it seems to me I met with an issue, I can't remember what...\nSo for the time being, let's stick with this patch, and once we get some time, we can look into optimizations.",
            "Applied patch in r1400671."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to implement a patch which calls the multiplyFast with x if 0<=x<RADIX, and otherwise create a new Dfp instance with x and call multiply(Dfp) with it. The patch was approved by Sebastien and was applied in r1400671."
    },
    "Compress_18_src/main/java/org/apache/commons/compress/archivers/tar/TarArchiveOutputStream.java_454_491": {
        "src": "void writePaxHeaders(String entryName,\n                         Map<String, String> headers) throws IOException {\n        String name = \"./PaxHeaders.X/\" + stripTo7Bits(entryName);\n            // TarEntry's constructor would think this is a directory\n            // and not allow any data to be written\n        if (name.length() >= TarConstants.NAMELEN) {\n            name = name.substring(0, TarConstants.NAMELEN - 1);\n        }\n        TarArchiveEntry pex = new TarArchiveEntry(name,\n                                                  TarConstants.LF_PAX_EXTENDED_HEADER_LC);\n\n        StringWriter w = new StringWriter();\n        for (Map.Entry<String, String> h : headers.entrySet()) {\n            String key = h.getKey();\n            String value = h.getValue();\n            int len = key.length() + value.length()\n                + 3 /* blank, equals and newline */\n                + 2 /* guess 9 < actual length < 100 */;\n            String line = len + \" \" + key + \"=\" + value + \"\\n\";\n            int actualLength = line.getBytes(CharsetNames.UTF_8).length;\n            while (len != actualLength) {\n                // Adjust for cases where length < 10 or > 100\n                // or where UTF-8 encoding isn't a single octet\n                // per character.\n                // Must be in loop as size may go from 99 to 100 in\n                // first pass so we'd need a second.\n                len = actualLength;\n                line = len + \" \" + key + \"=\" + value + \"\\n\";\n                actualLength = line.getBytes(CharsetNames.UTF_8).length;\n            }\n            w.write(line);\n        }\n        byte[] data = w.toString().getBytes(CharsetNames.UTF_8);\n        pex.setSize(data.length);\n        putArchiveEntry(pex);\n        write(data);\n        closeArchiveEntry();\n    }",
        "src_wo_comments": "void writePaxHeaders ( String entryName , Map < String , String > headers ) throws IOException { String name = \"./PaxHeaders.X/\" + stripTo7Bits ( entryName ) ; if ( name . length ( ) >= TarConstants . NAMELEN ) { name = name . substring ( 0 , TarConstants . NAMELEN - 1 ) ; } TarArchiveEntry pex = new TarArchiveEntry ( name , TarConstants . LF_PAX_EXTENDED_HEADER_LC ) ; StringWriter w = new StringWriter ( ) ; for ( Map . Entry < String , String > h : headers . entrySet ( ) ) { String key = h . getKey ( ) ; String value = h . getValue ( ) ; int len = key . length ( ) + value . length ( ) + 3 + 2 ; String line = len + \" \" + key + \"=\" + value + \"\\n\" ; int actualLength = line . getBytes ( CharsetNames . UTF_8 ) . length ; while ( len != actualLength ) { len = actualLength ; line = len + \" \" + key + \"=\" + value + \"\\n\" ; actualLength = line . getBytes ( CharsetNames . UTF_8 ) . length ; } w . write ( line ) ; } byte [ ] data = w . toString ( ) . getBytes ( CharsetNames . UTF_8 ) ; pex . setSize ( data . length ) ; putArchiveEntry ( pex ) ; write ( data ) ; closeArchiveEntry ( ) ; }",
        "fixed_src": "void writePaxHeaders(String entryName,\n                         Map<String, String> headers) throws IOException {\n        String name = \"./PaxHeaders.X/\" + stripTo7Bits(entryName);\n        while (name.endsWith(\"/\")) {\n            // TarEntry's constructor would think this is a directory\n            // and not allow any data to be written\n            name = name.substring(0, name.length() - 1);\n        }\n        if (name.length() >= TarConstants.NAMELEN) {\n            name = name.substring(0, TarConstants.NAMELEN - 1);\n        }\n        TarArchiveEntry pex = new TarArchiveEntry(name,\n                                                  TarConstants.LF_PAX_EXTENDED_HEADER_LC);\n\n        StringWriter w = new StringWriter();\n        for (Map.Entry<String, String> h : headers.entrySet()) {\n            String key = h.getKey();\n            String value = h.getValue();\n            int len = key.length() + value.length()\n                + 3 /* blank, equals and newline */\n                + 2 /* guess 9 < actual length < 100 */;\n            String line = len + \" \" + key + \"=\" + value + \"\\n\";\n            int actualLength = line.getBytes(CharsetNames.UTF_8).length;\n            while (len != actualLength) {\n                // Adjust for cases where length < 10 or > 100\n                // or where UTF-8 encoding isn't a single octet\n                // per character.\n                // Must be in loop as size may go from 99 to 100 in\n                // first pass so we'd need a second.\n                len = actualLength;\n                line = len + \" \" + key + \"=\" + value + \"\\n\";\n                actualLength = line.getBytes(CharsetNames.UTF_8).length;\n            }\n            w.write(line);\n        }\n        byte[] data = w.toString().getBytes(CharsetNames.UTF_8);\n        pex.setSize(data.length);\n        putArchiveEntry(pex);\n        write(data);\n        closeArchiveEntry();\n    }",
        "fixed_src_wo_comments": "void writePaxHeaders ( String entryName , Map < String , String > headers ) throws IOException { String name = \"./PaxHeaders.X/\" + stripTo7Bits ( entryName ) ; while ( name . endsWith ( \"/\" ) ) { name = name . substring ( 0 , name . length ( ) - 1 ) ; } if ( name . length ( ) >= TarConstants . NAMELEN ) { name = name . substring ( 0 , TarConstants . NAMELEN - 1 ) ; } TarArchiveEntry pex = new TarArchiveEntry ( name , TarConstants . LF_PAX_EXTENDED_HEADER_LC ) ; StringWriter w = new StringWriter ( ) ; for ( Map . Entry < String , String > h : headers . entrySet ( ) ) { String key = h . getKey ( ) ; String value = h . getValue ( ) ; int len = key . length ( ) + value . length ( ) + 3 + 2 ; String line = len + \" \" + key + \"=\" + value + \"\\n\" ; int actualLength = line . getBytes ( CharsetNames . UTF_8 ) . length ; while ( len != actualLength ) { len = actualLength ; line = len + \" \" + key + \"=\" + value + \"\\n\" ; actualLength = line . getBytes ( CharsetNames . UTF_8 ) . length ; } w . write ( line ) ; } byte [ ] data = w . toString ( ) . getBytes ( CharsetNames . UTF_8 ) ; pex . setSize ( data . length ) ; putArchiveEntry ( pex ) ; write ( data ) ; closeArchiveEntry ( ) ; }",
        "summary": "Long directory names can not be stored in a tar archive because of error when writing PAX headers",
        "Description": "Trying to add a directory to the TAR Archive that has a name longer than 100 bytes generates an exception with a stack trace similar to the following:\n\n{noformat}\njava.io.IOException: request to write '114' bytes exceeds size in header of '0' bytes for entry './PaxHeaders.X/layers/openstreetmap__osm.disy.net/.tiles/1.0.0/openstreetmap__osm.disy.net/default/'\n\n            at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.write(TarArchiveOutputStream.java:385)\n\n            at java.io.OutputStream.write(Unknown Source)\n\n            at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.writePaxHeaders(TarArchiveOutputStream.java:485)\n\n            at org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.putArchiveEntry(TarArchiveOutputStream.java:312)\n\n            at net.disy.lib.io.tar.TarUtilities.addFile(TarUtilities.java:116)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:158)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.addDirectory(TarUtilities.java:162)\n\n            at net.disy.lib.io.tar.TarUtilities.tar(TarUtilities.java:77)\n\n            at net.disy.lib.io.tar.TarUtilities.tar(TarUtilities.java:42)\n\n            at net.disy.gisterm.tilecacheset.export.TileCacheSetExporter.tarTreeStructure(TileCacheSetExporter.java:262)\n\n            at net.disy.gisterm.tilecacheset.export.TileCacheSetExporter.export(TileCacheSetExporter.java:111)\n\n            at net.disy.gisterm.tilecacheset.desktop.controller.ExportController$1.run(ExportController.java:81)\n\n            ... 2 more\n{noformat}\n\nInformal source code investigation points to the problem being that for directory entries the code assumes that the length is 0 in putArchiveEntry (see TarArchiveOutputStream:321 ) but when writing the data, it actually writes some data (the filename) and the length written (filename size) is larger than the length expected (0).",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-203",
        "comments": [
            "The problem is not with directories with long names per se, it happens when the name of the PAX header entry ends with a slash - which could happen for short non-ASCII directory names or long file names if the name is truncated in the wrong place.  I'm working on a fix.",
            "fixed with svn revision 1426334",
            "We are running in troubles with an example like this:\n\n{{0123456789/012345678901234567890123456789012345678901234567890123456789012345678901}}*\u00dc*{{345678901234567890123456789.txt}}\n\nPlease notice the character *'\u00dc'* in the file name.\n\n{{stripTo7Bits(entryName)}} transforms the name to this string:\n\n{{./PaxHeaders.X/0123456789/012345678901234567890123456789012345678901234567890123456789012345678901\\345678901234567890123456789.txt}}\n\nThen it gets truncated to:\n\n{{./PaxHeaders.X/0123456789/012345678901234567890123456789012345678901234567890123456789012345678901}}\\\n\nAfter that, the constructor of TarArchiveEntry does a normalization on the file name to change windows file separators to slashes:\n\n{{./PaxHeaders.X/0123456789/012345678901234567890123456789012345678901234567890123456789012345678901/}}\n\nAt the end {{isDir}} is true:\n\n{{boolean isDir = name.endsWith(\"/\");}}\n\nOur suggestion would be to change this:\n\n{code:title=TarArchiveOutputStream.java|borderStyle=solid}\nwhile (name.endsWith(\"/\")) {\n    // TarEntry's constructor would think this is a directory\n    // and not allow any data to be written\n     name = name.substring(0, name.length() - 1);\n}\n{code}\n\t\t\nTo something like this:\n\n{code:title=TarArchiveOutputStream.java|borderStyle=solid}\nwhile (name.endsWith(\"/\") || name.endsWith(\"\\\\\")) {\n    // TarEntry's constructor would think this is a directory\n    // and not allow any data to be written\n    name = name.substring(0, name.length() - 1);\n}\n{code}\n\nWe think that the problem also could be avoided by modifying the method {{stripTo7Bits}}.\n",
            "Patrik, could you please open a new ticket for this as it is certainly related but still a different problem?  And we'd also get better tracking of which version is affected by which of the two bugs.\n\nBTW, I agree that fixing {{stripTo7Bits}} is the better place to avoid the problem.",
            "I've opened COMPRESS-265 for the new issue"
        ],
        "summarized_discussion": "\n\nThe solution to the source code bug is to modify the TarArchiveOutputStream.java file by changing the while loop to check for both forward slashes and backslashes. Additionally, the method stripTo7Bits should also be modified to avoid the problem. A new ticket (COMPRESS-265) has been opened for the issue."
    },
    "Cli_17_src/java/org/apache/commons/cli/PosixParser.java_282_310": {
        "src": "protected void burstToken(String token, boolean stopAtNonOption)\n    {\n        for (int i = 1; i < token.length(); i++)\n        {\n            String ch = String.valueOf(token.charAt(i));\n\n            if (options.hasOption(ch))\n            {\n                tokens.add(\"-\" + ch);\n                currentOption = options.getOption(ch);\n\n                if (currentOption.hasArg() && (token.length() != (i + 1)))\n                {\n                    tokens.add(token.substring(i + 1));\n\n                    break;\n                }\n            }\n            else if (stopAtNonOption)\n            {\n                process(token.substring(i));\n            }\n            else\n            {\n                tokens.add(token);\n                break;\n            }\n        }\n    }",
        "src_wo_comments": "protected void burstToken ( String token , boolean stopAtNonOption ) { for ( int i = 1 ; i < token . length ( ) ; i ++ ) { String ch = String . valueOf ( token . charAt ( i ) ) ; if ( options . hasOption ( ch ) ) { tokens . add ( \"-\" + ch ) ; currentOption = options . getOption ( ch ) ; if ( currentOption . hasArg ( ) && ( token . length ( ) != ( i + 1 ) ) ) { tokens . add ( token . substring ( i + 1 ) ) ; break ; } } else if ( stopAtNonOption ) { process ( token . substring ( i ) ) ; } else { tokens . add ( token ) ; break ; } } }",
        "fixed_src": "protected void burstToken(String token, boolean stopAtNonOption)\n    {\n        for (int i = 1; i < token.length(); i++)\n        {\n            String ch = String.valueOf(token.charAt(i));\n\n            if (options.hasOption(ch))\n            {\n                tokens.add(\"-\" + ch);\n                currentOption = options.getOption(ch);\n\n                if (currentOption.hasArg() && (token.length() != (i + 1)))\n                {\n                    tokens.add(token.substring(i + 1));\n\n                    break;\n                }\n            }\n            else if (stopAtNonOption)\n            {\n                process(token.substring(i));\n                break;\n            }\n            else\n            {\n                tokens.add(token);\n                break;\n            }\n        }\n    }",
        "fixed_src_wo_comments": "protected void burstToken ( String token , boolean stopAtNonOption ) { for ( int i = 1 ; i < token . length ( ) ; i ++ ) { String ch = String . valueOf ( token . charAt ( i ) ) ; if ( options . hasOption ( ch ) ) { tokens . add ( \"-\" + ch ) ; currentOption = options . getOption ( ch ) ; if ( currentOption . hasArg ( ) && ( token . length ( ) != ( i + 1 ) ) ) { tokens . add ( token . substring ( i + 1 ) ) ; break ; } } else if ( stopAtNonOption ) { process ( token . substring ( i ) ) ; break ; } else { tokens . add ( token ) ; break ; } } }",
        "summary": "PosixParser keeps bursting tokens even if a non option character is found",
        "Description": "PosixParser doesn't stop the bursting process of a token if stopAtNonOption is enabled and a non option character is encountered.\n\nFor example if the options a and b are defined, with stopAtNonOption=true the following command line:\n\n{code}-azb{code}\n\nis turned into:\n\n{code}-a zb -b{code}\n\nthe right output should be:\n\n{code}-a zb{code}\n",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-163",
        "comments": [],
        "summarized_discussion": "\n\nThe bug in the source code was causing the program to crash when a certain button was pressed. The solution to the bug was to debug the code and identify the cause of the crash. Once the cause was identified, the code was modified to fix the bug and the program was tested to ensure the bug was fixed."
    },
    "JacksonDatabind_24_src/main/java/com/fasterxml/jackson/databind/cfg/BaseSettings.java_230_238": {
        "src": "public BaseSettings withDateFormat(DateFormat df) {\n        if (_dateFormat == df) {\n            return this;\n        }\n        TimeZone tz = (df == null) ? _timeZone : df.getTimeZone();\n        return new BaseSettings(_classIntrospector, _annotationIntrospector, _visibilityChecker, _propertyNamingStrategy, _typeFactory,\n                _typeResolverBuilder, df, _handlerInstantiator, _locale,\n                tz, _defaultBase64);\n    }",
        "src_wo_comments": "public BaseSettings withDateFormat ( DateFormat df ) { if ( _dateFormat == df ) { return this ; } TimeZone tz = ( df == null ) ? _timeZone : df . getTimeZone ( ) ; return new BaseSettings ( _classIntrospector , _annotationIntrospector , _visibilityChecker , _propertyNamingStrategy , _typeFactory , _typeResolverBuilder , df , _handlerInstantiator , _locale , tz , _defaultBase64 ) ; }",
        "fixed_src": "public BaseSettings withDateFormat(DateFormat df) {\n        if (_dateFormat == df) {\n            return this;\n        }\n        return new BaseSettings(_classIntrospector, _annotationIntrospector, _visibilityChecker, _propertyNamingStrategy, _typeFactory,\n                _typeResolverBuilder, df, _handlerInstantiator, _locale,\n                _timeZone, _defaultBase64);\n    }",
        "fixed_src_wo_comments": "public BaseSettings withDateFormat ( DateFormat df ) { if ( _dateFormat == df ) { return this ; } return new BaseSettings ( _classIntrospector , _annotationIntrospector , _visibilityChecker , _propertyNamingStrategy , _typeFactory , _typeResolverBuilder , df , _handlerInstantiator , _locale , _timeZone , _defaultBase64 ) ; }",
        "summary": "Configuring an ObjectMapper's DateFormat changes time zone when serialising Joda DateTime",
        "Description": "The serialisation of Joda `DateTime` instances behaves differently in 2.6.0 vs 2.5.4 when the `ObjectMapper`'s had its `DateFormat` configured. The behaviour change is illustrated by the following code:\n\n``` java\npublic static void main(String[] args) throws JsonProcessingException {\n    System.out.println(createObjectMapper()\n            .writeValueAsString(new DateTime(1988, 6, 25, 20, 30, DateTimeZone.UTC)));\n}\n\nprivate static ObjectMapper createObjectMapper() {\n    ObjectMapper mapper = new ObjectMapper();\n    mapper.registerModule(createJodaModule());\n    mapper.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);\n    System.out.println(mapper.getSerializationConfig().getTimeZone());\n    mapper.setDateFormat(new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"));\n    System.out.println(mapper.getSerializationConfig().getTimeZone());\n    return mapper;\n}\n\nprivate static SimpleModule createJodaModule() {\n    SimpleModule module = new SimpleModule();\n    module.addSerializer(DateTime.class, new DateTimeSerializer(\n            new JacksonJodaDateFormat(DateTimeFormat.forPattern(\"yyyy-MM-dd HH:mm:ss\")\n                    .withZoneUTC())));\n        return module;\n    }\n```\n\nWhen run with Jackson 2.5.4 the output is:\n\n```\nsun.util.calendar.ZoneInfo[id=\"GMT\",offset=0,dstSavings=0,useDaylight=false,transitions=0,lastRule=null]\nsun.util.calendar.ZoneInfo[id=\"GMT\",offset=0,dstSavings=0,useDaylight=false,transitions=0,lastRule=null]\n\"1988-06-25 20:30:00\"\n```\n\nWhen run with Jackson 2.6.0 the output is:\n\n```\nsun.util.calendar.ZoneInfo[id=\"GMT\",offset=0,dstSavings=0,useDaylight=false,transitions=0,lastRule=null]\nsun.util.calendar.ZoneInfo[id=\"Europe/London\",offset=0,dstSavings=3600000,useDaylight=true,transitions=242,lastRule=java.util.SimpleTimeZone[id=Europe/London,offset=0,dstSavings=3600000,useDaylight=true,startYear=0,startMode=2,startMonth=2,startDay=-1,startDayOfWeek=1,startTime=3600000,startTimeMode=2,endMode=2,endMonth=9,endDay=-1,endDayOfWeek=1,endTime=3600000,endTimeMode=2]]\n\"1988-06-25 21:30:00\"\n```\n\nIt looks like the fix for #824 is the cause. In 2.6, the call to `mapper.setDateFormat` causes the `ObjectMapper`'s time zone to be set to the JVM's default time zone. In 2.5.x, calling `mapper.setDateFormat` has no effect on its time zone.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Ok, this does sound like a problem. Unfortunately timezone handling is a rather messy area of Jackson databinding. But sounds like in this case solution might be relatively easy, by trying to retain existing configured time zone and not taking it from `DateFormat`\n"
            },
            {
                "content": "The idea with the change was to allow piggy-backing timezone configuration, but all in all it is probably better not to try to be too clever. And since it changed behavior for existing code, it's a no-no.\nThank you for reporting this, and apologies for breakage. Fix will be in 2.6.1; we are getting enough fixes to release that relatively soon now, esp. considering couple of other issue with Joda / Java8 datetime datatype modules.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug in the source code was related to timezone handling in Jackson databinding. The solution is to retain the existing configured time zone and not take it from DateFormat. The fix will be in 2.6.1 and it will be released soon."
    },
    "Compress_19_src/main/java/org/apache/commons/compress/archivers/zip/Zip64ExtendedInformationExtraField.java_249_287": {
        "src": "public void reparseCentralDirectoryData(boolean hasUncompressedSize,\n                                            boolean hasCompressedSize,\n                                            boolean hasRelativeHeaderOffset,\n                                            boolean hasDiskStart)\n        throws ZipException {\n        if (rawCentralDirectoryData != null) {\n            int expectedLength = (hasUncompressedSize ? DWORD : 0)\n                + (hasCompressedSize ? DWORD : 0)\n                + (hasRelativeHeaderOffset ? DWORD : 0)\n                + (hasDiskStart ? WORD : 0);\n            if (rawCentralDirectoryData.length != expectedLength) {\n                throw new ZipException(\"central directory zip64 extended\"\n                                       + \" information extra field's length\"\n                                       + \" doesn't match central directory\"\n                                       + \" data.  Expected length \"\n                                       + expectedLength + \" but is \"\n                                       + rawCentralDirectoryData.length);\n            }\n            int offset = 0;\n            if (hasUncompressedSize) {\n                size = new ZipEightByteInteger(rawCentralDirectoryData, offset);\n                offset += DWORD;\n            }\n            if (hasCompressedSize) {\n                compressedSize = new ZipEightByteInteger(rawCentralDirectoryData,\n                                                         offset);\n                offset += DWORD;\n            }\n            if (hasRelativeHeaderOffset) {\n                relativeHeaderOffset =\n                    new ZipEightByteInteger(rawCentralDirectoryData, offset);\n                offset += DWORD;\n            }\n            if (hasDiskStart) {\n                diskStart = new ZipLong(rawCentralDirectoryData, offset);\n                offset += WORD;\n            }\n        }\n    }",
        "src_wo_comments": "public void reparseCentralDirectoryData ( boolean hasUncompressedSize , boolean hasCompressedSize , boolean hasRelativeHeaderOffset , boolean hasDiskStart ) throws ZipException { if ( rawCentralDirectoryData != null ) { int expectedLength = ( hasUncompressedSize ? DWORD : 0 ) + ( hasCompressedSize ? DWORD : 0 ) + ( hasRelativeHeaderOffset ? DWORD : 0 ) + ( hasDiskStart ? WORD : 0 ) ; if ( rawCentralDirectoryData . length != expectedLength ) { throw new ZipException ( \"central directory zip64 extended\" + \" information extra field's length\" + \" doesn't match central directory\" + \" data.  Expected length \" + expectedLength + \" but is \" + rawCentralDirectoryData . length ) ; } int offset = 0 ; if ( hasUncompressedSize ) { size = new ZipEightByteInteger ( rawCentralDirectoryData , offset ) ; offset += DWORD ; } if ( hasCompressedSize ) { compressedSize = new ZipEightByteInteger ( rawCentralDirectoryData , offset ) ; offset += DWORD ; } if ( hasRelativeHeaderOffset ) { relativeHeaderOffset = new ZipEightByteInteger ( rawCentralDirectoryData , offset ) ; offset += DWORD ; } if ( hasDiskStart ) { diskStart = new ZipLong ( rawCentralDirectoryData , offset ) ; offset += WORD ; } } }",
        "fixed_src": "public void reparseCentralDirectoryData(boolean hasUncompressedSize,\n                                            boolean hasCompressedSize,\n                                            boolean hasRelativeHeaderOffset,\n                                            boolean hasDiskStart)\n        throws ZipException {\n        if (rawCentralDirectoryData != null) {\n            int expectedLength = (hasUncompressedSize ? DWORD : 0)\n                + (hasCompressedSize ? DWORD : 0)\n                + (hasRelativeHeaderOffset ? DWORD : 0)\n                + (hasDiskStart ? WORD : 0);\n            if (rawCentralDirectoryData.length < expectedLength) {\n                throw new ZipException(\"central directory zip64 extended\"\n                                       + \" information extra field's length\"\n                                       + \" doesn't match central directory\"\n                                       + \" data.  Expected length \"\n                                       + expectedLength + \" but is \"\n                                       + rawCentralDirectoryData.length);\n            }\n            int offset = 0;\n            if (hasUncompressedSize) {\n                size = new ZipEightByteInteger(rawCentralDirectoryData, offset);\n                offset += DWORD;\n            }\n            if (hasCompressedSize) {\n                compressedSize = new ZipEightByteInteger(rawCentralDirectoryData,\n                                                         offset);\n                offset += DWORD;\n            }\n            if (hasRelativeHeaderOffset) {\n                relativeHeaderOffset =\n                    new ZipEightByteInteger(rawCentralDirectoryData, offset);\n                offset += DWORD;\n            }\n            if (hasDiskStart) {\n                diskStart = new ZipLong(rawCentralDirectoryData, offset);\n                offset += WORD;\n            }\n        }\n    }",
        "fixed_src_wo_comments": "public void reparseCentralDirectoryData ( boolean hasUncompressedSize , boolean hasCompressedSize , boolean hasRelativeHeaderOffset , boolean hasDiskStart ) throws ZipException { if ( rawCentralDirectoryData != null ) { int expectedLength = ( hasUncompressedSize ? DWORD : 0 ) + ( hasCompressedSize ? DWORD : 0 ) + ( hasRelativeHeaderOffset ? DWORD : 0 ) + ( hasDiskStart ? WORD : 0 ) ; if ( rawCentralDirectoryData . length < expectedLength ) { throw new ZipException ( \"central directory zip64 extended\" + \" information extra field's length\" + \" doesn't match central directory\" + \" data.  Expected length \" + expectedLength + \" but is \" + rawCentralDirectoryData . length ) ; } int offset = 0 ; if ( hasUncompressedSize ) { size = new ZipEightByteInteger ( rawCentralDirectoryData , offset ) ; offset += DWORD ; } if ( hasCompressedSize ) { compressedSize = new ZipEightByteInteger ( rawCentralDirectoryData , offset ) ; offset += DWORD ; } if ( hasRelativeHeaderOffset ) { relativeHeaderOffset = new ZipEightByteInteger ( rawCentralDirectoryData , offset ) ; offset += DWORD ; } if ( hasDiskStart ) { diskStart = new ZipLong ( rawCentralDirectoryData , offset ) ; offset += WORD ; } } }",
        "summary": "ZipException on reading valid zip64 file",
        "Description": "ZipFile zip = new ZipFile(new File(\"ordertest-64.zip\")); throws ZipException \"central directory zip64 extended information extra field's length doesn't match central directory data.  Expected length 16 but is 28\".\n\nThe archive was created by using DotNetZip-WinFormsTool uzing zip64 flag (forces always to make zip64 archives).\n\nZip file is tested from the console: $zip -T ordertest-64.zip\n\nOutput:\ntest of ordertest-64.zip OK\n\nI can open the archive with FileRoller without problem on my machine, browse and extract it.\n",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-228",
        "comments": [
            "-Can we include your test archive in Compress' source distribution or does it contain sensitive data?-\n\nshould have looked inside the archive first :-)\n\nThanks.",
            "To me it is quite clear that the archive you've attached violates the spec of http://www.pkware.com/documents/casestudies/APPNOTE.TXT section 4.5.3\n\n{quote}\nThe order of the fields in the zip64 extended \ninformation record is fixed, but the fields MUST\nonly appear if the corresponding Local or Central\ndirectory record field is set to 0xFFFF or 0xFFFFFFFF.\n{quote}\n\noffset and disk number in the central directory entry are not set to all-1 values but the corresponding fields inside the extra information field are present - and make up for the 12 unexpected bytes.\n\nIt is also obvious that InfoZIP based implementations can deal with this kind of violation.  I'll dig into their source code to see what sort of heuristic they use.",
            "well, maybe the non-native speaker in me is the problem here.  To me \"MUST only appear if\" translates to \"must not be present unless\", but maybe it means \"can be there, but if the record field is set to all-1s, then it must be present\".\n\nAnyway, unzip 6.10 simply ignores excess data, we can do so as well.",
            "Yes, you are welcome. I believe you are right. ;) Deal with it as you think it is appropriate.",
            "fixed with svn revision 1486437",
            "From zip specification: \"Use of the term MUST or SHALL indicates a required element.\" \nSo, \"MUST only appear if\" could be read as \"required only appear if\". As I understand the fields may be present in any case, but are +required+ \"if the corresponding Local or Central directory record field is set to 0xFFFF or 0xFFFFFFFF\"."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to use the zip specification that states that the fields may be present in any case, but are required if the corresponding Local or Central directory record field is set to 0xFFFF or 0xFFFFFFFF. This was fixed with svn revision 1486437."
    },
    "Cli_40_src/main/java/org/apache/commons/cli/TypeHandler.java_62_105": {
        "src": "@SuppressWarnings(\"unchecked\") // returned value will have type T because it is fixed by clazz\n    public static <T> T createValue(final String str, final Class<T> clazz) throws ParseException\n    {\n        if (PatternOptionBuilder.STRING_VALUE == clazz)\n        {\n            return (T) str;\n        }\n        else if (PatternOptionBuilder.OBJECT_VALUE == clazz)\n        {\n            return (T) createObject(str);\n        }\n        else if (PatternOptionBuilder.NUMBER_VALUE == clazz)\n        {\n            return (T) createNumber(str);\n        }\n        else if (PatternOptionBuilder.DATE_VALUE == clazz)\n        {\n            return (T) createDate(str);\n        }\n        else if (PatternOptionBuilder.CLASS_VALUE == clazz)\n        {\n            return (T) createClass(str);\n        }\n        else if (PatternOptionBuilder.FILE_VALUE == clazz)\n        {\n            return (T) createFile(str);\n        }\n        else if (PatternOptionBuilder.EXISTING_FILE_VALUE == clazz)\n        {\n            return (T) openFile(str);\n        }\n        else if (PatternOptionBuilder.FILES_VALUE == clazz)\n        {\n            return (T) createFiles(str);\n        }\n        else if (PatternOptionBuilder.URL_VALUE == clazz)\n        {\n            return (T) createURL(str);\n        }\n        else\n        {\n            return null;\n        }\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"unchecked\" ) public static < T > T createValue ( final String str , final Class < T > clazz ) throws ParseException { if ( PatternOptionBuilder . STRING_VALUE == clazz ) { return ( T ) str ; } else if ( PatternOptionBuilder . OBJECT_VALUE == clazz ) { return ( T ) createObject ( str ) ; } else if ( PatternOptionBuilder . NUMBER_VALUE == clazz ) { return ( T ) createNumber ( str ) ; } else if ( PatternOptionBuilder . DATE_VALUE == clazz ) { return ( T ) createDate ( str ) ; } else if ( PatternOptionBuilder . CLASS_VALUE == clazz ) { return ( T ) createClass ( str ) ; } else if ( PatternOptionBuilder . FILE_VALUE == clazz ) { return ( T ) createFile ( str ) ; } else if ( PatternOptionBuilder . EXISTING_FILE_VALUE == clazz ) { return ( T ) openFile ( str ) ; } else if ( PatternOptionBuilder . FILES_VALUE == clazz ) { return ( T ) createFiles ( str ) ; } else if ( PatternOptionBuilder . URL_VALUE == clazz ) { return ( T ) createURL ( str ) ; } else { return null ; } }",
        "fixed_src": "@SuppressWarnings(\"unchecked\") // returned value will have type T because it is fixed by clazz\n    public static <T> T createValue(final String str, final Class<T> clazz) throws ParseException\n    {\n        if (PatternOptionBuilder.STRING_VALUE == clazz)\n        {\n            return (T) str;\n        }\n        else if (PatternOptionBuilder.OBJECT_VALUE == clazz)\n        {\n            return (T) createObject(str);\n        }\n        else if (PatternOptionBuilder.NUMBER_VALUE == clazz)\n        {\n            return (T) createNumber(str);\n        }\n        else if (PatternOptionBuilder.DATE_VALUE == clazz)\n        {\n            return (T) createDate(str);\n        }\n        else if (PatternOptionBuilder.CLASS_VALUE == clazz)\n        {\n            return (T) createClass(str);\n        }\n        else if (PatternOptionBuilder.FILE_VALUE == clazz)\n        {\n            return (T) createFile(str);\n        }\n        else if (PatternOptionBuilder.EXISTING_FILE_VALUE == clazz)\n        {\n            return (T) openFile(str);\n        }\n        else if (PatternOptionBuilder.FILES_VALUE == clazz)\n        {\n            return (T) createFiles(str);\n        }\n        else if (PatternOptionBuilder.URL_VALUE == clazz)\n        {\n            return (T) createURL(str);\n        }\n        else\n        {\n            throw new ParseException(\"Unable to handle the class: \" + clazz);\n        }\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"unchecked\" ) public static < T > T createValue ( final String str , final Class < T > clazz ) throws ParseException { if ( PatternOptionBuilder . STRING_VALUE == clazz ) { return ( T ) str ; } else if ( PatternOptionBuilder . OBJECT_VALUE == clazz ) { return ( T ) createObject ( str ) ; } else if ( PatternOptionBuilder . NUMBER_VALUE == clazz ) { return ( T ) createNumber ( str ) ; } else if ( PatternOptionBuilder . DATE_VALUE == clazz ) { return ( T ) createDate ( str ) ; } else if ( PatternOptionBuilder . CLASS_VALUE == clazz ) { return ( T ) createClass ( str ) ; } else if ( PatternOptionBuilder . FILE_VALUE == clazz ) { return ( T ) createFile ( str ) ; } else if ( PatternOptionBuilder . EXISTING_FILE_VALUE == clazz ) { return ( T ) openFile ( str ) ; } else if ( PatternOptionBuilder . FILES_VALUE == clazz ) { return ( T ) createFiles ( str ) ; } else if ( PatternOptionBuilder . URL_VALUE == clazz ) { return ( T ) createURL ( str ) ; } else { throw new ParseException ( \"Unable to handle the class: \" + clazz ) ; } }",
        "summary": "TypeHandler should throw ParseException for an unsupported class",
        "Description": "JavaDoc for TypeHandler states that createValue will\r\n{noformat}\r\n* @throws ParseException if the value creation for the given object type failedtype{noformat}\r\n\u00a0However createValue(String str, Class<?> clazz) will return null if the clazz is unknown.",
        "issue_url": "https://issues.apache.org/jira//browse/CLI-282",
        "comments": [
            "GitHub user deepy opened a pull request:\n\n    https://github.com/apache/commons-cli/pull/23\n\n    CLI-282: Throw an exception if we cannot handle the class.\n\n    \n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/deepy/commons-cli CLI-282\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/commons-cli/pull/23.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #23\n    \n----\ncommit 583a1d661816483ceb201a7b1632f35cd9c3139c\nAuthor: Alex Nordlund <deep.alexander@...>\nDate:   2018-02-26T12:46:55Z\n\n    CLI-282: Throw an exception if we cannot handle the class.\n\n----\n",
            "GitHub user deepy opened a pull request:\n\n    https://github.com/apache/commons-cli/pull/23\n\n    CLI-282: Throw an exception if we cannot handle the class.\n\n    \n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/deepy/commons-cli CLI-282\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/commons-cli/pull/23.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #23\n    \n----\ncommit 583a1d661816483ceb201a7b1632f35cd9c3139c\nAuthor: Alex Nordlund <deep.alexander@...>\nDate:   2018-02-26T12:46:55Z\n\n    CLI-282: Throw an exception if we cannot handle the class.\n\n----\n",
            "Hi\u00a0[~deepy],\r\n\r\nIn git master, please verify the code and close this issue.\r\n\r\nThank you!\r\n\r\nGary",
            "(y)",
            "Github user deepy commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/23\n  \n    #b0024d482050a08efc36c3cabee37c0af0e57a10\n",
            "Github user deepy commented on the issue:\n\n    https://github.com/apache/commons-cli/pull/23\n  \n    #b0024d482050a08efc36c3cabee37c0af0e57a10\n",
            "Github user deepy closed the pull request at:\n\n    https://github.com/apache/commons-cli/pull/23\n",
            "Github user deepy closed the pull request at:\n\n    https://github.com/apache/commons-cli/pull/23\n"
        ],
        "summarized_discussion": "\nSolution: The solution to the bug is for Github user deepy to verify the code and close the issue by making a commit to the master/trunk branch with the message \"This closes #23\"."
    },
    "Math_41_src/main/java/org/apache/commons/math/stat/descriptive/moment/Variance.java_501_532": {
        "src": "public double evaluate(final double[] values, final double[] weights,\n                           final double mean, final int begin, final int length) {\n\n        double var = Double.NaN;\n\n        if (test(values, weights, begin, length)) {\n            if (length == 1) {\n                var = 0.0;\n            } else if (length > 1) {\n                double accum = 0.0;\n                double dev = 0.0;\n                double accum2 = 0.0;\n                for (int i = begin; i < begin + length; i++) {\n                    dev = values[i] - mean;\n                    accum += weights[i] * (dev * dev);\n                    accum2 += weights[i] * dev;\n                }\n\n                double sumWts = 0;\n                for (int i = 0; i < weights.length; i++) {\n                    sumWts += weights[i];\n                }\n\n                if (isBiasCorrected) {\n                    var = (accum - (accum2 * accum2 / sumWts)) / (sumWts - 1.0);\n                } else {\n                    var = (accum - (accum2 * accum2 / sumWts)) / sumWts;\n                }\n            }\n        }\n        return var;\n    }",
        "src_wo_comments": "public double evaluate ( final double [ ] values , final double [ ] weights , final double mean , final int begin , final int length ) { double var = Double . NaN ; if ( test ( values , weights , begin , length ) ) { if ( length == 1 ) { var = 0.0 ; } else if ( length > 1 ) { double accum = 0.0 ; double dev = 0.0 ; double accum2 = 0.0 ; for ( int i = begin ; i < begin + length ; i ++ ) { dev = values [ i ] - mean ; accum += weights [ i ] * ( dev * dev ) ; accum2 += weights [ i ] * dev ; } double sumWts = 0 ; for ( int i = 0 ; i < weights . length ; i ++ ) { sumWts += weights [ i ] ; } if ( isBiasCorrected ) { var = ( accum - ( accum2 * accum2 / sumWts ) ) / ( sumWts - 1.0 ) ; } else { var = ( accum - ( accum2 * accum2 / sumWts ) ) / sumWts ; } } } return var ; }",
        "fixed_src": "public double evaluate(final double[] values, final double[] weights,\n                           final double mean, final int begin, final int length) {\n\n        double var = Double.NaN;\n\n        if (test(values, weights, begin, length)) {\n            if (length == 1) {\n                var = 0.0;\n            } else if (length > 1) {\n                double accum = 0.0;\n                double dev = 0.0;\n                double accum2 = 0.0;\n                for (int i = begin; i < begin + length; i++) {\n                    dev = values[i] - mean;\n                    accum += weights[i] * (dev * dev);\n                    accum2 += weights[i] * dev;\n                }\n\n                double sumWts = 0;\n                for (int i = begin; i < begin + length; i++) {\n                    sumWts += weights[i];\n                }\n\n                if (isBiasCorrected) {\n                    var = (accum - (accum2 * accum2 / sumWts)) / (sumWts - 1.0);\n                } else {\n                    var = (accum - (accum2 * accum2 / sumWts)) / sumWts;\n                }\n            }\n        }\n        return var;\n    }",
        "fixed_src_wo_comments": "public double evaluate ( final double [ ] values , final double [ ] weights , final double mean , final int begin , final int length ) { double var = Double . NaN ; if ( test ( values , weights , begin , length ) ) { if ( length == 1 ) { var = 0.0 ; } else if ( length > 1 ) { double accum = 0.0 ; double dev = 0.0 ; double accum2 = 0.0 ; for ( int i = begin ; i < begin + length ; i ++ ) { dev = values [ i ] - mean ; accum += weights [ i ] * ( dev * dev ) ; accum2 += weights [ i ] * dev ; } double sumWts = 0 ; for ( int i = begin ; i < begin + length ; i ++ ) { sumWts += weights [ i ] ; } if ( isBiasCorrected ) { var = ( accum - ( accum2 * accum2 / sumWts ) ) / ( sumWts - 1.0 ) ; } else { var = ( accum - ( accum2 * accum2 / sumWts ) ) / sumWts ; } } } return var ; }",
        "summary": "One of Variance.evaluate() methods does not work correctly",
        "Description": "The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset.\nSimilar method in Mean class seems to work.\nI did not check other methods taking the part of the array; they may have the same problem.\n\nWorkaround: I had to shrink my arrays and use the method without the length.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-704",
        "comments": [
            "I can't seem to make this fail and I have added tests that would indicate that array segments are being handled properly.  Can you provide an example showing a failure?",
            "I have found a small bug wrt this bug report. The sum of the weights is calculated on the whole weight array rather than the specified sub-array [begin, begin + length).\n\nSee the attached patch for more details. Still need a testcase to verify that it finally addresses the original bug report.",
            "Good catch, Thomas.  Thanks!  Patch, with test case added, committed in r1208291."
        ],
        "summarized_discussion": "\n\nThe bug was fixed by Thomas in r1208291. The patch added a test case to verify that the sum of the weights was calculated on the specified sub-array [begin, begin + length) rather than the whole weight array."
    },
    "Math_57_src/main/java/org/apache/commons/math/stat/clustering/KMeansPlusPlusClusterer.java_161_198": {
        "src": "private static <T extends Clusterable<T>> List<Cluster<T>>\n        chooseInitialCenters(final Collection<T> points, final int k, final Random random) {\n\n        final List<T> pointSet = new ArrayList<T>(points);\n        final List<Cluster<T>> resultSet = new ArrayList<Cluster<T>>();\n\n        // Choose one center uniformly at random from among the data points.\n        final T firstPoint = pointSet.remove(random.nextInt(pointSet.size()));\n        resultSet.add(new Cluster<T>(firstPoint));\n\n        final double[] dx2 = new double[pointSet.size()];\n        while (resultSet.size() < k) {\n            // For each data point x, compute D(x), the distance between x and\n            // the nearest center that has already been chosen.\n            int sum = 0;\n            for (int i = 0; i < pointSet.size(); i++) {\n                final T p = pointSet.get(i);\n                final Cluster<T> nearest = getNearestCluster(resultSet, p);\n                final double d = p.distanceFrom(nearest.getCenter());\n                sum += d * d;\n                dx2[i] = sum;\n            }\n\n            // Add one new data point as a center. Each point x is chosen with\n            // probability proportional to D(x)2\n            final double r = random.nextDouble() * sum;\n            for (int i = 0 ; i < dx2.length; i++) {\n                if (dx2[i] >= r) {\n                    final T p = pointSet.remove(i);\n                    resultSet.add(new Cluster<T>(p));\n                    break;\n                }\n            }\n        }\n\n        return resultSet;\n\n    }",
        "src_wo_comments": "private static < T extends Clusterable < T > > List < Cluster < T > > chooseInitialCenters ( final Collection < T > points , final int k , final Random random ) { final List < T > pointSet = new ArrayList < T > ( points ) ; final List < Cluster < T > > resultSet = new ArrayList < Cluster < T > > ( ) ; final T firstPoint = pointSet . remove ( random . nextInt ( pointSet . size ( ) ) ) ; resultSet . add ( new Cluster < T > ( firstPoint ) ) ; final double [ ] dx2 = new double [ pointSet . size ( ) ] ; while ( resultSet . size ( ) < k ) { int sum = 0 ; for ( int i = 0 ; i < pointSet . size ( ) ; i ++ ) { final T p = pointSet . get ( i ) ; final Cluster < T > nearest = getNearestCluster ( resultSet , p ) ; final double d = p . distanceFrom ( nearest . getCenter ( ) ) ; sum += d * d ; dx2 [ i ] = sum ; } final double r = random . nextDouble ( ) * sum ; for ( int i = 0 ; i < dx2 . length ; i ++ ) { if ( dx2 [ i ] >= r ) { final T p = pointSet . remove ( i ) ; resultSet . add ( new Cluster < T > ( p ) ) ; break ; } } } return resultSet ; }",
        "fixed_src": "private static <T extends Clusterable<T>> List<Cluster<T>>\n        chooseInitialCenters(final Collection<T> points, final int k, final Random random) {\n\n        final List<T> pointSet = new ArrayList<T>(points);\n        final List<Cluster<T>> resultSet = new ArrayList<Cluster<T>>();\n\n        // Choose one center uniformly at random from among the data points.\n        final T firstPoint = pointSet.remove(random.nextInt(pointSet.size()));\n        resultSet.add(new Cluster<T>(firstPoint));\n\n        final double[] dx2 = new double[pointSet.size()];\n        while (resultSet.size() < k) {\n            // For each data point x, compute D(x), the distance between x and\n            // the nearest center that has already been chosen.\n            double sum = 0;\n            for (int i = 0; i < pointSet.size(); i++) {\n                final T p = pointSet.get(i);\n                final Cluster<T> nearest = getNearestCluster(resultSet, p);\n                final double d = p.distanceFrom(nearest.getCenter());\n                sum += d * d;\n                dx2[i] = sum;\n            }\n\n            // Add one new data point as a center. Each point x is chosen with\n            // probability proportional to D(x)2\n            final double r = random.nextDouble() * sum;\n            for (int i = 0 ; i < dx2.length; i++) {\n                if (dx2[i] >= r) {\n                    final T p = pointSet.remove(i);\n                    resultSet.add(new Cluster<T>(p));\n                    break;\n                }\n            }\n        }\n\n        return resultSet;\n\n    }",
        "fixed_src_wo_comments": "private static < T extends Clusterable < T > > List < Cluster < T > > chooseInitialCenters ( final Collection < T > points , final int k , final Random random ) { final List < T > pointSet = new ArrayList < T > ( points ) ; final List < Cluster < T > > resultSet = new ArrayList < Cluster < T > > ( ) ; final T firstPoint = pointSet . remove ( random . nextInt ( pointSet . size ( ) ) ) ; resultSet . add ( new Cluster < T > ( firstPoint ) ) ; final double [ ] dx2 = new double [ pointSet . size ( ) ] ; while ( resultSet . size ( ) < k ) { double sum = 0 ; for ( int i = 0 ; i < pointSet . size ( ) ; i ++ ) { final T p = pointSet . get ( i ) ; final Cluster < T > nearest = getNearestCluster ( resultSet , p ) ; final double d = p . distanceFrom ( nearest . getCenter ( ) ) ; sum += d * d ; dx2 [ i ] = sum ; } final double r = random . nextDouble ( ) * sum ; for ( int i = 0 ; i < dx2 . length ; i ++ ) { if ( dx2 [ i ] >= r ) { final T p = pointSet . remove ( i ) ; resultSet . add ( new Cluster < T > ( p ) ) ; break ; } } } return resultSet ; }",
        "summary": "Truncation issue in KMeansPlusPlusClusterer",
        "Description": "The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable\n  int sum = 0;\nThis variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1.\n\nAs an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-546",
        "comments": [
            "I've a patch to fix this bug.\n\nThis is my first contribution to this project, so apologies if I've screwed something up :)",
            "Fixed in revision 1081744.\nThanks for the report and the patch.\n\nLeaving open until an answer can be provided concerning the \"EmptyClusterStrategy\" question.\n",
            "The empty cluster strategy is needed regardless of this bug. It may appear with different conditions and is a feature commonly found in clustering implementations.\nThis issue can be marked as resolved if the patch has been applied and works.\n\nThanks to Nate for reporting and fixing the issue, thanks to Gilles for reviewing and applying the patch."
        ],
        "summarized_discussion": "\n\nThe bug has been fixed with a patch in revision 1081744. The empty cluster strategy is needed regardless of this bug, and it may appear with different conditions. The issue can be marked as resolved if the patch has been applied and works, and thanks to Nate and Gilles for reporting and fixing the issue."
    },
    "Math_101_src/java/org/apache/commons/math/complex/ComplexFormat.java_320_389": {
        "src": "public Complex parse(String source, ParsePosition pos) {\n        int initialIndex = pos.getIndex();\n\n        // parse whitespace\n        parseAndIgnoreWhitespace(source, pos);\n\n        // parse real\n        Number re = parseNumber(source, getRealFormat(), pos);\n        if (re == null) {\n            // invalid real number\n            // set index back to initial, error index should already be set\n            // character examined.\n            pos.setIndex(initialIndex);\n            return null;\n        }\n\n        // parse sign\n        int startIndex = pos.getIndex();\n        char c = parseNextCharacter(source, pos);\n        int sign = 0;\n        switch (c) {\n        case 0 :\n            // no sign\n            // return real only complex number\n            return new Complex(re.doubleValue(), 0.0);\n        case '-' :\n            sign = -1;\n            break;\n        case '+' :\n            sign = 1;\n            break;\n        default :\n            // invalid sign\n            // set index back to initial, error index should be the last\n            // character examined.\n            pos.setIndex(initialIndex);\n            pos.setErrorIndex(startIndex);\n            return null;\n        }\n\n        // parse whitespace\n        parseAndIgnoreWhitespace(source, pos);\n\n        // parse imaginary\n        Number im = parseNumber(source, getRealFormat(), pos);\n        if (im == null) {\n            // invalid imaginary number\n            // set index back to initial, error index should already be set\n            // character examined.\n            pos.setIndex(initialIndex);\n            return null;\n        }\n\n        // parse imaginary character\n        int n = getImaginaryCharacter().length();\n        startIndex = pos.getIndex();\n        int endIndex = startIndex + n;\n        if (\n            source.substring(startIndex, endIndex).compareTo(\n            getImaginaryCharacter()) != 0) {\n            // set index back to initial, error index should be the start index\n            // character examined.\n            pos.setIndex(initialIndex);\n            pos.setErrorIndex(startIndex);\n            return null;\n        }\n        pos.setIndex(endIndex);\n\n        return new Complex(re.doubleValue(), im.doubleValue() * sign);\n    }",
        "src_wo_comments": "public Complex parse ( String source , ParsePosition pos ) { int initialIndex = pos . getIndex ( ) ; parseAndIgnoreWhitespace ( source , pos ) ; Number re = parseNumber ( source , getRealFormat ( ) , pos ) ; if ( re == null ) { pos . setIndex ( initialIndex ) ; return null ; } int startIndex = pos . getIndex ( ) ; char c = parseNextCharacter ( source , pos ) ; int sign = 0 ; switch ( c ) { case 0 : return new Complex ( re . doubleValue ( ) , 0.0 ) ; case '-' : sign = - 1 ; break ; case '+' : sign = 1 ; break ; default : pos . setIndex ( initialIndex ) ; pos . setErrorIndex ( startIndex ) ; return null ; } parseAndIgnoreWhitespace ( source , pos ) ; Number im = parseNumber ( source , getRealFormat ( ) , pos ) ; if ( im == null ) { pos . setIndex ( initialIndex ) ; return null ; } int n = getImaginaryCharacter ( ) . length ( ) ; startIndex = pos . getIndex ( ) ; int endIndex = startIndex + n ; if ( source . substring ( startIndex , endIndex ) . compareTo ( getImaginaryCharacter ( ) ) != 0 ) { pos . setIndex ( initialIndex ) ; pos . setErrorIndex ( startIndex ) ; return null ; } pos . setIndex ( endIndex ) ; return new Complex ( re . doubleValue ( ) , im . doubleValue ( ) * sign ) ; }",
        "fixed_src": "public Complex parse(String source, ParsePosition pos) {\n        int initialIndex = pos.getIndex();\n\n        // parse whitespace\n        parseAndIgnoreWhitespace(source, pos);\n\n        // parse real\n        Number re = parseNumber(source, getRealFormat(), pos);\n        if (re == null) {\n            // invalid real number\n            // set index back to initial, error index should already be set\n            // character examined.\n            pos.setIndex(initialIndex);\n            return null;\n        }\n\n        // parse sign\n        int startIndex = pos.getIndex();\n        char c = parseNextCharacter(source, pos);\n        int sign = 0;\n        switch (c) {\n        case 0 :\n            // no sign\n            // return real only complex number\n            return new Complex(re.doubleValue(), 0.0);\n        case '-' :\n            sign = -1;\n            break;\n        case '+' :\n            sign = 1;\n            break;\n        default :\n            // invalid sign\n            // set index back to initial, error index should be the last\n            // character examined.\n            pos.setIndex(initialIndex);\n            pos.setErrorIndex(startIndex);\n            return null;\n        }\n\n        // parse whitespace\n        parseAndIgnoreWhitespace(source, pos);\n\n        // parse imaginary\n        Number im = parseNumber(source, getRealFormat(), pos);\n        if (im == null) {\n            // invalid imaginary number\n            // set index back to initial, error index should already be set\n            // character examined.\n            pos.setIndex(initialIndex);\n            return null;\n        }\n\n        // parse imaginary character\n        int n = getImaginaryCharacter().length();\n        startIndex = pos.getIndex();\n        int endIndex = startIndex + n;\n        if ((startIndex >= source.length()) ||\n            (endIndex > source.length()) ||\n            source.substring(startIndex, endIndex).compareTo(\n            getImaginaryCharacter()) != 0) {\n            // set index back to initial, error index should be the start index\n            // character examined.\n            pos.setIndex(initialIndex);\n            pos.setErrorIndex(startIndex);\n            return null;\n        }\n        pos.setIndex(endIndex);\n\n        return new Complex(re.doubleValue(), im.doubleValue() * sign);\n    }",
        "fixed_src_wo_comments": "public Complex parse ( String source , ParsePosition pos ) { int initialIndex = pos . getIndex ( ) ; parseAndIgnoreWhitespace ( source , pos ) ; Number re = parseNumber ( source , getRealFormat ( ) , pos ) ; if ( re == null ) { pos . setIndex ( initialIndex ) ; return null ; } int startIndex = pos . getIndex ( ) ; char c = parseNextCharacter ( source , pos ) ; int sign = 0 ; switch ( c ) { case 0 : return new Complex ( re . doubleValue ( ) , 0.0 ) ; case '-' : sign = - 1 ; break ; case '+' : sign = 1 ; break ; default : pos . setIndex ( initialIndex ) ; pos . setErrorIndex ( startIndex ) ; return null ; } parseAndIgnoreWhitespace ( source , pos ) ; Number im = parseNumber ( source , getRealFormat ( ) , pos ) ; if ( im == null ) { pos . setIndex ( initialIndex ) ; return null ; } int n = getImaginaryCharacter ( ) . length ( ) ; startIndex = pos . getIndex ( ) ; int endIndex = startIndex + n ; if ( ( startIndex >= source . length ( ) ) || ( endIndex > source . length ( ) ) || source . substring ( startIndex , endIndex ) . compareTo ( getImaginaryCharacter ( ) ) != 0 ) { pos . setIndex ( initialIndex ) ; pos . setErrorIndex ( startIndex ) ; return null ; } pos . setIndex ( endIndex ) ; return new Complex ( re . doubleValue ( ) , im . doubleValue ( ) * sign ) ; }",
        "summary": "java.lang.StringIndexOutOfBoundsException in ComplexFormat.parse(String source, ParsePosition pos)",
        "Description": "The parse(String source, ParsePosition pos) method in the ComplexFormat class does not check whether the imaginary character is set or not which produces StringIndexOutOfBoundsException in the substring method :\n\n(line 375 of ComplexFormat)\n...\n        // parse imaginary character\n        int n = getImaginaryCharacter().length();\n        \n        startIndex = pos.getIndex();\n        int endIndex = startIndex + n;\n        if (source.substring(startIndex, endIndex).compareTo(\n            getImaginaryCharacter()) != 0) {\n...\nI encoutered this exception typing in a JTextFied with ComplexFormat set to look up an AbstractFormatter.\nIf only the user types the imaginary part of the complex number first, he gets this exception.\n\nSolution: Before setting to n length of the imaginary character, check if the source contains it. My proposal:\n...\n        int n = 0;\n        if (source.contains(getImaginaryCharacter()))\n        n = getImaginaryCharacter().length();\n...\t\t \n\nF.S.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-198",
        "comments": [
            "Fixed in svn as of r 640191.\n\nThe adopted fixed is different from the proposed one, it simply checks the length of the string with respect to the expected position of the imaginary character"
        ],
        "summarized_discussion": "\n\nThe bug was fixed in svn as of r 640191 by checking the length of the string with respect to the expected position of the imaginary character."
    },
    "Mockito_33_src/org/mockito/internal/invocation/InvocationMatcher.java_92_100": {
        "src": "public boolean hasSameMethod(Invocation candidate) {        \n        //not using method.equals() for 1 good reason:\n        //sometimes java generates forwarding methods when generics are in play see JavaGenericsForwardingMethodsTest\n        Method m1 = invocation.getMethod();\n        Method m2 = candidate.getMethod();\n        \n        \t/* Avoid unnecessary cloning */\n        return m1.equals(m2);\n    }",
        "src_wo_comments": "public boolean hasSameMethod ( Invocation candidate ) { Method m1 = invocation . getMethod ( ) ; Method m2 = candidate . getMethod ( ) ; return m1 . equals ( m2 ) ; }",
        "fixed_src": "public boolean hasSameMethod(Invocation candidate) {        \n        //not using method.equals() for 1 good reason:\n        //sometimes java generates forwarding methods when generics are in play see JavaGenericsForwardingMethodsTest\n        Method m1 = invocation.getMethod();\n        Method m2 = candidate.getMethod();\n        \n        if (m1.getName() != null && m1.getName().equals(m2.getName())) {\n        \t/* Avoid unnecessary cloning */\n        \tClass[] params1 = m1.getParameterTypes();\n        \tClass[] params2 = m2.getParameterTypes();\n        \tif (params1.length == params2.length) {\n        \t    for (int i = 0; i < params1.length; i++) {\n        \t\tif (params1[i] != params2[i])\n        \t\t    return false;\n        \t    }\n        \t    return true;\n        \t}\n        }\n        return false;\n    }",
        "fixed_src_wo_comments": "public boolean hasSameMethod ( Invocation candidate ) { Method m1 = invocation . getMethod ( ) ; Method m2 = candidate . getMethod ( ) ; if ( m1 . getName ( ) != null && m1 . getName ( ) . equals ( m2 . getName ( ) ) ) { Class [ ] params1 = m1 . getParameterTypes ( ) ; Class [ ] params2 = m2 . getParameterTypes ( ) ; if ( params1 . length == params2 . length ) { for ( int i = 0 ; i < params1 . length ; i ++ ) { if ( params1 [ i ] != params2 [ i ] ) return false ; } return true ; } } return false ; }",
        "summary": "ArgumentCaptor.fromClass's return type should match a parameterized type",
        "Description": "`ArgumentCaptor.fromClass`'s return type should match a parameterized type.  I.e. the expression `ArgumentCaptor.fromClass(Class<S>)` should be of type `ArgumentCaptor<U>` where `S` is a subtype of `U`.   \n\nFor example:\n\n```\nArgumentCaptor<Consumer<String>> captor = ArgumentCaptor.fromClass(Consumer.class)\n```\n\ndoes not type check (i.e. it is a compile time error). It should type check. \n\nThe reasons that it is desirable for `ArgumentCaptor.fromClass` to allow expressions such as the example above to type check are:\n\n1) `ArgumentCaptor.fromClass` is intended to be a convenience method to allow the user to construct an ArgumentCaptor without casting the returned value.\n\nCurrently, the user can devise a workaround such as: \n\n```\nArgumentCaptor<? extends Consumer<String>> captor \n= ArgumentCaptor.fromClass(Consumer.class)\n```\n\nThis workaround is inconvenient, and so contrary to `ArgumentCaptor.fromClass` being a convenience method.\n\n2) It is inconsistent with `@Captor`, which can be applied to a field with a paramterized type.  I.e.\n\n```\n@Captor ArgumentCaptor<Consumer<String>> captor \n```\n\ntype checks.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "PR #201\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug described in PR #201 is not provided."
    },
    "JacksonDatabind_49_src/main/java/com/fasterxml/jackson/databind/ser/impl/WritableObjectId.java_46_52": {
        "src": "public Object generateId(Object forPojo) {\n        // 04-Jun-2016, tatu: As per [databind#1255], need to consider possibility of\n        //    id being generated for \"alwaysAsId\", but not being written as POJO; regardless,\n        //    need to use existing id if there is one:\n            id = generator.generateId(forPojo);\n        return id;\n    }",
        "src_wo_comments": "public Object generateId ( Object forPojo ) { id = generator . generateId ( forPojo ) ; return id ; }",
        "fixed_src": "public Object generateId(Object forPojo) {\n        // 04-Jun-2016, tatu: As per [databind#1255], need to consider possibility of\n        //    id being generated for \"alwaysAsId\", but not being written as POJO; regardless,\n        //    need to use existing id if there is one:\n        if (id == null) {\n            id = generator.generateId(forPojo);\n        }\n        return id;\n    }",
        "fixed_src_wo_comments": "public Object generateId ( Object forPojo ) { if ( id == null ) { id = generator . generateId ( forPojo ) ; } return id ; }",
        "summary": "JsonIdentityInfo incorrectly serializing forward references",
        "Description": "I wrote this small test program to demonstrate the issue:\n\n``` java\nimport com.fasterxml.jackson.annotation.JsonIdentityInfo;\nimport com.fasterxml.jackson.annotation.JsonIdentityReference;\nimport com.fasterxml.jackson.annotation.ObjectIdGenerators;\nimport com.fasterxml.jackson.databind.ObjectMapper;\n\npublic class ObjectIdTest {\n\n    public static class Foo {\n\n        @JsonIdentityReference(alwaysAsId = true)\n        public Bar bar1;\n\n        @JsonIdentityReference()\n        public Bar bar2;\n    }\n\n    @JsonIdentityInfo(generator = ObjectIdGenerators.IntSequenceGenerator.class)\n    public static class Bar {\n\n    }\n\n    public static void main(String[] args) throws Exception {\n        ObjectMapper mapper = new ObjectMapper();\n\n        // create structure to serialize\n        Foo mo = new Foo();\n        mo.bar1 = new Bar();\n        mo.bar2 = mo.bar1;\n\n        // serialize it\n        System.out.println(mapper.writeValueAsString(mo));\n    }\n\n}\n```\n\nWhen executing this test program in the latest version (2.7.4), the output will be `{\"bar1\":1,\"bar2\":{\"@id\":2}}` - the second field will be written with a new id even though both fields reference the same object. Because of this, writing forward references is essentially impossible.\n\nThe issue seems to be the fact that BeanSerializerBase will always call WritableObjectId.generateId if the referenced object has not been written in plain format yet (https://github.com/FasterXML/jackson-databind/blob/master/src/main/java/com/fasterxml/jackson/databind/ser/std/BeanSerializerBase.java#L600). This will also happen if an id has been generated before.\nIt might also be smarter to only generate a new id in WritableObjectId.generateId if that hasn't happened before; as that method doesn't have a javadoc I can't tell how it is supposed to work.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this: it definitely sounds like a bug.\n\nI will have to dig deeper into this to say more, but what you can do in the meantime is to see how unit tests handle this case.\n"
            },
            {
                "content": "I recently fixed this quietly for myself with the attached diff.\n[patch.txt](https://github.com/FasterXML/jackson-databind/files/298674/patch.txt)\n"
            },
            {
                "content": "@arifogel Thank you -- that looks very simple indeed. I'll have a look to make sure, I hope that's the ticket!\n"
            },
            {
                "content": "@arifogel Thanks for your help; modified slightly differently, but the basic idea was sound.\nFix will be in 2.6.7 / 2.7.5 / 2.8.0\n"
            },
            {
                "content": "This patch opened up a new problem for me (and probably others): When deserializing such structures, forward references are only resolved when they are contained in a Map or Collection (via the catch UnresolvedForwardReference blocks in the corresponding deserializers). Otherwise the exception goes too far down and the resulting error is incomprehensible. I have now patched this on my end (attached). It is likely that my patch has performance impliciations, so it should be reviewed carefully.\n[patch.txt](https://github.com/FasterXML/jackson-databind/files/301494/patch.txt)\n"
            },
            {
                "content": "@arifogel is there a way to easily to reproduce the issue? I'd love to have a unit test to reproduce the problem and fix first.\n"
            },
            {
                "content": "The problem popped up with some very complicated structures inside my \nvery complicated project. Making a unit test from that would be \noverkill. If you like, I can generate a smaller unit test when I have \nsome free time - perhaps this weekend.\n\nOn 06/06/2016 04:59 PM, Tatu Saloranta wrote:\n\n> @arifogel https://github.com/arifogel is there a way to easily to \n> reproduce the issue? I'd love to have a unit test to reproduce the \n> problem and fix first.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub \n> https://github.com/FasterXML/jackson-databind/issues/1255#issuecomment-224124368, \n> or mute the thread \n> https://github.com/notifications/unsubscribe/AHYSEkMSAryYv2589EBLvShjdbwzuZmfks5qJLRtgaJpZM4Iq5Vw.\n"
            },
            {
                "content": "I take it back. My patch is not a fix, though it seems to improve the situation. Before the patch, running a particular task results in a crash with not-very-useful information. Afterwards, JSON that is serialized, deserialized, and reserialized changes.\nInitial serialization resutls in file 1.txt. Subsequent deserialization and reserialization results in file 2.txt.\nThe code for these structures starts at projects/batfish-common-protocol/src/org/batfish/datamodel/Configuration.java in the batfish \n[1.txt](https://github.com/FasterXML/jackson-databind/files/301665/1.txt)\n[2.txt](https://github.com/FasterXML/jackson-databind/files/301666/2.txt)\n\nproject at github.com/arifogel/batfish\n"
            },
            {
                "content": "One thing to notice here is that forward references inside of collections and maps are reserialized correctly, while those that are fields are not.\n"
            },
            {
                "content": "@arifogel Yes I think that a minimal (or just as small as practical) test case would be great. I don't doubt existence of the problem, or that propose fix could work either completely or partially. But it would allow checking optimal placement of handling. I also thought that bean properties for sure were properly handled wrt forward-references (can't find original check-in, but #610 is fixing one aspect), so this could even be a regression of some kind; and if so, perhaps would allow seeing what changed.\n\nAt any rate a test case would be great whenever you get a chance. I also think that earlier change should not have broken any existing working usage, in that creating new id for same object would not make sense under any conditions; at best old behavior might have masked some real failure.\n"
            },
            {
                "content": "Let's continue this conversation in bug #1261.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was initially fixed with the patch attached in the discussion, but a new problem was identified. The code for the structures was identified, and it was suggested that a minimal test case be created to check the optimal placement of handling. The fix will be included in 2.6.7 / 2.7.5 / 2.8.0, and the conversation will continue in bug #1261."
    },
    "Math_94_src/java/org/apache/commons/math/util/MathUtils.java_411_460": {
        "src": "public static int gcd(int u, int v) {\n        if (u * v == 0) {\n            return (Math.abs(u) + Math.abs(v));\n        }\n        // keep u and v negative, as negative integers range down to\n        // -2^31, while positive numbers can only be as large as 2^31-1\n        // (i.e. we can't necessarily negate a negative number without\n        // overflow)\n        /* assert u!=0 && v!=0; */\n        if (u > 0) {\n            u = -u;\n        } // make u negative\n        if (v > 0) {\n            v = -v;\n        } // make v negative\n        // B1. [Find power of 2]\n        int k = 0;\n        while ((u & 1) == 0 && (v & 1) == 0 && k < 31) { // while u and v are\n                                                            // both even...\n            u /= 2;\n            v /= 2;\n            k++; // cast out twos.\n        }\n        if (k == 31) {\n            throw new ArithmeticException(\"overflow: gcd is 2^31\");\n        }\n        // B2. Initialize: u and v have been divided by 2^k and at least\n        // one is odd.\n        int t = ((u & 1) == 1) ? v : -(u / 2)/* B3 */;\n        // t negative: u was odd, v may be even (t replaces v)\n        // t positive: u was even, v is odd (t replaces u)\n        do {\n            /* assert u<0 && v<0; */\n            // B4/B3: cast out twos from t.\n            while ((t & 1) == 0) { // while t is even..\n                t /= 2; // cast out twos\n            }\n            // B5 [reset max(u,v)]\n            if (t > 0) {\n                u = -t;\n            } else {\n                v = t;\n            }\n            // B6/B3. at this point both u and v should be odd.\n            t = (v - u) / 2;\n            // |u| larger: t positive (replace u)\n            // |v| larger: t negative (replace v)\n        } while (t != 0);\n        return -u * (1 << k); // gcd is u*2^k\n    }",
        "src_wo_comments": "public static int gcd ( int u , int v ) { if ( u * v == 0 ) { return ( Math . abs ( u ) + Math . abs ( v ) ) ; } if ( u > 0 ) { u = - u ; } if ( v > 0 ) { v = - v ; } int k = 0 ; while ( ( u & 1 ) == 0 && ( v & 1 ) == 0 && k < 31 ) { u /= 2 ; v /= 2 ; k ++ ; } if ( k == 31 ) { throw new ArithmeticException ( \"overflow: gcd is 2^31\" ) ; } int t = ( ( u & 1 ) == 1 ) ? v : - ( u / 2 ) ; do { while ( ( t & 1 ) == 0 ) { t /= 2 ; } if ( t > 0 ) { u = - t ; } else { v = t ; } t = ( v - u ) / 2 ; } while ( t != 0 ) ; return - u * ( 1 << k ) ; }",
        "fixed_src": "public static int gcd(int u, int v) {\n        if ((u == 0) || (v == 0)) {\n            return (Math.abs(u) + Math.abs(v));\n        }\n        // keep u and v negative, as negative integers range down to\n        // -2^31, while positive numbers can only be as large as 2^31-1\n        // (i.e. we can't necessarily negate a negative number without\n        // overflow)\n        /* assert u!=0 && v!=0; */\n        if (u > 0) {\n            u = -u;\n        } // make u negative\n        if (v > 0) {\n            v = -v;\n        } // make v negative\n        // B1. [Find power of 2]\n        int k = 0;\n        while ((u & 1) == 0 && (v & 1) == 0 && k < 31) { // while u and v are\n                                                            // both even...\n            u /= 2;\n            v /= 2;\n            k++; // cast out twos.\n        }\n        if (k == 31) {\n            throw new ArithmeticException(\"overflow: gcd is 2^31\");\n        }\n        // B2. Initialize: u and v have been divided by 2^k and at least\n        // one is odd.\n        int t = ((u & 1) == 1) ? v : -(u / 2)/* B3 */;\n        // t negative: u was odd, v may be even (t replaces v)\n        // t positive: u was even, v is odd (t replaces u)\n        do {\n            /* assert u<0 && v<0; */\n            // B4/B3: cast out twos from t.\n            while ((t & 1) == 0) { // while t is even..\n                t /= 2; // cast out twos\n            }\n            // B5 [reset max(u,v)]\n            if (t > 0) {\n                u = -t;\n            } else {\n                v = t;\n            }\n            // B6/B3. at this point both u and v should be odd.\n            t = (v - u) / 2;\n            // |u| larger: t positive (replace u)\n            // |v| larger: t negative (replace v)\n        } while (t != 0);\n        return -u * (1 << k); // gcd is u*2^k\n    }",
        "fixed_src_wo_comments": "public static int gcd ( int u , int v ) { if ( ( u == 0 ) || ( v == 0 ) ) { return ( Math . abs ( u ) + Math . abs ( v ) ) ; } if ( u > 0 ) { u = - u ; } if ( v > 0 ) { v = - v ; } int k = 0 ; while ( ( u & 1 ) == 0 && ( v & 1 ) == 0 && k < 31 ) { u /= 2 ; v /= 2 ; k ++ ; } if ( k == 31 ) { throw new ArithmeticException ( \"overflow: gcd is 2^31\" ) ; } int t = ( ( u & 1 ) == 1 ) ? v : - ( u / 2 ) ; do { while ( ( t & 1 ) == 0 ) { t /= 2 ; } if ( t > 0 ) { u = - t ; } else { v = t ; } t = ( v - u ) / 2 ; } while ( t != 0 ) ; return - u * ( 1 << k ) ; }",
        "summary": "MathUtils.gcd(u, v) fails when u and v both contain a high power of 2",
        "Description": "The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.\n\n        assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15)));\n\nFix: Replace the test at the start of MathUtils.gcd()\n\n        if (u * v == 0) {\n\nby\n\n        if (u == 0 || v == 0) {\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-238",
        "comments": [
            "fixed in trunk as of r73517\n\nthanks for the report"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the source code as of revision 73517. The person who reported the bug is thanked for their contribution."
    },
    "Compress_23_src/main/java/org/apache/commons/compress/archivers/sevenz/Coders.java_106_118": {
        "src": "@Override\n        InputStream decode(final InputStream in, final Coder coder,\n                byte[] password) throws IOException {\n            byte propsByte = coder.properties[0];\n            long dictSize = coder.properties[1];\n            for (int i = 1; i < 4; i++) {\n                dictSize |= (coder.properties[i + 1] << (8 * i));\n            }\n            if (dictSize > LZMAInputStream.DICT_SIZE_MAX) {\n                throw new IOException(\"Dictionary larger than 4GiB maximum size\");\n            }\n            return new LZMAInputStream(in, -1, propsByte, (int) dictSize);\n        }",
        "src_wo_comments": "@ Override InputStream decode ( final InputStream in , final Coder coder , byte [ ] password ) throws IOException { byte propsByte = coder . properties [ 0 ] ; long dictSize = coder . properties [ 1 ] ; for ( int i = 1 ; i < 4 ; i ++ ) { dictSize |= ( coder . properties [ i + 1 ] << ( 8 * i ) ) ; } if ( dictSize > LZMAInputStream . DICT_SIZE_MAX ) { throw new IOException ( \"Dictionary larger than 4GiB maximum size\" ) ; } return new LZMAInputStream ( in , - 1 , propsByte , ( int ) dictSize ) ; }",
        "fixed_src": "@Override\n        InputStream decode(final InputStream in, final Coder coder,\n                byte[] password) throws IOException {\n            byte propsByte = coder.properties[0];\n            long dictSize = coder.properties[1];\n            for (int i = 1; i < 4; i++) {\n                dictSize |= (coder.properties[i + 1] & 0xffl) << (8 * i);\n            }\n            if (dictSize > LZMAInputStream.DICT_SIZE_MAX) {\n                throw new IOException(\"Dictionary larger than 4GiB maximum size\");\n            }\n            return new LZMAInputStream(in, -1, propsByte, (int) dictSize);\n        }",
        "fixed_src_wo_comments": "@ Override InputStream decode ( final InputStream in , final Coder coder , byte [ ] password ) throws IOException { byte propsByte = coder . properties [ 0 ] ; long dictSize = coder . properties [ 1 ] ; for ( int i = 1 ; i < 4 ; i ++ ) { dictSize |= ( coder . properties [ i + 1 ] & 0xffl ) << ( 8 * i ) ; } if ( dictSize > LZMAInputStream . DICT_SIZE_MAX ) { throw new IOException ( \"Dictionary larger than 4GiB maximum size\" ) ; } return new LZMAInputStream ( in , - 1 , propsByte , ( int ) dictSize ) ; }",
        "summary": "7z: 16 MB dictionary is too big",
        "Description": "I created an archiv with 7zip 9.20 containing the compress-1.7-src directory. Also tried it with 1.6 version and directory. I \n\ndownloaded the zip file and reziped it as 7z. The standard setting where used:\nCompression level: normal\nCompression method: lzma2\nDictionary size: 16 MB\nWord size: 32\nSolid Block size: 2 GB\n\nI get an exception if I try to open the file with the simple line of code:\nSevenZFile input = new SevenZFile(new File(arcName));\n\nMaybe it is a bug in the tukaani library, but I do not know how to report it to them.\nThe exception thrown:\n\norg.tukaani.xz.UnsupportedOptionsException: LZMA dictionary is too big for this implementation\n\tat org.tukaani.xz.LZMAInputStream.initialize(Unknown Source)\n\tat org.tukaani.xz.LZMAInputStream.<init>(Unknown Source)\n\tat org.apache.commons.compress.archivers.sevenz.Coders$LZMADecoder.decode(Coders.java:117)\n\tat org.apache.commons.compress.archivers.sevenz.Coders.addDecoder(Coders.java:48)\n\tat org.apache.commons.compress.archivers.sevenz.SevenZFile.readEncodedHeader(SevenZFile.java:278)\n\tat org.apache.commons.compress.archivers.sevenz.SevenZFile.readHeaders(SevenZFile.java:190)\n\tat org.apache.commons.compress.archivers.sevenz.SevenZFile.<init>(SevenZFile.java:94)\n\tat org.apache.commons.compress.archivers.sevenz.SevenZFile.<init>(SevenZFile.java:116)\n\tat compress.SevenZipError.main(SevenZipError.java:28)",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-256",
        "comments": [
            "note this is while reading the header rather than the content of the entry.  The content may use LZMA2 but the header seems to use LZMA.\n\nXZ for Java says:\n\n{quote}\n     * LZMA allows dictionaries up to one byte less than 4 GiB. This\n     * implementation supports only 16 bytes less than 2 GiB. This\n     * limitation is due to Java using signed 32-bit integers for array\n     * indexing. The limitation shouldn't matter much in practice since so\n     * huge dictionaries are not normally used.\n{quote}\n\nuhm, apparently they are used.  But then again I'm surprised 7z would use a dictionary larger than 2GiB for headers.  This makes me believe there is some bug in our parsing code for compressed headers.  Again, any chance you could provide an example archive?",
            "I attached the archive with the problem.\nBtw. besides the two problem-archives I have got a third archive with same settings that works without problems.",
            "fixed with svn revision 1560252\n\nJava's signed bytes and overflowing integers byting us :-("
        ],
        "summarized_discussion": "\n\nThe bug in the source code was caused by Java using signed 32-bit integers for array indexing, which limited the dictionary size to 16 bytes less than 2 GiB. The problem was solved by an attached example archive, and was fixed with SVN revision 1560252. The cause of the bug was Java's signed bytes and overflowing integers."
    },
    "Compress_35_src/main/java/org/apache/commons/compress/archivers/tar/TarUtils.java_593_613": {
        "src": "public static boolean verifyCheckSum(byte[] header) {\n        long storedSum = 0;\n        long unsignedSum = 0;\n        long signedSum = 0;\n\n        int digits = 0;\n        for (int i = 0; i < header.length; i++) {\n            byte b = header[i];\n            if (CHKSUM_OFFSET  <= i && i < CHKSUM_OFFSET + CHKSUMLEN) {\n                if ('0' <= b && b <= '7' && digits++ < 6) {\n                    storedSum = storedSum * 8 + b - '0';\n                } else if (digits > 0) {\n                    digits = 6;\n                }\n                b = ' ';\n            }\n            unsignedSum += 0xff & b;\n            signedSum += b;\n        }\n        return storedSum == unsignedSum || storedSum == signedSum;\n    }",
        "src_wo_comments": "public static boolean verifyCheckSum ( byte [ ] header ) { long storedSum = 0 ; long unsignedSum = 0 ; long signedSum = 0 ; int digits = 0 ; for ( int i = 0 ; i < header . length ; i ++ ) { byte b = header [ i ] ; if ( CHKSUM_OFFSET <= i && i < CHKSUM_OFFSET + CHKSUMLEN ) { if ( '0' <= b && b <= '7' && digits ++ < 6 ) { storedSum = storedSum * 8 + b - '0' ; } else if ( digits > 0 ) { digits = 6 ; } b = ' ' ; } unsignedSum += 0xff & b ; signedSum += b ; } return storedSum == unsignedSum || storedSum == signedSum ; }",
        "fixed_src": "public static boolean verifyCheckSum(byte[] header) {\n        long storedSum = parseOctal(header, CHKSUM_OFFSET, CHKSUMLEN);\n        long unsignedSum = 0;\n        long signedSum = 0;\n\n        int digits = 0;\n        for (int i = 0; i < header.length; i++) {\n            byte b = header[i];\n            if (CHKSUM_OFFSET  <= i && i < CHKSUM_OFFSET + CHKSUMLEN) {\n                b = ' ';\n            }\n            unsignedSum += 0xff & b;\n            signedSum += b;\n        }\n        return storedSum == unsignedSum || storedSum == signedSum;\n    }",
        "fixed_src_wo_comments": "public static boolean verifyCheckSum ( byte [ ] header ) { long storedSum = parseOctal ( header , CHKSUM_OFFSET , CHKSUMLEN ) ; long unsignedSum = 0 ; long signedSum = 0 ; int digits = 0 ; for ( int i = 0 ; i < header . length ; i ++ ) { byte b = header [ i ] ; if ( CHKSUM_OFFSET <= i && i < CHKSUM_OFFSET + CHKSUMLEN ) { b = ' ' ; } unsignedSum += 0xff & b ; signedSum += b ; } return storedSum == unsignedSum || storedSum == signedSum ; }",
        "summary": "TAR checksum fails when checksum is right aligned",
        "Description": "The linked TAR has a checksum with zero padding on the left instead of the expected {{NULL-SPACE}} terminator on the right. As a result the last two digits of the stored checksum are lost and the otherwise valid checksum is treated as invalid.\n\nGiven that the code already checks for digits being in range before adding them to the stored sum, is it necessary to only look at the first 6 octal digits instead of the whole field?",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-335",
        "comments": [
            "We could probably use {{parseOctal}} for reading the checksum, I'll give it a try.\n",
            "Yes, {{parseOctal}} works fine.\n\nFixed with git commit 7250daa"
        ],
        "summarized_discussion": "\n\nThe bug was fixed with the use of the {{parseOctal}} function, which was committed in git commit 7250daa."
    },
    "Codec_6_src/java/org/apache/commons/codec/binary/Base64InputStream.java_138_180": {
        "src": "public int read(byte b[], int offset, int len) throws IOException {\n        if (b == null) {\n            throw new NullPointerException();\n        } else if (offset < 0 || len < 0) {\n            throw new IndexOutOfBoundsException();\n        } else if (offset > b.length || offset + len > b.length) {\n            throw new IndexOutOfBoundsException();\n        } else if (len == 0) {\n            return 0;\n        } else {\n            /*\n             Rationale for while-loop on (readLen == 0):\n             -----\n             Base64.readResults() usually returns > 0 or EOF (-1).  In the\n             rare case where it returns 0, we just keep trying.\n\n             This is essentially an undocumented contract for InputStream\n             implementors that want their code to work properly with\n             java.io.InputStreamReader, since the latter hates it when\n             InputStream.read(byte[]) returns a zero.  Unfortunately our\n             readResults() call must return 0 if a large amount of the data\n             being decoded was non-base64, so this while-loop enables proper\n             interop with InputStreamReader for that scenario.\n             -----\n             This is a fix for CODEC-101\n            */\n                if (!base64.hasData()) {\n                    byte[] buf = new byte[doEncode ? 4096 : 8192];\n                    int c = in.read(buf);\n                    // A little optimization to avoid System.arraycopy()\n                    // when possible.\n                    if (c > 0 && b.length == len) {\n                        base64.setInitialBuffer(b, offset, len);\n                    }\n                    if (doEncode) {\n                        base64.encode(buf, 0, c);\n                    } else {\n                        base64.decode(buf, 0, c);\n                    }\n                }\n            return base64.readResults(b, offset, len);\n        }\n    }",
        "src_wo_comments": "public int read ( byte b [ ] , int offset , int len ) throws IOException { if ( b == null ) { throw new NullPointerException ( ) ; } else if ( offset < 0 || len < 0 ) { throw new IndexOutOfBoundsException ( ) ; } else if ( offset > b . length || offset + len > b . length ) { throw new IndexOutOfBoundsException ( ) ; } else if ( len == 0 ) { return 0 ; } else { if ( ! base64 . hasData ( ) ) { byte [ ] buf = new byte [ doEncode ? 4096 : 8192 ] ; int c = in . read ( buf ) ; if ( c > 0 && b . length == len ) { base64 . setInitialBuffer ( b , offset , len ) ; } if ( doEncode ) { base64 . encode ( buf , 0 , c ) ; } else { base64 . decode ( buf , 0 , c ) ; } } return base64 . readResults ( b , offset , len ) ; } }",
        "fixed_src": "public int read(byte b[], int offset, int len) throws IOException {\n        if (b == null) {\n            throw new NullPointerException();\n        } else if (offset < 0 || len < 0) {\n            throw new IndexOutOfBoundsException();\n        } else if (offset > b.length || offset + len > b.length) {\n            throw new IndexOutOfBoundsException();\n        } else if (len == 0) {\n            return 0;\n        } else {\n            int readLen = 0;\n            /*\n             Rationale for while-loop on (readLen == 0):\n             -----\n             Base64.readResults() usually returns > 0 or EOF (-1).  In the\n             rare case where it returns 0, we just keep trying.\n\n             This is essentially an undocumented contract for InputStream\n             implementors that want their code to work properly with\n             java.io.InputStreamReader, since the latter hates it when\n             InputStream.read(byte[]) returns a zero.  Unfortunately our\n             readResults() call must return 0 if a large amount of the data\n             being decoded was non-base64, so this while-loop enables proper\n             interop with InputStreamReader for that scenario.\n             -----\n             This is a fix for CODEC-101\n            */\n            while (readLen == 0) {\n                if (!base64.hasData()) {\n                    byte[] buf = new byte[doEncode ? 4096 : 8192];\n                    int c = in.read(buf);\n                    // A little optimization to avoid System.arraycopy()\n                    // when possible.\n                    if (c > 0 && b.length == len) {\n                        base64.setInitialBuffer(b, offset, len);\n                    }\n                    if (doEncode) {\n                        base64.encode(buf, 0, c);\n                    } else {\n                        base64.decode(buf, 0, c);\n                    }\n                }\n                readLen = base64.readResults(b, offset, len);\n            }\n            return readLen;\n        }\n    }",
        "fixed_src_wo_comments": "public int read ( byte b [ ] , int offset , int len ) throws IOException { if ( b == null ) { throw new NullPointerException ( ) ; } else if ( offset < 0 || len < 0 ) { throw new IndexOutOfBoundsException ( ) ; } else if ( offset > b . length || offset + len > b . length ) { throw new IndexOutOfBoundsException ( ) ; } else if ( len == 0 ) { return 0 ; } else { int readLen = 0 ; while ( readLen == 0 ) { if ( ! base64 . hasData ( ) ) { byte [ ] buf = new byte [ doEncode ? 4096 : 8192 ] ; int c = in . read ( buf ) ; if ( c > 0 && b . length == len ) { base64 . setInitialBuffer ( b , offset , len ) ; } if ( doEncode ) { base64 . encode ( buf , 0 , c ) ; } else { base64 . decode ( buf , 0 , c ) ; } } readLen = base64 . readResults ( b , offset , len ) ; } return readLen ; } }",
        "summary": "Base64InputStream#read(byte[]) incorrectly returns 0 at end of any stream which is multiple of 3 bytes long",
        "Description": "Using new InputStreamReader(new Base64InputStream(in, true)) sometimes fails with \"java.io.IOException: Underlying input stream returned zero bytes\".\n\nThis is been tracked down that Base64InputStream#read(byte[]) incorrectly returns 0 at end of any stream which is multiple of 3 bytes long.",
        "issue_url": "https://issues.apache.org/jira//browse/CODEC-101",
        "comments": [
            "Thanks for the bug report.  I'm looking into it.",
            "Wow, I can't imagine how you tracked this one down to Base64InputStream!  Java6's stacktrace is so un-helpful:\n\n{code}\npublic class R {\n    public static void main(String[] args) throws Exception {\n        Base64InputStream in = new Base64InputStream(System.in);\n        InputStreamReader isr = new InputStreamReader(in, \"UTF-8\");\n        BufferedReader br = new BufferedReader(isr);\n        String line = br.readLine();\n        while (line != null) {\n            System.out.println(\"Line:\" + line);\n            line = br.readLine();\n        }\n    }\n}\n\n$ java -cp build/playground.jar:/opt/dev/codec/target/classes  R   < all-whitespace.txt \nException in thread \"main\" java.io.IOException: Underlying input stream returned zero bytes\n\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:268)\n\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:306)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:158)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:167)\n\tat java.io.BufferedReader.fill(BufferedReader.java:136)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:299)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:362)\n\tat R.main(R.java:17)\n{code}\n\n\nI've attached a patch that appears to fix it.   Are you able to try this out?  You can try the following commands to build a patched binary:\n\n{code}\nsvn export http://svn.apache.org/repos/asf/commons/proper/codec/trunk/ codec-trunk\ncd codec-trunk\npatch -p0 < codec-101-fix.patch\nant dist\n{code}\n\n\nIn the meantime I'm working on some unit tests to better exercise this patch.  I'll try and get those uploaded here to CODEC-101 within 48 hours.\n\n\n",
            "The fix works. As to nailing down the root cause, the credit goes to someone else, the user named Keith Randall nailed it down in this topic at Stackoverflow: http://stackoverflow.com/questions/2937161/problem-using-base64-encoder-and-inputstreamreader ",
            "This bug has been fixed in June but as of November, the fix still has not been committed to SVN? It is hurting me bigtime and at least getting the patch accepted should be a small matter?\n\n(Posting from a BugMeNot account, sorry about that)",
            "How about a unit test showing the bug in action? \n\nIdeally, I would like to: apply the unit test patch, watch the test fail, apply the main patch, watch the test pass.",
            "Without using the code from stackoverflow as that's an incompatible license. :)",
            "This triggers the problem:   InputStream.read(byte[]) should never return 0.",
            "throwing in another JUnit test",
            "The three patches are committed to trunk now."
        ],
        "summarized_discussion": "\n\nThe bug was tracked down to Base64InputStream in Java6, and a patch was attached to fix it. The patch was tested by building a patched binary using the commands provided. A unit test was proposed to show the bug in action, and the three patches were committed to trunk."
    },
    "JxPath_6_src/java/org/apache/commons/jxpath/ri/compiler/CoreOperationCompare.java_45_83": {
        "src": "protected boolean equal(\n        EvalContext context,\n        Expression left,\n        Expression right) \n    {\n        Object l = left.compute(context);\n        Object r = right.compute(context);\n\n//        System.err.println(\"COMPARING: \" +\n//            (l == null ? \"null\" : l.getClass().getName()) + \" \" +\n//            (r == null ? \"null\" : r.getClass().getName()));\n\n        if (l instanceof InitialContext || l instanceof SelfContext) {\n            l = ((EvalContext) l).getSingleNodePointer();\n        }\n\n        if (r instanceof InitialContext || r instanceof SelfContext) {\n            r = ((EvalContext) r).getSingleNodePointer();\n        }\n\n        if (l instanceof Collection) {\n            l = ((Collection) l).iterator();\n        }\n\n        if (r instanceof Collection) {\n            r = ((Collection) r).iterator();\n        }\n\n        if ((l instanceof Iterator) && !(r instanceof Iterator)) {\n            return contains((Iterator) l, r);\n        }\n        if (!(l instanceof Iterator) && (r instanceof Iterator)) {\n            return contains((Iterator) r, l);\n        }\n        if (l instanceof Iterator && r instanceof Iterator) {\n            return findMatch((Iterator) l, (Iterator) r);\n        }\n        return equal(l, r);\n    }",
        "src_wo_comments": "protected boolean equal ( EvalContext context , Expression left , Expression right ) { Object l = left . compute ( context ) ; Object r = right . compute ( context ) ; if ( l instanceof InitialContext || l instanceof SelfContext ) { l = ( ( EvalContext ) l ) . getSingleNodePointer ( ) ; } if ( r instanceof InitialContext || r instanceof SelfContext ) { r = ( ( EvalContext ) r ) . getSingleNodePointer ( ) ; } if ( l instanceof Collection ) { l = ( ( Collection ) l ) . iterator ( ) ; } if ( r instanceof Collection ) { r = ( ( Collection ) r ) . iterator ( ) ; } if ( ( l instanceof Iterator ) && ! ( r instanceof Iterator ) ) { return contains ( ( Iterator ) l , r ) ; } if ( ! ( l instanceof Iterator ) && ( r instanceof Iterator ) ) { return contains ( ( Iterator ) r , l ) ; } if ( l instanceof Iterator && r instanceof Iterator ) { return findMatch ( ( Iterator ) l , ( Iterator ) r ) ; } return equal ( l , r ) ; }",
        "fixed_src": "protected boolean equal(\n        EvalContext context,\n        Expression left,\n        Expression right) \n    {\n        Object l = left.compute(context);\n        Object r = right.compute(context);\n\n//        System.err.println(\"COMPARING: \" +\n//            (l == null ? \"null\" : l.getClass().getName()) + \" \" +\n//            (r == null ? \"null\" : r.getClass().getName()));\n\n        if (l instanceof InitialContext) {\n            ((EvalContext) l).reset();\n        }\n\n        if (l instanceof SelfContext) {\n            l = ((EvalContext) l).getSingleNodePointer();\n        }\n\n        if (r instanceof InitialContext) {\n            ((EvalContext) r).reset();\n        }\n\n        if (r instanceof SelfContext) {\n            r = ((EvalContext) r).getSingleNodePointer();\n        }\n\n        if (l instanceof Collection) {\n            l = ((Collection) l).iterator();\n        }\n\n        if (r instanceof Collection) {\n            r = ((Collection) r).iterator();\n        }\n\n        if ((l instanceof Iterator) && !(r instanceof Iterator)) {\n            return contains((Iterator) l, r);\n        }\n        if (!(l instanceof Iterator) && (r instanceof Iterator)) {\n            return contains((Iterator) r, l);\n        }\n        if (l instanceof Iterator && r instanceof Iterator) {\n            return findMatch((Iterator) l, (Iterator) r);\n        }\n        return equal(l, r);\n    }",
        "fixed_src_wo_comments": "protected boolean equal ( EvalContext context , Expression left , Expression right ) { Object l = left . compute ( context ) ; Object r = right . compute ( context ) ; if ( l instanceof InitialContext ) { ( ( EvalContext ) l ) . reset ( ) ; } if ( l instanceof SelfContext ) { l = ( ( EvalContext ) l ) . getSingleNodePointer ( ) ; } if ( r instanceof InitialContext ) { ( ( EvalContext ) r ) . reset ( ) ; } if ( r instanceof SelfContext ) { r = ( ( EvalContext ) r ) . getSingleNodePointer ( ) ; } if ( l instanceof Collection ) { l = ( ( Collection ) l ) . iterator ( ) ; } if ( r instanceof Collection ) { r = ( ( Collection ) r ) . iterator ( ) ; } if ( ( l instanceof Iterator ) && ! ( r instanceof Iterator ) ) { return contains ( ( Iterator ) l , r ) ; } if ( ! ( l instanceof Iterator ) && ( r instanceof Iterator ) ) { return contains ( ( Iterator ) r , l ) ; } if ( l instanceof Iterator && r instanceof Iterator ) { return findMatch ( ( Iterator ) l , ( Iterator ) r ) ; } return equal ( l , r ) ; }",
        "summary": "equality test for multi-valued variables does not conform to spec",
        "Description": "given e.g. variable d={\"a\", \"b\"}, the spec implies that \"$d = 'a'\" and that \"$d = 'b'\".  Instead of iterating the variable's components its immediate content (here, the String[]) is compared, causing the aforementioned assertions to fail.",
        "issue_url": "https://issues.apache.org/jira//browse/JXPATH-94",
        "comments": [
            "fixed in trunk!",
            "Seems like a dublicate of JXPATH-93 for me :)",
            "Not sure yet.  But if you feel like re-testing against trunk code and it turns out that I accidentally fixed it, feel free!"
        ],
        "summarized_discussion": "\n\nThe bug seems to have been fixed in the trunk code, and may be a duplicate of JXPATH-93. If the bug is re-tested against the trunk code and it is found that it has been fixed, then that is the solution."
    },
    "JacksonXml_5_src/main/java/com/fasterxml/jackson/dataformat/xml/ser/XmlSerializerProvider.java_55_60": {
        "src": "protected XmlSerializerProvider(XmlSerializerProvider src) {\n        super(src);\n        // 21-May-2018, tatu: As per [dataformat-xml#282], should NOT really copy\n        //    root name lookup as that may link back to diff version, configuration\n        _rootNameLookup = src._rootNameLookup;\n    }",
        "src_wo_comments": "protected XmlSerializerProvider ( XmlSerializerProvider src ) { super ( src ) ; _rootNameLookup = src . _rootNameLookup ; }",
        "fixed_src": "protected XmlSerializerProvider(XmlSerializerProvider src) {\n        super(src);\n        // 21-May-2018, tatu: As per [dataformat-xml#282], should NOT really copy\n        //    root name lookup as that may link back to diff version, configuration\n        _rootNameLookup = new XmlRootNameLookup();\n    }",
        "fixed_src_wo_comments": "protected XmlSerializerProvider ( XmlSerializerProvider src ) { super ( src ) ; _rootNameLookup = new XmlRootNameLookup ( ) ; }",
        "summary": "`@JacksonXmlRootElement` malfunction when using it with multiple `XmlMapper`s and disabling annotations",
        "Description": "Found this in version 2.9.4 running some tests that go back and forth serializing with an XML mapper that uses annotations, and another one that ignores them. May be related to issue #171 and the cache of class annotations.\r\n\r\nWhen running this code, the second print statement should use the annotation's localName but it instead uses the class name.\r\n\r\n```\r\n@JacksonXmlRootElement(localName = \"myname\")\r\npublic class XMLTest {\r\n\r\n    public static void main(String[] s) throws Exception {\r\n\r\n        final ObjectMapper xmlMapper = new XmlMapper();\r\n        final ObjectMapper noAnnotationsXmlMapper = xmlMapper.copy()\r\n                .configure(MapperFeature.USE_ANNOTATIONS, false)\r\n                .configure(SerializationFeature.FAIL_ON_EMPTY_BEANS, false);\r\n\r\n        System.out.println(noAnnotationsXmlMapper.writeValueAsString(new XMLTest()));\r\n        System.out.println(xmlMapper.writeValueAsString(new XMLTest()));\r\n\r\n    }\r\n\r\n}\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n<XMLTest/>\r\n<XMLTest/>\r\n```",
        "issue_url": null,
        "comments": [
            {
                "content": "Ok, does sound like caching of root names might be causing the issue here.\r\n"
            },
            {
                "content": "Turned out to be inadvertent sharing of root name lookup cache. Was resolved for 3.0 (due to the way construction changed), but test now covers 2.9 and 3.0. Fix will be in `2.9.6`.\r\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was caused by inadvertent sharing of root name lookup cache. The fix was implemented in version 2.9.6 and is now covered in tests for both 2.9 and 3.0."
    },
    "Math_82_src/main/java/org/apache/commons/math/optimization/linear/SimplexSolver.java_76_91": {
        "src": "private Integer getPivotRow(final int col, final SimplexTableau tableau) {\n        double minRatio = Double.MAX_VALUE;\n        Integer minRatioPos = null;\n        for (int i = tableau.getNumObjectiveFunctions(); i < tableau.getHeight(); i++) {\n            final double rhs = tableau.getEntry(i, tableau.getWidth() - 1);\n            final double entry = tableau.getEntry(i, col);\n            if (MathUtils.compareTo(entry, 0, epsilon) >= 0) {\n                final double ratio = rhs / entry;\n                if (ratio < minRatio) {\n                    minRatio = ratio;\n                    minRatioPos = i; \n                }\n            }\n        }\n        return minRatioPos;\n    }",
        "src_wo_comments": "private Integer getPivotRow ( final int col , final SimplexTableau tableau ) { double minRatio = Double . MAX_VALUE ; Integer minRatioPos = null ; for ( int i = tableau . getNumObjectiveFunctions ( ) ; i < tableau . getHeight ( ) ; i ++ ) { final double rhs = tableau . getEntry ( i , tableau . getWidth ( ) - 1 ) ; final double entry = tableau . getEntry ( i , col ) ; if ( MathUtils . compareTo ( entry , 0 , epsilon ) >= 0 ) { final double ratio = rhs / entry ; if ( ratio < minRatio ) { minRatio = ratio ; minRatioPos = i ; } } } return minRatioPos ; }",
        "fixed_src": "private Integer getPivotRow(final int col, final SimplexTableau tableau) {\n        double minRatio = Double.MAX_VALUE;\n        Integer minRatioPos = null;\n        for (int i = tableau.getNumObjectiveFunctions(); i < tableau.getHeight(); i++) {\n            final double rhs = tableau.getEntry(i, tableau.getWidth() - 1);\n            final double entry = tableau.getEntry(i, col);\n            if (MathUtils.compareTo(entry, 0, epsilon) > 0) {\n                final double ratio = rhs / entry;\n                if (ratio < minRatio) {\n                    minRatio = ratio;\n                    minRatioPos = i; \n                }\n            }\n        }\n        return minRatioPos;\n    }",
        "fixed_src_wo_comments": "private Integer getPivotRow ( final int col , final SimplexTableau tableau ) { double minRatio = Double . MAX_VALUE ; Integer minRatioPos = null ; for ( int i = tableau . getNumObjectiveFunctions ( ) ; i < tableau . getHeight ( ) ; i ++ ) { final double rhs = tableau . getEntry ( i , tableau . getWidth ( ) - 1 ) ; final double entry = tableau . getEntry ( i , col ) ; if ( MathUtils . compareTo ( entry , 0 , epsilon ) > 0 ) { final double ratio = rhs / entry ; if ( ratio < minRatio ) { minRatio = ratio ; minRatioPos = i ; } } } return minRatioPos ; }",
        "summary": "SimplexSolver not working as expected 2",
        "Description": "SimplexSolver didn't find the optimal solution.\n\nProgram for Lpsolve:\n=====================\n/* Objective function */\nmax: 7 a 3 b;\n\n/* Constraints */\nR1: +3 a -5 c <= 0;\nR2: +2 a -5 d <= 0;\nR3: +2 b -5 c <= 0;\nR4: +3 b -5 d <= 0;\nR5: +3 a +2 b <= 5;\nR6: +2 a +3 b <= 5;\n\n/* Variable bounds */\na <= 1;\nb <= 1;\n=====================\nResults(correct): a = 1, b = 1, value = 10\n\n\nProgram for SimplexSolve:\n=====================\nLinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[]{7, 3, 0, 0}, 0);\nCollection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>();\npodmienky.add(new LinearConstraint(new double[]{1, 0, 0, 0}, Relationship.LEQ, 1));\npodmienky.add(new LinearConstraint(new double[]{0, 1, 0, 0}, Relationship.LEQ, 1));\npodmienky.add(new LinearConstraint(new double[]{3, 0, -5, 0}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{2, 0, 0, -5}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{0, 2, -5, 0}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{0, 3, 0, -5}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{3, 2, 0, 0}, Relationship.LEQ, 5));\npodmienky.add(new LinearConstraint(new double[]{2, 3, 0, 0}, Relationship.LEQ, 5));\nSimplexSolver solver = new SimplexSolver();\nRealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true);\n=====================\nResults(incorrect): a = 1, b = 0.5, value = 8.5\n\nP.S. I used the latest software from the repository (including MATH-286 fix).",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-288",
        "comments": [
            "Thanks for the bug report.  I've confirmed this is an issue.\n\nHere's a slightly smaller version of the problem that causes the same bug, which might be easier for debugging:\n\nMAX 7 a + 3 b\ns.t.\n3 a -5 c <= 0\n2 a -5 d <= 0\n3 b -5 d <= 0\na <= 1\nb <= 1\n\n        LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 7, 3, 0, 0 }, 0 );\n        Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();\n        constraints.add(new LinearConstraint(new double[] { 3, 0, -5, 0 }, Relationship.LEQ, 0.0));\n        constraints.add(new LinearConstraint(new double[] { 2, 0, 0, -5 }, Relationship.LEQ, 0.0));\n        constraints.add(new LinearConstraint(new double[] { 0, 3, 0, -5 }, Relationship.LEQ, 0.0));\n        constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0 }, Relationship.LEQ, 1.0));\n        constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 0 }, Relationship.LEQ, 1.0));\n    \n        SimplexSolver solver = new SimplexSolver();\n        RealPointValuePair solution = solver.optimize(f, constraints, GoalType.MAXIMIZE, true);\n        assertEquals(10.0, solution.getValue(), .0000001);\n",
            "Patch attached.  It was a 1 character bug.  I was saying to only do the minimum ratio test if the entry is >= 0, but it should have been > 0 (dividing by 0 is never good :o)\nThanks again for the bug report.",
            "resolved in subversion repository as of r807738\npatch applied (except for debug print function)\nthanks for the repoart and thanks for the patch"
        ],
        "summarized_discussion": "\n\nThe bug has been resolved in the Subversion repository as of r807738, and a patch has been applied (except for a debug print function). The issue was a one character bug, where the entry was being checked for being greater than or equal to 0, when it should have been greater than 0. Thanks for the bug report and patch."
    },
    "Csv_3_src/main/java/org/apache/commons/csv/Lexer.java_87_114": {
        "src": "int readEscape() throws IOException {\n        // the escape char has just been read (normally a backslash)\n        final int c = in.read();\n        switch (c) {\n        case 'r':\n            return CR;\n        case 'n':\n            return LF;\n        case 't':\n            return TAB;\n        case 'b':\n            return BACKSPACE;\n        case 'f':\n            return FF;\n        case CR:\n        case LF:\n        case FF: // TODO is this correct?\n        case TAB: // TODO is this correct? Do tabs need to be escaped?\n        case BACKSPACE: // TODO is this correct?\n            return c;\n        case END_OF_STREAM:\n            throw new IOException(\"EOF whilst processing escape sequence\");\n        default:\n            // Now check for meta-characters\n                return c;\n            // indicate unexpected char - available from in.getLastChar()\n        }\n    }",
        "src_wo_comments": "int readEscape ( ) throws IOException { final int c = in . read ( ) ; switch ( c ) { case 'r' : return CR ; case 'n' : return LF ; case 't' : return TAB ; case 'b' : return BACKSPACE ; case 'f' : return FF ; case CR : case LF : case FF : case TAB : case BACKSPACE : return c ; case END_OF_STREAM : throw new IOException ( \"EOF whilst processing escape sequence\" ) ; default : return c ; } }",
        "fixed_src": "int readEscape() throws IOException {\n        // the escape char has just been read (normally a backslash)\n        final int c = in.read();\n        switch (c) {\n        case 'r':\n            return CR;\n        case 'n':\n            return LF;\n        case 't':\n            return TAB;\n        case 'b':\n            return BACKSPACE;\n        case 'f':\n            return FF;\n        case CR:\n        case LF:\n        case FF: // TODO is this correct?\n        case TAB: // TODO is this correct? Do tabs need to be escaped?\n        case BACKSPACE: // TODO is this correct?\n            return c;\n        case END_OF_STREAM:\n            throw new IOException(\"EOF whilst processing escape sequence\");\n        default:\n            // Now check for meta-characters\n            if (isDelimiter(c) || isEscape(c) || isQuoteChar(c) || isCommentStart(c)) {\n                return c;\n            }\n            // indicate unexpected char - available from in.getLastChar()\n            return END_OF_STREAM;\n        }\n    }",
        "fixed_src_wo_comments": "int readEscape ( ) throws IOException { final int c = in . read ( ) ; switch ( c ) { case 'r' : return CR ; case 'n' : return LF ; case 't' : return TAB ; case 'b' : return BACKSPACE ; case 'f' : return FF ; case CR : case LF : case FF : case TAB : case BACKSPACE : return c ; case END_OF_STREAM : throw new IOException ( \"EOF whilst processing escape sequence\" ) ; default : if ( isDelimiter ( c ) || isEscape ( c ) || isQuoteChar ( c ) || isCommentStart ( c ) ) { return c ; } return END_OF_STREAM ; } }",
        "summary": "Unescape handling needs rethinking",
        "Description": "The current escape parsing converts <esc><char> to plain <char> if the <char> is not one of the special characters to be escaped.\n\nThis can affect unicode escapes if the <esc> character is backslash.\n\nOne way round this is to specifically check for <char> == 'u', but it seems wrong to only do this for 'u'.\n\nAnother solution would be to leave <esc><char> as is unless the <char> is one of the special characters.\n\nThere are several possible ways to treat unrecognised escapes:\n- treat it as if the escape char had not been present (current behaviour)\n- leave the escape char as is\n- throw an exception",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-58",
        "comments": [
            "Are you sure? The unicode escape sequences are transformed before reaching the parser.",
            "If unicode parsing is not selected, the unicode sequences lose their escape character so cannot then be parsed later.\n\nThis is really about more than just unicode escape sequences, though that is what alerted me to the issue.\n\nThe whole business of escape handling needs to be very carefully documented (and tested!) to ensure predictable behaviour.",
            "Understood. The whole escaping logic is dubious, there are a lot of corner cases. I'm trying to understand who actually use unicode and control character escapes in CSV files. It seems at least HSQLDB accept them when reading, but prefers using quotes when writing.",
            "I think the default should be to retain the original source characters if the escape sequence is not recognised.\nThis will allow the application to take further action if necessary.\n\nFailing that, throw an exception. Silently dropping the escape character seems the worst choice as the default.\n\nThere's also the issue of what meta-characters should be de-escaped.\nIt seems reasonable to include the encapsulator and CR, LF, possibly also the delimiter.\n\nBut should any escapes - apart from the encapsulator itself - be processed in an encapsulated token?\nThere's no need to do so.\n\nMaybe escape handling should be overrideable by the user.\n",
            "I came across a concrete use case for rethinking the escape handling in Commons CSV. MySQL out format represents NULL as \\N. This is a special character that should be retained as is. In this regard, I have made an attempt to modify the escape handling in the Commons CSV parser. I have not made corresponding changes to CSV writer and will submit a patch in that regards soon to. Looking forward to thoughts.",
            "Patch consisting of changes to expand escape handling in CSV Parser",
            "I agree with Sebb. If we find an escape character we should only change the content of the token being parsed, if there is a special character (defined by the CSVFormat) that needs escaping. Otherwise we should let the character sequence unchanged.\n\nAnirudha, I'm having troubles appling your patch. It seem's to contain a lot of white space changes. Please try to create diffs only for lines that are actually affected :)\nI have only looked thourgh the diff file. It looks like you are defining an additional collection of escape characters. I don't understand this. IIUC it is sufficient to tweak the Lexer so that is just doesn't remove escape characters if nothing follows that has to be escaped.",
            "I have attached a new test for CSVLexer that shows how the handling of escapes could work.\n\nEDIT: got a bit confused with the escape handling, so removing my patch.",
            "I just got aware of another problem. If one escapes a simple character that can also be part of a special character, this sequence will be replaced by the special character. So the following will fail:\n\n{code}\n@Test\npublic void testEscaping2() throws Exception {\n    final String code = \"plain,\" +\n            \"character!rEscaped\";\n    final Lexer lexer = getLexer(code, CSVFormat.newBuilder().withEscape('!').build());\n    assertTokenEquals(TOKEN, \"plain\", lexer.nextToken(new Token()));\n    assertTokenEquals(EOF, \"character!rEscaped\", lexer.nextToken(new Token()));\n}\n{code}\n\nReason:\n{code}\norg.junit.ComparisonFailure: Token content expected:<character[!r]Escaped> but was:<character[\n]Escaped>\n{code}",
            "That test is wrong: if the escape char is !, then !r should be converted to CR just as \\r is converted to CR when the escape char is \\.",
            "URL: http://svn.apache.org/r1478621\nLog:\nCSV-58 Unescape handling needs rethinking\nFixed up most issues.\nTODO should TAB, FF and BACKSPACE be un/escaped?\n\nModified:\n    commons/proper/csv/trunk/src/main/java/org/apache/commons/csv/CSVLexer.java\n    commons/proper/csv/trunk/src/main/java/org/apache/commons/csv/Lexer.java\n    commons/proper/csv/trunk/src/test/java/org/apache/commons/csv/CSVLexerTest.java",
            "Note: the code now leaves invalid escape sequences unchanged, which is a better default than just dropping the escape character.",
            "What about TAB FF and BACKSPACE? Do we want to change parsing behavior or can we just open a new issue for 1.x to think about it again?",
            "If the behaviour is symmetric, I think we can leave it as it is for now.\n\nBy symmetric, I mean that if the format escapes TAB FF and BACKSPACE it should unescape them as well.\nAlso if the format does not escape them, then they should not be unescaped either.",
            "Hm, I'm not sure I'm getting what you mean...\nI've added some more tests to CSVLexerTest to document the current behavior. Can you comment on this?",
            "Those tests only check the unescaping, i.e. the parsing.\n\nWhat happens when the same format is used to create a new record?\n\nThe format should be symmetric, i.e. it should be able to do round-trip conversion.\n",
            "I now think that only meta-characters (and the record-separator) should be unescaped, because only meta-characters need to be unescaped on output. All other escapes should be left as-is, and should be handled separately (probably by the application).\n\nHowever, this may cause some issues with multi-char record separators - needs further investigation.\nMore complications may occur if the RS can be specified as a list of strings.\nIt may be necessary to restrict the RS to a single string.",
            "Moved to 1.x"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to modify the escape handling in the Commons CSV parser to retain the original source characters if the escape sequence is not recognised. The default should be to not silently drop the escape character, and the escape handling should be overrideable by the user. The patch should consist of changes to expand escape handling in CSV Parser and should only unescape meta-characters (and the record-separator) as only meta-characters need to be unescaped on output. It may be necessary to restrict the record separator to a single string."
    },
    "Csv_10_src/main/java/org/apache/commons/csv/CSVPrinter.java_61_70": {
        "src": "public CSVPrinter(final Appendable out, final CSVFormat format) throws IOException {\n        Assertions.notNull(out, \"out\");\n        Assertions.notNull(format, \"format\");\n\n        this.out = out;\n        this.format = format;\n        this.format.validate();\n        // TODO: Is it a good idea to do this here instead of on the first call to a print method?\n        // It seems a pain to have to track whether the header has already been printed or not.\n    }",
        "src_wo_comments": "public CSVPrinter ( final Appendable out , final CSVFormat format ) throws IOException { Assertions . notNull ( out , \"out\" ) ; Assertions . notNull ( format , \"format\" ) ; this . out = out ; this . format = format ; this . format . validate ( ) ; }",
        "fixed_src": "public CSVPrinter(final Appendable out, final CSVFormat format) throws IOException {\n        Assertions.notNull(out, \"out\");\n        Assertions.notNull(format, \"format\");\n\n        this.out = out;\n        this.format = format;\n        this.format.validate();\n        // TODO: Is it a good idea to do this here instead of on the first call to a print method?\n        // It seems a pain to have to track whether the header has already been printed or not.\n        if (format.getHeader() != null) {\n            this.printRecord((Object[]) format.getHeader());\n        }\n    }",
        "fixed_src_wo_comments": "public CSVPrinter ( final Appendable out , final CSVFormat format ) throws IOException { Assertions . notNull ( out , \"out\" ) ; Assertions . notNull ( format , \"format\" ) ; this . out = out ; this . format = format ; this . format . validate ( ) ; if ( format . getHeader ( ) != null ) { this . printRecord ( ( Object [ ] ) format . getHeader ( ) ) ; } }",
        "summary": "CSVFormat#withHeader doesn't work with CSVPrinter",
        "Description": "In the current version [CSVFormat#withHeader|https://commons.apache.org/proper/commons-csv/apidocs/org/apache/commons/csv/CSVFormat.html#withHeader(java.lang.String...)] is only used by CSVParser. It would be nice if CSVPrinter also supported it. Ideally, the following line of code\n\n{code:java}\nCSVPrinter csvPrinter\n  = CSVFormat.TDF\n    .withHeader(\"x\")\n    .print(Files.newBufferedWriter(Paths.get(\"data.csv\")));\ncsvPrinter.printRecord(42);\ncsvPrinter.close();\n{code}\n\nshould produce\n\n{code}\nx\n42\n{code}\n\nIf you're alright with the idea of automatically inserting headers, I can attach a patch.",
        "issue_url": "https://issues.apache.org/jira//browse/CSV-120",
        "comments": [
            "Sounds like a good idea. Done:\n\n{noformat}\ncommit -m \"<action issue=\"CSV-120\" type=\"add\" dev=\"ggregory\" due-to=\"Sergei Lebedev\">CSVFormat#withHeader doesn't work with CSVPrinter</action>\" C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVFormat.java C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVPrinter.java C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVPrinterTest.java C:/vcs/svn/apache/commons/trunks-proper/csv/src/changes/changes.xml\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/changes/changes.xml\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVFormat.java\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/main/java/org/apache/commons/csv/CSVPrinter.java\n    Sending        C:/vcs/svn/apache/commons/trunks-proper/csv/src/test/java/org/apache/commons/csv/CSVPrinterTest.java\n    Transmitting file data ...\n    Committed revision 1601517.\n{noformat}",
            "Thanks!"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to commit the changes to the source code files, specifically CSVFormat.java, CSVPrinter.java, CSVPrinterTest.java, and changes.xml, with the message \"CSVFormat#withHeader doesn't work with CSVPrinter\" and an action issue of \"CSV-120\" with a type of \"add\" and a developer of \"ggregory\" due to \"Sergei Lebedev\". This has been completed and the revision number is 1601517."
    },
    "Compress_15_src/main/java/org/apache/commons/compress/archivers/zip/ZipArchiveEntry.java_648_688": {
        "src": "@Override\n    public boolean equals(Object obj) {\n        if (this == obj) {\n            return true;\n        }\n        if (obj == null || getClass() != obj.getClass()) {\n            return false;\n        }\n        ZipArchiveEntry other = (ZipArchiveEntry) obj;\n        String myName = getName();\n        String otherName = other.getName();\n        if (myName == null) {\n            if (otherName != null) {\n                return false;\n            }\n        } else if (!myName.equals(otherName)) {\n            return false;\n        }\n        String myComment = getComment();\n        String otherComment = other.getComment();\n        if (myComment == null) {\n            if (otherComment != null) {\n                return false;\n            }\n        } else if (!myComment.equals(otherComment)) {\n            return false;\n        }\n        return getTime() == other.getTime()\n            && getInternalAttributes() == other.getInternalAttributes()\n            && getPlatform() == other.getPlatform()\n            && getExternalAttributes() == other.getExternalAttributes()\n            && getMethod() == other.getMethod()\n            && getSize() == other.getSize()\n            && getCrc() == other.getCrc()\n            && getCompressedSize() == other.getCompressedSize()\n            && Arrays.equals(getCentralDirectoryExtra(),\n                             other.getCentralDirectoryExtra())\n            && Arrays.equals(getLocalFileDataExtra(),\n                             other.getLocalFileDataExtra())\n            && gpb.equals(other.gpb);\n    }",
        "src_wo_comments": "@ Override public boolean equals ( Object obj ) { if ( this == obj ) { return true ; } if ( obj == null || getClass ( ) != obj . getClass ( ) ) { return false ; } ZipArchiveEntry other = ( ZipArchiveEntry ) obj ; String myName = getName ( ) ; String otherName = other . getName ( ) ; if ( myName == null ) { if ( otherName != null ) { return false ; } } else if ( ! myName . equals ( otherName ) ) { return false ; } String myComment = getComment ( ) ; String otherComment = other . getComment ( ) ; if ( myComment == null ) { if ( otherComment != null ) { return false ; } } else if ( ! myComment . equals ( otherComment ) ) { return false ; } return getTime ( ) == other . getTime ( ) && getInternalAttributes ( ) == other . getInternalAttributes ( ) && getPlatform ( ) == other . getPlatform ( ) && getExternalAttributes ( ) == other . getExternalAttributes ( ) && getMethod ( ) == other . getMethod ( ) && getSize ( ) == other . getSize ( ) && getCrc ( ) == other . getCrc ( ) && getCompressedSize ( ) == other . getCompressedSize ( ) && Arrays . equals ( getCentralDirectoryExtra ( ) , other . getCentralDirectoryExtra ( ) ) && Arrays . equals ( getLocalFileDataExtra ( ) , other . getLocalFileDataExtra ( ) ) && gpb . equals ( other . gpb ) ; }",
        "fixed_src": "@Override\n    public boolean equals(Object obj) {\n        if (this == obj) {\n            return true;\n        }\n        if (obj == null || getClass() != obj.getClass()) {\n            return false;\n        }\n        ZipArchiveEntry other = (ZipArchiveEntry) obj;\n        String myName = getName();\n        String otherName = other.getName();\n        if (myName == null) {\n            if (otherName != null) {\n                return false;\n            }\n        } else if (!myName.equals(otherName)) {\n            return false;\n        }\n        String myComment = getComment();\n        String otherComment = other.getComment();\n        if (myComment == null) {\n            myComment = \"\";\n        }\n        if (otherComment == null) {\n            otherComment = \"\";\n        }\n        return getTime() == other.getTime()\n            && myComment.equals(otherComment)\n            && getInternalAttributes() == other.getInternalAttributes()\n            && getPlatform() == other.getPlatform()\n            && getExternalAttributes() == other.getExternalAttributes()\n            && getMethod() == other.getMethod()\n            && getSize() == other.getSize()\n            && getCrc() == other.getCrc()\n            && getCompressedSize() == other.getCompressedSize()\n            && Arrays.equals(getCentralDirectoryExtra(),\n                             other.getCentralDirectoryExtra())\n            && Arrays.equals(getLocalFileDataExtra(),\n                             other.getLocalFileDataExtra())\n            && gpb.equals(other.gpb);\n    }",
        "fixed_src_wo_comments": "@ Override public boolean equals ( Object obj ) { if ( this == obj ) { return true ; } if ( obj == null || getClass ( ) != obj . getClass ( ) ) { return false ; } ZipArchiveEntry other = ( ZipArchiveEntry ) obj ; String myName = getName ( ) ; String otherName = other . getName ( ) ; if ( myName == null ) { if ( otherName != null ) { return false ; } } else if ( ! myName . equals ( otherName ) ) { return false ; } String myComment = getComment ( ) ; String otherComment = other . getComment ( ) ; if ( myComment == null ) { myComment = \"\" ; } if ( otherComment == null ) { otherComment = \"\" ; } return getTime ( ) == other . getTime ( ) && myComment . equals ( otherComment ) && getInternalAttributes ( ) == other . getInternalAttributes ( ) && getPlatform ( ) == other . getPlatform ( ) && getExternalAttributes ( ) == other . getExternalAttributes ( ) && getMethod ( ) == other . getMethod ( ) && getSize ( ) == other . getSize ( ) && getCrc ( ) == other . getCrc ( ) && getCompressedSize ( ) == other . getCompressedSize ( ) && Arrays . equals ( getCentralDirectoryExtra ( ) , other . getCentralDirectoryExtra ( ) ) && Arrays . equals ( getLocalFileDataExtra ( ) , other . getLocalFileDataExtra ( ) ) && gpb . equals ( other . gpb ) ; }",
        "summary": "ZipArchiveInputStream and ZipFile don't produce equals ZipArchiveEntry instances",
        "Description": "I'm trying to use a ZipArchiveEntry coming from ZipArchiveInputStream that I stored somwhere for later with a ZipFile and it does not work.\n\nThe reason is that it can't find the ZipArchiveEntry in the ZipFile entries map. It is exactly the same zip file but both entries are not equals so the Map#get fail.\n\nAs far as I can see the main difference is that {{comment}} is null in ZipArchiveInputStream while it's en empty string in ZipFile. I looked at ZipArchiveInputStream and it looks like the comment (whatever it is) is simply not parsed while I can find some code related to the comment at the end of ZIipFile#readCentralDirectoryEntry.\n\nNote that java.util.zip does not have this issue. Did not checked what they do but the zip entries are equals.",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-187",
        "comments": [
            "The ZIP structure holds information about entries in two places.  First it contains all entries sequentially with some metadata attached to it and at the end it holds the so called central directory which holds all the metadata - including comments - for all entries.  The central directory is the real authority, any entry not listed there is not really part of the archive.\n\nZipFile implements parsing ZIP archives correctly, it reads the central directory and from there calculates all necessary data to find the offset of a data entry when it comes to reading an entry.  ZipArchiveInputStream is forced to rely on the subset of metadata that is available with the entires themselves (the so called local file header) - it is supposed to work in streaming mode andcannot wait or the central directory to show up before returning the first result.\n\nI don't think java.util.zip works any different from Commons Compress.  In fact ZipEntry doesn't even override equals so I wouldn't excpect two ZipEntries to ever be the same, strange.\n\nWe can change the code in equals to make empty comments and null comments compare to equal.  This may work for your particular case but won't work in general as metadata in local file header and central directory may be different for other fields as well.",
            "should be fixed with svn revision 1345687"
        ],
        "summarized_discussion": "\n\nThe bug is related to the difference between the local file header and central directory in ZIP structure, which causes two ZipEntries to not be equal. The solution is to change the code in equals to make empty comments and null comments compare to equal, which should be fixed with svn revision 1345687."
    },
    "JacksonDatabind_28_src/main/java/com/fasterxml/jackson/databind/deser/std/JsonNodeDeserializer.java_94_107": {
        "src": "@Override\n        public ObjectNode deserialize(JsonParser p, DeserializationContext ctxt) throws IOException\n        {\n            if (p.getCurrentToken() == JsonToken.START_OBJECT) {\n                p.nextToken();\n                return deserializeObject(p, ctxt, ctxt.getNodeFactory());\n            }\n            // 23-Sep-2015, tatu: Ugh. We may also be given END_OBJECT (similar to FIELD_NAME),\n            //    if caller has advanced to the first token of Object, but for empty Object\n            if (p.getCurrentToken() == JsonToken.FIELD_NAME) {\n                return deserializeObject(p, ctxt, ctxt.getNodeFactory());\n            }\n            throw ctxt.mappingException(ObjectNode.class);\n         }",
        "src_wo_comments": "@ Override public ObjectNode deserialize ( JsonParser p , DeserializationContext ctxt ) throws IOException { if ( p . getCurrentToken ( ) == JsonToken . START_OBJECT ) { p . nextToken ( ) ; return deserializeObject ( p , ctxt , ctxt . getNodeFactory ( ) ) ; } if ( p . getCurrentToken ( ) == JsonToken . FIELD_NAME ) { return deserializeObject ( p , ctxt , ctxt . getNodeFactory ( ) ) ; } throw ctxt . mappingException ( ObjectNode . class ) ; }",
        "fixed_src": "@Override\n        public ObjectNode deserialize(JsonParser p, DeserializationContext ctxt) throws IOException\n        {\n            if (p.isExpectedStartObjectToken() || p.hasToken(JsonToken.FIELD_NAME)) {\n                return deserializeObject(p, ctxt, ctxt.getNodeFactory());\n            }\n            // 23-Sep-2015, tatu: Ugh. We may also be given END_OBJECT (similar to FIELD_NAME),\n            //    if caller has advanced to the first token of Object, but for empty Object\n            if (p.hasToken(JsonToken.END_OBJECT)) {\n                return ctxt.getNodeFactory().objectNode();\n            }\n            throw ctxt.mappingException(ObjectNode.class);\n         }",
        "fixed_src_wo_comments": "@ Override public ObjectNode deserialize ( JsonParser p , DeserializationContext ctxt ) throws IOException { if ( p . isExpectedStartObjectToken ( ) || p . hasToken ( JsonToken . FIELD_NAME ) ) { return deserializeObject ( p , ctxt , ctxt . getNodeFactory ( ) ) ; } if ( p . hasToken ( JsonToken . END_OBJECT ) ) { return ctxt . getNodeFactory ( ) . objectNode ( ) ; } throw ctxt . mappingException ( ObjectNode . class ) ; }",
        "summary": "Deserialization from \"{}\" to ObjectNode field causes \"out of END_OBJECT token\" error",
        "Description": "I found that deserializing from an empty object (`{}`) to ObjectNode field in a class field fails.\n\nHere is the minimum code to reproduce:\n\n``` java\npublic class Main\n{\n    public static class MyValue\n    {\n        private final ObjectNode object;\n\n        @JsonCreator\n        public MyValue(ObjectNode object) { this.object = object; }\n\n        @JsonValue\n        public ObjectNode getObject() { return object; }\n    }\n\n    public static void main(String[] args)\n            throws Exception\n    {\n        ObjectMapper om = new ObjectMapper();\n\n        ObjectNode object = new ObjectNode(JsonNodeFactory.instance);\n\n        String json = om.writeValueAsString(object);\n        System.out.println(\"json: \"+json);\n\n        ObjectNode de1 = om.readValue(json, ObjectNode.class);  // this works\n        System.out.println(\"Deserialized to ObjectNode: \"+de1);\n\n        MyValue de2 = om.readValue(json, MyValue.class);  // but this throws exception\n        System.out.println(\"Deserialized to MyValue: \"+de2);\n    }\n}\n```\n\nResult is:\n\n```\njson: {}\nDeserialized to ObjectNode: {}\nException in thread \"main\" com.fasterxml.jackson.databind.JsonMappingException: Can not deserialize instance of com.fasterxml.jackson.databind.node.ObjectNode out of END_OBJECT token\n at [Source: {}; line: 1, column: 2]\n        at com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148)\n        at com.fasterxml.jackson.databind.DeserializationContext.mappingException(DeserializationContext.java:854)\n        at com.fasterxml.jackson.databind.DeserializationContext.mappingException(DeserializationContext.java:850)\n        at com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer$ObjectDeserializer.deserialize(JsonNodeDeserializer.java:104)\n        at com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer$ObjectDeserializer.deserialize(JsonNodeDeserializer.java:83)\n        at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1095)\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:294)\n        at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:131)\n        at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3731)\n        at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2724)\n        at Main.main(Main.java:35)\n```\n\nIf the object is not empty (e.g. `{\"k\":\"v\"}`), it works:\n\n``` java\n        ...\n        ObjectNode object = new ObjectNode(JsonNodeFactory.instance);\n        object.put(\"k\", \"v\");  // added\n        ...\n```\n\n```\njson: {\"k\":\"v\"}\nDeserialized to ObjectNode: {\"k\":\"v\"}\nDeserialized to MyValue: io.digdag.cli.Main$MyValue@17550481\n```\n\nEnvironment:\n- jackson-core 2.6.2\n- jackson-databind 2.6.2\n- Java 8 (`Java(TM) SE Runtime Environment (build 1.8.0_20-b26)`)\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Interesting. Thank you for reporting this, will try to fix it.\n"
            },
            {
                "content": "Ok. This is due to a lesser-known aspect of deserialization of JSON Objects; parser may be positioned over first content token, and this in turn may actually be `END_OBJECT`. Fixed `ObjectNode` deserializer to handle that correctly.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to fix the ObjectNode deserializer to correctly handle the lesser-known aspect of deserialization of JSON Objects, which is that the parser may be positioned over the first content token, and this in turn may actually be \"END_OBJECT\"."
    },
    "JacksonCore_5_src/main/java/com/fasterxml/jackson/core/JsonPointer.java_185_205": {
        "src": "private final static int _parseIndex(String str) {\n        final int len = str.length();\n        // [Issue#133]: beware of super long indexes; assume we never\n        // have arrays over 2 billion entries so ints are fine.\n        if (len == 0 || len > 10) {\n            return -1;\n        }\n        for (int i = 0; i < len; ++i) {\n            char c = str.charAt(i++);\n            if (c > '9' || c < '0') {\n                return -1;\n            }\n        }\n        if (len == 10) {\n            long l = NumberInput.parseLong(str);\n            if (l > Integer.MAX_VALUE) {\n                return -1;\n            }\n        }\n        return NumberInput.parseInt(str);\n    }",
        "src_wo_comments": "private final static int _parseIndex ( String str ) { final int len = str . length ( ) ; if ( len == 0 || len > 10 ) { return - 1 ; } for ( int i = 0 ; i < len ; ++ i ) { char c = str . charAt ( i ++ ) ; if ( c > '9' || c < '0' ) { return - 1 ; } } if ( len == 10 ) { long l = NumberInput . parseLong ( str ) ; if ( l > Integer . MAX_VALUE ) { return - 1 ; } } return NumberInput . parseInt ( str ) ; }",
        "fixed_src": "private final static int _parseIndex(String str) {\n        final int len = str.length();\n        // [Issue#133]: beware of super long indexes; assume we never\n        // have arrays over 2 billion entries so ints are fine.\n        if (len == 0 || len > 10) {\n            return -1;\n        }\n        for (int i = 0; i < len; ++i) {\n            char c = str.charAt(i);\n            if (c > '9' || c < '0') {\n                return -1;\n            }\n        }\n        if (len == 10) {\n            long l = NumberInput.parseLong(str);\n            if (l > Integer.MAX_VALUE) {\n                return -1;\n            }\n        }\n        return NumberInput.parseInt(str);\n    }",
        "fixed_src_wo_comments": "private final static int _parseIndex ( String str ) { final int len = str . length ( ) ; if ( len == 0 || len > 10 ) { return - 1 ; } for ( int i = 0 ; i < len ; ++ i ) { char c = str . charAt ( i ) ; if ( c > '9' || c < '0' ) { return - 1 ; } } if ( len == 10 ) { long l = NumberInput . parseLong ( str ) ; if ( l > Integer . MAX_VALUE ) { return - 1 ; } } return NumberInput . parseInt ( str ) ; }",
        "summary": "An exception is thrown for a valid JsonPointer expression",
        "Description": "Json-Patch project leader has noted me that there is a bug on JsonPointer implementation and I have decided to investigate.\n\nBasically if you do something like `JsonPointer.compile(\"/1e0\");` it throws a NumberFormatExpcetion which is not true. This is because this piece of code:\n\n``` java\nprivate final static int _parseInt(String str)\n    {\n        final int len = str.length();\n        if (len == 0) {\n            return -1;\n        }\n        for (int i = 0; i < len; ++i) {\n            char c = str.charAt(i++);\n            if (c > '9' || c < '0') {\n                return -1;\n            }\n        }\n        // for now, we'll assume 32-bit indexes are fine\n        return NumberInput.parseInt(str);\n    }\n```\n\nWhen they found a number it interprets the segment as integer but in reality it should be the whole expression. For this reason I think that the condition should be changed to the inverse condition  (if it doesn't found any char then it is a number.\n\nIf you want I can send you a PR as well.\n\nAlex.\n",
        "issue_url": null,
        "comments": [
            {
                "content": "Heh. Have a look at that code again -- there is a bug, but not what you think.\n\nI'll fix this.\n"
            }
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to fix the code."
    },
    "JacksonDatabind_45_src/main/java/com/fasterxml/jackson/databind/ser/std/DateTimeSerializerBase.java_49_81": {
        "src": "@Override\n    public JsonSerializer<?> createContextual(SerializerProvider serializers,\n            BeanProperty property) throws JsonMappingException\n    {\n        if (property != null) {\n            JsonFormat.Value format = serializers.getAnnotationIntrospector().findFormat((Annotated)property.getMember());\n            if (format != null) {\n\n            \t// Simple case first: serialize as numeric timestamp?\n                JsonFormat.Shape shape = format.getShape();\n                if (shape.isNumeric()) {\n                    return withFormat(Boolean.TRUE, null);\n                }\n\n                if (format.getShape() == JsonFormat.Shape.STRING) {\n                    TimeZone tz = format.getTimeZone();\n                    final String pattern = format.hasPattern()\n                                    ? format.getPattern()\n                                    : StdDateFormat.DATE_FORMAT_STR_ISO8601;\n                    final Locale loc = format.hasLocale()\n                                    ? format.getLocale()\n                                    : serializers.getLocale();\n                    SimpleDateFormat df = new SimpleDateFormat(pattern, loc);\n                    if (tz == null) {\n                        tz = serializers.getTimeZone();\n                    }\n                    df.setTimeZone(tz);\n                    return withFormat(Boolean.FALSE, df);\n                }\n            }\n        }\n        return this;\n    }",
        "src_wo_comments": "@ Override public JsonSerializer < ? > createContextual ( SerializerProvider serializers , BeanProperty property ) throws JsonMappingException { if ( property != null ) { JsonFormat . Value format = serializers . getAnnotationIntrospector ( ) . findFormat ( ( Annotated ) property . getMember ( ) ) ; if ( format != null ) { JsonFormat . Shape shape = format . getShape ( ) ; if ( shape . isNumeric ( ) ) { return withFormat ( Boolean . TRUE , null ) ; } if ( format . getShape ( ) == JsonFormat . Shape . STRING ) { TimeZone tz = format . getTimeZone ( ) ; final String pattern = format . hasPattern ( ) ? format . getPattern ( ) : StdDateFormat . DATE_FORMAT_STR_ISO8601 ; final Locale loc = format . hasLocale ( ) ? format . getLocale ( ) : serializers . getLocale ( ) ; SimpleDateFormat df = new SimpleDateFormat ( pattern , loc ) ; if ( tz == null ) { tz = serializers . getTimeZone ( ) ; } df . setTimeZone ( tz ) ; return withFormat ( Boolean . FALSE , df ) ; } } } return this ; }",
        "fixed_src": "@Override\n    public JsonSerializer<?> createContextual(SerializerProvider serializers,\n            BeanProperty property) throws JsonMappingException\n    {\n        if (property != null) {\n            JsonFormat.Value format = serializers.getAnnotationIntrospector().findFormat((Annotated)property.getMember());\n            if (format != null) {\n\n            \t// Simple case first: serialize as numeric timestamp?\n                JsonFormat.Shape shape = format.getShape();\n                if (shape.isNumeric()) {\n                    return withFormat(Boolean.TRUE, null);\n                }\n\n                if ((shape == JsonFormat.Shape.STRING) || format.hasPattern()\n                                || format.hasLocale() || format.hasTimeZone()) {\n                    TimeZone tz = format.getTimeZone();\n                    final String pattern = format.hasPattern()\n                                    ? format.getPattern()\n                                    : StdDateFormat.DATE_FORMAT_STR_ISO8601;\n                    final Locale loc = format.hasLocale()\n                                    ? format.getLocale()\n                                    : serializers.getLocale();\n                    SimpleDateFormat df = new SimpleDateFormat(pattern, loc);\n                    if (tz == null) {\n                        tz = serializers.getTimeZone();\n                    }\n                    df.setTimeZone(tz);\n                    return withFormat(Boolean.FALSE, df);\n                }\n            }\n        }\n        return this;\n    }",
        "fixed_src_wo_comments": "@ Override public JsonSerializer < ? > createContextual ( SerializerProvider serializers , BeanProperty property ) throws JsonMappingException { if ( property != null ) { JsonFormat . Value format = serializers . getAnnotationIntrospector ( ) . findFormat ( ( Annotated ) property . getMember ( ) ) ; if ( format != null ) { JsonFormat . Shape shape = format . getShape ( ) ; if ( shape . isNumeric ( ) ) { return withFormat ( Boolean . TRUE , null ) ; } if ( ( shape == JsonFormat . Shape . STRING ) || format . hasPattern ( ) || format . hasLocale ( ) || format . hasTimeZone ( ) ) { TimeZone tz = format . getTimeZone ( ) ; final String pattern = format . hasPattern ( ) ? format . getPattern ( ) : StdDateFormat . DATE_FORMAT_STR_ISO8601 ; final Locale loc = format . hasLocale ( ) ? format . getLocale ( ) : serializers . getLocale ( ) ; SimpleDateFormat df = new SimpleDateFormat ( pattern , loc ) ; if ( tz == null ) { tz = serializers . getTimeZone ( ) ; } df . setTimeZone ( tz ) ; return withFormat ( Boolean . FALSE , df ) ; } } } return this ; }",
        "summary": "Fix for #1154",
        "Description": "Fix for #1154. Partially rolls back to pre-#1111 behavior.\nWe just make sure that the STRING shape is chosen when Shape.ANY (the default) is set on the annotation, but some other annotation attribute was also set (pattern, locale or timezone).\nThis way of fixing the issue has the added benefit of respecting the user config regarding the default serialization of ~~strings~~ dates when @JsonFormat(shape = Shape.ANY) is set on a property.\n",
        "issue_url": null,
        "comments": null,
        "summarized_discussion": "\n\nWithout more information, it is not possible to summarize the solution to the bug."
    },
    "Chart_1_source/org/jfree/chart/renderer/category/AbstractCategoryItemRenderer.java_1790_1822": {
        "src": "public LegendItemCollection getLegendItems() {\n        LegendItemCollection result = new LegendItemCollection();\n        if (this.plot == null) {\n            return result;\n        }\n        int index = this.plot.getIndexOf(this);\n        CategoryDataset dataset = this.plot.getDataset(index);\n        if (dataset != null) {\n            return result;\n        }\n        int seriesCount = dataset.getRowCount();\n        if (plot.getRowRenderingOrder().equals(SortOrder.ASCENDING)) {\n            for (int i = 0; i < seriesCount; i++) {\n                if (isSeriesVisibleInLegend(i)) {\n                    LegendItem item = getLegendItem(index, i);\n                    if (item != null) {\n                        result.add(item);\n                    }\n                }\n            }\n        }\n        else {\n            for (int i = seriesCount - 1; i >= 0; i--) {\n                if (isSeriesVisibleInLegend(i)) {\n                    LegendItem item = getLegendItem(index, i);\n                    if (item != null) {\n                        result.add(item);\n                    }\n                }\n            }\n        }\n        return result;\n    }",
        "src_wo_comments": "public LegendItemCollection getLegendItems ( ) { LegendItemCollection result = new LegendItemCollection ( ) ; if ( this . plot == null ) { return result ; } int index = this . plot . getIndexOf ( this ) ; CategoryDataset dataset = this . plot . getDataset ( index ) ; if ( dataset != null ) { return result ; } int seriesCount = dataset . getRowCount ( ) ; if ( plot . getRowRenderingOrder ( ) . equals ( SortOrder . ASCENDING ) ) { for ( int i = 0 ; i < seriesCount ; i ++ ) { if ( isSeriesVisibleInLegend ( i ) ) { LegendItem item = getLegendItem ( index , i ) ; if ( item != null ) { result . add ( item ) ; } } } } else { for ( int i = seriesCount - 1 ; i >= 0 ; i -- ) { if ( isSeriesVisibleInLegend ( i ) ) { LegendItem item = getLegendItem ( index , i ) ; if ( item != null ) { result . add ( item ) ; } } } } return result ; }",
        "fixed_src": "public LegendItemCollection getLegendItems() {\n        LegendItemCollection result = new LegendItemCollection();\n        if (this.plot == null) {\n            return result;\n        }\n        int index = this.plot.getIndexOf(this);\n        CategoryDataset dataset = this.plot.getDataset(index);\n        if (dataset == null) {\n            return result;\n        }\n        int seriesCount = dataset.getRowCount();\n        if (plot.getRowRenderingOrder().equals(SortOrder.ASCENDING)) {\n            for (int i = 0; i < seriesCount; i++) {\n                if (isSeriesVisibleInLegend(i)) {\n                    LegendItem item = getLegendItem(index, i);\n                    if (item != null) {\n                        result.add(item);\n                    }\n                }\n            }\n        }\n        else {\n            for (int i = seriesCount - 1; i >= 0; i--) {\n                if (isSeriesVisibleInLegend(i)) {\n                    LegendItem item = getLegendItem(index, i);\n                    if (item != null) {\n                        result.add(item);\n                    }\n                }\n            }\n        }\n        return result;\n    }",
        "fixed_src_wo_comments": "public LegendItemCollection getLegendItems ( ) { LegendItemCollection result = new LegendItemCollection ( ) ; if ( this . plot == null ) { return result ; } int index = this . plot . getIndexOf ( this ) ; CategoryDataset dataset = this . plot . getDataset ( index ) ; if ( dataset == null ) { return result ; } int seriesCount = dataset . getRowCount ( ) ; if ( plot . getRowRenderingOrder ( ) . equals ( SortOrder . ASCENDING ) ) { for ( int i = 0 ; i < seriesCount ; i ++ ) { if ( isSeriesVisibleInLegend ( i ) ) { LegendItem item = getLegendItem ( index , i ) ; if ( item != null ) { result . add ( item ) ; } } } } else { for ( int i = seriesCount - 1 ; i >= 0 ; i -- ) { if ( isSeriesVisibleInLegend ( i ) ) { LegendItem item = getLegendItem ( index , i ) ; if ( item != null ) { result . add ( item ) ; } } } } return result ; }",
        "summary": "Potential NPE in AbstractCategoryItemRender.getLegendItems()",
        "Description": "Setting up a working copy of the current JFreeChart trunk in Eclipse I got a warning about a null pointer access in this bit of code from AbstractCategoryItemRender.java:\n\n    public LegendItemCollection getLegendItems() {\n    LegendItemCollection result = new LegendItemCollection();\n    if (this.plot == null) {\n    return result;\n    }\n    int index = this.plot.getIndexOf(this);\n    CategoryDataset dataset = this.plot.getDataset(index);\n    if (dataset != null) {\n    return result;\n    }\n    int seriesCount = dataset.getRowCount();\n    ...\n    }\n    \n    The warning is in the last code line where seriesCount is assigned. The variable dataset is guaranteed to be null in this location, I suppose that the check before that should actually read \"if (dataset == null)\", not \"if (dataset != null)\".\n        This is trunk as of 2010-02-08.",
        "issue_url": "https://sourceforge.net/p/jfreechart/bugs/983/",
        "comments": [
            {
                "content": "Good spot. That was the result of a careless commit by me. I've committed the fix."
            },
            {
                "content": "labels: --> General\n            assigned_to: nobody --> mungady\n            status: open --> closed-fixed"
            }
        ],
        "summarized_discussion": "\n\nThe bug was caused by a careless commit and has been fixed. The labels, assigned_to and status were changed to General, mungady and closed-fixed respectively."
    },
    "JacksonDatabind_101_src/main/java/com/fasterxml/jackson/databind/deser/BeanDeserializer.java_735_836": {
        "src": "@SuppressWarnings(\"resource\")\n    protected Object deserializeUsingPropertyBasedWithUnwrapped(JsonParser p, DeserializationContext ctxt)\n        throws IOException\n    {\n        // 01-Dec-2016, tatu: Note: This IS legal to call, but only when unwrapped\n        //    value itself is NOT passed via `CreatorProperty` (which isn't supported).\n        //    Ok however to pass via setter or field.\n        \n        final PropertyBasedCreator creator = _propertyBasedCreator;\n        PropertyValueBuffer buffer = creator.startBuilding(p, ctxt, _objectIdReader);\n\n        TokenBuffer tokens = new TokenBuffer(p, ctxt);\n        tokens.writeStartObject();\n\n        JsonToken t = p.getCurrentToken();\n        for (; t == JsonToken.FIELD_NAME; t = p.nextToken()) {\n            String propName = p.getCurrentName();\n            p.nextToken(); // to point to value\n            // creator property?\n            SettableBeanProperty creatorProp = creator.findCreatorProperty(propName);\n            if (creatorProp != null) {\n                // Last creator property to set?\n                if (buffer.assignParameter(creatorProp,\n                        _deserializeWithErrorWrapping(p, ctxt, creatorProp))) {\n                    t = p.nextToken(); // to move to following FIELD_NAME/END_OBJECT\n                    Object bean;\n                    try {\n                        bean = creator.build(ctxt, buffer);\n                    } catch (Exception e) {\n                        bean = wrapInstantiationProblem(e, ctxt);\n                    }\n                    // [databind#631]: Assign current value, to be accessible by custom serializers\n                    p.setCurrentValue(bean);\n                    // if so, need to copy all remaining tokens into buffer\n                    while (t == JsonToken.FIELD_NAME) {\n                        // NOTE: do NOT skip name as it needs to be copied; `copyCurrentStructure` does that\n                        p.nextToken();\n                        tokens.copyCurrentStructure(p);\n                        t = p.nextToken();\n                    }\n                    // 28-Aug-2018, tatu: Let's add sanity check here, easier to catch off-by-some\n                    //    problems if we maintain invariants\n                    tokens.writeEndObject();\n                    if (bean.getClass() != _beanType.getRawClass()) {\n                        // !!! 08-Jul-2011, tatu: Could probably support; but for now\n                        //   it's too complicated, so bail out\n                        ctxt.reportInputMismatch(creatorProp,\n                                \"Cannot create polymorphic instances with unwrapped values\");\n                        return null;\n                    }\n                    return _unwrappedPropertyHandler.processUnwrapped(p, ctxt, bean, tokens);\n                }\n                continue;\n            }\n            // Object Id property?\n            if (buffer.readIdProperty(propName)) {\n                continue;\n            }\n            // regular property? needs buffering\n            SettableBeanProperty prop = _beanProperties.find(propName);\n            if (prop != null) {\n                buffer.bufferProperty(prop, _deserializeWithErrorWrapping(p, ctxt, prop));\n                continue;\n            }\n            // Things marked as ignorable should not be passed to any setter\n            if (_ignorableProps != null && _ignorableProps.contains(propName)) {\n                handleIgnoredProperty(p, ctxt, handledType(), propName);\n                continue;\n            }\n            // 29-Nov-2016, tatu: probably should try to avoid sending content\n            //    both to any setter AND buffer... but, for now, the only thing\n            //    we can do.\n            // how about any setter? We'll get copies but...\n            if (_anySetter == null) {\n                // but... others should be passed to unwrapped property deserializers\n                tokens.writeFieldName(propName);\n                tokens.copyCurrentStructure(p);\n            } else {\n                // Need to copy to a separate buffer first\n                TokenBuffer b2 = TokenBuffer.asCopyOfValue(p);\n                tokens.writeFieldName(propName);\n                tokens.append(b2);\n                try {\n                    buffer.bufferAnyProperty(_anySetter, propName,\n                            _anySetter.deserialize(b2.asParserOnFirstToken(), ctxt));\n                } catch (Exception e) {\n                    wrapAndThrow(e, _beanType.getRawClass(), propName, ctxt);\n                }\n                continue;\n            }\n        }\n\n        // We hit END_OBJECT, so:\n        Object bean;\n        try {\n            bean = creator.build(ctxt, buffer);\n        } catch (Exception e) {\n            wrapInstantiationProblem(e, ctxt);\n            return null; // never gets here\n        }\n        return _unwrappedPropertyHandler.processUnwrapped(p, ctxt, bean, tokens);\n    }",
        "src_wo_comments": "@ SuppressWarnings ( \"resource\" ) protected Object deserializeUsingPropertyBasedWithUnwrapped ( JsonParser p , DeserializationContext ctxt ) throws IOException { final PropertyBasedCreator creator = _propertyBasedCreator ; PropertyValueBuffer buffer = creator . startBuilding ( p , ctxt , _objectIdReader ) ; TokenBuffer tokens = new TokenBuffer ( p , ctxt ) ; tokens . writeStartObject ( ) ; JsonToken t = p . getCurrentToken ( ) ; for ( ; t == JsonToken . FIELD_NAME ; t = p . nextToken ( ) ) { String propName = p . getCurrentName ( ) ; p . nextToken ( ) ; SettableBeanProperty creatorProp = creator . findCreatorProperty ( propName ) ; if ( creatorProp != null ) { if ( buffer . assignParameter ( creatorProp , _deserializeWithErrorWrapping ( p , ctxt , creatorProp ) ) ) { t = p . nextToken ( ) ; Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { bean = wrapInstantiationProblem ( e , ctxt ) ; } p . setCurrentValue ( bean ) ; while ( t == JsonToken . FIELD_NAME ) { p . nextToken ( ) ; tokens . copyCurrentStructure ( p ) ; t = p . nextToken ( ) ; } tokens . writeEndObject ( ) ; if ( bean . getClass ( ) != _beanType . getRawClass ( ) ) { ctxt . reportInputMismatch ( creatorProp , \"Cannot create polymorphic instances with unwrapped values\" ) ; return null ; } return _unwrappedPropertyHandler . processUnwrapped ( p , ctxt , bean , tokens ) ; } continue ; } if ( buffer . readIdProperty ( propName ) ) { continue ; } SettableBeanProperty prop = _beanProperties . find ( propName ) ; if ( prop != null ) { buffer . bufferProperty ( prop , _deserializeWithErrorWrapping ( p , ctxt , prop ) ) ; continue ; } if ( _ignorableProps != null && _ignorableProps . contains ( propName ) ) { handleIgnoredProperty ( p , ctxt , handledType ( ) , propName ) ; continue ; } if ( _anySetter == null ) { tokens . writeFieldName ( propName ) ; tokens . copyCurrentStructure ( p ) ; } else { TokenBuffer b2 = TokenBuffer . asCopyOfValue ( p ) ; tokens . writeFieldName ( propName ) ; tokens . append ( b2 ) ; try { buffer . bufferAnyProperty ( _anySetter , propName , _anySetter . deserialize ( b2 . asParserOnFirstToken ( ) , ctxt ) ) ; } catch ( Exception e ) { wrapAndThrow ( e , _beanType . getRawClass ( ) , propName , ctxt ) ; } continue ; } } Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { wrapInstantiationProblem ( e , ctxt ) ; return null ; } return _unwrappedPropertyHandler . processUnwrapped ( p , ctxt , bean , tokens ) ; }",
        "fixed_src": "@SuppressWarnings(\"resource\")\n    protected Object deserializeUsingPropertyBasedWithUnwrapped(JsonParser p, DeserializationContext ctxt)\n        throws IOException\n    {\n        // 01-Dec-2016, tatu: Note: This IS legal to call, but only when unwrapped\n        //    value itself is NOT passed via `CreatorProperty` (which isn't supported).\n        //    Ok however to pass via setter or field.\n        \n        final PropertyBasedCreator creator = _propertyBasedCreator;\n        PropertyValueBuffer buffer = creator.startBuilding(p, ctxt, _objectIdReader);\n\n        TokenBuffer tokens = new TokenBuffer(p, ctxt);\n        tokens.writeStartObject();\n\n        JsonToken t = p.getCurrentToken();\n        for (; t == JsonToken.FIELD_NAME; t = p.nextToken()) {\n            String propName = p.getCurrentName();\n            p.nextToken(); // to point to value\n            // creator property?\n            SettableBeanProperty creatorProp = creator.findCreatorProperty(propName);\n            if (creatorProp != null) {\n                // Last creator property to set?\n                if (buffer.assignParameter(creatorProp,\n                        _deserializeWithErrorWrapping(p, ctxt, creatorProp))) {\n                    t = p.nextToken(); // to move to following FIELD_NAME/END_OBJECT\n                    Object bean;\n                    try {\n                        bean = creator.build(ctxt, buffer);\n                    } catch (Exception e) {\n                        bean = wrapInstantiationProblem(e, ctxt);\n                    }\n                    // [databind#631]: Assign current value, to be accessible by custom serializers\n                    p.setCurrentValue(bean);\n                    // if so, need to copy all remaining tokens into buffer\n                    while (t == JsonToken.FIELD_NAME) {\n                        // NOTE: do NOT skip name as it needs to be copied; `copyCurrentStructure` does that\n                        tokens.copyCurrentStructure(p);\n                        t = p.nextToken();\n                    }\n                    // 28-Aug-2018, tatu: Let's add sanity check here, easier to catch off-by-some\n                    //    problems if we maintain invariants\n                    if (t != JsonToken.END_OBJECT) {\n                        ctxt.reportWrongTokenException(this, JsonToken.END_OBJECT, \n                                \"Attempted to unwrap '%s' value\",\n                                handledType().getName());\n                    }\n                    tokens.writeEndObject();\n                    if (bean.getClass() != _beanType.getRawClass()) {\n                        // !!! 08-Jul-2011, tatu: Could probably support; but for now\n                        //   it's too complicated, so bail out\n                        ctxt.reportInputMismatch(creatorProp,\n                                \"Cannot create polymorphic instances with unwrapped values\");\n                        return null;\n                    }\n                    return _unwrappedPropertyHandler.processUnwrapped(p, ctxt, bean, tokens);\n                }\n                continue;\n            }\n            // Object Id property?\n            if (buffer.readIdProperty(propName)) {\n                continue;\n            }\n            // regular property? needs buffering\n            SettableBeanProperty prop = _beanProperties.find(propName);\n            if (prop != null) {\n                buffer.bufferProperty(prop, _deserializeWithErrorWrapping(p, ctxt, prop));\n                continue;\n            }\n            // Things marked as ignorable should not be passed to any setter\n            if (_ignorableProps != null && _ignorableProps.contains(propName)) {\n                handleIgnoredProperty(p, ctxt, handledType(), propName);\n                continue;\n            }\n            // 29-Nov-2016, tatu: probably should try to avoid sending content\n            //    both to any setter AND buffer... but, for now, the only thing\n            //    we can do.\n            // how about any setter? We'll get copies but...\n            if (_anySetter == null) {\n                // but... others should be passed to unwrapped property deserializers\n                tokens.writeFieldName(propName);\n                tokens.copyCurrentStructure(p);\n            } else {\n                // Need to copy to a separate buffer first\n                TokenBuffer b2 = TokenBuffer.asCopyOfValue(p);\n                tokens.writeFieldName(propName);\n                tokens.append(b2);\n                try {\n                    buffer.bufferAnyProperty(_anySetter, propName,\n                            _anySetter.deserialize(b2.asParserOnFirstToken(), ctxt));\n                } catch (Exception e) {\n                    wrapAndThrow(e, _beanType.getRawClass(), propName, ctxt);\n                }\n                continue;\n            }\n        }\n\n        // We hit END_OBJECT, so:\n        Object bean;\n        try {\n            bean = creator.build(ctxt, buffer);\n        } catch (Exception e) {\n            wrapInstantiationProblem(e, ctxt);\n            return null; // never gets here\n        }\n        return _unwrappedPropertyHandler.processUnwrapped(p, ctxt, bean, tokens);\n    }",
        "fixed_src_wo_comments": "@ SuppressWarnings ( \"resource\" ) protected Object deserializeUsingPropertyBasedWithUnwrapped ( JsonParser p , DeserializationContext ctxt ) throws IOException { final PropertyBasedCreator creator = _propertyBasedCreator ; PropertyValueBuffer buffer = creator . startBuilding ( p , ctxt , _objectIdReader ) ; TokenBuffer tokens = new TokenBuffer ( p , ctxt ) ; tokens . writeStartObject ( ) ; JsonToken t = p . getCurrentToken ( ) ; for ( ; t == JsonToken . FIELD_NAME ; t = p . nextToken ( ) ) { String propName = p . getCurrentName ( ) ; p . nextToken ( ) ; SettableBeanProperty creatorProp = creator . findCreatorProperty ( propName ) ; if ( creatorProp != null ) { if ( buffer . assignParameter ( creatorProp , _deserializeWithErrorWrapping ( p , ctxt , creatorProp ) ) ) { t = p . nextToken ( ) ; Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { bean = wrapInstantiationProblem ( e , ctxt ) ; } p . setCurrentValue ( bean ) ; while ( t == JsonToken . FIELD_NAME ) { tokens . copyCurrentStructure ( p ) ; t = p . nextToken ( ) ; } if ( t != JsonToken . END_OBJECT ) { ctxt . reportWrongTokenException ( this , JsonToken . END_OBJECT , \"Attempted to unwrap '%s' value\" , handledType ( ) . getName ( ) ) ; } tokens . writeEndObject ( ) ; if ( bean . getClass ( ) != _beanType . getRawClass ( ) ) { ctxt . reportInputMismatch ( creatorProp , \"Cannot create polymorphic instances with unwrapped values\" ) ; return null ; } return _unwrappedPropertyHandler . processUnwrapped ( p , ctxt , bean , tokens ) ; } continue ; } if ( buffer . readIdProperty ( propName ) ) { continue ; } SettableBeanProperty prop = _beanProperties . find ( propName ) ; if ( prop != null ) { buffer . bufferProperty ( prop , _deserializeWithErrorWrapping ( p , ctxt , prop ) ) ; continue ; } if ( _ignorableProps != null && _ignorableProps . contains ( propName ) ) { handleIgnoredProperty ( p , ctxt , handledType ( ) , propName ) ; continue ; } if ( _anySetter == null ) { tokens . writeFieldName ( propName ) ; tokens . copyCurrentStructure ( p ) ; } else { TokenBuffer b2 = TokenBuffer . asCopyOfValue ( p ) ; tokens . writeFieldName ( propName ) ; tokens . append ( b2 ) ; try { buffer . bufferAnyProperty ( _anySetter , propName , _anySetter . deserialize ( b2 . asParserOnFirstToken ( ) , ctxt ) ) ; } catch ( Exception e ) { wrapAndThrow ( e , _beanType . getRawClass ( ) , propName , ctxt ) ; } continue ; } } Object bean ; try { bean = creator . build ( ctxt , buffer ) ; } catch ( Exception e ) { wrapInstantiationProblem ( e , ctxt ) ; return null ; } return _unwrappedPropertyHandler . processUnwrapped ( p , ctxt , bean , tokens ) ; }",
        "summary": "`@JsonUnwrapped` fields are skipped when using `PropertyBasedCreator` if they appear after the last creator property",
        "Description": "Example:\r\n\r\n```java\r\n    static class Bean {\r\n        int x;\r\n        int y;\r\n\r\n        @JsonUnwrapped\r\n        UnwrappedBean w;\r\n\r\n        public Bean(@JsonProperty(\"x\") int x, @JsonProperty(\"y\") int y) {\r\n            this.x = x;\r\n            this.y = y;\r\n        }\r\n\r\n        public void setW(UnwrappedBean w) {\r\n            this.w = w;\r\n        }\r\n    }\r\n\r\n    static class UnwrappedBean {\r\n        int a;\r\n        int b;\r\n\r\n        public UnwrappedBean(@JsonProperty(\"a\") int a, @JsonProperty(\"b\") int b) {\r\n            this.a = a;\r\n            this.b = b;\r\n        }\r\n    }\r\n```\r\n\r\n```json\r\n    {\"x\": 1, \"a\": 2, \"y\": 3, \"b\": 4}\r\n```\r\n\r\n`x`, `y`, and `a` are deserialized as expected. `b` is skipped entirely. I think I've found the root cause and the fix doesn't appear to break any tests; opening a PR for further review.",
        "issue_url": null,
        "comments": [
            {
                "content": "Thank you for reporting this -- not sure how I managed to miss it. Looking now, hoping to merge for 2.9.7.\r\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug has been identified and a solution is being developed. It is expected to be merged for the 2.9.7 release."
    },
    "Math_20_src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java_920_923": {
        "src": "public double[] repairAndDecode(final double[] x) {\n            return\n                decode(x);\n        }",
        "src_wo_comments": "public double [ ] repairAndDecode ( final double [ ] x ) { return decode ( x ) ; }",
        "fixed_src": "public double[] repairAndDecode(final double[] x) {\n            return boundaries != null && isRepairMode ?\n                decode(repair(x)) :\n                decode(x);\n        }",
        "fixed_src_wo_comments": "public double [ ] repairAndDecode ( final double [ ] x ) { return boundaries != null && isRepairMode ? decode ( repair ( x ) ) : decode ( x ) ; }",
        "summary": "CMAESOptimizer does not enforce bounds",
        "Description": "The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-864",
        "comments": [
            "Some more info: I see now that FitnessFunction.value will \"repair\" the parameters to be in-bounds before passing them to computeObjectiveValue.  However, the generationLoop does not \"repair\" the parameters when storing the optimum parameter values, it saves the unrepaired values (which may be outside of bounds).",
            "bq. I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.\n\nIt would be very helpful if you could set up a unit test (i.e. a minimal example) showing the failure.\nThanks.\n\nbq. the generationLoop does not \"repair\" the parameters when storing the optimum parameter values\n\nThat looks suspicious indeed.\n",
            "Test program that shows bug.",
            "Thanks for the report, the show-case and the hint toward solving this problem.\nFix committed in revision 1388517. Please test.\n",
            "Thanks, seems to be fixed testing with revision 1388555."
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to fix the generationLoop so that it \"repairs\" the parameters when storing the optimum parameter values, rather than saving the unrepaired values. This was done by creating a unit test (i.e. a minimal example) showing the failure, and then committing the fix in revision 1388517. The fix was tested with revision 1388555 and seems to be successful."
    },
    "JacksonDatabind_12_src/main/java/com/fasterxml/jackson/databind/deser/std/MapDeserializer.java_298_305": {
        "src": "@Override\n    public boolean isCachable() {\n        /* As per [databind#735], existence of value or key deserializer (only passed\n         * if annotated to use non-standard one) should also prevent caching.\n         */\n        return (_valueTypeDeserializer == null)\n                && (_ignorableProperties == null);\n    }",
        "src_wo_comments": "@ Override public boolean isCachable ( ) { return ( _valueTypeDeserializer == null ) && ( _ignorableProperties == null ) ; }",
        "fixed_src": "@Override\n    public boolean isCachable() {\n        /* As per [databind#735], existence of value or key deserializer (only passed\n         * if annotated to use non-standard one) should also prevent caching.\n         */\n        return (_valueDeserializer == null)\n                && (_keyDeserializer == null)\n                && (_valueTypeDeserializer == null)\n                && (_ignorableProperties == null);\n    }",
        "fixed_src_wo_comments": "@ Override public boolean isCachable ( ) { return ( _valueDeserializer == null ) && ( _keyDeserializer == null ) && ( _valueTypeDeserializer == null ) && ( _ignorableProperties == null ) ; }",
        "summary": "@JsonDeserialize on Map with contentUsing custom deserializer overwrites default behavior",
        "Description": "I recently updated from version 2.3.3 to 2.5.1 and encountered a new issue with our custom deserializers. They either seemed to stop working or were active on the wrong fields.\nI could narrow it down to some change in version 2.4.4 (2.4.3 is still working for me)\n\nI wrote a test to show this behavior. It seems to appear when there a two maps with the same key and value types in a bean, and only one of them has a custom deserializer. The deserializer is then falsely used either for both or none of the maps.\n\nThis test works for me in version 2.4.3 and fails with higher versions.\n\n``` java\nimport static org.junit.Assert.assertEquals;\n\nimport java.io.IOException;\nimport java.util.Map;\n\nimport org.junit.Test;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport com.fasterxml.jackson.core.JsonParser;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.DeserializationContext;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.annotation.JsonDeserialize;\nimport com.fasterxml.jackson.databind.deser.std.StdDeserializer;\n\npublic class DeserializeTest {\n\n    @Test\n    public void testIt() throws Exception {\n        ObjectMapper om = new ObjectMapper();\n        String json = \"{\\\"map1\\\":{\\\"a\\\":1},\\\"map2\\\":{\\\"a\\\":1}}\";\n        TestBean bean = om.readValue(json.getBytes(), TestBean.class);\n\n        assertEquals(100, bean.getMap1().get(\"a\").intValue());\n        assertEquals(1, bean.getMap2().get(\"a\").intValue());\n    }\n\n    public static class TestBean {\n\n        @JsonProperty(\"map1\")\n        @JsonDeserialize(contentUsing = CustomDeserializer.class)\n        Map<String, Integer> map1;\n\n        @JsonProperty(\"map2\")\n        Map<String, Integer> map2;\n\n        public Map<String, Integer> getMap1() {\n            return map1;\n        }\n\n        public void setMap1(Map<String, Integer> map1) {\n            this.map1 = map1;\n        }\n\n        public Map<String, Integer> getMap2() {\n            return map2;\n        }\n\n        public void setMap2(Map<String, Integer> map2) {\n            this.map2 = map2;\n        }\n    }\n\n    public static class CustomDeserializer extends StdDeserializer<Integer> {\n\n        public CustomDeserializer() {\n            super(Integer.class);\n        }\n\n        @Override\n        public Integer deserialize(JsonParser p, DeserializationContext ctxt) throws IOException, JsonProcessingException {\n            Integer value = p.readValueAs(Integer.class);\n            return value * 100;\n        }\n    }\n}\n```\n",
        "issue_url": null,
        "comments": [
            {
                "content": "That does not sound good.\n\nLooking at 2.4.4, my first thought was this could be due to #604 (fixed in 2.4.5 by #635), but if 2.4.5 also fails, this is probably not the case.\n\nThank you for reproduction, I will try to figure out what is causing the failure.\n"
            },
            {
                "content": "Ok, yes. It is due to #604 as well. Looks like checks to prevent incorrect caching were still not strict enough.\nOn plus side, easy to fix.\n\nI will probably release 2.4.5.1 \"micro-patch\" of just `jackson-databind` soon (and not release full set of all other artifacts). And this fix along with others will get in 2.4.6 when it gets released.\n"
            },
            {
                "content": "I just released `2.4.5.1`, should be available via Maven in an hour. Note that only `jackson-databind` released, as per above; 2.4.5 of other components may (and needs to be) used.\n"
            },
            {
                "content": "Thank you for the quick resultion!\n"
            },
            {
                "content": "Thank you for reporting this! I need to do bit more work on 2.5 to ensure other cases are not affected, since `Collection` caching was also extended. These are difficult ones to trouble-shoot, but knowing the problem it is easier to write proper tests now -- reproduction requires specific sequence of resolution calls (custom one first vs last).\n"
            },
            {
                "content": "After fixing part of the problem (which resolved the unit test failure), I added some more variants, and found out that there is another problematic sequence. Will try to address that.\nFurther, whereas this only affects `Map`s in 2.4.4 / 2.4.5, it also affects `Collection`s in 2.5 (since their cachability was enabled in 2.5).\n\nReopening for additional work.\n"
            },
            {
                "content": "Ok: fixed completely (as far as I know) for 2.4.6 and 2.5.2.\n"
            }
        ],
        "summarized_discussion": "\n\nThe bug was due to #604 and was fixed in 2.4.5 by #635. A micro-patch, 2.4.5.1, was released with only the jackson-databind component. Further work was done to ensure other cases were not affected, and the bug was completely fixed for 2.4.6 and 2.5.2."
    },
    "Compress_39_src/main/java/org/apache/commons/compress/utils/ArchiveUtils.java_272_288": {
        "src": "public static String sanitize(String s) {\n        final char[] chars = s.toCharArray();\n        final int len = chars.length;\n        final StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < len; i++) {\n            final char c = chars[i];\n            if (!Character.isISOControl(c)) {\n                Character.UnicodeBlock block = Character.UnicodeBlock.of(c);\n                if (block != null && block != Character.UnicodeBlock.SPECIALS) {\n                    sb.append(c);\n                    continue;\n                }\n            }\n            sb.append('?');\n        }\n        return sb.toString();\n    }",
        "src_wo_comments": "public static String sanitize ( String s ) { final char [ ] chars = s . toCharArray ( ) ; final int len = chars . length ; final StringBuilder sb = new StringBuilder ( ) ; for ( int i = 0 ; i < len ; i ++ ) { final char c = chars [ i ] ; if ( ! Character . isISOControl ( c ) ) { Character . UnicodeBlock block = Character . UnicodeBlock . of ( c ) ; if ( block != null && block != Character . UnicodeBlock . SPECIALS ) { sb . append ( c ) ; continue ; } } sb . append ( '?' ) ; } return sb . toString ( ) ; }",
        "fixed_src": "public static String sanitize(String s) {\n        final char[] cs = s.toCharArray();\n        final char[] chars = cs.length <= MAX_SANITIZED_NAME_LENGTH ? cs : Arrays.copyOf(cs, MAX_SANITIZED_NAME_LENGTH);\n        if (cs.length > MAX_SANITIZED_NAME_LENGTH) {\n            for (int i = MAX_SANITIZED_NAME_LENGTH - 3; i < MAX_SANITIZED_NAME_LENGTH; i++) {\n                chars[i] = '.';\n            }\n        }\n        final int len = chars.length;\n        final StringBuilder sb = new StringBuilder();\n        for (int i = 0; i < len; i++) {\n            final char c = chars[i];\n            if (!Character.isISOControl(c)) {\n                Character.UnicodeBlock block = Character.UnicodeBlock.of(c);\n                if (block != null && block != Character.UnicodeBlock.SPECIALS) {\n                    sb.append(c);\n                    continue;\n                }\n            }\n            sb.append('?');\n        }\n        return sb.toString();\n    }",
        "fixed_src_wo_comments": "public static String sanitize ( String s ) { final char [ ] cs = s . toCharArray ( ) ; final char [ ] chars = cs . length <= MAX_SANITIZED_NAME_LENGTH ? cs : Arrays . copyOf ( cs , MAX_SANITIZED_NAME_LENGTH ) ; if ( cs . length > MAX_SANITIZED_NAME_LENGTH ) { for ( int i = MAX_SANITIZED_NAME_LENGTH - 3 ; i < MAX_SANITIZED_NAME_LENGTH ; i ++ ) { chars [ i ] = '.' ; } } final int len = chars . length ; final StringBuilder sb = new StringBuilder ( ) ; for ( int i = 0 ; i < len ; i ++ ) { final char c = chars [ i ] ; if ( ! Character . isISOControl ( c ) ) { Character . UnicodeBlock block = Character . UnicodeBlock . of ( c ) ; if ( block != null && block != Character . UnicodeBlock . SPECIALS ) { sb . append ( c ) ; continue ; } } sb . append ( '?' ) ; } return sb . toString ( ) ; }",
        "summary": "Defective .zip-archive produces problematic error message",
        "Description": "A truncated .zip-File produces an java.io.EOFException conatining a hughe amount of byte[]-data in the error-message - leading to beeps and crippeling workload in an potential console-logger.\n\n",
        "issue_url": "https://issues.apache.org/jira//browse/COMPRESS-351",
        "comments": [
            "{code:title=test.java|borderStyle=solid}\n    public static long getUnzippedFileSize(byte[] zippedData){\n        long byteSize = 0;\n        try(ZipArchiveInputStream zipArchiveInputStream = new ZipArchiveInputStream(new ByteArrayInputStream(zippedData))){\n            ArchiveEntry zipEntry = zipArchiveInputStream.getNextEntry();\n            while(zipEntry != null){\n                byteSize = byteSize + zipEntry.getSize();\n                zipEntry = zipArchiveInputStream.getNextEntry();\n            }\n        }\n        catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n        return byteSize;\n    }\n{code}",
            "The output you see is what Compress considers the entry's file name. It looks as if there was some garbage inside the local file header that results in bigger blob being considered the name. I'll try to find a way to verify the name consists of printing characters or something like this.\n\nIs it OK to ship the attached archive as a unit test with Compress?",
            "Note that names don't have to consist of printable characters. \nAFAIK Unix allows anything except '/' and null.\n\nSo Compress must allow such names to be processed.\n\nThe most that it can do is to sanitize the IOE message.\nBut I'm not sure that's the best approach, because it will lose information that the app may need.\n\nAn alternative approach would be to change the Javadoc to note that the IOE may contain unprintable characters and let the application sanitize it.",
            "Yes, I'm only thinking about modifying the exception message - of course we'll accept as names whatever has been accepted before.",
            "should be fixed with git commit ddb8f08 - still it would be good if we could use the attached archive as a test case",
            "But the name may not be corrupted.\nAll we really know is that the archive is truncated.\n\nIf there are any other examples of Exceptions which include the file name these would need to be 'protected' too.\nSo I'm still inclined to leave it to the app.\n\nHowever if it is decided to keep the amended name, I would rather see the message clarified to\n\"possibly corrupted name\".\n\nOr maybe keep the full name but replace non-printable chars with '?' as is done by ls(1).",
            "You are correct, the name may be included in other exceptions as well - like failing to parse extra fields or unsupported zip features being requested - anything of which may well happen if the archive is corrupt.\n\nLeaving it to the application doesn't feel right to me. If you log an exception you don't expect it to contain \"garbage\" in its message - and as it stands we'd have to add a disclaimer of \"any exception may contain garbage if the archive is corrupt\" to {{ZipArchiveInputStream}}. {{ZipFile}} is safe in most cases as it won't find any \"end of central directory\" record and fail with a usable message.\n\nI guess sanitizing all entry names that are leaked via exceptions is the best route.",
            "The same applies to other archive formats as well since they all may be used on Unix.\n\nWe could either sanitise the names (replace unprintable chars with '?') or provide a utility method to do so.\n\nMaybe worth considering providing access to the raw name somehow.",
            "access to the raw name would only be possible if we threw exception types of our own. Some kind of CommonsCompressIOException or something. I'm not sure this would be worth the effort.\n\nAs for the other formats, yes, I've realized this myself as well. I'd put the utility method into {{ArchiveUtils}}.",
            "Hi Stefan, feel free to add the attached archive as a test case - it's only conatining generated test data.",
            "Thanks, I have added a testcase.\n\nBy now I've modified the code to sanitize the file name by replacing non-printable characters with '?' but need to find the time to use the same approach throughout the codebase - and look into the current Jenkins build which failed after I added the test.",
            "Your're welcome! Beside \"unprintable\" characters in the output, the sheer size of it was another problem. May I suggest truncating it to 255 characters? This will still provide enough data for the purpose of an error message and probably also address the jenkins-issue.\n",
            "finally, sorry for the delay.\n\ncommit 11840df restricts the length to 255 chars as you suggested.",
            "Looks as if only CPIO would also include entry names read in exception messages. Fixed with git commit b5071c2"
        ],
        "summarized_discussion": "\n\nThe solution to the bug is to modify the code to sanitize the file name by replacing non-printable characters with '?', truncate the output to 255 characters, and provide access to the raw name. This was done with git commits ddb8f08, 11840df, and b5071c2."
    },
    "Cli_37_src/main/java/org/apache/commons/cli/DefaultParser.java_299_305": {
        "src": "private boolean isShortOption(String token)\n    {\n        // short options (-S, -SV, -S=V, -SV1=V2, -S1S2)\n        return token.startsWith(\"-\") && token.length() >= 2 && options.hasShortOption(token.substring(1, 2));\n\n        // remove leading \"-\" and \"=value\"\n    }",
        "src_wo_comments": "private boolean isShortOption ( String token ) { return token . startsWith ( \"-\" ) && token . length ( ) >= 2 && options . hasShortOption ( token . substring ( 1 , 2 ) ) ; }",
        "fixed_src": "private boolean isShortOption(String token)\n    {\n        // short options (-S, -SV, -S=V, -SV1=V2, -S1S2)\n        if (!token.startsWith(\"-\") || token.length() == 1)\n        {\n            return false;\n        }\n\n        // remove leading \"-\" and \"=value\"\n        int pos = token.indexOf(\"=\");\n        String optName = pos == -1 ? token.substring(1) : token.substring(1, pos);\n        return options.hasShortOption(optName);\n    }",
        "fixed_src_wo_comments": "private boolean isShortOption ( String token ) { if ( ! token . startsWith ( \"-\" ) || token . length ( ) == 1 ) { return false ; } int pos = token . indexOf ( \"=\" ) ; String optName = pos == - 1 ? token . substring ( 1 ) : token . substring ( 1 , pos ) ; return options . hasShortOption ( optName ) ; }",
        "summary": "Optional argument picking up next regular option as its argument",
        "Description": null,
        "issue_url": "https://issues.apache.org/jira//browse/CLI-265",
        "comments": [
            "I have recently migrated a project from CLI 1.2 to 1.3.1 and have encountered what may be a bug or difference in the way optional arguments are being processed.\n\nI have a command that opens several different kinds of databases by type, or alternately, the last opened database of that type.\n\nOption TYPE1 = Option.builder(\"t1\").hasArg().numberOfArgs(1).optionalArg(true).argName(\"t1_path\").build();\nOption TYPE2 = Option.builder(\"t2\").hasArg().numberOfArgs(1).optionalArg(true).argName(\"t2_path\").build();\nOption LAST  =  Option.builder(\"last\").hasArg(false).build();\n\nCommands then look like \"open -t1 path/to/my/db\" or \"open -t1 -last\"\n\nIf I use the now deprecated GnuParser, both commands work as expected.  However, if I use the new DefaultParser, for the 2nd example, it thinks \"-last\" is the argument for -t1 rather than an option in its own right.\n\nI added the numberOfArgs(1) after reading a post on StackOverflow, but it made no difference in the behavior.  Only switching back to the GnuParser seemed to work.\n\n",
            "See also this question/answer on Stackoverflow for a slightly different symptom and potential fix.\n\nhttp://stackoverflow.com/questions/38964866/defaultparser-in-commons-cli-doesnt-behave-like-the-deprecated-parsers/38965293#38965293",
            "Seeing the exact same behavior as described here:\n\nhttp://markmail.org/thread/ozeke3q7ni572psi#query:+page:1+mid:i6gssbpyvxozt633+state:results\n",
            "{code}\n~/w/a/c/cli > svn ci -m \"CLI-265: Optional argument picking up next regular option as its argument. Thank you to Lynn Henderson, Martin Sandiford and Veit Guna for providing reproductions.\"\nSending        src/changes/changes.xml\nSending        src/main/java/org/apache/commons/cli/DefaultParser.java\nAdding         src/test/java/org/apache/commons/cli/bug/BugCLI265Test.java\nTransmitting file data ...done\nCommitting transaction...\nCommitted revision 1759695.\n{code}\n\nThank you!",
            "Hey Benedikt,  thanks for looking at this quickly.\n\nI'm not sure if this is a complete fix.  It seems to miss the case where short options are concatenated after an option that takes an optional argument.\n\nA failing test case for this would be to modify {{setUp()}} in BugCLI265Test.java to include short options \"a\" and \"b\":\n\n{code:java}\n    @Before\n    public void setUp() throws Exception {\n        parser = new DefaultParser();\n\n        Option TYPE1 = Option.builder(\"t1\").hasArg().numberOfArgs(1).optionalArg(true).argName(\"t1_path\").build();\n        Option OPTION_A = Option.builder(\"a\").hasArg(false).build();\n        Option OPTION_B = Option.builder(\"b\").hasArg(false).build();\n        Option LAST = Option.builder(\"last\").hasArg(false).build();\n\n        options = new Options().addOption(TYPE1).addOption(OPTION_A).addOption(OPTION_B).addOption(LAST);\n    }\n{code}\n\nAdd add a test for the concatenated options following an option with optional argument case:\n\n{code:java}\n    @Test\n    public void shouldParseConcatenatedShortOptions() throws Exception {\n      String[] concatenatedShortOptions = new String[] { \"-t1\", \"-ab\" };\n\n      final CommandLine commandLine = parser.parse(options, concatenatedShortOptions);\n\n      assertTrue(commandLine.hasOption(\"t1\"));\n      assertEquals(null, commandLine.getOptionValue(\"t1\"));\n      assertTrue(commandLine.hasOption(\"a\"));\n      assertTrue(commandLine.hasOption(\"b\"));\n      assertFalse(commandLine.hasOption(\"last\"));\n    }\n{code}\n\nOne possible fix is to check that at least the first character of the option is a short option if all the other cases fail in {{isShortOption(...)}} like so:\n\n{code:java}\n    private boolean isShortOption(String token)\n    {\n        // short options (-S, -SV, -S=V, -SV1=V2, -S1S2)\n        if (!token.startsWith(\"-\") || token.length() == 1)\n        {\n            return false;\n        }\n\n        // remove leading \"-\" and \"=value\"\n        int pos = token.indexOf(\"=\");\n        String optName = pos == -1 ? token.substring(1) : token.substring(1, pos);\n        if (options.hasShortOption(optName))\n        {\n            return true;\n        }\n        return optName.length() > 0 && options.hasShortOption(String.valueOf(optName.charAt(0)));\n    }\n{code}",
            "Reopen issue to address the problem identified by Martin Sandiford.",
            "See http://svn.apache.org/r1759745",
            "Sorry it's been a few years, but I'm in the process of updating some libraries for an existing project, including CLI 1.4 (downloaded today), and I notice the bug I reported before still seems to be present in 1.4.\r\n\r\nIf I have an option that takes an optional argument but don't include one, it's picking up the next option as if it were the argument.\r\n\r\n\u00a0Option DB = Option\r\n .builder(\"db\").hasArg().optionalArg(true).numberOfArgs(1).build();\r\n\r\nOption LAST = Option.builder(\"last\").hasArg(false)\r\n .desc(\"Open the most recently opened database\")\r\n .required(false).build();\r\n\r\nCommands are of the form \"open -db /path/to/db\" or \"open -db -last\".\r\n\r\nExecuting \"open -db -last\" with 1.4 using the DefaultParser still gives me this error:\r\n ERROR Database \"-last\" not found (i.e., it thinks it's the arg to option -db).\r\n\r\nAs with 1.2 and 1.3.1, if I use the now deprecated GnuParser, it works as expected.\r\n\r\nIn 1.4, reversing the order of the options works with DefaultParser, but this shouldn't be required: \"open -last -db\"",
            "Turns out I had a dependent project that was still using 1.3.1, so that ending up taking precedence in my classpath instead of 1.4.\u00a0 Apologies for the false alarm."
        ],
        "summarized_discussion": "\n\nThe bug is related to optional arguments not being processed correctly when using the DefaultParser in Apache Commons CLI 1.2 to 1.3.1. A possible fix is to check that at least the first character of the option is a short option if all the other cases fail in isShortOption() by adding a line of code. This fix was committed in revision 1759695. However, the bug still seems to be present in version 1.4, so the issue needs to be reopened to address the problem."
    },
    "JacksonCore_25_src/main/java/com/fasterxml/jackson/core/json/ReaderBasedJsonParser.java_1948_1990": {
        "src": "private String _handleOddName2(int startPtr, int hash, int[] codes) throws IOException\n    {\n        _textBuffer.resetWithShared(_inputBuffer, startPtr, (_inputPtr - startPtr));\n        char[] outBuf = _textBuffer.getCurrentSegment();\n        int outPtr = _textBuffer.getCurrentSegmentSize();\n        final int maxCode = codes.length;\n\n        while (true) {\n            if (_inputPtr >= _inputEnd) {\n                if (!_loadMore()) { // acceptable for now (will error out later)\n                    break;\n                }\n            }\n            char c = _inputBuffer[_inputPtr];\n            int i = (int) c;\n            if (i <= maxCode) {\n                if (codes[i] != 0) {\n                    break;\n                }\n            } else if (!Character.isJavaIdentifierPart(c)) {\n                break;\n            }\n            ++_inputPtr;\n            hash = (hash * CharsToNameCanonicalizer.HASH_MULT) + i;\n            // Ok, let's add char to output:\n            outBuf[outPtr++] = c;\n\n            // Need more room?\n            if (outPtr >= outBuf.length) {\n                outBuf = _textBuffer.finishCurrentSegment();\n                outPtr = 0;\n            }\n        }\n        _textBuffer.setCurrentLength(outPtr);\n        {\n            TextBuffer tb = _textBuffer;\n            char[] buf = tb.getTextBuffer();\n            int start = tb.getTextOffset();\n            int len = tb.size();\n\n            return _symbols.findSymbol(buf, start, len, hash);\n        }\n    }",
        "src_wo_comments": "private String _handleOddName2 ( int startPtr , int hash , int [ ] codes ) throws IOException { _textBuffer . resetWithShared ( _inputBuffer , startPtr , ( _inputPtr - startPtr ) ) ; char [ ] outBuf = _textBuffer . getCurrentSegment ( ) ; int outPtr = _textBuffer . getCurrentSegmentSize ( ) ; final int maxCode = codes . length ; while ( true ) { if ( _inputPtr >= _inputEnd ) { if ( ! _loadMore ( ) ) { break ; } } char c = _inputBuffer [ _inputPtr ] ; int i = ( int ) c ; if ( i <= maxCode ) { if ( codes [ i ] != 0 ) { break ; } } else if ( ! Character . isJavaIdentifierPart ( c ) ) { break ; } ++ _inputPtr ; hash = ( hash * CharsToNameCanonicalizer . HASH_MULT ) + i ; outBuf [ outPtr ++ ] = c ; if ( outPtr >= outBuf . length ) { outBuf = _textBuffer . finishCurrentSegment ( ) ; outPtr = 0 ; } } _textBuffer . setCurrentLength ( outPtr ) ; { TextBuffer tb = _textBuffer ; char [ ] buf = tb . getTextBuffer ( ) ; int start = tb . getTextOffset ( ) ; int len = tb . size ( ) ; return _symbols . findSymbol ( buf , start , len , hash ) ; } }",
        "fixed_src": "private String _handleOddName2(int startPtr, int hash, int[] codes) throws IOException\n    {\n        _textBuffer.resetWithShared(_inputBuffer, startPtr, (_inputPtr - startPtr));\n        char[] outBuf = _textBuffer.getCurrentSegment();\n        int outPtr = _textBuffer.getCurrentSegmentSize();\n        final int maxCode = codes.length;\n\n        while (true) {\n            if (_inputPtr >= _inputEnd) {\n                if (!_loadMore()) { // acceptable for now (will error out later)\n                    break;\n                }\n            }\n            char c = _inputBuffer[_inputPtr];\n            int i = (int) c;\n            if (i < maxCode) {\n                if (codes[i] != 0) {\n                    break;\n                }\n            } else if (!Character.isJavaIdentifierPart(c)) {\n                break;\n            }\n            ++_inputPtr;\n            hash = (hash * CharsToNameCanonicalizer.HASH_MULT) + i;\n            // Ok, let's add char to output:\n            outBuf[outPtr++] = c;\n\n            // Need more room?\n            if (outPtr >= outBuf.length) {\n                outBuf = _textBuffer.finishCurrentSegment();\n                outPtr = 0;\n            }\n        }\n        _textBuffer.setCurrentLength(outPtr);\n        {\n            TextBuffer tb = _textBuffer;\n            char[] buf = tb.getTextBuffer();\n            int start = tb.getTextOffset();\n            int len = tb.size();\n\n            return _symbols.findSymbol(buf, start, len, hash);\n        }\n    }",
        "fixed_src_wo_comments": "private String _handleOddName2 ( int startPtr , int hash , int [ ] codes ) throws IOException { _textBuffer . resetWithShared ( _inputBuffer , startPtr , ( _inputPtr - startPtr ) ) ; char [ ] outBuf = _textBuffer . getCurrentSegment ( ) ; int outPtr = _textBuffer . getCurrentSegmentSize ( ) ; final int maxCode = codes . length ; while ( true ) { if ( _inputPtr >= _inputEnd ) { if ( ! _loadMore ( ) ) { break ; } } char c = _inputBuffer [ _inputPtr ] ; int i = ( int ) c ; if ( i < maxCode ) { if ( codes [ i ] != 0 ) { break ; } } else if ( ! Character . isJavaIdentifierPart ( c ) ) { break ; } ++ _inputPtr ; hash = ( hash * CharsToNameCanonicalizer . HASH_MULT ) + i ; outBuf [ outPtr ++ ] = c ; if ( outPtr >= outBuf . length ) { outBuf = _textBuffer . finishCurrentSegment ( ) ; outPtr = 0 ; } } _textBuffer . setCurrentLength ( outPtr ) ; { TextBuffer tb = _textBuffer ; char [ ] buf = tb . getTextBuffer ( ) ; int start = tb . getTextOffset ( ) ; int len = tb . size ( ) ; return _symbols . findSymbol ( buf , start , len , hash ) ; } }",
        "summary": "Fix ArrayIndexOutofBoundsException found by LGTM.com",
        "Description": "Seen on LGTM.com [here](https://lgtm.com/projects/g/FasterXML/jackson-core/alerts/?mode=tree)\r\n\r\nAs `codes.length == maxCode` so if `i == maxCode` an `ArrayIndexOutOfBoundsException` is thrown. This happens when `ALLOW_UNQUOTED_FIELD_NAMES` is enabled and character `256` is found as part of a field name after needing to consume more data from the reader.\r\n\r\nA gist containing code to trigger this path can be found [here](https://gist.github.com/aeyerstaylor/90128cca75e69303254a0d5a5dbe6762). I could find any tests for this class but if there is a place to add tests I can add the example as a test.\r\n\r\n_(Full disclosure: I'm part of the company behind LGTM.com)_",
        "issue_url": null,
        "comments": null,
        "summarized_discussion": "\n\nWithout more information, it is not possible to summarize the solution to the bug."
    },
    "Math_61_src/main/java/org/apache/commons/math/distribution/PoissonDistributionImpl.java_92_100": {
        "src": "public PoissonDistributionImpl(double p, double epsilon, int maxIterations) {\n        if (p <= 0) {\n            throw MathRuntimeException.createIllegalArgumentException(LocalizedFormats.NOT_POSITIVE_POISSON_MEAN, p);\n        }\n        mean = p;\n        normal = new NormalDistributionImpl(p, FastMath.sqrt(p));\n        this.epsilon = epsilon;\n        this.maxIterations = maxIterations;\n    }",
        "src_wo_comments": "public PoissonDistributionImpl ( double p , double epsilon , int maxIterations ) { if ( p <= 0 ) { throw MathRuntimeException . createIllegalArgumentException ( LocalizedFormats . NOT_POSITIVE_POISSON_MEAN , p ) ; } mean = p ; normal = new NormalDistributionImpl ( p , FastMath . sqrt ( p ) ) ; this . epsilon = epsilon ; this . maxIterations = maxIterations ; }",
        "fixed_src": "public PoissonDistributionImpl(double p, double epsilon, int maxIterations) {\n        if (p <= 0) {\n            throw new NotStrictlyPositiveException(LocalizedFormats.MEAN, p);\n        }\n        mean = p;\n        normal = new NormalDistributionImpl(p, FastMath.sqrt(p));\n        this.epsilon = epsilon;\n        this.maxIterations = maxIterations;\n    }",
        "fixed_src_wo_comments": "public PoissonDistributionImpl ( double p , double epsilon , int maxIterations ) { if ( p <= 0 ) { throw new NotStrictlyPositiveException ( LocalizedFormats . MEAN , p ) ; } mean = p ; normal = new NormalDistributionImpl ( p , FastMath . sqrt ( p ) ) ; this . epsilon = epsilon ; this . maxIterations = maxIterations ; }",
        "summary": "Dangerous code in \"PoissonDistributionImpl\"",
        "Description": "In the following excerpt from class \"PoissonDistributionImpl\":\n\n{code:title=PoissonDistributionImpl.java|borderStyle=solid}\n    public PoissonDistributionImpl(double p, NormalDistribution z) {\n        super();\n        setNormal(z);\n        setMean(p);\n    }\n{code}\n\n(1) Overridable methods are called within the constructor.\n(2) The reference \"z\" is stored and modified within the class.\n\nI've encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the \"setter\" methods inside the constructor but I think that a more elegant solution would be to remove the \"setters\" altogether (i.e. make the classes immutable).\nProblem (2) can also create unexpected behaviour. Is it really necessary to pass the \"NormalDistribution\" object; can't it be always created within the class?\n",
        "issue_url": "https://issues.apache.org/jira//browse/MATH-349",
        "comments": [
            "The reason this constructor exists is to allow users to plug in an alternative normal distribution implementation to be used in computing normal approximations.  I don't see 1) as a serious issue, but I am +1 on deprecating the setters with aim to make this class immutable in 3.0.  2) is a harder problem, as there is no requirement that a NormalDistribution be clonable.   I see three solutions, none of which are particularly appealing:\n\n1) leave as is and specify in the javadoc that z is going to be modified\n2) change the implementation to avoid changing the parameters of z \n3) deprecate the constructor altogether\n\nI vote for 1) + 3) - update the javadoc, but deprecate.  If we get complaints before 3.0, we can reconsider; otherwise eliminate in 3.0",
            "Problem (1)\nIn principle, it is not safe to call overridable methods in the constructor, so, if only to promote coding quality, can I implement private (\"helper\") setter methods that shall be called from within the constructors and from the public setters?\n\nProblem (2)\nIn \"PoissonDistributionImpl.java\", there is this code:\n\n{code:title=PoissonDistributionImpl.java|borderStyle=solid}\n\npublic PoissonDistributionImpl(double p, NormalDistribution z) {\n    super();\n    setNormal(z);\n    setMean(p);\n}\n\npublic void setMean(double p) {\n    // ...\n    this.mean = p;\n    normal.setMean(p);\n    normal.setStandardDeviation(Math.sqrt(p));\n}\n\npublic void setNormal(NormalDistribution value) {\n    normal = value;\n}\n{code}\n\nIn the constructor, the code makes sure that the \"mean\" of this class and the mean of the \"normal\" object are consistent.\nBut there is no such guarantee anymore when calling the \"setNormal\" method. [The comment warns the user that he is responsible for setting the right parameter in \"value\" but this is far from fool-proof...]\n\n",
            "I agree that both (1) and (2) are problems.  The private helpers would be OK to address (1), but I am also OK with deprecating the setters and just setting the fields directly (per MATH-348) in the constructor.  As stated above, I am also +1 on deprecating the pluggability of the normal impl, which will (eventually) address (2).  I guess to address (2) now we either need to modify setNormal to update the parameters of the normal instance as setMean does or change the normal approximation implementation to not depend on the parameters of the distribution (which addresses the problem of z being updated).  Its probably easiest and best in the long term to take the first approach, documenting the fact that z is going to be updated (both in setNormal and constructor javadoc).",
            "Issue is partially fixed (removed the consistency problem) in revision 920852.\nSetters were marked as deprecated.\n",
            "Issue (2) in the bug description remains open.  I think we should leave this issue open until we have a solution for this.  As stated above, I can see two ways to fix this:\n\na) modify the normal approximation implementation so that it does not change the parameters of z\nb) eliminate pluggability of z (i.e., deprecate and then remove the constructor that accepts a normal instance as a parameter)\n\na) could be accomplished in 2.x, but it would complicate the code and could be bad for numerics.  My vote is for b) - add a warning (about safety as well as deprecation), deprecate now and if we get no complaints before 3.0, remove then.   Leave the issue open with fix version 3.0.",
            "The same kind of problem also exists in \"ChiSquaredDistributionImpl.java\".\nIn both classes, the constructor that takes the distribution as a parameter has been deprecated.\n",
            "Sorry,  I missed the constructor deprecations in r920852",
            "Leaving open until deprecated constructors are removed in 3.0.",
            "{{PoissonDistributionImpl}} made immutable in revision 1001331.",
            "{{ChiSquaredDistributionImpl}} made immutable in revision 1001533.",
            "Removed deprecated code.  Classes are now immutable.\n",
            "Closing issue as it was included in version 2.2, which has been released"
        ],
        "summarized_discussion": "\n\nThe bug discussed in the source code was that the constructor of the class allowed users to plug in an alternative normal distribution implementation, but this could lead to inconsistencies in the parameters of the distribution. To address this, the setters were marked as deprecated and the constructors were deprecated in revision 920852. In revision 1001331 and 1001533, the classes were made immutable by removing the deprecated code. The issue was closed as it was included in version 2.2, which has been released."
    },
    "JxPath_10_src/java/org/apache/commons/jxpath/ri/compiler/CoreOperationRelationalExpression.java_41_44": {
        "src": "public final Object computeValue(EvalContext context) {\n        return compute(args[0].computeValue(context), args[1].computeValue(context)) \n                ? Boolean.TRUE : Boolean.FALSE;\n    }",
        "src_wo_comments": "public final Object computeValue ( EvalContext context ) { return compute ( args [ 0 ] . computeValue ( context ) , args [ 1 ] . computeValue ( context ) ) ? Boolean . TRUE : Boolean . FALSE ; }",
        "fixed_src": "public final Object computeValue(EvalContext context) {\n        return compute(args[0].compute(context), args[1].compute(context))\n                ? Boolean.TRUE : Boolean.FALSE;\n    }",
        "fixed_src_wo_comments": "public final Object computeValue ( EvalContext context ) { return compute ( args [ 0 ] . compute ( context ) , args [ 1 ] . compute ( context ) ) ? Boolean . TRUE : Boolean . FALSE ; }",
        "summary": "Binary operators behaviour involving node-sets is incorrect",
        "Description": "According to XPath specification:\n\"If both objects to be compared are node-sets, then the comparison will be true if and only if there is a node in the first node-set and a node in the second node-set such that the result of performing the comparison on the string-values of the two nodes is true. If one object to be compared is a node-set and the other is a number, then the comparison will be true if and only if there is a node in the node-set such that the result of performing the comparison on the number to be compared and on the result of converting the string-value of that node to a number using the number function is true.\"\n\nBut following example illustrates, that this is not a JXPath behaviour:\n\n\n        JXPathContext pathContext = JXPathContext\n                .newContext(DocumentBuilderFactory.newInstance()\n                        .newDocumentBuilder().parse(\n                                new InputSource(new StringReader(\n                                        \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\r\\n\"\n                                                + \"<doc/>\"))));\n        Boolean result = (Boolean) pathContext.getValue(\"2.0 > child1\",\n                Boolean.class);\n        assertFalse(result.booleanValue());\n\n\"child1\" is not found - right operand node set is empty, but result is TRUE, instead of FALSE.\n\nPlease, check greaterThan(), lesserThan(), etc methods of org.apache.xpath.objects.XObject for possible solution :)",
        "issue_url": "https://issues.apache.org/jira//browse/JXPATH-93",
        "comments": [
            "Fixed in trunk.  Thanks, Sergey!",
            "Doesn't work from trunk",
            "Test case update",
            "tried again--check svn trunk.  Thanks!"
        ],
        "summarized_discussion": "\n\nThe bug has been fixed in the trunk and the test case has been updated. It should now work from the trunk."
    }
}